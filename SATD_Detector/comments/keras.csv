file path,line #,comment,satd
keras/,33,if clipnorm == 0 no need to add ops to the graph,
keras/,36,tf require using a special op to multiply IndexedSliced by scalar,
keras/,42,saving the shape to avoid converting sparse tensor to dense,
keras/,160,Legacy support.,
keras/,199,momentum,
keras/,205,velocity,
keras/,213,Apply constraints.,
keras/,273,update accumulator,
keras/,278,Apply constraints.,
keras/,287,Override set_weights for backward compatibility of Keras 2.2.4 optimizer,
keras/,288,since it does not include iteration at head of the weight list. Set,
keras/,289,iteration to 0.,
keras/,348,update accumulator,
keras/,352,Apply constraints.,
keras/,361,Override set_weights for backward compatibility of Keras 2.2.4 optimizer,
keras/,362,since it does not include iteration at head of the weight list. Set,
keras/,363,iteration to 0.,
keras/,430,update accumulator,
keras/,434,use the new accumulator and the *old* delta_accumulator,
keras/,438,Apply constraints.,
keras/,444,update delta_accumulator,
keras/,451,Override set_weights for backward compatibility of Keras 2.2.4 optimizer,
keras/,452,since it does not include iteration at head of the weight list. Set,
keras/,453,iteration to 0.,
keras/,549,Apply constraints.,
keras/,610,zero init of 1st moment,
keras/,613,zero init of exponentially weighted infinity norm,
keras/,628,Apply constraints.,
keras/,686,"Due to the recommendations in [2], i.e. warming momentum schedule",
keras/,704,the following equations given in [1],
keras/,720,Apply constraints.,
keras/,729,Override set_weights for backward compatibility of Keras 2.2.4 optimizer,
keras/,730,since it does not include m_schedule at head of the weight list. Set,
keras/,731,m_schedule to 1.,
keras/,789,Aliases.,
keras/,827,Make deserialization case-insensitive for built-in optimizers.,
keras/,854,Wrap TF optimizer instances,
keras/,27,Initializers saved from `tf.keras`,
keras/,28,may contain an unused `dtype` argument.,
keras/,220,"0.879... = scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.)",
keras/,269,Pick the one with the correct shape.,
keras/,448,Compatibility aliases,
keras/,459,Utility functions,
keras/,482,"Assuming convolution kernels (1D, 2D or 3D).",
keras/,483,"TH kernel shape: (depth, input_depth, ...)",
keras/,484,"TF kernel shape: (..., input_depth, depth)",
keras/,496,No specific assumptions.,
keras/,67,If we are wrapping a lambda function strip '<>' from the name as it is not,
keras/,68,accepted in scope name.,
keras/,740,Convert the binary labels to -1 or 1.,
keras/,749,Aliases.,
keras/,146,Aliases.,
keras/,154,Legacy aliases.,
keras/,63,All metric layers are stateful.,
keras/,81,For TF,
keras/,84,We are adding the metric object as metadata on the result tensor.,
keras/,85,This is required when we want to use a metric with `add_metric` API on,
keras/,86,a Model/Layer in graph mode. This metric instance will later be used,
keras/,87,to reset variable state after each epoch of training.,
keras/,88,Example:,
keras/,89,model = Model(),
keras/,90,mean = Mean(),
keras/,91,"model.add_metric(mean(values), name='mean')",
keras/,121,For use by subclasses,
keras/,135,End: For use by subclasses,
keras/,176,Update dimensions of weights to match with values if possible.,
keras/,180,Broadcast weights if possible.,
keras/,188,Exit early if the reduction doesn't have a denominator.,
keras/,192,Update `count` for reductions that require a denominator.,
keras/,1183,"Compute `num_thresholds` thresholds in [0, 1]",
keras/,1256,Calculate specificities at all the thresholds.,
keras/,1262,Find the index of the threshold where the specificity is closest to the,
keras/,1263,given specificity.,
keras/,1268,Compute sensitivity at that index.,
keras/,1329,Calculate sensitivities at all the thresholds.,
keras/,1335,Find the index of the threshold where the sensitivity is closest to the,
keras/,1336,given specificity.,
keras/,1341,Compute specificity at that index.,
keras/,1641,Validate configurations.,
keras/,1654,Update properties.,
keras/,1656,"If specified, use the supplied thresholds.",
keras/,1663,"Otherwise, linearly interpolate (num_thresholds - 2) thresholds in",
keras/,1664,"(0, 1).",
keras/,1669,"Add an endpoint ""threshold"" below zero and above one for either",
keras/,1670,threshold method to account for floating point imprecisions.,
keras/,1684,Create metric variables,
keras/,1766,Logical and,
keras/,1791,This use case is different and is handled separately.,
keras/,1794,Set `x` and `y` values for the curves based on `curve` config.,
keras/,1808,curve == 'PR'.,
keras/,1816,Find the rectangle heights based on `summation_method`.,
keras/,1818,"Note: the case ('PR', 'interpolation') has been handled above.",
keras/,1822,self.summation_method = metrics_utils.AUCSummationMethod.MAJORING:,
keras/,1825,Sum up the areas of all the rectangles.,
keras/,1837,We remove the endpoint thresholds as an inverse of how the thresholds,
keras/,1838,were initialized. This ensures that a metric initialized from this,
keras/,1839,config has the same thresholds.,
keras/,1912,"reshape in case it's in shape (num_samples, 1) instead of (num_samples,)",
keras/,1915,convert dense predictions to labels,
keras/,1926,"If the shape of y_true is (num_samples, 1), flatten to (num_samples,)",
keras/,1953,Aliases,
keras/,55,Cache for created layers.,
keras/,56,"Map {reference_tensor: (corresponding_tensor, mask)}",
keras/,58,Create placeholders to build the model on top of.,
keras/,67,Cache newly created input layer.,
keras/,73,Make sure that all input tensors come from a Keras layer.,
keras/,74,If tensor comes from an input layer: cache the input layer.,
keras/,83,Cache newly created input layer.,
keras/,92,"tensor, mask",
keras/,94,"Iterated over every node in the reference model, in depth order.",
keras/,100,Recover the corresponding layer.,
keras/,103,Get or create layer.,
keras/,105,Clone layer.,
keras/,110,Reuse previously cloned layer.,
keras/,112,Don't call InputLayer multiple times.,
keras/,116,Gather inputs to call the new layer.,
keras/,120,"If all previous input tensors are available in tensor_map,",
keras/,121,then call node.inbound_layer on them.,
keras/,122,"List of tuples (input, mask).",
keras/,128,Call layer.,
keras/,162,Update tensor_map.,
keras/,168,"Check that we did compute the model outputs,",
keras/,169,then instantiate a new model from inputs and outputs.,
keras/,21,Also importable from root,
keras/,50,Aliases.,
keras/,572,Model methods,
keras/,645,"Old interface: (params, constraints, loss)",
keras/,646,"New interface: (loss, params)",
keras/,651,Assuming old interface.,
keras/,125,"no activation, this layer is only linear.",
keras/,486,"build an all-zero tensor of shape (samples, output_dim)",
keras/,487,"(samples, timesteps, input_dim)",
keras/,488,"(samples,)",
keras/,489,"(samples, 1)",
keras/,490,"(samples, output_dim)",
keras/,500,"If there are multiple inputs, then",
keras/,501,they should be the main input and `initial_state`,
keras/,502,e.g. when loading model from file,
keras/,508,"If `initial_state` is specified,",
keras/,509,"and if it a Keras tensor,",
keras/,510,then add it to the inputs and temporarily,
keras/,511,modify the input spec to include the state.,
keras/,525,"Compute the full input spec, including state",
keras/,532,"Compute the full inputs, including state",
keras/,535,Perform the call,
keras/,538,Restore original input spec,
keras/,546,"input shape: `(samples, time (padded with zeros), input_dim)`",
keras/,547,note that the .build() method of subclasses MUST define,
keras/,548,self.input_spec and self.state_spec with complete input shapes.,
keras/,597,Properly set learning phase,
keras/,628,initialize state if None,
keras/,133,indexing trick,
keras/,205,"expand mask so that `mask[:, t].ndim == x.ndim`",
keras/,220,"tm1 means ""t minus one"" as in ""previous timestep""",
keras/,559,Handle negative axes,
keras/,581,ignore batch dimension,
keras/,17,"Set Keras base dir path given KERAS_HOME env variable, if applicable.",
keras/,18,Otherwise either ~/.keras or /tmp.,
keras/,27,Default backend: TensorFlow.,
keras/,30,Attempt to read Keras config file.,
keras/,52,"Save config file, if possible.",
keras/,57,Except permission denied and potential race conditions,
keras/,58,in multi-threaded environments.,
keras/,72,Except permission denied.,
keras/,75,"Set backend based on KERAS_BACKEND flag, if applicable.",
keras/,81,Import backend functions.,
keras/,92,Try and load external backend.,
keras/,96,Check if valid backend.,
keras/,97,Module is a valid backend if it has the required entries.,
keras/,104,"Make sure we don't override any entries from common, such as epsilon.",
keras/,32,INTERNAL UTILS,
keras/,34,This list holds the available devices.,
keras/,35,It is populated when `_get_available_gpus()` is called for the first time.,
keras/,36,We assume our devices don't change during our lifetime.,
keras/,47,Set initial config,
keras/,53,Private TF Keras utils,
keras/,55,learning_phase_scope = tf_keras_backend.learning_phase_scope  # TODO,
keras/,428,"This step is expensive, so we only run it on variables",
keras/,429,not already marked as initialized.,
keras/,441,DEVICE MANIPULATION AND PROBING,
keras/,447,NOTE(robieta): This differs from tf.keras in that self.device is a,
keras/,448,DeviceSpec rather than a string. This is done for compatibility,
keras/,449,with a range of TensorFlow versions.,
keras/,526,VARIABLE MANIPULATION,
keras/,1140,ensure that randomness is conditioned by the Numpy RNG,
keras/,1178,ensure that randomness is conditioned by the Numpy RNG,
keras/,1242,UPDATES OPS,
keras/,1304,LINEAR ALGEBRA,
keras/,1475,"if tuple, convert to list.",
keras/,1478,convert negative indices.,
keras/,1484,sanity checks,
keras/,1501,backup ndims. Need them later.,
keras/,1505,"if rank is 2, expand to 3.",
keras/,1514,bring x's dimension to be reduced to last axis.,
keras/,1522,bring y's dimension to be reduced to axis 1.,
keras/,1530,normalize both inputs to rank 3.,
keras/,1532,squash middle dimensions of x.,
keras/,1543,squash trailing dimensions of y.,
keras/,1555,"if inputs were squashed, we have to reshape the matmul output.",
keras/,1572,"if the inputs were originally rank 2, we remove the added 1 dim.",
keras/,1632,ELEMENT-WISE OPERATIONS,
keras/,2349,The CPU implementation of FusedBatchNorm only support NHWC,
keras/,2361,The mean / var / beta / gamma may be processed by broadcast,
keras/,2362,"so it may have extra axes with 1,",
keras/,2363,it is not needed and should be removed,
keras/,2402,default,
keras/,2406,SHAPE OPERATIONS,
keras/,2561,For static axis,
keras/,2563,slices along the repeat axis,
keras/,2565,repeat each slice the given number of reps,
keras/,2569,Here we use tf.tile to mimic behavior of np.repeat so that,
keras/,2570,we can handle dynamic shapes (that include None).,
keras/,2571,"To do that, we need an auxiliary axis to repeat elements along",
keras/,2572,it and then merge them along the desired axis.,
keras/,2574,Repeating,
keras/,2582,Merging,
keras/,2589,Fix shape representation,
keras/,2635,Match the behavior of numpy and Theano by returning an empty sequence.,
keras/,2641,Handle case where start is a tensor,
keras/,2683,Padding the axis,
keras/,2912,VALUE MANIPULATION,
keras/,3001,GRAPH MANIPULATION,
keras/,3046,CONTROL FLOW,
keras/,3149,tf.where needs its condition tensor,
keras/,3150,to be the same shape as its two,
keras/,3151,result tensors,
keras/,3213,else: assume learning phase is a placeholder tensor.,
keras/,3241,NN OPERATIONS,
keras/,3277,computes x for x > threshold else 0,
keras/,3280,"if no threshold, then can use nn.relu6 native TF op for performance",
keras/,3523,Note that the order of the 2 first positional arguments,
keras/,3524,has been inverted in TF 2.,
keras/,3530,CONVOLUTIONS,
keras/,3543,tensorflow doesn't support float64 for conv layer before 1.8.0,
keras/,3547,to pass TF Conv2dNative operations,
keras/,3550,NCW -> NWC,
keras/,3568,tensorflow doesn't support float64 for conv layer before 1.8.0,
keras/,3575,NCHW -> NHWC,
keras/,3591,tensorflow doesn't support float64 for conv layer before 1.8.0,
keras/,3652,causal (dilated) convolution:,
keras/,3659,TF 2 arg conversion,
keras/,3674,NWC -> NCW,
keras/,3705,TF 2 arg conversion,
keras/,3719,NHWC -> NCHW,
keras/,3747,tf.nn.atrous_conv2d_transpose input only supports NHWC format,
keras/,3781,NHWC -> NCHW,
keras/,3828,TF 2 arg conversion,
keras/,3844,NWC -> NCW,
keras/,3879,TF 2 arg conversion,
keras/,3892,NHWC -> NCHW,
keras/,3925,TF 2 arg conversion,
keras/,3938,NHWC -> NCHW,
keras/,3968,TF 2 arg conversion,
keras/,4081,NHWC -> NCHW,
keras/,4166,"Shape: `(output_length, batch_size, filters)`.",
keras/,4302,RANDOMNESS,
keras/,4416,CTC,
keras/,4417,"TensorFlow has a native implementation, but it uses sparse tensors",
keras/,4418,and therefore requires a wrapper for Keras. The functions below convert,
keras/,4419,dense to sparse tensors and also wraps up the beam search code that is,
keras/,4420,in TensorFlow's CTC implementation,
keras/,4557,HIGH ORDER FUNCTIONS,
keras/,7,the type of float to use throughout the session.,
keras/,192,Legacy methods,
keras/,28,Legacy functions,
keras/,37,INTERNAL UTILS,
keras/,39,"0 = test, 1 = train",
keras/,45,"False = test, True = train",
keras/,84,VARIABLE MANIPULATION,
keras/,145,"Support for RandomStreams().normal(), .uniform().",
keras/,380,We don't want those compilation to show up in Theano profiler.,
keras/,401,UPDATES OPS,
keras/,420,LINEAR ALGEBRA,
keras/,487,behaves like tf.batch_matmul as default,
keras/,518,Expand dims if ndim == 1,
keras/,546,ELEMENT-WISE OPERATIONS,
keras/,599,bool is available since theano v0.9dev,
keras/,698,Theano has a built-in optimization for logsumexp,
keras/,699,(see https://github.com/Theano/Theano/pull/4736),
keras/,700,so we can just write the expression directly:,
keras/,777,TODO remove this if statement when Theano without,
keras/,778,T.nnet.bn.batch_normalization_train is deprecated,
keras/,802,TODO remove this if statement when Theano without,
keras/,803,T.nnet.bn.batch_normalization_test is deprecated,
keras/,813,based on TensorFlow's default: normalize along rightmost dimension,
keras/,822,TODO remove this function when Theano without,
keras/,823,T.nnet.bn.batch_normalization_train is deprecated,
keras/,825,pragma: no cover,
keras/,873,TODO remove this if statement when Theano without,
keras/,874,T.nnet.bn.batch_normalization_test is deprecated,
keras/,876,pragma: no cover,
keras/,885,"in TensorFlow's batch_normalization, if the parameters are vectors",
keras/,886,the batch normalization should be applied along the rightmost axis.,
keras/,887,Theano expects the parameters to always have x.ndim dimensions.,
keras/,923,SHAPE OPERATIONS,
keras/,1098,Padding the axis,
keras/,1369,VALUE MANIPULATION,
keras/,1406,GRAPH MANIPULATION,
keras/,1478,CONTROL FLOW,
keras/,1550,tf.where needs its condition tensor,
keras/,1551,to be the same shape as its two,
keras/,1552,result tensors,
keras/,1597,"build an all-zero tensor of shape (samples, output_dim)",
keras/,1600,Theano gets confused by broadcasting patterns in the scan op,
keras/,1610,output previous output if masked.,
keras/,1626,deal with Theano API inconsistency,
keras/,1662,Theano likes to make shape==1 dimensions,
keras/,1663,in the initial states (outputs_info) broadcastable,
keras/,1674,deal with Theano API inconsistency,
keras/,1751,else: assume learning phase is a placeholder tensor.,
keras/,1768,NN OPERATIONS,
keras/,1838,"If the channels are not in the last axis, move them to be there:",
keras/,1847,scale preds so that the class probas of each sample sum to 1,
keras/,1849,avoid numerical instability with _EPSILON clipping,
keras/,1862,"If the channels are not in the last axis, move them to be there:",
keras/,1877,avoid numerical instability with _EPSILON clipping,
keras/,1946,handle k < 1 and k >= predictions.shape[1] cases to match TF behavior,
keras/,1948,dtype='bool' is only available since Theano 0.9.0,
keras/,1965,CONVOLUTIONS,
keras/,1969,"TF uses the last dimension as channel dimension,",
keras/,1970,instead of the 2nd one.,
keras/,1971,"TH input shape: (samples, input_depth, rows, cols)",
keras/,1972,"TF input shape: (samples, rows, cols, input_depth)",
keras/,1979,"TF uses the last dimension as channel dimension,",
keras/,1980,instead of the 2nd one.,
keras/,1981,"TH input shape: (samples, input_depth, rows, cols, slices)",
keras/,1982,"TF input shape: (samples, rows, cols, slices, input_depth)",
keras/,1988,"As of Keras 2.0.0, all kernels are normalized",
keras/,1989,"on the format `(rows, cols, input_depth, depth)`,",
keras/,1990,independently of `data_format`.,
keras/,1991,"Theano expects `(depth, input_depth, rows, cols)`.",
keras/,1997,"As of Keras 2.0.0, all kernels are normalized",
keras/,1998,"on the format `(rows, cols, input_depth, depth)`,",
keras/,1999,independently of `data_format`.,
keras/,2000,"Theano expects `(input_depth * depth, 1, rows, cols)`",
keras/,2001,for depthwise convolution.,
keras/,2009,"As of Keras 2.0.0, all kernels are normalized",
keras/,2010,"on the format `(space, input_depth, depth)`,",
keras/,2011,independently of `data_format`.,
keras/,2012,"Theano expects `(depth, input_depth, space)`.",
keras/,2030,Theano might not accept long type,
keras/,2046,Theano might not accept long type,
keras/,2062,Theano might not accept long type,
keras/,2077,Theano might not accept long type,
keras/,2092,Theano might not accept long type,
keras/,2154,causal (dilated) convolution:,
keras/,2162,"original shape: (batch, length, input_dim)",
keras/,2163,"add dim to x to have (batch, length, 1, input_dim)",
keras/,2165,update x._keras_shape,
keras/,2169,"original shape: (batch, input_dim, length)",
keras/,2170,"add dim to x to have (batch, input_dim, length, 1)",
keras/,2172,update x._keras_shape,
keras/,2175,"update dilation rate, strides",
keras/,2178,add dim to kernel (always same format independently of data_format),
keras/,2179,"i.e. (rows, 1, input_depth, depth)",
keras/,2184,remove added dim,
keras/,2209,in case of a shared variable,
keras/,2255,in case of a shared variable,
keras/,2320,in case of a shared variable,
keras/,2326,in case of a shared variable,
keras/,2383,in case of a shared variable,
keras/,2389,in case of a shared variable,
keras/,2444,in case of a shared variable,
keras/,2484,in case of a shared variable,
keras/,2530,in case of a shared variable,
keras/,2688,RANDOMNESS,
keras/,2730,Poor man's truncated normal: we literally clip the tensor,
keras/,2734,Theano implementation of CTC,
keras/,2735,Used with permission from Shawn Tan,
keras/,2736,https://github.com/shawntan/,
keras/,2737,Note that TensorFlow's native CTC code is significantly,
keras/,2738,faster than this,
keras/,2764,copy over,
keras/,2766,previous transitions,
keras/,2768,skip transitions,
keras/,2787,there should be a shortcut to calculating this,
keras/,2817,batchifies original CTC code,
keras/,2850,HIGH ORDER FUNCTIONS,
keras/,2884,We need to change the order of the arguments because theano accepts x as,
keras/,2885,first parameter and accumulator as second,
keras/,2907,We need to change the order of the arguments because theano accepts x as,
keras/,2908,first parameter and accumulator as second,
keras/,2927,"Shape: `(output_length, batch_size, filters)`.",
keras/,30,A learning phase is a bool tensor used to run Keras models in,
keras/,31,either train mode (learning_phase == 1) or test mode (learning_phase == 0).,
keras/,32,LEARNING_PHASE_PLACEHOLDER is the placeholder for dynamic learning phase,
keras/,37,"static learning phase flag, if it is not 0 or 1, we will go with dynamic",
keras/,38,learning phase tensor.,
keras/,42,"cntk doesn't support gradient as symbolic op, to hook up with keras model,",
keras/,43,"we will create gradient as a constant placeholder, here use this global",
keras/,44,map to keep the mapping from grad placeholder to parameter,
keras/,64,"If _LEARNING_PHASE is not 0 or 1, return dynamic learning phase tensor",
keras/,97,"CNTK currently don't support cond op, so here we use",
keras/,98,element_select approach as workaround. It may have,
keras/,99,"perf issue, will resolve it later with cntk cond op.",
keras/,109,if _LEARNING_PHASE is static,
keras/,130,"cntk only running with float,",
keras/,131,try to cast to float to run the model,
keras/,174,"we don't support init parameter with symbolic op, so eval it first as",
keras/,175,workaround,
keras/,183,"TODO: remove the conversion when cntk supports int32, int64",
keras/,184,https://www.cntk.ai/pythondocs/cntk.variables.html#cntk.variables.Parameter,
keras/,394,ensure that randomness is conditioned by the Numpy RNG,
keras/,419,ensure that randomness is conditioned by the Numpy RNG,
keras/,432,ensure that randomness is conditioned by the Numpy RNG,
keras/,462,ensure that randomness is conditioned by the Numpy RNG,
keras/,492,ensure that randomness is conditioned by the Numpy RNG,
keras/,566,"cntk calculate everything in float, so don't need case from bool / int",
keras/,626,"if tuple, convert to list",
keras/,629,convert negative indices,
keras/,649,Input shapes:,
keras/,650,"x: (b_size, x1, ..., d, ..., xn)",
keras/,651,"y: (b_size, y1, ..., d, ..., yn)",
keras/,652,where d is the dimension to reduce.,
keras/,654,Bring d to the last dimension in x,
keras/,655,"x: (b_size, ..., d)",
keras/,664,Bring d to the second dimension in y,
keras/,665,"y: (b_size, d, ...)",
keras/,673,Expand to rank 3 if needed,
keras/,689,batch size might be lost at this point,
keras/,712,for older versions of CNTK,
keras/,734,There is a bug in cntk gather op which may cause crash.,
keras/,735,We have made a fix but not catched in CNTK 2.1 release.,
keras/,736,Will update with gather op in next release,
keras/,749,"sequence axis is removed by default, so don't need reshape on it",
keras/,866,Padding the axis,
keras/,894,"Current cntk does not support shape like (1, batch). so using the workaround",
keras/,895,here to mapping the correct axis. Will remove this tricky after we add support,
keras/,896,in native cntk op,
keras/,1096,"cntk does not support gradients as symbolic op,",
keras/,1097,to hook up with keras model,
keras/,1098,"we will return a constant as place holder, the cntk learner will apply",
keras/,1099,the gradient during training.,
keras/,1170,need broadcasting,
keras/,1173,skip the batch axis,
keras/,1202,Compute true mean while keeping the dims for proper broadcasting.,
keras/,1226,The mean / var / beta / gamma may be processed by broadcast,
keras/,1227,"so it may have an extra batch axis with 1, it is not needed",
keras/,1228,"in cntk, need to remove those dummy axis.",
keras/,1281,collapse axis with batch axis,
keras/,1291,"no collapse, then first need to padding the shape",
keras/,1375,this is a workaround for recurrent layer,
keras/,1376,"if n is inferred dimension,",
keras/,1377,we can't figure out how to repeat it in cntk now,
keras/,1378,return the same x to take cntk broadcast feature,
keras/,1379,to make the recurrent layer work.,
keras/,1380,need to be fixed in GA.,
keras/,1411,"if the second axis is static axis, CNTK will do unroll by default",
keras/,1438,remove dummy dimension,
keras/,1468,remove dummy dimension,
keras/,1496,add the time_step axis back,
keras/,1500,add the time_step axis back,
keras/,1603,create place holder,
keras/,1676,causal (dilated) convolution:,
keras/,1684,"As of Keras 2.0.0, all kernels are normalized",
keras/,1685,"on the format `(steps, input_depth, depth)`,",
keras/,1686,independently of `data_format`.,
keras/,1687,"CNTK expects `(depth, input_depth, steps)`.",
keras/,1876,cntk output_shape does not include batch axis,
keras/,1878,"in keras2, need handle output shape in different format",
keras/,1980,"cntk's batch axis is not in shape,",
keras/,1981,so just flatten all the dim in x.shape,
keras/,2001,"Here, unlike other backends, the tensors lack a batch dimension:",
keras/,2010,"If the channels are not in the last axis, move them to be there:",
keras/,2019,"cntk's result shape is (batch, 1), while keras expect (batch, )",
keras/,2022,scale preds so that the class probas of each sample sum to 1,
keras/,2024,avoid numerical instability with epsilon clipping,
keras/,2030,"Here, unlike other backends, the tensors lack a batch dimension:",
keras/,2055,need group update by gradient place holder,
keras/,2111,"cntk only could handle loss and 1 metric in trainer, for metrics more",
keras/,2112,"than 2, need manual eval",
keras/,2136,"cntk only support calculate on float, do auto cast here",
keras/,2145,in current version cntk can't support input with variable,
keras/,2146,length. Will support it in next release.,
keras/,2185,"Some ops (like dropout) won't be applied during ""eval"" in cntk.",
keras/,2186,"They only evaluated in training phase. To make it work, call",
keras/,2187,"""forward"" method to let cntk know we want to evaluate them.from",
keras/,2188,"But the assign ops won't be executed under this mode, that's why",
keras/,2189,we need this check.,
keras/,2235,pragma: no cover,
keras/,2266,pragma: no cover,
keras/,2400,cntk output_shape does not include batch axis,
keras/,2402,"in keras2, need handle output shape in different format",
keras/,2430,"TF uses the last dimension as channel dimension,",
keras/,2431,instead of the 2nd one.,
keras/,2432,"TH input shape: (samples, input_depth, rows, cols)",
keras/,2433,"TF input shape: (samples, rows, cols, input_depth)",
keras/,2439,"As of Keras 2.0.0, all kernels are normalized",
keras/,2440,"on the format `(rows, cols, input_depth, depth)`,",
keras/,2441,independently of `data_format`.,
keras/,2442,"CNTK expects `(depth, input_depth, rows, cols)`.",
keras/,2465,"TF uses the last dimension as channel dimension,",
keras/,2466,instead of the 2nd one.,
keras/,2467,"TH input shape: (samples, input_depth, conv_dim1, conv_dim2, conv_dim3)",
keras/,2468,"TF input shape: (samples, conv_dim1, conv_dim2, conv_dim3,",
keras/,2469,input_depth),
keras/,2537,"transpose kernel to output_filters first, to apply broadcast",
keras/,2539,"Shape: (batch, filters, output_length, input_length * kernel_size)",
keras/,2541,"Shape: (batch, filters, output_length)",
keras/,2543,"Shape: (batch, output_length, filters)",
keras/,2574,transpose kernel to put filters first,
keras/,2576,"shape: batch, filters, output_length, input_length * kernel_size",
keras/,2578,"shape: batch, filters, output_length",
keras/,2580,"shape: batch, filters, row, col",
keras/,2585,"shape: batch, row, col, filters",
keras/,2610,there is a bug in cntk 2.1's unpack_batch implementation,
keras/,2623,"for hot fix, ignore all the . except the first one.",
keras/,1,-*- coding: utf-8 -*-,
keras/,28,decode utf8,
keras/,1,-*- coding: utf-8 -*-,
keras/,45,Legacy support,
keras/,76,"by convention, use 2 as OOV word",
keras/,77,reserve 'index_from' (=3 by default) characters:,
keras/,78,"0 (padding), 1 (start), 2 (OOV)",
keras/,47,Legacy support,
keras/,90,"by convention, use 2 as OOV word",
keras/,91,reserve 'index_from' (=3 by default) characters:,
keras/,92,"0 (padding), 1 (start), 2 (OOV)",
keras/,52,"We treat subclassed models as a simple sequence of layers,",
keras/,53,for logging purposes.,
keras/,61,if the model has multiple nodes,
keras/,62,or if the nodes have multiple inbound_layers,
keras/,63,the model is no longer sequential,
keras/,68,search for shared layers,
keras/,86,header names for the different log elements,
keras/,93,header names for the different log elements,
keras/,141,node is not part of the current network,
keras/,202,Note: SeparableConvolution not included,
keras/,203,since only supported by TF.,
keras/,248,last -> first,
keras/,253,first -> last,
keras/,283,"Reached an Input layer, stop recursion.",
keras/,295,Avoid input redundancy.,
keras/,101,Broadcast weights if possible.,
keras/,109,Raise error if ndim of weights is > values.,
keras/,115,"Expand dim of weights to match ndim of values, if required.",
keras/,124,Cannot be broadcasted.,
keras/,164,Update dimensions of `sample_weight` to match with `losses` if possible.,
keras/,168,Broadcast weights if possible.,
keras/,171,Apply weights to losses.,
keras/,174,Apply reduction function to the individual weighted losses.,
keras/,176,Convert the result back to the input type.,
keras/,61,generate input data,
keras/,80,instantiation,
keras/,83,"test get_weights , set_weights at layer level",
keras/,89,test in functional API,
keras/,97,check with the functional API,
keras/,110,"test serialization, weight setting at model level",
keras/,119,test training mode (e.g. useful when the layer has a,
keras/,120,different behavior at training and testing time).,
keras/,125,test instantiation from layer config,
keras/,130,for further checks in the caller function,
keras/,183,will mock gcs locally for tests,
keras/,191,will use real bucket for tests,
keras/,199,check that bucket exists and is accessible,
keras/,165,Logical and,
keras/,268,Reshape predictions and labels.,
keras/,273,Tile the thresholds for every prediction.,
keras/,282,Tile the predictions for every threshold.,
keras/,285,Compare predictions and threshold.,
keras/,288,Tile labels by number of thresholds,
keras/,103,Assume list/iterable,
keras/,198,Flag to check if a dict is user defined data or a sub group:,
keras/,224,We have to remember to unpickle in __getitem__,
keras/,235,scalar,
keras/,240,Check that no item in `data` is larger than `HDF5_OBJECT_HEADER_LIMIT`,
keras/,241,because in that case even chunking the array would not make the saving,
keras/,242,possible.,
keras/,245,Expecting this to never be true.,
keras/,254,convert to bytes,
keras/,262,This will never loop forever thanks to the test above.,
keras/,305,could be chunked,
keras/,388,Implementation based on suggestion solution here:,
keras/,389,https://github.com/keras-team/keras/issues/9343#issuecomment-440903847,
keras/,396,name does not matter,
keras/,417,note that filename does not matter here.,
keras/,427,We can't use isinstance here because it would require,
keras/,428,us to add pathlib2 to the Python 2 dependencies.,
keras/,154,Using all visible GPUs when not specifying `gpus`,
keras/,155,"e.g. CUDA_VISIBLE_DEVICES=0,2 python keras_mgpu.py",
keras/,200,Relocate the model definition under CPU device scope if needed,
keras/,209,"Place a copy of the model on each GPU,",
keras/,210,each getting a slice of the inputs.,
keras/,215,Retrieve a slice of the input.,
keras/,217,In-place input splitting which is not only,
keras/,218,5% ~ 12% faster but also less GPU memory,
keras/,219,duplication.,
keras/,228,Apply model on slice,
keras/,229,(creating a model replica on the target device).,
keras/,233,Save the outputs for merging back together later.,
keras/,237,Deduplicate output names to handle Siamese networks.,
keras/,252,Merge outputs under expected scope.,
keras/,171,noqa,
keras/,195,File found; verify integrity if a hash was provided.,
keras/,210,Maintain progbar for the lifetime of download.,
keras/,211,This design was chosen for Python 2.7 compatibility.,
keras/,381,Global variables to be shared across processes,
keras/,383,We use a Value to provide unique id to different processes.,
keras/,440,In this case the OS does not allow us to use,
keras/,441,multiprocessing. We resort to an int,
keras/,442,for enqueuer indexing.,
keras/,449,Doing Multiprocessing.Value += x is not process-safe.,
keras/,474,We do not need the init since it's threads.,
keras/,485,For new processes that may spawn,
keras/,567,Share the initial sequence,
keras/,580,"Done with the current epoch, waiting for the final batches",
keras/,584,We're done,
keras/,587,Call the internal on epoch end.,
keras/,589,communicate on_epoch_end to the main thread,
keras/,689,Share the initial generator,
keras/,723,Special case for finite generators,
keras/,727,Wait for them to complete,
keras/,729,Keep the good ones,
keras/,159,Get the dilated kernel size,
keras/,162,"Infer length if output padding is None, else compute the exact length",
keras/,10,"`pydot` is an optional dependency,",
keras/,11,see `extras_require` in `setup.py`.,
keras/,26,Attempt to create an image of a blank graph,
keras/,27,to check the pydot/graphviz installation.,
keras/,101,Create graph nodes.,
keras/,105,"Append a wrapped layer's label to node's label, if it exists.",
keras/,115,sub_w : submodel_wrapper,
keras/,130,sub_n : submodel_not_wrapper,
keras/,136,Create node's label.,
keras/,142,Rebuild the label as a table including input/output shapes.,
keras/,163,Connect nodes with edges.,
keras/,176,if inbound_layer is not Model or wrapped Model,
keras/,179,if current layer is not Model or wrapped Model,
keras/,186,if current layer is Model,
keras/,190,if current layer is wrapped Model,
keras/,197,if inbound_layer is Model,
keras/,205,if inbound_layer is wrapped Model,
keras/,247,"Return the image as a Jupyter Image object, to be displayed in-line.",
keras/,10,Globally-importable utils.,
keras/,126,In this case we are dealing with a Keras config dictionary.,
keras/,151,Then `cls` may be a function returning a class.,
keras/,152,in this case by convention `config` holds,
keras/,153,the kwargs of the function.,
keras/,205,unpack previous dump,
keras/,221,just access it so it gets captured in .__closure__,
keras/,235,backwards compatibility for models serialized prior to 2.1.2,
keras/,370,Stateful metrics output a numeric value.  This representation,
keras/,371,"means ""take an average from a single value"" but keeps the",
keras/,372,numeric formatting.,
keras/,552,hdf5 datasets only support list objects as indices,
keras/,13,TODO: make it public?,
keras/,112,it's possible to callback a different model than itself,
keras/,113,(used by Sequential models),
keras/,134,"To prevent a slowdown,",
keras/,135,we find beforehand the arrays that need conversion.,
keras/,168,Same labels assumed.,
keras/,182,Do not slice the training phase flag.,
keras/,205,Last batch.,
keras/,212,Same labels assumed.,
keras/,252,Check if callbacks have not been already configured,
keras/,280,Step-based predictions.,
keras/,281,Since we do not know how many samples,
keras/,282,"we will see, we cannot pre-allocate",
keras/,283,the returned Numpy arrays.,
keras/,284,"Instead, we store one array per batch seen",
keras/,285,and concatenate them upon returning.,
keras/,308,Sample-based predictions.,
keras/,315,Do not slice the training phase flag.,
keras/,327,Pre-allocate the results arrays.,
keras/,374,Check if callbacks have not been already configured,
keras/,396,"To prevent a slowdown,",
keras/,397,we find beforehand the arrays that need conversion.,
keras/,418,Index 0 == `Loss`,
keras/,433,Index 0 == `Loss`,
keras/,440,Do not slice the training phase flag.,
keras/,454,Index 0 == `Loss`,
keras/,469,Index 0 == `Loss`,
keras/,124,Check shapes compatibility.,
keras/,222,return a set with the variation between,
keras/,223,"different shapes, with None => 0",
keras/,345,to reshape we need to be cleanly divisible by batch size,
keras/,346,we stash extra items and reappend them after shuffling,
keras/,365,round up,
keras/,399,score_array has ndim >= 2,
keras/,402,Cast the mask to floatX to avoid float64 upcasting in Theano,
keras/,404,mask should have the same shape as score_array,
keras/,406,the loss per batch should be proportional,
keras/,407,to the number of unmasked samples.,
keras/,410,apply sample weighting,
keras/,412,reduce score_array to same ndim as weight array,
keras/,508,subtract the sets to pick all missing classes,
keras/,524,Everything has weight 1 by default.,
keras/,579,Edge case where ins == [static_learning_phase],
keras/,605,TODO Dref360: Decide which pattern to follow. First needs a new TF Version.,
keras/,631,`epoch` is 0-indexed internally but 1-indexed in the public API.,
keras/,677,In case of nested models: recover the first layer,
keras/,678,of the deepest model to infer input shape and dtype.,
keras/,679,Subclassed Models may not have been built so can't be checked.,
keras/,695,"Deserialize loss configuration, if needed.",
keras/,699,Custom callable class.,
keras/,703,"Wrap loss function with signature `(y_true, y_pred, **kwargs)`",
keras/,704,in `LossFunctionWrapper` class.,
keras/,707,"For losses which are given as strings/functions in the compile API,",
keras/,708,we always set the loss reduction type to be `SUM_OVER_BATCH_SIZE`..,
keras/,916,User has provided a list of len = len(outputs).,
keras/,919,If it is a single list we then apply all metrics to all outputs.,
keras/,945,"If the metric function is not stateful, we create a stateful version.",
keras/,965,We keep the string that the user has set in compile as the metric name.,
keras/,1003,"If the output_shape[-1] is not 1, then we know output is `categorical`.",
keras/,1004,We assume it is sparse categorical only if loss is explicitly given,
keras/,1005,as sparse categorical crossentropy loss.,
keras/,1024,Use mask as sample weight.,
keras/,1027,Update dimensions of weights to match with mask.,
keras/,1034,For TF,
keras/,1037,`Mean` metric only takes a single value.,
keras/,1039,For TF,
keras/,102,List of stateful metric functions. Used for resetting metric state during,
keras/,103,training/eval.,
keras/,105,List of metric wrappers on output losses.,
keras/,109,Model is not compilable because,
keras/,110,it does not know its number of inputs,
keras/,111,"and outputs, nor their shapes and names.",
keras/,112,We will compile after the first,
keras/,113,time the model gets called on training data.,
keras/,117,"Prepare list of loss functions, same size as model outputs.",
keras/,126,"if loss function is None, then this output will be skipped during total",
keras/,127,loss calculation and feed targets preparation.,
keras/,135,Prepare output masks.,
keras/,141,"Prepare list loss weights, same size of model outputs.",
keras/,145,Prepare targets of model.,
keras/,206,Prepare sample weights.,
keras/,210,Save all metric attributes per output of the model.,
keras/,213,Set metric attributes on model.,
keras/,216,Invoke metric functions (unweighted) for all the outputs.,
keras/,224,Compute total loss.,
keras/,225,Used to keep track of the total loss value (stateless).,
keras/,226,"eg., total_loss = loss_weight_1 * output_1_loss_fn(...) +",
keras/,227,loss_weight_2 * output_2_loss_fn(...) +,
keras/,228,layer losses.,
keras/,231,"Functions for train, test and predict will",
keras/,232,be compiled lazily when required.,
keras/,233,This saves time when the user is not using all functions.,
keras/,240,"Collected trainable weights, sorted in topological order.",
keras/,259,Add output loss metric names to the metric names list.,
keras/,267,Add compile metrics/weighted metrics' names to the metric names list.,
keras/,270,Add metric names from layers.,
keras/,327,Gets loss and metrics. Updates weights at each call.,
keras/,354,"Return loss and metrics, no gradient updates.",
keras/,355,Does update the network states.,
keras/,371,Gets network outputs. Does not update weights.,
keras/,372,Does update the network states.,
keras/,410,Note: we can't test whether the model,
keras/,411,is `Sequential` via `isinstance`,
keras/,412,since `Sequential` depends on `Model`.,
keras/,422,On-the-fly setting of symbolic model inputs,
keras/,423,"(either by using the tensor provided,",
keras/,424,or by creating a placeholder if Numpy data was provided).,
keras/,440,We fix the placeholder shape except the batch size.,
keras/,441,"This is suboptimal, but it is the best we can do with the info",
keras/,442,we have. The user should call `model._set_inputs(placeholders)`,
keras/,443,to specify custom placeholders if the need arises.,
keras/,451,Assumed tensor - TODO(fchollet) additional type check?,
keras/,459,Obtain symbolic outputs by calling the model.,
keras/,478,We need to use `x` to set the model inputs.,
keras/,479,We type-check that `x` and `y` are either single arrays,
keras/,480,or lists of arrays.,
keras/,500,Build the model using the retrieved inputs (value or symbolic).,
keras/,501,"If values, then in symbolic-mode placeholders will be created",
keras/,502,to match the value shapes.,
keras/,512,On-the-fly compilation of the model.,
keras/,513,We need to use `y` to set the model targets.,
keras/,530,Typecheck that all inputs are *either* value *or* symbolic.,
keras/,540,Handle target tensors if any passed.,
keras/,552,"If `x` and `y` were all symbolic,",
keras/,553,then the model should not be fed any inputs and targets.,
keras/,554,"Note: in this case, `any` and `all` are equivalent since we disallow",
keras/,555,mixed symbolic/value inputs.,
keras/,559,"What follows is input validation and standardization to list format,",
keras/,560,in the case where all inputs are value arrays.,
keras/,563,Case: symbolic-mode subclassed network.,
keras/,564,Do not do shape validation.,
keras/,568,Case: symbolic-mode graph network.,
keras/,569,"In this case, we run extensive shape validation checks.",
keras/,573,Standardize the inputs.,
keras/,578,Don't enforce the batch size.,
keras/,585,Sample weighting not supported in this case.,
keras/,586,TODO: consider supporting it.,
keras/,607,If the given loss is not an instance of the `Loss` class,
keras/,608,(custom class) or if the loss function that is wrapped is,
keras/,609,"not in the `losses` module, then it is a user-defined loss",
keras/,610,and we make no assumptions about it.,
keras/,615,Standardize the outputs.,
keras/,620,Don't enforce the batch size.,
keras/,623,Generate sample-wise weight values given the `sample_weight` and,
keras/,624,`class_weight` arguments.,
keras/,635,Check that all arrays have the same length.,
keras/,639,Additional checks to avoid users mistakenly,
keras/,640,using improper loss fns.,
keras/,648,"Check that for stateful networks, number of samples is a multiple",
keras/,649,of the static batch size.,
keras/,681,Update weights with mask.,
keras/,685,Update dimensions of weights to match with mask.,
keras/,697,For TF,
keras/,711,Add regularization penalties and other layer-specific losses.,
keras/,787,Update the name on the metric class to be the unique generated name.,
keras/,790,Keep track of metric function.,
keras/,811,Create a metric wrapper for each output loss. This computes mean of an,
keras/,812,output loss across mini-batches (irrespective of how we reduce within a,
keras/,813,batch).,
keras/,862,Invoke all metrics added using `compile`.,
keras/,915,Avoids the override in Sequential.,
keras/,921,Check `batch_size` argument is consistent with InputLayer.,
keras/,928,Set inferred batch size from the InputLayer.,
keras/,933,Backwards compatibility,
keras/,1112,Legacy support,
keras/,1128,"Case 1: generator-like. Input is Python generator,",
keras/,1129,"or Sequence object, or iterator.",
keras/,1149,Case 2: Symbolic tensors or Numpy array-like.,
keras/,1156,Prepare validation data.,
keras/,1208,Prepare input arrays and training function.,
keras/,1216,Prepare display labels.,
keras/,1226,Delegate logic to `fit_loop`.,
keras/,1328,"Case 1: generator-like. Input is Python generator, or Sequence object.",
keras/,1340,Case 2: Symbolic tensors or Numpy array-like.,
keras/,1345,Validate user data.,
keras/,1350,"Prepare inputs, delegate logic to `test_loop`.",
keras/,1424,"Case 1: generator-like. Input is Python generator, or Sequence object.",
keras/,1440,Case 2: Symbolic tensors or Numpy array-like.,
keras/,1451,"Prepare inputs, delegate logic to `predict_loop`.",
keras/,1862,We cannot call 'metrics' on the model because we do not want to,
keras/,1863,include the metrics that were added in compile API of a nested model.,
keras/,114,These properties will be set upon call of self.build(),
keras/,123,A list of metric instances corresponding to the metric tensors added using,
keras/,124,the `add_metric` API.,
keras/,127,These lists will be filled via successive calls,
keras/,128,to self._add_inbound_node().,
keras/,132,These properties should be set by the user via keyword arguments.,
keras/,133,"note that 'dtype', 'input_shape' and 'batch_input_shape'",
keras/,134,are only applicable to input layers: do not pass these keywords,
keras/,135,to non-input layers.,
keras/,143,legacy,
keras/,156,In this case we will later create an input layer,
keras/,157,to insert before the current layer,
keras/,166,Set dtype.,
keras/,335,Check ndim.,
keras/,359,Check dtype.,
keras/,367,Check specific shape axes.,
keras/,384,Check shape.,
keras/,442,"Handle laying building (weight creating, input spec locking).",
keras/,444,Raise exceptions in case the input is not compatible,
keras/,445,with the input_spec specified in the layer constructor.,
keras/,448,Collect input shapes to build layer.,
keras/,466,Load weights that were specified at layer instantiation.,
keras/,470,Raise exceptions in case the input is not compatible,
keras/,471,with the input_spec set at build time.,
keras/,474,Handle mask propagation.,
keras/,478,The previous layer generated a mask.,
keras/,481,"If mask is explicitly passed to __call__,",
keras/,482,we should override the default mask.,
keras/,484,Handle automatic shape inference (only useful for Theano).,
keras/,487,"Actually call the layer,",
keras/,488,"collecting output(s), mask(s), and shape(s).",
keras/,492,"If the layer returns tensors from its inputs, unmodified,",
keras/,493,we copy them to avoid loss of tensor metadata.,
keras/,503,Inferring the output shape is only relevant for Theano.,
keras/,515,Augment the mask to match the length of the output.,
keras/,518,"Add an inbound node to the layer, so that it keeps track",
keras/,519,of the call and of all new variables created during the call.,
keras/,520,This also updates the layer history of the output tensor(s).,
keras/,521,"If the input tensor(s) had not previous Keras history,",
keras/,522,this does nothing.,
keras/,531,Apply activity regularizer if any:,
keras/,565,Collect input tensor(s) coordinates.,
keras/,580,"Create node, add it to inbound nodes.",
keras/,595,"Update tensor history, _keras_shape and _uses_learning_phase.",
keras/,649,masking not explicitly supported: return None as mask,
keras/,651,"if masking is explicitly supported, by default",
keras/,652,carry over the input mask,
keras/,984,We track the instance using the metadata on the result tensor.,
keras/,985,Use case: model.add_metric(metrics.Mean(name='metric_2')(y)),
keras/,988,"Use cases: model.add_metric(K.sum(y), name='metric_1')",
keras/,1009,Update self.losses,
keras/,1015,Update self._per_input_updates,
keras/,1021,Updates indexed by None are unconditional,
keras/,1022,rather than input-dependent,
keras/,1043,Update self.updates,
keras/,1049,Update self._per_input_updates,
keras/,1055,Updates indexed by None are unconditional,
keras/,1056,rather than input-dependent,
keras/,1212,Keep track of metric instance created in subclassed model/layer.,
keras/,1213,We do this so that we can maintain the correct order of metrics by adding,
keras/,1214,the instance to the `metrics` list as soon as it is created.,
keras/,1224,Automatically track layers set as attributes.,
keras/,1231,Automatically track variables set as attributes.,
keras/,1258,For TF,
keras/,1360,Layer instance (NOT a list).,
keras/,1361,this is the layer that takes a list of input tensors,
keras/,1362,and turns them into a list of output tensors.,
keras/,1363,the current node will be added to,
keras/,1364,the inbound_nodes of outbound_layer.,
keras/,1367,The following 3 properties describe where,
keras/,1368,"the input tensors come from: which layers,",
keras/,1369,"and for each layer, which node and which",
keras/,1370,tensor output of each node.,
keras/,1372,List of layer instances.,
keras/,1374,"List of integers, 1:1 mapping with inbound_layers.",
keras/,1376,"List of integers, 1:1 mapping with inbound_layers.",
keras/,1379,Following 2 properties:,
keras/,1380,tensor inputs and outputs of outbound_layer.,
keras/,1382,List of tensors. 1:1 mapping with inbound_layers.,
keras/,1384,"List of tensors, created by outbound_layer.call().",
keras/,1387,Following 2 properties: input and output masks.,
keras/,1388,"List of tensors, 1:1 mapping with input_tensor.",
keras/,1390,"List of tensors, created by outbound_layer.compute_mask().",
keras/,1393,Following 2 properties: input and output shapes.,
keras/,1395,"List of shape tuples, shapes of input_tensors.",
keras/,1397,"List of shape tuples, shapes of output_tensors.",
keras/,1400,Optional keyword arguments to layer's `call`.,
keras/,1403,Add nodes to all layers involved.,
keras/,1451,"If the class is private the name starts with ""_"" which is not secure",
keras/,1452,"for creating scopes. We prefix the name with ""private"" in this case.",
keras/,91,Add to the model any layers passed to the constructor.,
keras/,98,"Historically, `sequential.layers` only returns layers that were added",
keras/,99,"via `add`, and omits the auto-generated `InputLayer`",
keras/,100,that comes at the bottom of the stack.,
keras/,107,"Historically, `Sequential` was once",
keras/,108,implemented as a wrapper for `Model` which maintained,
keras/,109,its underlying `Model` as the `model` property.,
keras/,110,We keep it for compatibility reasons.,
keras/,137,First layer in model: check that it is an input layer.,
keras/,139,Create an input tensor and call `layer` on the input tensor.,
keras/,140,"First, we need to infer the expected input shape and dtype.",
keras/,143,We were passed a model as first layer.,
keras/,144,This requires a specific way to figure out the,
keras/,145,input shape and dtype.,
keras/,149,In case of nested models: recover the first layer,
keras/,150,of the deepest model to infer input shape and dtype.,
keras/,158,Instantiate the input layer.,
keras/,163,This will build the current layer,
keras/,164,and create the node connecting the current layer,
keras/,165,to the input layer we just created.,
keras/,169,Corner case where the user passes an InputLayer via `add`.,
keras/,295,legacy config file,
keras/,41,getargspec() is deprecated since Python 3.0,
keras/,99,if obj is a serializable Keras class instance,
keras/,100,"e.g. optimizer, layer",
keras/,105,if obj is any numpy type,
keras/,112,misc functions (e.g. loss function),
keras/,116,if obj is a python 'type',
keras/,194,Default values of symbolic_weights is /variable,
keras/,195,for Theano and CNTK,
keras/,310,We batch weight value assignments in a single backend call,
keras/,311,which provides a speedup in TensorFlow.,
keras/,350,Recover loss functions and metrics.,
keras/,351,Deserialize loss class.,
keras/,356,Earlier versions of keras didn't dump weighted_metrics properly. Use,
keras/,357,a get to avoid failing if the key is missing,
keras/,363,Compile model.,
keras/,371,Set optimizer weights.,
keras/,373,Build train function (to get weight updates).,
keras/,543,write as binary stream,
keras/,678,Check that no item in `data` is larger than `HDF5_OBJECT_HEADER_LIMIT`,
keras/,679,because in that case even chunking the array would not make the saving,
keras/,680,possible.,
keras/,683,Expecting this to never be true.,
keras/,695,This will never loop forever thanks to the test above.,
keras/,747,Sort model layers by layer name to ensure that group names are strictly,
keras/,748,growing to avoid prefix issues.,
keras/,765,scalar,
keras/,830,trainable weights,
keras/,841,non-trainable weights,
keras/,855,Convert layers nested in Bidirectional/TimeDistributed/Model/Sequential.,
keras/,856,Both transformation should be ran for both Keras 1->2 conversion,
keras/,857,and for conversion of CuDNN layers.,
keras/,874,Handle Keras 1.1 format,
keras/,876,Legacy shape:,
keras/,877,"(filters, input_dim, filter_length, 1)",
keras/,885,"old: (filters, stack_size, kernel_rows, kernel_cols)",
keras/,886,"new: (kernel_rows, kernel_cols, stack_size, filters)",
keras/,891,"old: (kernel_rows, kernel_cols, stack_size, filters)",
keras/,892,"new: (kernel_rows, kernel_cols, filters, stack_size)",
keras/,895,"old: (filters, stack_size, kernel_rows, kernel_cols)",
keras/,896,"new: (kernel_rows, kernel_cols, filters, stack_size)",
keras/,901,"old: (filters, stack_size, ...)",
keras/,902,"new: (..., stack_size, filters)",
keras/,920,"old: i, c, f, o",
keras/,921,"new: i, f, c, o",
keras/,951,"old: (filters, stack_size, kernel_rows, kernel_cols)",
keras/,952,"new: (kernel_rows, kernel_cols, stack_size, filters)",
keras/,986,convert CuDNN layers,
keras/,1058,convert the weights between CuDNNLSTM and LSTM,
keras/,1060,determine if we're loading a CuDNNLSTM layer,
keras/,1061,from the number of bias weights:,
keras/,1062,CuDNNLSTM has (units * 8) weights; while LSTM has (units * 4),
keras/,1063,"if there's no bias weight in the file, skip this conversion",
keras/,1076,transpose (and reshape) input and recurrent kernels,
keras/,1082,merge input and recurrent biases into a single set,
keras/,1085,Split single set of biases evenly to two sets. The way of,
keras/,1086,splitting doesn't matter as long as the two sets sum is kept.,
keras/,1093,convert the weights between CuDNNGRU and GRU(reset_after=True),
keras/,1095,We can determine the source of the weights from the shape of the bias.,
keras/,1096,If there is no bias we skip the conversion,
keras/,1097,since CuDNNGRU always has biases.,
keras/,1127,only convert between different types,
keras/,1155,backend information not available,
keras/,1161,"By default, do not convert the kernels if the original backend is unknown",
keras/,1166,Assume unknown backends use correlation,
keras/,1213,We batch weight value assignments in a single backend call,
keras/,1214,which provides a speedup in TensorFlow.,
keras/,1272,New file format.,
keras/,1275,Reverse index of layer name to list of layers with name.,
keras/,1281,We batch weight value assignments in a single backend call,
keras/,1282,which provides a speedup in TensorFlow.,
keras/,1312,Set values.,
keras/,89,Signature detection,
keras/,93,Graph network,
keras/,96,Subclassed network,
keras/,100,The following are implemented as property functions:,
keras/,101,self.trainable_weights,
keras/,102,self.non_trainable_weights,
keras/,103,self.input_spec,
keras/,104,self.losses,
keras/,105,self.updates,
keras/,107,Handle `name` argument.,
keras/,113,This acts just like the `trainable` attribute of any layer instance.,
keras/,114,"It does not affect users of the underlying layers, only users of the",
keras/,115,Network instance.,
keras/,126,Don't reset optimizer if already set.,
keras/,129,Private attributes to implement compatibility with Layer.,
keras/,137,A list of metric instances corresponding to the metric tensors added using,
keras/,138,the `add_metric` API.,
keras/,141,All layers in order of horizontal graph traversal.,
keras/,142,Entries are unique. Includes input and output layers.,
keras/,145,Used only in conjunction with graph-networks,
keras/,151,"Normalize and set self.inputs, self.outputs.",
keras/,155,User-provided argument validation.,
keras/,156,Check for redundancy in inputs.,
keras/,163,Check that x has appropriate `_keras_history` metadata.,
keras/,170,Check that x is an input tensor.,
keras/,203,"A Network does not create weights of its own,",
keras/,204,thus it is already built.,
keras/,213,This is for performance optimization when calling the Network on new,
keras/,214,"inputs. Every time the Network is called on a set on input tensors,",
keras/,215,"we compute the output tensors,",
keras/,216,"output masks and output shapes in one pass,",
keras/,217,"then cache them here. When any of these outputs is queried later, we",
keras/,218,retrieve it from there instead of recomputing it.,
keras/,223,Build self._output_layers:,
keras/,229,Build self._input_layers:,
keras/,232,"It's supposed to be an input layer, so only one node",
keras/,233,and one tensor output.,
keras/,239,Keep track of the network's nodes and layers.,
keras/,247,Create the node linking internal inputs to internal outputs.,
keras/,254,No network-level masking for now.,
keras/,260,Fill in the output mask cache.,
keras/,278,Build self.input_names and self.output_names.,
keras/,285,Check that layer is an InputLayer.,
keras/,313,Automatically track layers set as Model,
keras/,314,attributes for subclassed Models.,
keras/,346,It would be unreliable to build a dictionary,
keras/,347,"based on layer names, because names can potentially",
keras/,348,be changed at any point by the user,
keras/,349,without the network being notified of it.,
keras/,385,Collect updates that are dependent on inputs,
keras/,386,that are part of the model.,
keras/,390,The model owns this layer node.,
keras/,393,Collect unconditional updates.,
keras/,415,Collect losses that are dependent on inputs,
keras/,416,that are part of the model.,
keras/,420,The model owns this layer node.,
keras/,423,Collect unconditional losses.,
keras/,428,Add any potential unconditional model-level loss.,
keras/,538,TODO: support it in subclassed networks after inputs are set.,
keras/,605,Must be implemented by subclasses.,
keras/,621,"Bad luck, we have to run the graph manually.",
keras/,626,"It's an input layer: compute_output_shape is identity,",
keras/,627,and there is only one node and one tensor output.,
keras/,633,"Iterate over nodes, by depth level.",
keras/,638,"This is always a single layer, never a list.",
keras/,641,We've already covered the input layers,
keras/,642,a few lines above.,
keras/,644,"Potentially redundant list,",
keras/,645,same size of node.input_tensors.,
keras/,665,Read final output shapes from layers_to_output_shapes.,
keras/,678,Store in cache.,
keras/,701,Dictionary mapping reference tensors to tuples,
keras/,702,"(computed tensor, compute mask)",
keras/,703,we assume a 1:1 mapping from tensor to mask,
keras/,704,TODO: raise exception when a `.compute_mask()` call,
keras/,705,does not return a list the same size as `call`,
keras/,715,"This is always a single layer, never a list.",
keras/,720,"If all previous input tensors are available in tensor_map,",
keras/,721,then call node.inbound_layer on them.,
keras/,722,"List of tuples (input, mask).",
keras/,728,call layer,
keras/,749,computed_masks might be used in the future.,
keras/,765,Apply activity regularizer if any:,
keras/,781,Update model updates and losses:,
keras/,782,Keep track of updates that depend on the inputs,
keras/,783,(e.g. BN updates).,
keras/,785,Keep track of unconditional updates (e.g. a counter).,
keras/,787,Keep track of losses that depend on the inputs,
keras/,788,(e.g. activity regularizers).,
keras/,790,Keep track of unconditional losses,
keras/,791,(e.g. weight regularizers).,
keras/,794,Update _keras_shape.,
keras/,807,Update tensor_map.,
keras/,827,Update cache;,
keras/,828,keys are based on ids on input tensors and inputs masks.,
keras/,848,Subclassed networks are not serializable,
keras/,849,(unless serialization is implemented by,
keras/,850,the author of the subclassed network).,
keras/,857,Build a map from a layer unique name (self._node_key),
keras/,858,to the index of the nodes that are saved in the config.,
keras/,859,Only nodes in network_nodes are saved.,
keras/,863,Networks start with a pre-existing node,
keras/,864,linking their input to output.,
keras/,871,i.e. we mark it to be saved,
keras/,875,serialize and save the layers in layer_configs,
keras/,877,From the earliest layers on.,
keras/,884,The node is relevant to the model:,
keras/,885,add to filtered_inbound_nodes.,
keras/,925,Gather info about inputs and outputs.,
keras/,968,Layer instances created during,
keras/,969,the graph reconstruction process,
keras/,972,Dictionary mapping layer instances to,
keras/,973,node data that specifies a layer call.,
keras/,974,It acts as a queue that maintains any unprocessed,
keras/,975,layer call until it becomes possible to process it,
keras/,976,(i.e. until the input tensors to the call all exist).,
keras/,1014,Raise an error if the corresponding layer node,
keras/,1015,has not yet been created,
keras/,1022,"Call layer on its inputs, thus creating the node",
keras/,1023,and building the layer if needed.,
keras/,1038,Instantiate layer.,
keras/,1045,Gather layer inputs.,
keras/,1048,We don't process nodes (i.e. make layer calls),
keras/,1049,"on the fly because the inbound node may not yet exist,",
keras/,1050,in case of layer shared at different topological depths,
keras/,1051,(e.g. a model such as A(B(A(B(x))))),
keras/,1054,"First, we create all layers and enqueue nodes to be processed",
keras/,1058,Then we process nodes in order of layer depth.,
keras/,1059,Nodes that cannot yet be processed (if the inbound node,
keras/,1060,"does not yet exist) are re-enqueued, and the process",
keras/,1061,is repeated until all nodes are processed.,
keras/,1066,"Process all nodes in layer, if not yet processed",
keras/,1070,Process nodes in order,
keras/,1077,If the node does not have all inbound layers,
keras/,1078,"available, stop processing and continue later",
keras/,1084,If not all nodes processed then store unprocessed nodes,
keras/,1087,If all nodes processed remove the layer,
keras/,1091,Create lits of input and output tensors and return new class,
keras/,1178,If file exists and should not be overwritten:,
keras/,1267,If obj is any numpy type,
keras/,1274,If obj is a python 'type',
keras/,1360,Network_nodes: set of nodes included in the graph of layers,
keras/,1361,(not all nodes included in the layers are relevant to the current graph).,
keras/,1362,ids of all nodes relevant to the Network,
keras/,1363,dict {node: depth value},
keras/,1364,dict {layer: depth value},
keras/,1365,dict {layer: index in traversal},
keras/,1395,Prevent cycles.,
keras/,1400,Don't repeat work for shared subgraphs,
keras/,1405,Update network_nodes.,
keras/,1408,Store the traversal order for layer sorting.,
keras/,1414,Propagate to all previous tensors connected to this node.,
keras/,1437,"If the depth is not set, the node has no outbound nodes (depth 0).",
keras/,1440,Update the depth of the corresponding layer,
keras/,1442,"If we've seen this layer before at a higher depth,",
keras/,1443,we should use that depth instead of the node depth.,
keras/,1444,This is necessary for shared layers that have inputs at different,
keras/,1445,depth levels in the graph.,
keras/,1450,Update the depth of inbound nodes.,
keras/,1451,"The ""depth"" of a node is the max of the depths",
keras/,1452,of all layers it is connected to.,
keras/,1460,Build a dict {depth: list of nodes with this depth},
keras/,1467,Build a dict {depth: list of layers with this depth},
keras/,1474,Get sorted list of layer depths.,
keras/,1478,Set self.layers and self._layers_by_depth.,
keras/,1482,Network.layers needs to have a deterministic order:,
keras/,1483,here we order them by traversal order.,
keras/,1487,Get sorted list of node depths.,
keras/,1491,Check that all tensors required are computable.,
keras/,1492,computable_tensors: all tensors in the graph,
keras/,1493,that can be computed from the inputs provided.,
keras/,1498,To provide a better error msg.,
keras/,1516,"Ensure name unicity, which will be crucial for serialization",
keras/,1517,(since serialized nodes refer to layers by their name).,
keras/,1,"note: `Node` is an internal class,",
keras/,2,it isn't meant to be used by Keras users.,
keras/,54,if generator is instance of Sequence and steps_per_epoch are not provided -,
keras/,55,recompute steps_per_epoch after each epoch,
keras/,68,"python 2 has 'next', 3 has '__next__'",
keras/,69,avoid any explicit version checks,
keras/,81,Prepare display labels.,
keras/,85,prepare callbacks,
keras/,97,it's possible to callback a different model than self:,
keras/,116,Create an Enqueuer that can be reused,
keras/,138,Prepare data for validation,
keras/,177,Construct epoch logs.,
keras/,204,Handle data tensors support when no input given,
keras/,205,step-size = 1 for data tensors,
keras/,213,build batch logs,
keras/,231,Epoch finished.,
keras/,235,Note that `callbacks` here is an instance of,
keras/,236,`keras.callbacks.CallbackList`,
keras/,244,No need for try/except because,
keras/,245,data has already been validated.,
keras/,253,Same labels assumed.,
keras/,272,recomute steps per epochs in case if Sequence changes it's length,
keras/,275,update callbacks to make sure params are valid each epoch,
keras/,327,Check if callbacks have not been already configured,
keras/,383,Handle data tensors support when no input given,
keras/,384,step-size = 1 for data tensors,
keras/,420,index 0 = 'loss',
keras/,455,Check if callbacks have not been already configured,
keras/,493,Compatibility with the generators,
keras/,494,used for training.,
keras/,505,Assumes a generator that only,
keras/,506,yields inputs (not targets and sample weights).,
keras/,510,Handle data tensors support when no input given,
keras/,511,step-size = 1 for data tensors,
keras/,52,"If input_tensor is set, and batch_input_shape is not set:",
keras/,53,Attempt automatic input shape inference.,
keras/,91,Create an input node to add to self.outbound_node,
keras/,92,and set output_tensors' _keras_history.,
keras/,179,Return tensor including _keras_shape and _keras_history.,
keras/,180,Note that in this case train_output and test_output are the same pointer.,
keras/,68,"input shape: `(samples, time (padded with zeros), input_dim)`",
keras/,69,note that the .build() method of subclasses MUST define,
keras/,70,self.input_spec and self.state_spec with complete input shapes.,
keras/,88,Reverse time axis.,
keras/,1,-*- coding: utf-8 -*-,
keras/,1,-*- coding: utf-8 -*-,
keras/,121,Set input spec,
keras/,1,-*- coding: utf-8 -*-,
keras/,19,imports for backwards namespace compatibility,
keras/,150,Set input spec.,
keras/,804,Set input spec.,
keras/,824,Infer the dynamic output shape:,
keras/,1076,Set input spec.,
keras/,1099,Infer the dynamic output shape:,
keras/,1357,Set input spec.,
keras/,1839,Set input spec.,
keras/,1916,"self.rank is 1 for UpSampling1D, 2 for UpSampling2D.",
keras/,2096,"self.rank is 1 for ZeroPadding1D, 2 for ZeroPadding2D.",
keras/,2337,"self.rank is 1 for Cropping1D, 2 for Cropping2D...",
keras/,2571,Aliases,
keras/,2582,Legacy aliases,
keras/,1,-*- coding: utf-8 -*-,
keras/,157,Get affine transformation params,
keras/,161,Apply mask,
keras/,164,Do affine transformation,
keras/,1,-*- coding: utf-8 -*-,
keras/,134,Prepare broadcasting shape.,
keras/,141,Determines whether broadcasting is needed.,
keras/,146,In this case we must explicitly broadcast all parameters.,
keras/,178,If the learning phase is *static* and set to inference:,
keras/,182,"If the learning is either dynamic, or set to training:",
keras/,194,sample variance - unbiased estimator of population variance,
keras/,205,Pick the normalized form corresponding to the training phase.,
keras/,1,-*- coding: utf-8 -*-,
keras/,52,add dummy last dimension,
keras/,58,remove dummy last dimension,
keras/,779,Aliases,
keras/,66,Used purely for shape validation.,
keras/,92,"If the inputs have different ranks, we have to reshape them",
keras/,93,to make them broadcastable.,
keras/,107,"If ranks of all inputs are available,",
keras/,108,we simply expand each of them at axis=1,
keras/,109,until all of them have the same rank.,
keras/,118,Transpose all inputs so that batch size is the last dimension.,
keras/,119,"(batch_size, dim1, dim2, ... ) -> (dim1, dim2, ... , batch_size)",
keras/,139,We don't transpose inputs if they are,
keras/,140,1D vectors or scalars.,
keras/,145,"If inputs have been transposed,",
keras/,146,we have to transpose the output too.,
keras/,347,Used purely for shape validation.,
keras/,392,Make a list of masks while making sure,
keras/,393,the dimensionality of each mask,
keras/,394,is the same as the corresponding input.,
keras/,398,"Input is unmasked. Append all 1s to masks,",
keras/,401,"Mask is smaller than the input, expand it",
keras/,452,Used purely for shape validation.,
keras/,125,input_length can be tuple if input is 3D or higher,
keras/,1,-*- coding: utf-8 -*-,
keras/,33,Tracks mapping of Wrapper inputs to inner layer inputs. Useful when,
keras/,34,the inner layer has update ops that depend on its inputs (as opposed,
keras/,35,to the inputs to the Wrapper layer).,
keras/,72,"If the wrapper modifies the inputs, use the modified inputs to",
keras/,73,get the updates from the inner layer.,
keras/,184,replace all None in int_shape by K.shape,
keras/,219,"batch size matters, use rnn-based implementation",
keras/,234,"No batch size specified, therefore the layer will be able",
keras/,235,to process batches of any size.,
keras/,236,We can go with reshape-based implementation for performance.,
keras/,241,"Shape: (num_samples * timesteps, ...). And track the",
keras/,242,transformation in self._input_map.,
keras/,246,"(num_samples * timesteps, ...)",
keras/,253,"Shape: (num_samples, timesteps, ...)",
keras/,259,Apply activity regularizer if any:,
keras/,295,cases need to call the layer.compute_mask when input_mask is None:,
keras/,296,Masking layer and Embedding layer with mask_zero,
keras/,299,"batch size matters, we currently do not handle mask explicitly",
keras/,311,"input_mask is not None, and output_mask is None:",
keras/,312,we should return a not-None mask,
keras/,317,output_mask is not None. We need to reshape it,
keras/,323,"if the output_mask does not have a static shape,",
keras/,324,its shape must be the same as mask's,
keras/,385,This is isolated in its own method in order to use,
keras/,386,the disable_tracking decorator without altering the,
keras/,387,visible signature of __init__.,
keras/,439,Applies the same workaround as in `RNN.__call__`,
keras/,443,Check if `initial_state` can be splitted into half,
keras/,482,"Compute the full input spec, including state",
keras/,486,Perform the call with temporarily replaced input_spec,
keras/,560,Properly set learning phase,
keras/,58,Aliases (not in the docs),
keras/,78,Aliases (not in the docs),
keras/,128,Legacy imports,
keras/,162,All layers.,
keras/,1,-*- coding: utf-8 -*-,
keras/,22,Legacy support.,
keras/,59,reverse_state_order determines whether the state size will be in a,
keras/,60,reverse order of the cells' state. User might want to set this to True,
keras/,61,to keep the existing behavior. This is only useful when use,
keras/,62,`RNN(return_state=True)` since the state will be returned as the same,
keras/,63,order of state_size.,
keras/,75,States are a flat list of the individual cell state size.,
keras/,76,"e.g. states of a 2-layer LSTM would be `[h1, c1, h2, c2]`.",
keras/,77,"(assuming one LSTM has states [h, c])",
keras/,78,"In the case of reverse_state_order=True, the state_size will be",
keras/,79,"`[h2, c2, h1, c1]`.",
keras/,98,Recover per-cell states.,
keras/,110,Call the cells in order and store the returned states.,
keras/,121,Format the new states as a flat list,
keras/,122,in reverse cell order.,
keras/,427,This is isolated in its own method in order to use,
keras/,428,the disable_tracking decorator without altering the,
keras/,429,visible signature of __init__.,
keras/,482,Note input_shape will be list of shapes of initial states and,
keras/,483,constants if these are passed in __call__.,
keras/,496,allow cell (if layer) to build before we set or validate state_spec,
keras/,504,set or validate state_spec,
keras/,511,"initial_state was passed in call, check compatibility",
keras/,526,"build an all-zero tensor of shape (samples, output_dim)",
keras/,527,"(samples, timesteps, input_dim)",
keras/,528,"(samples,)",
keras/,529,"(samples, 1)",
keras/,543,If any of `initial_state` or `constants` are specified and are Keras,
keras/,544,"tensors, then add them to the inputs and temporarily modify the",
keras/,545,input_spec to include them.,
keras/,563,at this point additional_inputs cannot be empty,
keras/,574,"Compute the full input spec, including state and constants",
keras/,577,Perform the call with temporarily replaced input_spec,
keras/,600,"input shape: `(samples, time (padded with zeros), input_dim)`",
keras/,601,note that the .build() method of subclasses MUST define,
keras/,602,self.input_spec and self.state_spec with complete input shapes.,
keras/,607,get initial_state from full input spec,
keras/,608,as they could be copied to multiple GPU.,
keras/,694,Properly set learning phase,
keras/,721,initialize state if None,
keras/,754,TODO: consider batch calls to `set_value`.,
keras/,947,Properly set learning phase on output tensor.,
keras/,1327,separate biases for input and recurrent kernels,
keras/,1328,Note: the shape is intentionally different from CuDNNGRU biases,
keras/,1329,"`(2 * 3 * self.units,)`, so that we can distinguish the classes",
keras/,1330,when loading and converting saved weights.,
keras/,1340,"NOTE: need to flatten, since slicing in CNTK gives 2D array",
keras/,1346,update gate,
keras/,1349,reset gate,
keras/,1354,new gate,
keras/,1359,bias for inputs,
keras/,1363,bias for hidden state - just for compatibility with CuDNN,
keras/,1380,previous memory,
keras/,1396,dropout matrices for input units,
keras/,1398,dropout matrices for recurrent units,
keras/,1437,reset gate applied after/before matrix multiplication,
keras/,1451,inputs projected by all gate matrices at once,
keras/,1454,"biases: bias_z_i, bias_r_i, bias_h_i",
keras/,1464,hidden state projected by all gate matrices at once,
keras/,1469,hidden state projected separately for update/reset and new,
keras/,1487,previous and candidate state mixed by update gate,
keras/,1985,dropout matrices for input units,
keras/,1987,dropout matrices for recurrent units,
keras/,1990,previous memory state,
keras/,1991,previous carry state,
keras/,1,-*- coding: utf-8 -*-,
keras/,142,The StackedConvRNN2DCells isn't implemented yet.,
keras/,191,Note input_shape will be list of shapes of initial states and,
keras/,192,constants if these are passed in __call__.,
keras/,204,allow cell (if layer) to build before we set or validate state_spec,
keras/,212,set or validate state_spec,
keras/,219,"initial_state was passed in call, check compatibility",
keras/,243,"(samples, timesteps, rows, cols, filters)",
keras/,245,"(samples, rows, cols, filters)",
keras/,251,We need to force this to be a tensor,
keras/,252,"and not a variable, to avoid variable initialization",
keras/,253,issues.,
keras/,261,Fix for Theano because it needs,
keras/,262,K.int_shape to work in call() with initial_state.,
keras/,290,If any of `initial_state` or `constants` are specified and are Keras,
keras/,291,"tensors, then add them to the inputs and temporarily modify the",
keras/,292,input_spec to include them.,
keras/,303,Fix for Theano,
keras/,316,at this point additional_inputs cannot be empty,
keras/,324,"Compute the full input spec, including state and constants",
keras/,327,Perform the call with temporarily replaced input_spec,
keras/,342,note that the .build() method of subclasses MUST define,
keras/,343,self.input_spec and self.state_spec with complete input shapes.,
keras/,398,Properly set learning phase,
keras/,431,helper function,
keras/,442,initialize state if None,
keras/,475,TODO: consider batch calls to `set_value`.,
keras/,687,dropout matrices for input units,
keras/,689,dropout matrices for recurrent units,
keras/,692,previous memory state,
keras/,693,previous carry state,
keras/,1,-*- coding: utf-8 -*-,
keras/,393,input shape (partially) unknown? replace -1's with None's,
keras/,397,input shape known? then we can compute the output shape,
keras/,509,Ensure works for any dim,
keras/,669,"With TensorFlow or CNTK, we can infer the output shape directly:",
keras/,682,"Otherwise, we default to the input shape.",
keras/,757,Simple lookup in custom objects,
keras/,763,Unsafe deserialization from bytecode,
keras/,770,Simple lookup in custom objects,
keras/,776,Unsafe deserialization from bytecode,
keras/,781,"If arguments were numpy array, they have been saved as",
keras/,782,list. We need to recover the ndarray,
keras/,788,Overwrite the argument with its numpy translation,
keras/,131,It is the same as writing as frequently as possible.,
keras/,161,dense layer kernel case,
keras/,169,convnet case,
keras/,171,switch to channels_first to display,
keras/,172,every kernel as a separate image,
keras/,179,bias case,
keras/,185,not possible to handle 3D convnets etc.,
keras/,282,do not slice the learning phase,
keras/,296,We need a second forward-pass here because we're passing,
keras/,297,the `embeddings_data` explicitly. This design allows to pass,
keras/,298,arbitrary data as `embeddings_data` and results from the fact,
keras/,299,that we need to know the size of the `tf.Variable`s which,
keras/,300,"hold the embeddings in `set_model`. At this point, however,",
keras/,301,the `validation_data` is not yet set.,
keras/,303,More details in this discussion:,
keras/,304,https://github.com/keras-team/keras/pull/7766#issuecomment-329195622,
keras/,78,"Batch is ending, calculate batch time",
keras/,353,For backwards compatibility,
keras/,365,For backwards compatibility,
keras/,517,Make value available to next callbacks.,
keras/,600,Skip progbar update for the last batch;,
keras/,601,will be handled by on_epoch_end.,
keras/,807,Allow instances to be re-used,
keras/,929,new API,
keras/,931,old API for backward compatibility,
keras/,1006,Cooldown counter.,
keras/,1134,We set NA so that csv parsers do not fail for this last epoch.,
keras/,257,check if binary classification,
keras/,259,first column is probability of class 0 and second is of class 1,
keras/tests/test_loss_masking.py,43,Normally the trailing 1 is added by standardize_weights,
keras/tests/test_model_saving.py,62,cleanup,
keras/tests/test_model_saving.py,72,test that new updates are the same with both models,
keras/tests/test_model_saving.py,79,"test with custom optimizer, loss",
keras/tests/test_model_saving.py,104,cleanup,
keras/tests/test_model_saving.py,140,cleanup,
keras/tests/test_model_saving.py,158,test non-default options in h5,
keras/tests/test_model_saving.py,190,save directly to binary file,
keras/tests/test_model_saving.py,193,"Load the data the usual way, and make sure the model is intact.",
keras/tests/test_model_saving.py,205,save the model the usual way,
keras/tests/test_model_saving.py,208,"Load the data binary, and make sure the model is intact.",
keras/tests/test_model_saving.py,242,assure that model is working,
keras/tests/test_model_saving.py,300,"test with custom optimizer, loss",
keras/tests/test_model_saving.py,304,sequential model,
keras/tests/test_model_saving.py,321,delete and recreate model,
keras/tests/test_model_saving.py,328,load weights from first model,
keras/tests/test_model_saving.py,341,"only compare layers that have weights, skipping Flatten()",
keras/tests/test_model_saving.py,345,delete and recreate model with `use_bias=False`,
keras/tests/test_model_saving.py,361,delete and recreate model with `filters=10`,
keras/tests/test_model_saving.py,382,"test with custom optimizer, loss",
keras/tests/test_model_saving.py,386,sequential model,
keras/tests/test_model_saving.py,402,delete and recreate model using Functional API,
keras/tests/test_model_saving.py,406,add 2 layers (but maintain shapes),
keras/tests/test_model_saving.py,413,load weights from first model,
keras/tests/test_model_saving.py,429,biases init to 0,
keras/tests/test_model_saving.py,430,biases init to 0,
keras/tests/test_model_saving.py,439,"test with custom optimizer, loss",
keras/tests/test_model_saving.py,443,sequential model,
keras/tests/test_model_saving.py,459,delete and recreate model,
keras/tests/test_model_saving.py,463,different shape w.r.t. previous model,
keras/tests/test_model_saving.py,466,load weights from first model,
keras/tests/test_model_saving.py,467,expect UserWarning for skipping weights,
keras/tests/test_model_saving.py,471,assert layers 'rick' are equal,
keras/tests/test_model_saving.py,475,"assert layers 'morty' are not equal, since we skipped loading this layer",
keras/tests/test_model_saving.py,480,a function to be called from the Lambda layer,
keras/tests/test_model_saving.py,552,This layer name will make the `layers_name` HDF5 attribute blow,
keras/tests/test_model_saving.py,553,out of proportion. Note that it fits into the internal HDF5,
keras/tests/test_model_saving.py,554,attribute memory limit on its own but because h5py converts,
keras/tests/test_model_saving.py,555,"the list of layer names into numpy array, which uses the same",
keras/tests/test_model_saving.py,556,"amout of memory for every item, it increases the memory",
keras/tests/test_model_saving.py,557,requirements substantially.,
keras/tests/test_model_saving.py,578,Check that the HDF5 files contains chunked array,
keras/tests/test_model_saving.py,579,of layer names.,
keras/tests/test_model_saving.py,586,The chunking of layer names array should have happened.,
keras/tests/test_model_saving.py,599,This layer name will make the `weights_name`,
keras/tests/test_model_saving.py,600,HDF5 attribute blow out of proportion.,
keras/tests/test_model_saving.py,623,Check that the HDF5 files contains chunked array,
keras/tests/test_model_saving.py,624,of weight names.,
keras/tests/test_model_saving.py,632,The chunking of layer names array should have happened.,
keras/tests/test_model_saving.py,734,we should not use same filename in several tests to allow for parallel,
keras/tests/test_model_saving.py,735,execution,
keras/tests/test_model_saving.py,744,cleanup,
keras/tests/test_model_saving.py,784,we should not use same filename in several tests to allow for parallel,
keras/tests/test_model_saving.py,785,execution,
keras/tests/test_model_saving.py,804,cleanup,
keras/tests/test_model_saving.py,832,ensure biases are non-zero and properly converted,
keras/tests/test_model_saving.py,865,"example: make_nested_seq_model((1,), Dense(10), level=2).summary()",
keras/tests/test_model_saving.py,873,"example: make_nested_func_model((1,), Dense(10), level=2).summary()",
keras/tests/test_model_saving.py,912,ensure biases are non-zero and properly converted,
keras/tests/test_model_saving.py,950,A model is needed to initialize weights.,
keras/tests/test_model_saving.py,979,for theano backend,
keras/tests/test_model_saving.py,983,for theano backend,
keras/tests/test_model_pickling.py,40,test that new updates are the same with both models,
keras/tests/test_model_pickling.py,51,"test with custom optimizer, loss",
keras/tests/test_model_pickling.py,116,assure that model is working,
keras/tests/test_loss_weighting.py,42,convert class vectors to binary class matrices,
keras/tests/test_multiprocessing.py,120,Build a NN,
keras/tests/test_multiprocessing.py,125,"- Produce data on 4 worker processes, consume on main process:",
keras/tests/test_multiprocessing.py,126,- Each worker process runs OWN copy of generator,
keras/tests/test_multiprocessing.py,127,"- BUT on Windows, `multiprocessing` won't marshall generators across",
keras/tests/test_multiprocessing.py,128,process boundaries -> make sure `fit_generator()` raises ValueError,
keras/tests/test_multiprocessing.py,129,exception and does not attempt to run the generator.,
keras/tests/test_multiprocessing.py,150,"- Produce data on 4 worker threads, consume on main thread:",
keras/tests/test_multiprocessing.py,151,- All worker threads share the SAME generator,
keras/tests/test_multiprocessing.py,161,"- Produce data on 1 worker process, consume on main process:",
keras/tests/test_multiprocessing.py,162,- Worker process runs generator,
keras/tests/test_multiprocessing.py,163,"- BUT on Windows, `multiprocessing` won't marshall generators across",
keras/tests/test_multiprocessing.py,164,process boundaries -> make sure `fit_generator()` raises ValueError,
keras/tests/test_multiprocessing.py,165,exception and does not attempt to run the generator.,
keras/tests/test_multiprocessing.py,188,"- Produce data on 1 worker thread, consume on main thread:",
keras/tests/test_multiprocessing.py,189,- Worker thread is the only thread running the generator,
keras/tests/test_multiprocessing.py,200,"- Produce data on 1 worker process, consume on main process:",
keras/tests/test_multiprocessing.py,201,- Worker process runs generator,
keras/tests/test_multiprocessing.py,202,"- BUT on Windows, `multiprocessing` won't marshall generators across",
keras/tests/test_multiprocessing.py,203,process boundaries -> make sure `fit_generator()` raises ValueError,
keras/tests/test_multiprocessing.py,204,exception and does not attempt to run the generator.,
keras/tests/test_multiprocessing.py,223,"- Produce data on 1 worker thread AT A TIME, consume on main thread:",
keras/tests/test_multiprocessing.py,224,- Worker threads for training and validation run generator SEQUENTIALLY,
keras/tests/test_multiprocessing.py,233,- Produce and consume data without a queue on main thread,
keras/tests/test_multiprocessing.py,234,- Make sure the value of `use_multiprocessing` is ignored,
keras/tests/test_multiprocessing.py,250,Test invalid use cases,
keras/tests/test_multiprocessing.py,256,not specified `validation_steps`,
keras/tests/test_multiprocessing.py,266,validation data is neither a tuple nor a triple.,
keras/tests/test_multiprocessing.py,279,validation generator is neither a tuple nor a triple.,
keras/tests/test_multiprocessing.py,289,- For Sequence,
keras/tests/test_multiprocessing.py,327,Build a NN,
keras/tests/test_multiprocessing.py,332,"- Produce data on 4 worker processes, consume on main process:",
keras/tests/test_multiprocessing.py,333,- Each worker process runs OWN copy of generator,
keras/tests/test_multiprocessing.py,334,"- BUT on Windows, `multiprocessing` won't marshall generators across",
keras/tests/test_multiprocessing.py,335,process boundaries -> make sure `fit_generator()` raises ValueError,
keras/tests/test_multiprocessing.py,336,exception and does not attempt to run the generator.,
keras/tests/test_multiprocessing.py,357,"- Produce data on 1 worker process, consume on main process:",
keras/tests/test_multiprocessing.py,358,- Worker process runs generator,
keras/tests/test_multiprocessing.py,359,"- BUT on Windows, `multiprocessing` won't marshall generators across",
keras/tests/test_multiprocessing.py,360,process boundaries -> make sure `fit_generator()` raises ValueError,
keras/tests/test_multiprocessing.py,361,exception and does not attempt to run the generator.,
keras/tests/test_multiprocessing.py,382,"- Produce data on 1 worker thread, consume on main thread:",
keras/tests/test_multiprocessing.py,383,- Worker thread is the only thread running the generator,
keras/tests/test_multiprocessing.py,385,- Produce and consume data without a queue on main thread,
keras/tests/test_multiprocessing.py,386,- Make sure the value of `use_multiprocessing` is ignored,
keras/tests/test_multiprocessing.py,418,Build a NN,
keras/tests/test_multiprocessing.py,423,"- Produce data on 4 worker threads, consume on main thread:",
keras/tests/test_multiprocessing.py,424,- All worker threads share the SAME generator,
keras/tests/test_multiprocessing.py,434,"- Produce data on 1 worker thread, consume on main thread:",
keras/tests/test_multiprocessing.py,435,- Worker thread is the only thread running the generator,
keras/tests/test_multiprocessing.py,445,- Produce and consume data without a queue on main thread,
keras/tests/test_multiprocessing.py,446,- Make sure the value of `use_multiprocessing` is ignored,
keras/tests/test_multiprocessing.py,475,Build a NN,
keras/tests/test_multiprocessing.py,480,"- Produce data on 4 worker processes, consume on main process:",
keras/tests/test_multiprocessing.py,481,- Each worker process runs OWN copy of generator,
keras/tests/test_multiprocessing.py,482,"- BUT on Windows, `multiprocessing` won't marshall generators across",
keras/tests/test_multiprocessing.py,483,process boundaries -> make sure `predict_generator()` raises ValueError,
keras/tests/test_multiprocessing.py,484,exception and does not attempt to run the generator.,
keras/tests/test_multiprocessing.py,499,"- Produce data on 1 worker process, consume on main process:",
keras/tests/test_multiprocessing.py,500,- Worker process runs generator,
keras/tests/test_multiprocessing.py,501,"- BUT on Windows, `multiprocessing` won't marshall generators across",
keras/tests/test_multiprocessing.py,502,process boundaries -> make sure `predict_generator()` raises ValueError,
keras/tests/test_multiprocessing.py,503,exception and does not attempt to run the generator.,
keras/tests/test_multiprocessing.py,518,- Main thread runs the generator without a queue,
keras/tests/test_multiprocessing.py,519,- Make sure the value of `use_multiprocessing` is ignored,
keras/tests/test_multiprocessing.py,542,Build a NN,
keras/tests/test_multiprocessing.py,547,"- Produce data on 4 worker threads, consume on main thread:",
keras/tests/test_multiprocessing.py,548,- All worker threads share the SAME generator,
keras/tests/test_multiprocessing.py,555,"- Produce data on 1 worker thread, consume on main thread:",
keras/tests/test_multiprocessing.py,556,- Worker thread is the only thread running the generator,
keras/tests/test_multiprocessing.py,563,- Main thread runs the generator without a queue,
keras/tests/test_multiprocessing.py,564,- Make sure the value of `use_multiprocessing` is ignored,
keras/tests/test_multiprocessing.py,590,Build a NN,
keras/tests/test_multiprocessing.py,595,"- Produce data on 4 worker processes, consume on main process:",
keras/tests/test_multiprocessing.py,596,- Each worker process runs OWN copy of generator,
keras/tests/test_multiprocessing.py,597,"- BUT on Windows, `multiprocessing` won't marshall generators across",
keras/tests/test_multiprocessing.py,598,process boundaries,
keras/tests/test_multiprocessing.py,599,-> make sure `evaluate_generator()` raises raises ValueError,
keras/tests/test_multiprocessing.py,600,exception and does not attempt to run the generator.,
keras/tests/test_multiprocessing.py,615,"- Produce data on 1 worker process, consume on main process:",
keras/tests/test_multiprocessing.py,616,- Worker process runs generator,
keras/tests/test_multiprocessing.py,617,"- BUT on Windows, `multiprocessing` won't marshall generators across",
keras/tests/test_multiprocessing.py,618,process boundaries -> make sure `evaluate_generator()` raises ValueError,
keras/tests/test_multiprocessing.py,619,exception and does not attempt to run the generator.,
keras/tests/test_multiprocessing.py,634,- Produce and consume data without a queue on main thread,
keras/tests/test_multiprocessing.py,635,- Make sure the value of `use_multiprocessing` is ignored,
keras/tests/test_multiprocessing.py,660,Build a NN,
keras/tests/test_multiprocessing.py,665,"- Produce data on 4 worker threads, consume on main thread:",
keras/tests/test_multiprocessing.py,666,- All worker threads share the SAME generator,
keras/tests/test_multiprocessing.py,673,"- Produce data on 1 worker thread, consume on main thread:",
keras/tests/test_multiprocessing.py,674,- Worker thread is the only thread running the generator,
keras/tests/test_multiprocessing.py,681,- Produce and consume data without a queue on main thread,
keras/tests/test_multiprocessing.py,682,- Make sure the value of `use_multiprocessing` is ignored,
keras/tests/test_multiprocessing.py,716,"- Produce data on 4 worker processes, consume on main process:",
keras/tests/test_multiprocessing.py,717,- Each worker process runs OWN copy of generator,
keras/tests/test_multiprocessing.py,718,"- BUT on Windows, `multiprocessing` won't marshall generators across",
keras/tests/test_multiprocessing.py,719,process boundaries -> make sure `fit_generator()` raises ValueError,
keras/tests/test_multiprocessing.py,720,exception and does not attempt to run the generator.,
keras/tests/test_multiprocessing.py,721,"- On other platforms, make sure `RuntimeError` exception bubbles up",
keras/tests/test_multiprocessing.py,739,"- Produce data on 1 worker process, consume on main process:",
keras/tests/test_multiprocessing.py,740,- Worker process runs generator,
keras/tests/test_multiprocessing.py,741,"- BUT on Windows, `multiprocessing` won't marshall generators across",
keras/tests/test_multiprocessing.py,742,process boundaries -> make sure `fit_generator()` raises ValueError,
keras/tests/test_multiprocessing.py,743,exception and does not attempt to run the generator.,
keras/tests/test_multiprocessing.py,744,"- On other platforms, make sure `RuntimeError` exception bubbles up",
keras/tests/test_multiprocessing.py,762,- Make sure the value of `use_multiprocessing` is ignored,
keras/tests/test_multiprocessing.py,763,- Make sure `RuntimeError` exception bubbles up,
keras/tests/test_multiprocessing.py,798,"- Produce data on 4 worker threads, consume on main thread:",
keras/tests/test_multiprocessing.py,799,- All worker threads share the SAME generator,
keras/tests/test_multiprocessing.py,800,- Make sure `RuntimeError` exception bubbles up,
keras/tests/test_multiprocessing.py,809,"- Produce data on 1 worker thread, consume on main thread:",
keras/tests/test_multiprocessing.py,810,- Worker thread is the only thread running the generator,
keras/tests/test_multiprocessing.py,811,- Make sure `RuntimeError` exception bubbles up,
keras/tests/test_multiprocessing.py,820,- Produce and consume data without a queue on main thread,
keras/tests/test_multiprocessing.py,821,- Make sure the value of `use_multiprocessing` is ignored,
keras/tests/test_multiprocessing.py,822,- Make sure `RuntimeError` exception bubbles up,
keras/tests/test_multiprocessing.py,856,"- Produce data on 4 worker processes, consume on main process:",
keras/tests/test_multiprocessing.py,857,- Each worker process runs OWN copy of generator,
keras/tests/test_multiprocessing.py,858,"- BUT on Windows, `multiprocessing` won't marshall generators across",
keras/tests/test_multiprocessing.py,859,process boundaries -> make sure `evaluate_generator()` raises ValueError,
keras/tests/test_multiprocessing.py,860,exception and does not attempt to run the generator.,
keras/tests/test_multiprocessing.py,861,"- On other platforms, make sure `RuntimeError` exception bubbles up",
keras/tests/test_multiprocessing.py,877,"- Produce data on 1 worker process, consume on main process:",
keras/tests/test_multiprocessing.py,878,- Worker process runs generator,
keras/tests/test_multiprocessing.py,879,"- BUT on Windows, `multiprocessing` won't marshall generators across",
keras/tests/test_multiprocessing.py,880,process boundaries -> make sure `evaluate_generator()` raises ValueError,
keras/tests/test_multiprocessing.py,881,exception and does not attempt to run the generator.,
keras/tests/test_multiprocessing.py,882,"- On other platforms, make sure `RuntimeError` exception bubbles up",
keras/tests/test_multiprocessing.py,898,- Produce and consume data without a queue on main thread,
keras/tests/test_multiprocessing.py,899,- Make sure the value of `use_multiprocessing` is ignored,
keras/tests/test_multiprocessing.py,900,- Make sure `RuntimeError` exception bubbles up,
keras/tests/test_multiprocessing.py,932,"- Produce data on 4 worker threads, consume on main thread:",
keras/tests/test_multiprocessing.py,933,- All worker threads share the SAME generator,
keras/tests/test_multiprocessing.py,934,- Make sure `RuntimeError` exception bubbles up,
keras/tests/test_multiprocessing.py,942,"- Produce data on 1 worker thread, consume on main thread:",
keras/tests/test_multiprocessing.py,943,- Worker thread is the only thread running the generator,
keras/tests/test_multiprocessing.py,944,- Make sure `RuntimeError` exception bubbles up,
keras/tests/test_multiprocessing.py,952,- Produce and consume data without a queue on main thread,
keras/tests/test_multiprocessing.py,953,- Make sure the value of `use_multiprocessing` is ignored,
keras/tests/test_multiprocessing.py,954,- Make sure `RuntimeError` exception bubbles up,
keras/tests/test_multiprocessing.py,986,"- Produce data on 4 worker processes, consume on main process:",
keras/tests/test_multiprocessing.py,987,- Each worker process runs OWN copy of generator,
keras/tests/test_multiprocessing.py,988,"- BUT on Windows, `multiprocessing` won't marshall generators across",
keras/tests/test_multiprocessing.py,989,process boundaries -> make sure `predict_generator()` raises ValueError,
keras/tests/test_multiprocessing.py,990,exception and does not attempt to run the generator.,
keras/tests/test_multiprocessing.py,991,"- On other platforms, make sure `RuntimeError` exception bubbles up",
keras/tests/test_multiprocessing.py,1007,"- Produce data on 1 worker process, consume on main process:",
keras/tests/test_multiprocessing.py,1008,- Worker process runs generator,
keras/tests/test_multiprocessing.py,1009,"- BUT on Windows, `multiprocessing` won't marshall generators across",
keras/tests/test_multiprocessing.py,1010,process boundaries -> make sure `predict_generator()` raises ValueError,
keras/tests/test_multiprocessing.py,1011,exception and does not attempt to run the generator.,
keras/tests/test_multiprocessing.py,1012,"- On other platforms, make sure `RuntimeError` exception bubbles up",
keras/tests/test_multiprocessing.py,1028,- Produce and consume data without a queue on main thread,
keras/tests/test_multiprocessing.py,1029,- Make sure the value of `use_multiprocessing` is ignored,
keras/tests/test_multiprocessing.py,1030,- Make sure `RuntimeError` exception bubbles up,
keras/tests/test_multiprocessing.py,1061,"- Produce data on 4 worker threads, consume on main thread:",
keras/tests/test_multiprocessing.py,1062,- All worker threads share the SAME generator,
keras/tests/test_multiprocessing.py,1063,- Make sure `RuntimeError` exception bubbles up,
keras/tests/test_multiprocessing.py,1070,"- Produce data on 1 worker thread, consume on main thread:",
keras/tests/test_multiprocessing.py,1071,- Worker thread is the only thread running the generator,
keras/tests/test_multiprocessing.py,1072,- Make sure `RuntimeError` exception bubbles up,
keras/tests/test_multiprocessing.py,1080,- Produce and consume data without a queue on main thread,
keras/tests/test_multiprocessing.py,1081,- Make sure the value of `use_multiprocessing` is ignored,
keras/tests/test_multiprocessing.py,1082,- Make sure `RuntimeError` exception bubbles up,
keras/tests/test_dynamic_trainability.py,10,"with constructor argument, in Sequential",
keras/tests/test_dynamic_trainability.py,15,"by setting the `trainable` argument, in Sequential",
keras/tests/test_dynamic_trainability.py,23,"with constructor argument, in Model",
keras/tests/test_dynamic_trainability.py,29,"by setting the `trainable` argument, in Model",
keras/tests/test_dynamic_trainability.py,40,a non-trainable model has no trainable weights,
keras/tests/test_dynamic_trainability.py,47,same for Sequential,
keras/tests/test_dynamic_trainability.py,55,a Sequential inside a Model,
keras/tests/test_dynamic_trainability.py,69,a Sequential inside a Sequential,
keras/tests/test_dynamic_trainability.py,81,a Model inside a Model,
keras/tests/test_dynamic_trainability.py,95,a Model inside a Sequential,
keras/tests/,49,Test constraints.,
keras/tests/,67,Test saving.,
keras/tests/,36,1. Default returns linear,
keras/tests/,40,2. Passing in a layer raises a warning,
keras/tests/,45,3. Callables return themselves for some reason,
keras/tests/,49,4. Anything else is not a valid argument,
keras/tests/,77,One dimensional arrays are supposed to raise a value error,
keras/tests/,183,Test max_value,
keras/tests/,189,Test max_value == 6.,
keras/tests/,52,use one_hot embedding to convert sparse labels to equivalent dense labels,
keras/tests/,102,"Test correctness if the shape of y_true is (num_samples, 1)",
keras/tests/,104,"Test correctness if the shape of y_true is (num_samples,)",
keras/tests/,10,Need TensorFlow to use metric.__call__,
keras/tests/,24,Check save and restore config,
keras/tests/,85,Check save and restore config,
keras/tests/,143,Check save and restore config,
keras/tests/,200,Check save and restore config,
keras/tests/,259,Check save and restore config,
keras/tests/,325,Check save and restore config,
keras/tests/,389,"threshold values are [0 - 1e-7, 0.5, 1 + 1e-7]",
keras/tests/,390,"y_pred when threshold = 0 - 1e-7  : [1, 1, 1, 1]",
keras/tests/,391,"y_pred when threshold = 0.5       : [0, 0, 0, 1]",
keras/tests/,392,"y_pred when threshold = 1 + 1e-7  : [0, 0, 0, 0]",
keras/tests/,394,without sample_weight:,
keras/tests/,395,"tp = np.sum([[0, 0, 1, 1], [0, 0, 0, 1], [0, 0, 0, 0]], axis=1)",
keras/tests/,396,"fp = np.sum([[1, 1, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], axis=1)",
keras/tests/,397,"fn = np.sum([[0, 0, 0, 0], [0, 0, 1, 0], [0, 0, 1, 1]], axis=1)",
keras/tests/,398,"tn = np.sum([[0, 0, 0, 0], [1, 1, 0, 0], [1, 1, 0, 0]], axis=1)",
keras/tests/,400,"tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2]",
keras/tests/,402,with sample_weight:,
keras/tests/,403,"tp = np.sum([[0, 0, 3, 4], [0, 0, 0, 4], [0, 0, 0, 0]], axis=1)",
keras/tests/,404,"fp = np.sum([[1, 2, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], axis=1)",
keras/tests/,405,"fn = np.sum([[0, 0, 0, 0], [0, 0, 3, 0], [0, 0, 3, 4]], axis=1)",
keras/tests/,406,"tn = np.sum([[0, 0, 0, 0], [1, 2, 0, 0], [1, 2, 0, 0]], axis=1)",
keras/tests/,408,"tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]",
keras/tests/,422,Check save and restore config.,
keras/tests/,444,Check save and restore config.,
keras/tests/,463,"tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2]",
keras/tests/,464,"recall = [2/2, 1/(1+1), 0] = [1, 0.5, 0]",
keras/tests/,465,"fp_rate = [2/2, 0, 0] = [1, 0, 0]",
keras/tests/,466,"heights = [(1 + 0.5)/2, (0.5 + 0)/2] = [0.75, 0.25]",
keras/tests/,467,"widths = [(1 - 0), (0 - 0)] = [1, 0]",
keras/tests/,473,"Verify that when specified, thresholds are used instead of num_thresholds.",
keras/tests/,479,"tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2]",
keras/tests/,480,"recall = [2/2, 1/(1+1), 0] = [1, 0.5, 0]",
keras/tests/,481,"fp_rate = [2/2, 0, 0] = [1, 0, 0]",
keras/tests/,482,"heights = [(1 + 0.5)/2, (0.5 + 0)/2] = [0.75, 0.25]",
keras/tests/,483,"widths = [(1 - 0), (0 - 0)] = [1, 0]",
keras/tests/,492,"tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]",
keras/tests/,493,"recall = [7/7, 4/(4+3), 0] = [1, 0.571, 0]",
keras/tests/,494,"fp_rate = [3/3, 0, 0] = [1, 0, 0]",
keras/tests/,495,"heights = [(1 + 0.571)/2, (0.571 + 0)/2] = [0.7855, 0.2855]",
keras/tests/,496,"widths = [(1 - 0), (0 - 0)] = [1, 0]",
keras/tests/,506,"tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]",
keras/tests/,507,"recall = [7/7, 4/(4+3), 0] = [1, 0.571, 0]",
keras/tests/,508,"fp_rate = [3/3, 0, 0] = [1, 0, 0]",
keras/tests/,509,"heights = [max(1, 0.571), max(0.571, 0)] = [1, 0.571]",
keras/tests/,510,"widths = [(1 - 0), (0 - 0)] = [1, 0]",
keras/tests/,520,"tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]",
keras/tests/,521,"recall = [7/7, 4/(4+3), 0] = [1, 0.571, 0]",
keras/tests/,522,"fp_rate = [3/3, 0, 0] = [1, 0, 0]",
keras/tests/,523,"heights = [min(1, 0.571), min(0.571, 0)] = [0.571, 0]",
keras/tests/,524,"widths = [(1 - 0), (0 - 0)] = [1, 0]",
keras/tests/,536,"tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]",
keras/tests/,537,"precision = [7/(7+3), 4/4, 0] = [0.7, 1, 0]",
keras/tests/,538,"recall = [7/7, 4/(4+3), 0] = [1, 0.571, 0]",
keras/tests/,539,"heights = [max(0.7, 1), max(1, 0)] = [1, 1]",
keras/tests/,540,"widths = [(1 - 0.571), (0.571 - 0)] = [0.429, 0.571]",
keras/tests/,552,"tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]",
keras/tests/,553,"precision = [7/(7+3), 4/4, 0] = [0.7, 1, 0]",
keras/tests/,554,"recall = [7/7, 4/(4+3), 0] = [1, 0.571, 0]",
keras/tests/,555,"heights = [min(0.7, 1), min(1, 0)] = [0.7, 0]",
keras/tests/,556,"widths = [(1 - 0.571), (0.571 - 0)] = [0.429, 0.571]",
keras/tests/,565,auc = (slope / Total Pos) * [dTP - intercept * log(Pb/Pa)],
keras/tests/,567,"tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]",
keras/tests/,568,"P = tp + fp = [10, 4, 0]",
keras/tests/,569,"dTP = [7-4, 4-0] = [3, 4]",
keras/tests/,570,"dP = [10-4, 4-0] = [6, 4]",
keras/tests/,571,"slope = dTP/dP = [0.5, 1]",
keras/tests/,572,"intercept = (TPa+(slope*Pa) = [(4 - 0.5*4), (0 - 1*0)] = [2, 0]",
keras/tests/,573,"(Pb/Pa) = (Pb/Pa) if Pb > 0 AND Pa > 0 else 1 = [10/4, 4/0] = [2.5, 1]",
keras/tests/,574,"auc * TotalPos = [(0.5 * (3 + 2 * log(2.5))), (1 * (4 + 0))]",
keras/tests/,575,"= [2.416, 4]",
keras/tests/,576,"auc = [2.416, 4]/(tp[1:]+fn[1:])",
keras/tests/,577,expected_result = (2.416 / 7 + 4 / 7),
keras/tests/,610,Check save and restore config,
keras/tests/,759,Check save and restore config,
keras/tests/,915,"cm = [[1, 1],",
keras/tests/,916,"[1, 1]]",
keras/tests/,917,"sum_row = [2, 2], sum_col = [2, 2], true_positives = [1, 1]",
keras/tests/,918,iou = true_positives / (sum_row + sum_col - true_positives)),
keras/tests/,930,"cm = [[0.2, 0.3],",
keras/tests/,931,"[0.4, 0.1]]",
keras/tests/,932,"sum_row = [0.6, 0.4], sum_col = [0.5, 0.5], true_positives = [0.2, 0.1]",
keras/tests/,933,iou = true_positives / (sum_row + sum_col - true_positives)),
keras/tests/,945,"cm = [[0.2, 0.3],",
keras/tests/,946,"[0.4, 0.1]]",
keras/tests/,947,"sum_row = [0.6, 0.4], sum_col = [0.5, 0.5], true_positives = [0.2, 0.1]",
keras/tests/,948,iou = true_positives / (sum_row + sum_col - true_positives)),
keras/tests/,963,"cm = [[0, 0],",
keras/tests/,964,"[0, 1]]",
keras/tests/,965,"sum_row = [0, 1], sum_col = [0, 1], true_positives = [0, 1]",
keras/tests/,966,iou = true_positives / (sum_row + sum_col - true_positives)),
keras/tests/,11,Need TensorFlow to use metric.__call__,
keras/tests/,20,check config,
keras/tests/,26,check initial state,
keras/tests/,29,check __call__,
keras/tests/,33,check update_state() and result() + state accumulation + tensor input,
keras/tests/,36,100 + 1 + 5,
keras/tests/,38,check reset_states(),
keras/tests/,46,check scalar weight,
keras/tests/,51,check weights not scalar and weights rank matches values rank,
keras/tests/,54,50 + 1 + 5 * 0.2,
keras/tests/,57,check weights broadcast,
keras/tests/,59,52 + 0.5 + 1,
keras/tests/,62,check weights squeeze,
keras/tests/,64,53.5 + 1 + 1,
keras/tests/,67,check weights expand,
keras/tests/,69,55.5 + 1 + 1,
keras/tests/,72,check values reduced to the dimensions of weight,
keras/tests/,75,result = (prev: 57.5) + 0.5 + 1 + 1.5 + 1 + 0.25 + 2,
keras/tests/,85,check config,
keras/tests/,91,check initial state,
keras/tests/,95,check __call__(),
keras/tests/,100,check update_state() and result(),
keras/tests/,103,100 + 1 + 5,
keras/tests/,106,check reset_states(),
keras/tests/,111,Check save and restore config,
keras/tests/,122,check scalar weight,
keras/tests/,128,check weights not scalar and weights rank matches values rank,
keras/tests/,132,50 + 1 + 5 * 0.2,
keras/tests/,133,0.5 + 1.2,
keras/tests/,135,check weights broadcast,
keras/tests/,138,52 + 0.5 + 1,
keras/tests/,139,1.7 + 0.5 + 0.5,
keras/tests/,141,check weights squeeze,
keras/tests/,144,53.5 + 1 + 1,
keras/tests/,145,2.7 + 1.2,
keras/tests/,147,check weights expand,
keras/tests/,150,55.5 + 1 + 1,
keras/tests/,151,3.9 + 1.2,
keras/tests/,160,check initial state,
keras/tests/,166,check __call__(),
keras/tests/,186,check config,
keras/tests/,192,verify that correct value is returned,
keras/tests/,194,2/2,
keras/tests/,196,Check save and restore config,
keras/tests/,203,check with sample_weight,
keras/tests/,211,check config,
keras/tests/,217,verify that correct value is returned,
keras/tests/,220,2/2,
keras/tests/,222,check y_pred squeeze,
keras/tests/,227,check y_true squeeze,
keras/tests/,232,check with sample_weight,
keras/tests/,246,check config,
keras/tests/,252,verify that correct value is returned,
keras/tests/,256,2/2,
keras/tests/,258,check with sample_weight,
keras/tests/,263,2.5/2.7,
keras/tests/,268,check config,
keras/tests/,274,verify that correct value is returned,
keras/tests/,279,2/2,
keras/tests/,281,check with sample_weight,
keras/tests/,291,check config,
keras/tests/,297,verify that correct value is returned,
keras/tests/,300,2/2,
keras/tests/,302,check with sample_weight,
keras/tests/,316,Check save and restore config,
keras/tests/,349,Check save and restore config,
keras/tests/,381,Check save and restore config,
keras/tests/,415,Check save and restore config,
keras/tests/,459,both the samples match,
keras/tests/,461,With `k` < 5.,
keras/tests/,464,only sample #2 matches,
keras/tests/,466,With `k` > 5.,
keras/tests/,472,only 1 sample matches.,
keras/tests/,502,both the samples match,
keras/tests/,504,With `k` < 5.,
keras/tests/,507,only sample #2 matches,
keras/tests/,509,With `k` > 5.,
keras/tests/,514,only 1 sample matches.,
keras/tests/,660,Check save and restore config,
keras/tests/,699,Check save and restore config,
keras/tests/,733,Check save and restore config,
keras/tests/,767,Check save and restore config,
keras/tests/,801,Check save and restore config,
keras/tests/,847,"error = [-1, -1, -4], square(error) = [1, 1, 16], mean = 18/3 = 6",
keras/tests/,16,0 could possibly cause trouble,
keras/tests/,39,a more explicit example,
keras/tests/,61,"In the unit norm constraint, it should be equal to 1.",
keras/tests/,110,TODO: factor out,
keras/tests/,169,Test serialization,
keras/tests/,173,Model should be built.,
keras/tests/,236,Test serialization,
keras/tests/,318,Everything should work in a new session.,
keras/tests/,321,With placeholder creation,
keras/tests/,326,On top of new tensors,
keras/tests/,334,"On top of new, non-Keras tensors",
keras/tests/,346,Layer with single input and multiple outputs,
keras/tests/,358,Layer with multiple inputs and outputs,
keras/tests/,381,Everything should work in a new session.,
keras/tests/,384,With placeholder creation,
keras/tests/,389,On top of new tensor,
keras/tests/,396,"On top of new, non-Keras tensor",
keras/tests/,448,Test serialization,
keras/tests/,7,2D tensor test fixture,
keras/tests/,10,4D convolution in th order. This shape has the same effective shape as FC_SHAPE,
keras/tests/,157,Test that calling a same seeded random initializer,
keras/tests/,158,in succession results in different values.,
keras/tests/,32,"losses.SparseCategoricalCrossentropy,",
keras/tests/,155,"mse = [((4 - 1)^2 + (8 - 9)^2) / 2, ((12 - 2)^2 + (3 - 5)^2) / 2]",
keras/tests/,156,"mse = [5, 52]",
keras/tests/,157,"weighted_mse = [5 * 1.2, 52 * 0.5] = [6, 26]",
keras/tests/,158,reduced_weighted_mse = (6 + 26) / 2 =,
keras/tests/,465,Test with logits.,
keras/tests/,479,"EPSILON = 1e-7, y = y_true, y` = y_pred, Y_MAX = 0.9999999",
keras/tests/,480,"y` = clip(output, EPSILON, 1. - EPSILON)",
keras/tests/,481,"y` = [Y_MAX, Y_MAX, Y_MAX, EPSILON]",
keras/tests/,483,Loss = -(y log(y` + EPSILON) + (1 - y) log(1 - y` + EPSILON)),
keras/tests/,484,"= [-log(Y_MAX + EPSILON), -log(1 - Y_MAX + EPSILON),",
keras/tests/,485,"-log(Y_MAX + EPSILON), -log(1)]",
keras/tests/,486,"= [0, 15.33, 0, 0]",
keras/tests/,487,Reduced loss = 15.33 / 4,
keras/tests/,491,Test with logits.,
keras/tests/,497,"Loss = max(x, 0) - x * z + log(1 + exp(-abs(x)))",
keras/tests/,498,(where x = logits and z = y_true),
keras/tests/,499,= [((100 - 100 * 1 + log(1 + exp(-100))) +,
keras/tests/,500,(0 + 100 * 0 + log(1 + exp(-100))) +,
keras/tests/,501,"(100 - 100 * 1 + log(1 + exp(-100))),",
keras/tests/,502,((100 - 100 * 0 + log(1 + exp(-100))) +,
keras/tests/,503,(100 - 100 * 1 + log(1 + exp(-100))) +,
keras/tests/,504,(0 + 100 * 1 + log(1 + exp(-100))))],
keras/tests/,505,"= [(0 + 0 + 0) / 3, 200 / 3]",
keras/tests/,506,Reduced loss = (0 + 66.666) / 2,
keras/tests/,516,"EPSILON = 1e-7, y = y_true, y` = y_pred, Y_MAX = 0.9999999",
keras/tests/,517,"y` = clip(output, EPSILON, 1. - EPSILON)",
keras/tests/,518,"y` = [Y_MAX, Y_MAX, Y_MAX, EPSILON]",
keras/tests/,520,Loss = -(y log(y` + EPSILON) + (1 - y) log(1 - y` + EPSILON)),
keras/tests/,521,"= [-log(Y_MAX + EPSILON), -log(1 - Y_MAX + EPSILON),",
keras/tests/,522,"-log(Y_MAX + EPSILON), -log(1)]",
keras/tests/,523,"= [0, 15.33, 0, 0]",
keras/tests/,524,"Weighted loss = [0, 15.33 * 2.3, 0, 0]",
keras/tests/,525,Reduced loss = 15.33 * 2.3 / 4,
keras/tests/,529,Test with logits.,
keras/tests/,535,"Loss = max(x, 0) - x * z + log(1 + exp(-abs(x)))",
keras/tests/,536,(where x = logits and z = y_true),
keras/tests/,537,"Loss = [(0 + 0 + 0) / 3, 200 / 3]",
keras/tests/,538,"Weighted loss = [0 * 2.3, 66.666 * 2.3]",
keras/tests/,539,Reduced loss = (0 + 66.666 * 2.3) / 2,
keras/tests/,550,"EPSILON = 1e-7, y = y_true, y` = y_pred, Y_MAX = 0.9999999",
keras/tests/,551,"y` = clip(output, EPSILON, 1. - EPSILON)",
keras/tests/,552,"y` = [Y_MAX, Y_MAX, Y_MAX, EPSILON]",
keras/tests/,554,Loss = -(y log(y` + EPSILON) + (1 - y) log(1 - y` + EPSILON)),
keras/tests/,555,"= [-log(Y_MAX + EPSILON), -log(1 - Y_MAX + EPSILON),",
keras/tests/,556,"-log(Y_MAX + EPSILON), -log(1)]",
keras/tests/,557,"= [0, 15.33, 0, 0]",
keras/tests/,558,Reduced loss = 15.33 * 1.2 / 4,
keras/tests/,562,Test with logits.,
keras/tests/,569,"Loss = max(x, 0) - x * z + log(1 + exp(-abs(x)))",
keras/tests/,570,(where x = logits and z = y_true),
keras/tests/,571,"Loss = [(0 + 0 + 0)/3, 200 / 3]",
keras/tests/,572,"Weighted loss = [0 * 4, 66.666 * 3]",
keras/tests/,573,Reduced loss = (0 + 66.666 * 3) / 2,
keras/tests/,584,"Loss = max(x, 0) - x * z + log(1 + exp(-abs(x)))",
keras/tests/,585,(where x = logits and z = y_true),
keras/tests/,586,"Loss = [(0 + 0 + 0)/3, (200)/3]",
keras/tests/,594,"Loss: max(x, 0) - x * z + log(1 + exp(-abs(x)))",
keras/tests/,595,(where x = logits and z = y_true),
keras/tests/,596,Label smoothing: z' = z * (1 - L) + 0.5L,
keras/tests/,597,1  = 1 - 0.5L,
keras/tests/,598,0  = 0.5L,
keras/tests/,599,Applying the above two fns to the given input:,
keras/tests/,600,(100 - 100 * (1 - 0.5 L)  + 0 +,
keras/tests/,601,0   + 100 * (0.5 L)      + 0 +,
keras/tests/,602,0   + 100 * (1 - 0.5 L)  + 0) * (1/3),
keras/tests/,603,= (100 + 50L) * 1/3,
keras/tests/,627,Test with logits.,
keras/tests/,641,Test with logits.,
keras/tests/,655,Test with logits.,
keras/tests/,670,Test with logits.,
keras/tests/,688,Softmax Cross Entropy Loss: -\sum_i p_i \log q_i,
keras/tests/,689,where for a softmax activation,
keras/tests/,690,\log q_i = x_i - \log \sum_j \exp x_j,
keras/tests/,691,= x_i - x_max - \log \sum_j \exp (x_j - x_max),
keras/tests/,692,"For our activations, [100, -100, -100]",
keras/tests/,693,\log ( exp(0) + exp(-200) + exp(-200) ) = 0,
keras/tests/,694,"so our log softmaxes become: [0, -200, -200]",
keras/tests/,695,Label smoothing: z' = z * (1 - L) + L/n,
keras/tests/,696,1  = 1 - L + L/n,
keras/tests/,697,0  = L/n,
keras/tests/,698,Applying the above two fns to the given input:,
keras/tests/,699,-0 * (1 - L + L/n) + 200 * L/n + 200 * L/n = 400 L/n,
keras/tests/,723,Test with logits.,
keras/tests/,737,Test with logits.,
keras/tests/,751,Test with logits.,
keras/tests/,766,Test with logits.,
keras/tests/,73,"y_true_1 = [[2.], [4.], [6.], [8.]], y_pred = [[3.], [6.], [9.], [12.]]",
keras/tests/,74,"y_true_2 = [[1.], [2.], [3.], [4.]], y_pred = [[3.], [6.], [9.], [12.]]",
keras/tests/,76,Weighted metric `output_1`:,
keras/tests/,77,Total = ((3 - 2)^2 * 2  + (6 - 4)^2 * 3) +,
keras/tests/,78,((9 - 6)^2 * 4 + (12 - 8)^2 * 5),
keras/tests/,79,= 130,
keras/tests/,80,Count = (2 + 3) + (4 + 5),
keras/tests/,81,Result = 9.2857141,
keras/tests/,83,Weighted metric `output_2`:,
keras/tests/,84,Total = ((3 - 1)^2 * 3.5 + (6 - 2)^2 * 2.5) +,
keras/tests/,85,((9 - 3)^2 * 1.5 + (12 - 4)^2 * 0.5),
keras/tests/,86,= 140,
keras/tests/,87,Count = (3.5 + 2.5) + (1.5 + 0.5),
keras/tests/,88,Result = 17.5,
keras/tests/,90,Loss `output_1` with weights:,
keras/tests/,91,Total = ((3 - 2)^2 * 2  + (6 - 4)^2 * 3) +,
keras/tests/,92,((9 - 6)^2 * 4 + (12 - 8)^2 * 5),
keras/tests/,93,= 130,
keras/tests/,94,Count = 2 + 2,
keras/tests/,95,Result = 32.5,
keras/tests/,97,Loss `output_1` without weights/Metric `output_1`:,
keras/tests/,98,Total = ((3 - 2)^2 + (6 - 4)^2) + ((9 - 6)^2 + (12 - 8)^2) = 30,
keras/tests/,99,Count = 2 + 2,
keras/tests/,100,Result = 7.5,
keras/tests/,102,Loss `output_2` with weights:,
keras/tests/,103,Total = ((3 - 1)^2 * 3.5 + (6 - 2)^2 * 2.5) +,
keras/tests/,104,((9 - 3)^2 * 1.5 + (12 - 4)^2 * 0.5),
keras/tests/,105,= 140,
keras/tests/,106,Count = 2 + 2,
keras/tests/,107,Result = 35,
keras/tests/,109,Loss `output_2` without weights/Metric `output_2`:,
keras/tests/,110,Total = ((3 - 1)^2 + (6 - 2)^2) + ((9 - 3)^2 + (12 - 4)^2) = 120,
keras/tests/,111,Count = 2 + 2,
keras/tests/,112,Result = 30,
keras/tests/,114,Total loss with weights = 32.5 + 35 = 67.5,
keras/tests/,115,Total loss without weights = 7.5 + 30 = 37.5,
keras/tests/,147,"In the order: 'loss', 'output_1_loss', 'output_2_loss',",
keras/tests/,148,"'output_1_mean_squared_error', 'output_1_mean_squared_error_2',",
keras/tests/,149,"'output_2_mean_squared_error', 'output_2_mean_squared_error_2'",
keras/tests/,181,Set weights for one output (use batch size).,
keras/tests/,204,Set weights for one output.,
keras/tests/,232,Set weights for one output.,
keras/tests/,241,Verify that metric value is same with arbitrary weights and batch size.,
keras/tests/,265,Set weights for one output.,
keras/tests/,280,Set weights for one output.,
keras/tests/,302,Set weights for one output.,
keras/tests/,328,Set weights for one output.,
keras/tests/,350,Set weights for one output.,
keras/tests/,374,Set weights for one output.,
keras/tests/,42,Must wait for tf.keras to support sparse ops.,
keras/tests/,214,note that numpy reference implementation is independent of `unroll` argument,
keras/tests/,254,not supported learning_phase,
keras/tests/,312,Note : batch_dot implementation is different for,
keras/tests/,313,placeholders and variables in CNTK backend,
keras/tests/,336,test with placeholders,
keras/tests/,349,test with placeholders (no shape info),
keras/tests/,364,test with variables,
keras/tests/,396,Test shape inference when input,
keras/tests/,397,shape has `None` entries,
keras/tests/,433,test theano shape inference when,
keras/tests/,434,input shape has None entries,
keras/tests/,456,test theano shape inference when,
keras/tests/,457,input shape has None entries,
keras/tests/,481,TODO: somehow this capture mechanism doesn't work for TF,
keras/tests/,482,even though the TF op does print to stdout.,
keras/tests/,489,"Theano inserts ""__str__ = "" for no good reason",
keras/tests/,547,two-tensor ops,
keras/tests/,557,assumes first uid will always be the same,
keras/tests/,640,This test checks the consistency of the stop_gradient backend API.,
keras/tests/,641,It doesn't check the functionality (which is checked at the,
keras/tests/,642,test_gradient test).,
keras/tests/,664,Need to use `identity` to make this symbolic,
keras/tests/,665,(TODO: fix in tf.keras),
keras/tests/,679,Additional operations can be passed to tf.Session().run() via its,
keras/tests/,680,`fetches` arguments. In contrast to `updates` argument of,
keras/tests/,681,"KTF.function() these do not have control dependency on `outputs`, so",
keras/tests/,682,they can run in parallel. Also they should not contribute to output of,
keras/tests/,683,KTF.function().,
keras/tests/,701,Additional substitutions can be passed to `tf.Session().run()` via its,
keras/tests/,702,`feed_dict` arguments. Note that the feed_dict is passed once in the,
keras/tests/,703,constructor but we can modify the values in the dictionary. Through,
keras/tests/,704,this feed_dict we can provide additional substitutions besides Keras,
keras/tests/,705,inputs.,
keras/tests/,723,updated value in feed_dict will be modified within the K.function(),
keras/tests/,738,enable run_options.,
keras/tests/,746,disable run_options.,
keras/tests/,757,Test functions with string inputs.,
keras/tests/,767,implement a simple RNN,
keras/tests/,843,implement a simple RNN with an additional state,
keras/tests/,844,whose shape is different from that of the output,
keras/tests/,885,implement a simple RNN without states,
keras/tests/,919,implement a simple RNN,
keras/tests/,938,constants are appended to states in K.rnn,
keras/tests/,964,for second sample only,
keras/tests/,966,"a step function that just outputs inputs,",
keras/tests/,967,but increments states +1 per timestep,
keras/tests/,974,masking of two last timesteps for second sample only,
keras/tests/,978,outputs expected to be same as inputs for the first sample,
keras/tests/,980,but for the second sample all outputs in masked region should be the same,
keras/tests/,981,as last output before masked region,
keras/tests/,986,first state should be incremented for every timestep (no masking),
keras/tests/,988,second state should not be incremented for last two timesteps,
keras/tests/,991,verify same expected output for `unroll=true/false`,
keras/tests/,1020,final timestep masked for last sample,
keras/tests/,1023,"for the last sample, the final timestep (in masked region) should be the",
keras/tests/,1024,same as the second to final output (before masked region),
keras/tests/,1052,final two timesteps masked for first sample,
keras/tests/,1070,not updated last timestep:,
keras/tests/,1100,scalar,
keras/tests/,1108,non scalar,
keras/tests/,1125,"dropout patterns are different, only check mean",
keras/tests/,1133,"dropout patterns are different, only check mean",
keras/tests/,1137,Test invalid use cases,
keras/tests/,1142,standard relu,
keras/tests/,1143,set alpha only,
keras/tests/,1144,set max_value only,
keras/tests/,1145,set threshold only,
keras/tests/,1146,set alpha and max_value,
keras/tests/,1147,set alpha and threshold,
keras/tests/,1148,set max_value and threshold,
keras/tests/,1149,set all,
keras/tests/,1150,max_value is zero,
keras/tests/,1151,threshold is negative,
keras/tests/,1152,max_value > 6,
keras/tests/,1175,"toy label matrix (4 samples, 2 classes)",
keras/tests/,1192,"toy label matrix (2 samples, 3 classes)",
keras/tests/,1206,Random prediction test case,
keras/tests/,1210,(k == 0 or k > num_classes) does not raise an error,
keras/tests/,1211,but just return an unmeaningful tensor.,
keras/tests/,1218,Identical prediction test case:,
keras/tests/,1219,randomly set half of the predictions to an identical value,
keras/tests/,1411,TODO: make this a parameterized test,
keras/tests/,1450,assumption in initializers.VarianceScaling,
keras/tests/,1525,Test invalid use cases,
keras/tests/,1562,Test invalid use cases,
keras/tests/,1584,Check handling of dynamic shapes.,
keras/tests/,1590,Test invalid use cases,
keras/tests/,1606,Check handling of dynamic shapes.,
keras/tests/,1612,Test invalid use cases,
keras/tests/,1638,Test invalid use cases,
keras/tests/,1704,the Theano and TensorFlow CTC code use different methods to ensure,
keras/tests/,1705,numerical stability.  The Theano code subtracts out the max,
keras/tests/,1706,"before the final log, so the results are different but scale",
keras/tests/,1707,identically and still train properly,
keras/tests/,1714,simplified version of TensorFlow's test,
keras/tests/,1717,number of timesteps,
keras/tests/,1719,dimensions are batch x time x categories,
keras/tests/,1745,"test when batch_size = 1, that is, one sample only",
keras/tests/,1746,get only first sample from above test case,
keras/tests/,1783,t=0,
keras/tests/,1784,t=1,
keras/tests/,1785,t=2,
keras/tests/,1786,t=3,
keras/tests/,1787,t=4 (ignored),
keras/tests/,1788,t=5 (ignored),
keras/tests/,1792,dimensions are time x depth,
keras/tests/,1795,t=0,
keras/tests/,1796,t=1,
keras/tests/,1797,t=2,
keras/tests/,1798,t=3,
keras/tests/,1799,t=4,
keras/tests/,1800,t=5 (ignored),
keras/tests/,1803,len max_time_steps array of batch_size x depth matrices,
keras/tests/,1808,change tensorflow order to keras backend order,
keras/tests/,1811,batch_size length vector of sequence_lengths,
keras/tests/,1858,Random entry added in at time=5,
keras/tests/,1862,Add arbitrary offset - this is fine,
keras/tests/,1865,len max_time_steps array of batch_size x depth matrices,
keras/tests/,1867,Pad to max_time_steps = 8,
keras/tests/,1870,Take exponential as we directly apply ctc_decode_beam_search,
keras/tests/,1873,change tensorflow order to keras backend order,
keras/tests/,1876,batch_size length vector of sequence_lengths,
keras/tests/,1878,batch_size length vector of log probabilities,
keras/tests/,1881,output beam 0,
keras/tests/,1882,output beam 1,
keras/tests/,1910,"A simple CTC probability map with some repeating characters,",
keras/tests/,1911,"shape(batch, input_width, char_count)",
keras/tests/,1912,"Without merging should be decoded as: ""AABB"", with merging as: ""AB"".",
keras/tests/,1914,"blank, A ,B",
keras/tests/,1915,blank,
keras/tests/,1916,A,
keras/tests/,1917,blank,
keras/tests/,1918,A,
keras/tests/,1919,B,
keras/tests/,1920,blank,
keras/tests/,1921,B,
keras/tests/,1938,merged: A B,
keras/tests/,1940,not merged: A A B B,
keras/tests/,2007,"In stack, each array must have the same shape.",
keras/tests/,2020,make sure we can also walk the indexes in tensorflow which we,
keras/tests/,2021,can't without specifying dtype,
keras/tests/,2041,This test aims to make sure that we walk the array from right to left,
keras/tests/,2042,and checks it in the following way: multiplying left to right 1e-40,
keras/tests/,2043,cannot be held into a float32 so it causes an underflow while from,
keras/tests/,2044,right to left we have no such problem and the result is larger,
keras/tests/,2107,Keep track of the old value,
keras/tests/,2115,Keep track of the old value,
keras/tests/,2117,Check correct values,
keras/tests/,2120,Make sure that changes to the global floatx are effectively,
keras/tests/,2121,taken into account by the backend.,
keras/tests/,2123,Restore old value,
keras/tests/,2139,GitHub issue: 11435,
keras/tests/,156,Baseline,
keras/tests/,163,Training,
keras/tests/,169,Inference,
keras/tests/,190,Inference,
keras/tests/,213,Baseline,
keras/tests/,228,Change this,
keras/tests/,235,Training,
keras/tests/,288,TODO: resolve flakyness issue. Tracked with #11587,
keras/tests/,412,One epoch is completed so enqueuer will switch the Sequence,
keras/tests/,418,One epoch has been completed so enqueuer2 will switch,
keras/tests/,420,Be sure that both Sequence were updated,
keras/tests/,426,Tear down everything,
keras/tests/,489,TODO: resolve flakyness issue. Tracked with #11586,
keras/tests/,52,Keyword-only arguments (Python 3 only),
keras/tests/,57,lambda,
keras/tests/,71,Sometimes exec adds builtins to the context,
keras/tests/,124,this test ensures that models serialized prior to version 2.1.2 can still be,
keras/tests/,125,deserialized,
keras/tests/,127,see:,
keras/tests/,128,https://github.com/evhub/keras/blob/2.1.1/keras/utils/generic_utils.py#L166,
keras/tests/,45,Creating dataset to store features,
keras/tests/,48,Creating dataset to store labels,
keras/tests/,60,"Instantiating HDF5Matrix for the training set,",
keras/tests/,61,which is a slice of the first 150 elements,
keras/tests/,65,Likewise for the test set,
keras/tests/,69,HDF5Matrix behave more or less like Numpy matrices with regards to indexing,
keras/tests/,71,"But they do not support negative indices, so don't try print(X_train[-1])",
keras/tests/,84,Note: you have to use shuffle='batch' or False with HDF5Matrix,
keras/tests/,86,test that evalutation and prediction don't crash and,
keras/tests/,87,return reasonable results,
keras/tests/,95,test slicing for shortened array,
keras/tests/,98,test __getitem__,
keras/tests/,115,test normalizer,
keras/tests/,121,test resizing normalizer,
keras/tests/,127,test dtype changing normalizer,
keras/tests/,148,test both HDF5 and dict implementations,
keras/tests/,154,str,
keras/tests/,158,list<bytes>,
keras/tests/,162,ndarray,
keras/tests/,184,test both HDF5 and dict implementations,
keras/tests/,45,Test equivalence of convert_all_kernels_in_model,
keras/tests/,52,Test equivalence of convert_dense_weights_data_format,
keras/tests/,22,Check shape,
keras/tests/,24,Make sure there are only 0s and 1s,
keras/tests/,26,Make sure there is exactly one 1 in a row,
keras/tests/,28,Get original labels back from one hots,
keras/tests/,32,It will work for use_multiprocessing=False,
keras/tests/,144,weighted_masked_objective,
keras/tests/,171,test starting from non-zero initial epoch,
keras/tests/,181,define tracer callback,
keras/tests/,189,TODO: resolve flakyness issue. Tracked with #11560,
keras/tests/,204,training/testing doesn't work before compiling.,
keras/tests/,212,test train_on_batch,
keras/tests/,220,test fit,
keras/tests/,229,test validation_split,
keras/tests/,237,test validation data,
keras/tests/,256,test_on_batch,
keras/tests/,264,predict_on_batch,
keras/tests/,269,"predict, evaluate",
keras/tests/,281,with sample_weight,
keras/tests/,297,test accuracy metric,
keras/tests/,308,this should also work,
keras/tests/,319,and this as well,
keras/tests/,337,test starting from non-zero initial epoch for generator too,
keras/tests/,352,test with a custom metric function,
keras/tests/,361,total loss + 2 outputs * (loss + metric),
keras/tests/,381,enable verbose for evaluate_generator,
keras/tests/,383,pass generator directly so `is_generator_or_sequence`,
keras/tests/,384,doesn't get confused.,
keras/tests/,387,empty batch,
keras/tests/,403,x is not a list of numpy arrays.,
keras/tests/,407,x does not match _feed_input_names.,
keras/tests/,413,all input/output/weight arrays should have the same number of samples.,
keras/tests/,428,`sample_weight` is neither a dict nor a list.,
keras/tests/,434,`validation_data` is neither a tuple nor a triple.,
keras/tests/,441,`loss` does not match outputs.,
keras/tests/,445,`loss_weights` does not match output_names.,
keras/tests/,449,`loss_weights` does not match outputs.,
keras/tests/,453,`loss_weights` is invalid type.,
keras/tests/,457,`sample_weight_mode` does not match output_names.,
keras/tests/,462,`sample_weight_mode` does not match output_names.,
keras/tests/,466,`sample_weight_mode` matches output_names partially.,
keras/tests/,471,`loss` does not exist.,
keras/tests/,480,the rank of weight arrays should be 1.,
keras/tests/,491,the rank of output arrays should be at least 3D.,
keras/tests/,498,TODO: resolve flakyness issue. Tracked with #11560,
keras/tests/,536,steps_per_epoch will be equal to len of sequence if it's unspecified,
keras/tests/,547,the queue may be full.,
keras/tests/,559,the queue may be full.,
keras/tests/,561,test for workers = 0,
keras/tests/,587,fit_generator will throw an exception,
keras/tests/,588,if steps is unspecified for regular generator,
keras/tests/,599,Check if generator is only accessed an expected number of times,
keras/tests/,615,Need range check here as filling,
keras/tests/,616,of the queue depends on sleep in the enqueuers,
keras/tests/,620,12 = (epoch * workers * validation steps * max_queue_size),
keras/tests/,630,12 = (epoch * workers * validation steps * max_queue_size),
keras/tests/,631,Need range check here as filling,
keras/tests/,632,of the queue depends on sleep in the enqueuers,
keras/tests/,656,1st epoch -> ceil(20 / 3) = 7 batches,
keras/tests/,657,2nd epoch -> ceil(20 / 5) = 4 batches,
keras/tests/,658,3d  epoch -> ceil(20 / 7) = 3 batches,
keras/tests/,659,4th epoch -> ceil(20 / 9) = 3 batches,
keras/tests/,660,5th epoch -> ceil(20 /11) = 2 batches,
keras/tests/,676,1st epoch -> ceil(30 / 3) = 10 batches,
keras/tests/,677,2nd epoch -> ceil(30 / 5) =  6 batches,
keras/tests/,678,3d  epoch -> ceil(30 / 7) =  5 batches,
keras/tests/,679,4th epoch -> ceil(30 / 9) =  4 batches,
keras/tests/,680,5th epoch -> ceil(30 /11) =  3 batches,
keras/tests/,695,number of trained batches should match sum of steps per each epoch,
keras/tests/,720,1st epoch -> ceil(20 / 3) = 7 batches,
keras/tests/,721,2nd epoch -> ceil(20 / 5) = 4 batches,
keras/tests/,722,3d  epoch -> ceil(20 / 7) = 3 batches,
keras/tests/,723,4th epoch -> ceil(20 / 9) = 3 batches,
keras/tests/,724,5th epoch -> ceil(20 /11) = 2 batches,
keras/tests/,740,1st epoch -> ceil(30 / 3) = 10 batches,
keras/tests/,741,2nd epoch -> ceil(30 / 5) =  6 batches,
keras/tests/,742,3d  epoch -> ceil(30 / 7) =  5 batches,
keras/tests/,743,4th epoch -> ceil(30 / 9) =  4 batches,
keras/tests/,744,5th epoch -> ceil(30 /11) =  3 batches,
keras/tests/,759,number of trained batches should match sum of steps per each epoch,
keras/tests/,765,predict_generator output shape behavior should be consistent,
keras/tests/,773,Multiple outputs and one step.,
keras/tests/,785,Multiple outputs and multiple steps.,
keras/tests/,797,Create a model with a single output.,
keras/tests/,802,Single output and one step.,
keras/tests/,814,Single output and multiple steps.,
keras/tests/,946,test with nesting,
keras/tests/,1024,test train_on_batch,
keras/tests/,1033,test fit,
keras/tests/,1039,test evaluate,
keras/tests/,1045,test predict,
keras/tests/,1050,Now test a model with a single input,
keras/tests/,1051,i.e. we don't pass any data to fit the model.,
keras/tests/,1062,test train_on_batch,
keras/tests/,1075,test fit,
keras/tests/,1081,test evaluate,
keras/tests/,1087,test predict,
keras/tests/,1092,"Same, without learning phase",
keras/tests/,1093,i.e. we don't pass any data to fit the model.,
keras/tests/,1103,test train_on_batch,
keras/tests/,1116,test fit,
keras/tests/,1122,test evaluate,
keras/tests/,1128,test predict,
keras/tests/,1148,test train_on_batch,
keras/tests/,1151,fit,
keras/tests/,1153,evaluate,
keras/tests/,1156,Same without dropout.,
keras/tests/,1166,test train_on_batch,
keras/tests/,1169,fit,
keras/tests/,1171,evaluate,
keras/tests/,1178,"None loss, only regularization loss.",
keras/tests/,1194,test train_on_batch,
keras/tests/,1197,fit,
keras/tests/,1199,evaluate,
keras/tests/,1202,"No dropout, external loss.",
keras/tests/,1214,test train_on_batch,
keras/tests/,1217,fit,
keras/tests/,1219,evaluate,
keras/tests/,1222,Test fit with no external data at all.,
keras/tests/,1236,test train_on_batch,
keras/tests/,1241,test fit,
keras/tests/,1246,define a generator to produce x=None and y=None,
keras/tests/,1254,test fit_generator for framework-native data tensors,
keras/tests/,1258,test evaluate_generator for framework-native data tensors,
keras/tests/,1262,test fit with validation data,
keras/tests/,1273,test evaluate,
keras/tests/,1278,test predict,
keras/tests/,1284,Test multi-output model without external data.,
keras/tests/,1294,test train_on_batch,
keras/tests/,1299,test fit,
keras/tests/,1304,test fit with validation data,
keras/tests/,1315,test evaluate,
keras/tests/,1320,test predict,
keras/tests/,1330,"single-output, as list",
keras/tests/,1339,"single-output, as dict",
keras/tests/,1344,"single-output, as tensor",
keras/tests/,1349,test invalid arguments,
keras/tests/,1364,"multi-output, as list",
keras/tests/,1379,"multi-output, as dict",
keras/tests/,1385,"multi-output, not enough target tensors when `target_tensors` is not a dict",
keras/tests/,1399,test with sample weights,
keras/tests/,1426,test list of target tensors,
keras/tests/,1442,test dictionary of target_tensors,
keras/tests/,1449,test dictionary of target_tensors,
keras/tests/,1460,test with custom placeholder as target,
keras/tests/,1490,Should warn on .summary(),
keras/tests/,1497,And on .fit(),
keras/tests/,1504,And shouldn't warn if we recompile,
keras/tests/,1654,Test with dictionary inputs,
keras/tests/,1682,Test with validation data,
keras/tests/,1691,Test with validation split,
keras/tests/,1702,Test evaluation / prediction methods,
keras/tests/,1747,"Labels for testing 4-class sparse_categorical_crossentropy, 4-class",
keras/tests/,1748,"categorical_crossentropy, and 2-class binary_crossentropy:",
keras/tests/,1756,Compute one loss for each loss function in the list `losses_to_test`:,
keras/tests/,1762,"Evaluate a simple network with channels last, with all three loss",
keras/tests/,1763,functions:,
keras/tests/,1773,"Evaluate the same network with channels first, with all three loss",
keras/tests/,1774,functions:,
keras/tests/,1833,Only `sample_weights`.,
keras/tests/,1837,Only `class_weights`.,
keras/tests/,1841,Both 'sample_weights` and 'class_weights`.,
keras/tests/,1866,Test in training_arrays.py,
keras/tests/,1878,Test in training_generator.py,
keras/tests/,1947,Verify that the metrics added using `compile` and `add_metric` API are,
keras/tests/,1948,included,
keras/tests/,1979,Verify that the metrics added using `compile` and `add_metric` API are,
keras/tests/,1980,included,
keras/tests/,2055,Provide same name as in the instance created in __init__,
keras/tests/,2056,for eager mode,
keras/tests/,61,sequential model,
keras/tests/,124,test merge,
keras/tests/,128,Test recursion,
keras/tests/,140,try actually running graph,
keras/tests/,146,output a: nothing changes,
keras/tests/,148,output b: dropout applied,
keras/tests/,153,Test the ability to pass and serialize arguments to `call`.,
keras/tests/,160,Test that argument is kept when applying the model,
keras/tests/,165,Test that argument is kept after loading a model,
keras/tests/,172,,
keras/tests/,173,test basics,
keras/tests/,219,test layer properties,
keras/tests/,254,,
keras/tests/,255,test multi-input layer,
keras/tests/,286,we don't check names of first 2 layers (inputs) because,
keras/tests/,287,ordering of same-level layers is not fixed,
keras/tests/,293,actually run model,
keras/tests/,300,test get_source_inputs,
keras/tests/,305,serialization / deserialization,
keras/tests/,322,,
keras/tests/,323,test recursion,
keras/tests/,341,"g2, h2 = model([e, f])",
keras/tests/,346,test separate manipulation of different layer outputs,
keras/tests/,354,we don't check names of first 2 layers (inputs) because,
keras/tests/,355,ordering of same-level layers is not fixed,
keras/tests/,361,run recursive model,
keras/tests/,368,test serialization,
keras/tests/,379,,
keras/tests/,380,test multi-input multi-output,
keras/tests/,395,test with single output as 1-elem list,
keras/tests/,403,test with single output as tensor,
keras/tests/,409,note that the output of the K.function will still be a 1-elem list,
keras/tests/,412,test serialization,
keras/tests/,419,note that the output of the K.function will still be a 1-elem list,
keras/tests/,432,,
keras/tests/,433,test invalid graphs,
keras/tests/,435,input is not an Input tensor,
keras/tests/,444,disconnected graph,
keras/tests/,451,redundant outputs,
keras/tests/,455,this should work with a warning,
keras/tests/,458,redundant inputs,
keras/tests/,465,i have not idea what I'm doing: garbage as inputs/outputs,
keras/tests/,472,,
keras/tests/,473,test calling layers/models on placeholders,
keras/tests/,485,test merge,
keras/tests/,489,test tensor input,
keras/tests/,512,TimeDistributed Conv2D layer,
keras/tests/,513,use 'channels_first' data format to check that,
keras/tests/,514,the function is being called correctly for Conv2D,
keras/tests/,515,"old: (filters, stack_size, kernel_rows, kernel_cols)",
keras/tests/,516,"new: (kernel_rows, kernel_cols, stack_size, filters)",
keras/tests/,530,Bidirectional ConvLSTM2D layer,
keras/tests/,531,"old ConvLSTM2D took a list of 12 weight tensors,",
keras/tests/,532,returns a list of 3 concatenated larger tensors.,
keras/tests/,534,bidirectional,
keras/tests/,536,kernel,
keras/tests/,537,recurrent kernel,
keras/tests/,538,bias,
keras/tests/,584,A model is needed to initialize weights.,
keras/tests/,619,layer can be instantiated only for supported backends,
keras/tests/,621,A model is needed to initialize weights.,
keras/tests/,687,"Basic outline here: we have a shared embedding layer, and two inputs that",
keras/tests/,688,go through different depths of computation in the graph before,
keras/tests/,689,the final output.  We need the computed depth of the input layers to be,
keras/tests/,690,"the same, because they both pass through the embedding layer before anything",
keras/tests/,691,else happens.  That's what we're testing.,
keras/tests/,763,This tests for the bug in this issue,
keras/tests/,764,https://github.com/keras-team/keras/issues/11159,
keras/tests/,765,It occurs with layer sharing at heterogeneous depth when,
keras/tests/,766,the layers need to be applied in an order that differs from,
keras/tests/,767,the order that occurs in the config.,
keras/tests/,779,Note: if the order of the layers in the concat is,
keras/tests/,780,"changed to ([Aout1, Aout2]) the bug doesn't trigger",
keras/tests/,9,basic case,
keras/tests/,28,recursive case,
keras/tests/,47,subnetwork case,
keras/tests/,105,basic case,
keras/tests/,120,includes input layer,
keras/tests/,306,train once so that the states change,
keras/tests/,311,"if the state is not reset, output should be different",
keras/tests/,314,check that output changes after states are reset,
keras/tests/,315,(even though the model itself didn't change),
keras/tests/,320,check that container-level reset_states() works,
keras/tests/,325,check that the call to `predict` updated the states,
keras/tests/,343,test with Sequential model,
keras/tests/,351,test config,
keras/tests/,356,test stacked bidirectional layers,
keras/tests/,366,test with functional API,
keras/tests/,374,Bidirectional and stateful,
keras/tests/,51,max_value of ReLU layer cannot be negative value,
keras/tests/,56,negative_slope of ReLU layer cannot be negative value,
keras/tests/,66,Test that `relu` op gets used.,
keras/tests/,69,Test that `leakyrelu` op gets used.,
keras/tests/,72,Test that `relu6` op gets used.,
keras/tests/,15,"first, test with Dense layer",
keras/tests/,24,test config,
keras/tests/,27,test when specifying a batch_input_shape,
keras/tests/,42,test with Embedding,
keras/tests/,51,compare to not using batch_input_shape,
keras/tests/,65,test with Conv2D,
keras/tests/,78,test stacked layers,
keras/tests/,88,test wrapping Sequential model,
keras/tests/,97,test with functional API,
keras/tests/,105,test with BatchNormalization,
keras/tests/,111,Assert that mean and variance are 0 and 1.,
keras/tests/,115,Train,
keras/tests/,118,Assert that mean and variance changed.,
keras/tests/,121,Verify input_map has one mapping from inputs to reshaped inputs.,
keras/tests/,131,test layers that need learning_phase to be set,
keras/tests/,141,test layers that need learning_phase to be set,
keras/tests/,158,test with unspecified shape and Embeddings with mask_zero,
keras/tests/,162,"the shape so far: (N, t_1, t_2, 6)",
keras/tests/,177,embedding layer,
keras/tests/,178,first RNN layer,
keras/tests/,179,second RNN layer,
keras/tests/,183,final layer,
keras/tests/,187,test with Masking layer,
keras/tests/,239,test with Sequential model,
keras/tests/,248,test config,
keras/tests/,253,test stacked bidirectional layers,
keras/tests/,263,Bidirectional and stateful,
keras/tests/,275,test with functional API with dynamic length,
keras/tests/,316,basic case,
keras/tests/,331,test return_state,
keras/tests/,348,test if the state of a BiRNN is the concatenation of the underlying RNNs,
keras/tests/,398,test passing invalid initial_state: passing a tensor,
keras/tests/,403,test valid usage: passing a list,
keras/tests/,424,will (and should) raise if more than one constant passed,
keras/tests/,454,Test basic case.,
keras/tests/,469,Test basic case serialization.,
keras/tests/,483,test flat list inputs,
keras/tests/,504,will (and should) raise if more than one constant passed,
keras/tests/,534,Test basic case.,
keras/tests/,552,Test basic case serialization.,
keras/tests/,568,verify that state is used,
keras/tests/,573,test flat list inputs,
keras/tests/,584,test layers that need learning_phase to be set,
keras/tests/,40,test for return state:,
keras/tests/,61,test for output shape:,
keras/tests/,78,Tests for statefulness,
keras/tests/,93,train once so that the states change,
keras/tests/,98,"if the state is not reset, output should be different",
keras/tests/,101,check that output changes after states are reset,
keras/tests/,102,(even though the model itself didn't change),
keras/tests/,107,check that container-level reset_states() works,
keras/tests/,112,check that the call to `predict` updated the states,
keras/tests/,116,cntk doesn't support eval convolution with static,
keras/tests/,117,"variable, will enable it later",
keras/tests/,119,check regularizers,
keras/tests/,143,check dropout,
keras/tests/,154,check state initialization,
keras/tests/,34,Test invalid use case,
keras/tests/,66,Test invalid use case,
keras/tests/,192,Test invalid use case,
keras/tests/,223,Test with negative tuple of axes.,
keras/tests/,233,shapes provided,
keras/tests/,247,shapes not provided,
keras/tests/,261,ndim not provided,
keras/tests/,12,TensorFlow does not support full convolution.,
keras/tests/,24,Causal,
keras/tests/,28,Non-causal,
keras/tests/,32,Causal dilated with larger kernel size,
keras/tests/,228,Check dilated conv transpose returns expected output,
keras/tests/,286,Test invalid output padding for given stride. Output padding equal to stride,
keras/tests/,296,Output padding greater than stride,
keras/tests/,614,Test invalid use case,
keras/tests/,622,Test invalid output padding for given stride. Output padding equal,
keras/tests/,623,to stride,
keras/tests/,633,Output padding greater than stride,
keras/tests/,651,basic test,
keras/tests/,659,correctness test,
keras/tests/,845,tf,
keras/tests/,849,basic test,
keras/tests/,865,tf,
keras/tests/,869,compare with numpy,
keras/tests/,873,tf,
keras/tests/,893,tf,
keras/tests/,897,basic test,
keras/tests/,915,tf,
keras/tests/,935,tf,
keras/tests/,940,basic test,
keras/tests/,958,tf,
keras/tests/,963,compare with numpy,
keras/tests/,968,tf,
keras/tests/,1004,basic test,
keras/tests/,1009,correctness test,
keras/tests/,1015,compare with numpy,
keras/tests/,1036,another correctness test (no cropping),
keras/tests/,1043,compare with input,
keras/tests/,1046,Test invalid use cases,
keras/tests/,1069,basic test,
keras/tests/,1074,correctness test,
keras/tests/,1080,compare with numpy,
keras/tests/,1103,another correctness test (no cropping),
keras/tests/,1110,compare with input,
keras/tests/,1113,Test invalid use cases,
keras/tests/,54,"centered on 5.0, variance 10.0",
keras/tests/,73,"centered on 5.0, variance 10.0",
keras/tests/,109,This is a regression test for issue #4881 with the old,
keras/tests/,110,batch normalization functions in the Theano backend.,
keras/tests/,129,"centered on 5.0, variance 10.0",
keras/tests/,150,"centered on 5.0, variance 10.0",
keras/tests/,163,Test single layer reuse,
keras/tests/,177,Test model-level reuse,
keras/tests/,235,Simulates training-mode with trainable layer. Should use mini-batch statistics.,
keras/tests/,81,Test that dropout is applied during training,
keras/tests/,91,Test that dropout is not applied during testing,
keras/tests/,118,train once so that the states change,
keras/tests/,123,"if the state is not reset, output should be different",
keras/tests/,126,check that output changes after states are reset,
keras/tests/,127,(even though the model itself didn't change),
keras/tests/,132,check that container-level reset_states() works,
keras/tests/,137,check that the call to `predict` updated the states,
keras/tests/,144,Check masking: output with left padding and right padding,
keras/tests/,145,should be the same.,
keras/tests/,187,also equal to `output_size`,
keras/tests/,189,random inputs and state values,
keras/tests/,191,last timestep masked for first sample (all zero inputs masked by Masking layer),
keras/tests/,195,final outputs equal to last inputs,
keras/tests/,197,"except for first sample, where it is equal to second to last value due to mask",
keras/tests/,201,states are incremented `num_timesteps - 1` times for first sample,
keras/tests/,203,and `num_timesteps - 1` times for remaining samples,
keras/tests/,246,random inputs and state values,
keras/tests/,248,last timestep masked for first sample (all zero inputs masked by Masking layer),
keras/tests/,252,final outputs equal to last inputs concatenated,
keras/tests/,254,"except for first sample, where it is equal to second to last value due to mask",
keras/tests/,258,states are incremented `num_timesteps - 1` times for first sample,
keras/tests/,260,and `num_timesteps - 1` times for remaining samples,
keras/tests/,285,Without dropout,
keras/tests/,290,With dropout,
keras/tests/,297,Without bias,
keras/tests/,377,Test with Keras tensor,
keras/tests/,402,Test with non-Keras tensor,
keras/tests/,438,Test fit with invalid data,
keras/tests/,447,Test with Keras tensor,
keras/tests/,550,Basic test case.,
keras/tests/,559,Test stacking.,
keras/tests/,588,Basic test case.,
keras/tests/,597,Test stacking.,
keras/tests/,619,no time axis in the input shape passed to RNN cells,
keras/tests/,642,Test basic case.,
keras/tests/,651,Test basic case serialization.,
keras/tests/,664,Test stacking.,
keras/tests/,674,Test stacked RNN serialization.,
keras/tests/,690,Test basic case.,
keras/tests/,699,Test basic case serialization.,
keras/tests/,711,Test stacking.,
keras/tests/,721,Test stacked RNN serialization.,
keras/tests/,756,Test regularization losses,
keras/tests/,759,Test weights,
keras/tests/,765,Test `get_losses_for`,
keras/tests/,784,Test reverse_state_order = True for stacked cell.,
keras/tests/,823,will (and should) raise if more than one constant passed,
keras/tests/,853,Test basic case.,
keras/tests/,866,Test basic case serialization.,
keras/tests/,881,test flat list inputs,
keras/tests/,890,Test stacking.,
keras/tests/,903,Test stacked RNN serialization.,
keras/tests/,931,will (and should) raise if more than one constant passed,
keras/tests/,961,Test basic case.,
keras/tests/,975,Test basic case serialization.,
keras/tests/,991,verify that state is used,
keras/tests/,996,test flat list inputs,
keras/tests/,1048,theano does not support static shape inference.,
keras/tests/,37,len(input_length) should be equal to len(input_shape) - 1,
keras/tests/,114,Test GlobalAveragePooling1D supports masking,
keras/tests/,53,Test invalid use cases,
keras/tests/,62,with string argument,
keras/tests/,67,with function argument,
keras/tests/,175,only valid for 2D tensors,
keras/tests/,184,test layer with multiple outputs,
keras/tests/,215,test layer with multiple outputs and no,
keras/tests/,216,explicit mask,
keras/tests/,257,test serialization with function,
keras/tests/,265,test with lambda,
keras/tests/,272,test serialization with output_shape function,
keras/tests/,329,test in functional API,
keras/tests/,338,test serialization,
keras/tests/,44,Changing the default arguments of get_test_data.,
keras/tests/,89,"we must generate new callbacks for each test, as they aren't stateless",
keras/tests/,109,fit without validation data,
keras/tests/,114,fit with validation data and accuracy,
keras/tests/,120,fit generator without validation data,
keras/tests/,125,fit generator with validation data and accuracy,
keras/tests/,150,test a layer with a list of output tensors,
keras/tests/,162,"we must generate new callbacks for each test, as they aren't stateless",
keras/tests/,181,fit without validation data,
keras/tests/,186,fit with validation data and accuracy,
keras/tests/,194,fit generator without validation data,
keras/tests/,198,fit generator with validation data and accuracy,
keras/tests/,57,Changing the default arguments of get_test_data.,
keras/tests/,365,case 1 fit,
keras/tests/,439,case 1,
keras/tests/,458,case 2,
keras/tests/,467,case 3,
keras/tests/,479,case 4,
keras/tests/,490,case 5,
keras/tests/,564,This should allow training to go for at least `patience` epochs,
keras/tests/,586,"Should stop after epoch 3,",
keras/tests/,587,as the loss has not improved after patience=2 epochs.,
keras/tests/,630,All epochs should run because baseline was met in second epoch,
keras/tests/,632,Baseline was not met by second epoch and should stop,
keras/tests/,667,"The best configuration is in the epoch 2 (loss = 0.1000),",
keras/tests/,668,so with patience=2 we need to end up at epoch 4,
keras/tests/,693,The best configuration is in the epoch 2 (loss = 0.1000).,
keras/tests/,706,"The best configuration is in epoch 2 (loss = 0.1000),",
keras/tests/,707,"and while patience = 2, we're restoring the best weights,",
keras/tests/,708,"so we end up at the epoch with the best weights, i.e. epoch 2",
keras/tests/,749,This should reduce the LR after the first epoch (due to high epsilon).,
keras/tests/,797,The learning rates should be 1.0 except the last one,
keras/tests/,805,Check if warnings are disabled,
keras/tests/,832,"case 1, create new file with defined separator",
keras/tests/,845,"case 2, append data to existing file, skip header",
keras/tests/,851,"case 3, reuse of CSVLogger object",
keras/tests/,890,"callback validation data should always have x, y, and sample weights",
keras/tests/,909,Start an arbitrary process that should run during model training and,
keras/tests/,910,be terminated after training has completed.,
keras/tests/,191,Usage of sklearn's grid_search,
keras/tests/,192,from sklearn import grid_search,
keras/tests/,193,"parameters = dict(hidden_dims = [20, 30], batch_size=[64, 128],",
keras/tests/,194,"epochs=[2], verbose=[0])",
keras/tests/,195,classifier = Inherit_class_build_fn_clf(),
keras/tests/,196,"clf = grid_search.GridSearchCV(classifier, parameters)",
keras/tests/,197,"clf.fit(X_train, y_train)",
keras/tests/,198,"parameters = dict(hidden_dims = [20, 30], batch_size=[64, 128],",
keras/tests/,199,"epochs=[2], verbose=[0])",
keras/tests/,200,regressor = Inherit_class_build_fn_reg(),
keras/tests/,201,"reg = grid_search.GridSearchCV(regressor, parameters,",
keras/tests/,202,"scoring='mean_squared_error',",
keras/tests/,203,"n_jobs=1, cv=2, verbose=2)",
keras/tests/,204,"reg.fit(X_train_reg, y_train_reg)",
keras/tests/integration_tests/test_temporal_data_tasks.py,124,generate alphabet:,
keras/tests/integration_tests/test_temporal_data_tasks.py,125,http://stackoverflow.com/questions/16060899/alphabet-range-python,
keras/tests/integration_tests/test_temporal_data_tasks.py,129,generate char sequences of length 'sequence_length' out of alphabet and,
keras/tests/integration_tests/test_temporal_data_tasks.py,130,store the next char as label (e.g. 'ab'->'c'),
keras/tests/integration_tests/test_temporal_data_tasks.py,137,Transform sequences and labels into 'one-hot' encoding,
keras/tests/integration_tests/test_temporal_data_tasks.py,145,learn the alphabet with stacked LSTM,
keras/tests/integration_tests/test_temporal_data_tasks.py,155,prime the model with 'ab' sequence and let it generate the learned alphabet,
keras/tests/integration_tests/test_temporal_data_tasks.py,167,check that it did generate the alphabet correctly,
keras/tests/integration_tests/test_vector_data_tasks.py,26,Test with Sequential API,
keras/tests/integration_tests/test_vector_data_tasks.py,51,Test with functional API,
keras/tests/integration_tests/test_tensorflow_integration.py,46,Test saving.,
keras/tests/integration_tests/test_image_data_tasks.py,47,Dummy ImageDataGenerator,
keras/tests/integration_tests/imagenet_utils_test.py,11,Test image batch with float and int image input,
keras/tests/integration_tests/imagenet_utils_test.py,26,Test single image,
keras/tests/integration_tests/imagenet_utils_test.py,41,Test that writing over the input data works predictably,
keras/tests/integration_tests/imagenet_utils_test.py,49,Caffe mode works differently from the others,
keras/tests/integration_tests/imagenet_utils_test.py,59,Test image batch,
keras/tests/integration_tests/imagenet_utils_test.py,78,Test single image,
keras/tests/integration_tests/imagenet_utils_test.py,99,Disabled due to SSL issues on Travis.,
keras/tests/integration_tests/imagenet_utils_test.py,107,the numbers of columns and ImageNet classes are not identical.,
keras/tests/integration_tests/applications_test.py,26,Note that NASNetLarge is too heavy to test on Travis.,
keras/tests/integration_tests/applications_test.py,33,Create model in a subprocess so that,
keras/tests/integration_tests/applications_test.py,34,the memory consumed by InceptionResNetV2 will be,
keras/tests/integration_tests/applications_test.py,35,released back to the system after this test,
keras/tests/integration_tests/applications_test.py,36,(to deal with OOM error on CNTK backend).,
keras/tests/integration_tests/applications_test.py,37,TODO: remove the use of multiprocessing from these tests,
keras/tests/integration_tests/applications_test.py,38,once a memory clearing mechanism,
keras/tests/integration_tests/applications_test.py,39,is implemented in the CNTK backend.,
keras/tests/integration_tests/applications_test.py,47,The error in a subprocess won't propagate,
keras/tests/integration_tests/applications_test.py,48,"to the main process, so we check if the model",
keras/tests/integration_tests/applications_test.py,49,is successfully created by checking if the output shape,
keras/tests/integration_tests/applications_test.py,50,has been put into the queue,
keras/tests/integration_tests/test_datasets.py,15,only run data download tests 20% of the time,
keras/tests/integration_tests/test_datasets.py,16,to speed up frequent testing,
keras/tests/integration_tests/test_datasets.py,31,only run data download tests 20% of the time,
keras/tests/integration_tests/test_datasets.py,32,to speed up frequent testing,
keras/tests/integration_tests/test_datasets.py,47,only run data download tests 20% of the time,
keras/tests/integration_tests/test_datasets.py,48,to speed up frequent testing,
keras/tests/integration_tests/test_datasets.py,57,only run data download tests 20% of the time,
keras/tests/integration_tests/test_datasets.py,58,to speed up frequent testing,
keras/tests/integration_tests/test_datasets.py,70,only run data download tests 20% of the time,
keras/tests/integration_tests/test_datasets.py,71,to speed up frequent testing,
keras/tests/integration_tests/test_datasets.py,80,only run data download tests 20% of the time,
keras/tests/integration_tests/test_datasets.py,81,to speed up frequent testing,
keras/tests/integration_tests/preprocessing/sequence_test.py,18,test padding,
keras/tests/integration_tests/preprocessing/sequence_test.py,24,test truncating,
keras/tests/integration_tests/preprocessing/sequence_test.py,30,test value,
keras/tests/integration_tests/preprocessing/sequence_test.py,40,test padding,
keras/tests/integration_tests/preprocessing/sequence_test.py,50,test truncating,
keras/tests/integration_tests/preprocessing/sequence_test.py,61,test value,
keras/tests/integration_tests/preprocessing/sequence_test.py,75,test with no window size and binary labels,
keras/tests/integration_tests/preprocessing/sequence_test.py,80,test window size and categorical labels,
keras/tests/integration_tests/preprocessing/sequence_test.py,195,"All elements in range(length, 10) should be used as current step",
keras/tests/integration_tests/preprocessing/sequence_test.py,222,all batches have the same size when shuffle is True.,
keras/tests/integration_tests/preprocessing/sequence_test.py,226,last batch will be different if `(samples - length) / stride`,
keras/tests/integration_tests/preprocessing/sequence_test.py,227,is not a multiple of `batch_size`.,
keras/tests/integration_tests/preprocessing/text_test.py,1,-*- coding: utf-8 -*-,
keras/tests/integration_tests/preprocessing/text_test.py,110,"2 OOVs: some, unknown",
keras/tests/integration_tests/preprocessing/text_test.py,112,"Default, without OOV flag",
keras/tests/integration_tests/preprocessing/text_test.py,116,discards 2 OOVs,
keras/tests/integration_tests/preprocessing/text_test.py,118,With OOV feature,
keras/tests/integration_tests/preprocessing/text_test.py,122,OOVs marked in place,
keras/tests/integration_tests/preprocessing/image_test.py,66,Test with sample weights,
keras/tests/integration_tests/preprocessing/image_test.py,77,Test with `shuffle=True`,
keras/tests/integration_tests/preprocessing/image_test.py,82,Check that the sequence is shuffled.,
keras/tests/integration_tests/preprocessing/image_test.py,86,Test without y,
keras/tests/integration_tests/preprocessing/image_test.py,92,Check that the sequence is shuffled.,
keras/tests/integration_tests/preprocessing/image_test.py,95,Test with a single miscellaneous input data array,
keras/tests/integration_tests/preprocessing/image_test.py,107,Test with two miscellaneous inputs,
keras/tests/integration_tests/preprocessing/image_test.py,119,Test cases with `y = None`,
keras/tests/integration_tests/preprocessing/image_test.py,135,Test some failure cases:,
keras/tests/integration_tests/preprocessing/image_test.py,148,Test `flow` behavior as Sequence,
keras/tests/integration_tests/preprocessing/image_test.py,157,Test with `shuffle=True`,
keras/tests/integration_tests/preprocessing/image_test.py,162,Check that the sequence is shuffled.,
keras/tests/integration_tests/preprocessing/image_test.py,165,`on_epoch_end` should reshuffle the sequence.,
keras/tests/integration_tests/preprocessing/image_test.py,182,Test fit with invalid data,
keras/tests/integration_tests/preprocessing/image_test.py,187,Test flow with invalid data,
keras/tests/integration_tests/preprocessing/image_test.py,201,Test grayscale,
keras/tests/integration_tests/preprocessing/image_test.py,204,Test RBG,
keras/tests/integration_tests/preprocessing/image_test.py,207,Test more samples than dims,
keras/tests/integration_tests/preprocessing/image_test.py,217,Test grayscale,
keras/tests/integration_tests/preprocessing/image_test.py,220,Test RBG,
keras/tests/integration_tests/preprocessing/image_test.py,223,Test more samples than dims,
keras/tests/integration_tests/preprocessing/image_test.py,230,create folders and subfolders,
keras/tests/integration_tests/preprocessing/image_test.py,244,save the images in the paths,
keras/tests/integration_tests/preprocessing/image_test.py,249,rotate image class,
keras/tests/integration_tests/preprocessing/image_test.py,251,rotate subfolders,
keras/tests/integration_tests/preprocessing/image_test.py,259,create iterator,
keras/tests/integration_tests/preprocessing/image_test.py,263,check number of classes and images,
keras/tests/integration_tests/preprocessing/image_test.py,268,Test invalid use cases,
keras/tests/integration_tests/preprocessing/image_test.py,284,Test usage as Sequence,
keras/tests/integration_tests/preprocessing/image_test.py,305,save the images in the paths,
keras/tests/integration_tests/preprocessing/image_test.py,313,create iterator,
keras/tests/integration_tests/preprocessing/image_test.py,319,check if input and output have the same shape,
keras/tests/integration_tests/preprocessing/image_test.py,321,check if the input and output images are not the same numpy array,
keras/tests/integration_tests/preprocessing/image_test.py,337,create folders and subfolders,
keras/tests/integration_tests/preprocessing/image_test.py,351,save the images in the paths,
keras/tests/integration_tests/preprocessing/image_test.py,356,rotate image class,
keras/tests/integration_tests/preprocessing/image_test.py,358,rotate subfolders,
keras/tests/integration_tests/preprocessing/image_test.py,366,create iterator,
keras/tests/integration_tests/preprocessing/image_test.py,380,check number of classes and images,
keras/tests/integration_tests/preprocessing/image_test.py,390,Test th data format,
keras/tests/integration_tests/preprocessing/image_test.py,396,Test 2D,
keras/tests/integration_tests/preprocessing/image_test.py,403,Test tf data format,
keras/tests/integration_tests/preprocessing/image_test.py,409,Test 2D,
keras/tests/integration_tests/preprocessing/image_test.py,416,Test invalid use case,
keras/tests/integration_tests/preprocessing/image_test.py,418,not 3D,
keras/tests/integration_tests/preprocessing/image_test.py,420,unknown data_format,
keras/tests/integration_tests/preprocessing/image_test.py,423,neither RGB nor gray-scale,
keras/tests/integration_tests/preprocessing/image_test.py,426,unknown data_format,
keras/tests/integration_tests/preprocessing/image_test.py,429,neither RGB nor gray-scale,
keras/tests/integration_tests/preprocessing/image_test.py,441,Test get_random_transform with predefined seed,
keras/tests/integration_tests/preprocessing/image_test.py,473,Test get_random_transform without any randomness,
keras/tests/integration_tests/preprocessing/image_test.py,511,ImageDataGenerator.standardize should work on batches,
keras/tests/integration_tests/preprocessing/image_test.py,550,Test that loaded image is exactly equal to original.,
keras/tests/integration_tests/preprocessing/image_test.py,562,Test that nothing is changed when target size is equal to original.,
keras/tests/integration_tests/preprocessing/image_test.py,575,Test down-sampling with bilinear interpolation.,
keras/tests/integration_tests/preprocessing/image_test.py,586,Test down-sampling with nearest neighbor interpolation.,
keras/tests/integration_tests/preprocessing/image_test.py,594,Check that exception is raised if interpolation not supported.,
keras/tests/docs/test_documentation.py,21,Functions or classes with less than 'MIN_CODE_SIZE' lines can be ignored,
keras/tests/docs/test_documentation.py,52,We don't need to check this one.,
keras/tests/docs/test_documentation.py,138,Check arguments styling,
keras/tests/docs/test_documentation.py,147,Check arguments order,
keras/tests/docs/test_documentation.py,168,Only test keras' modules,
keras/examples/babi_memnn.py,58,Only select the related substory,
keras/examples/babi_memnn.py,62,Provide all the substories,
keras/examples/babi_memnn.py,110,"QA1 with 10,000 samples",
keras/examples/babi_memnn.py,113,"QA2 with 10,000 samples",
keras/examples/babi_memnn.py,130,Reserve 0 for masking via pad_sequences,
keras/examples/babi_memnn.py,166,placeholders,
keras/examples/babi_memnn.py,170,encoders,
keras/examples/babi_memnn.py,171,embed the input sequence into a sequence of vectors,
keras/examples/babi_memnn.py,176,"output: (samples, story_maxlen, embedding_dim)",
keras/examples/babi_memnn.py,178,embed the input into a sequence of vectors of size query_maxlen,
keras/examples/babi_memnn.py,183,"output: (samples, story_maxlen, query_maxlen)",
keras/examples/babi_memnn.py,185,embed the question into a sequence of vectors,
keras/examples/babi_memnn.py,191,"output: (samples, query_maxlen, embedding_dim)",
keras/examples/babi_memnn.py,193,encode input sequence and questions (which are indices),
keras/examples/babi_memnn.py,194,to sequences of dense vectors,
keras/examples/babi_memnn.py,199,compute a 'match' between the first input vector sequence,
keras/examples/babi_memnn.py,200,and the question vector sequence,
keras/examples/babi_memnn.py,201,"shape: `(samples, story_maxlen, query_maxlen)`",
keras/examples/babi_memnn.py,205,add the match matrix with the second input vector sequence,
keras/examples/babi_memnn.py,206,"(samples, story_maxlen, query_maxlen)",
keras/examples/babi_memnn.py,207,"(samples, query_maxlen, story_maxlen)",
keras/examples/babi_memnn.py,209,concatenate the match matrix with the question vector sequence,
keras/examples/babi_memnn.py,212,the original paper uses a matrix multiplication for this reduction step.,
keras/examples/babi_memnn.py,213,we choose to use a RNN instead.,
keras/examples/babi_memnn.py,214,"(samples, 32)",
keras/examples/babi_memnn.py,216,one regularization layer -- more would probably be needed.,
keras/examples/babi_memnn.py,218,"(samples, vocab_size)",
keras/examples/babi_memnn.py,219,we output a probability distribution over the vocabulary,
keras/examples/babi_memnn.py,222,build the final model,
keras/examples/babi_memnn.py,227,train,
keras/examples/mnist_hierarchical_rnn.py,43,Training parameters.,
keras/examples/mnist_hierarchical_rnn.py,48,Embedding dimensions.,
keras/examples/mnist_hierarchical_rnn.py,52,"The data, split between train and test sets.",
keras/examples/mnist_hierarchical_rnn.py,55,Reshapes data to 4D for Hierarchical RNN.,
keras/examples/mnist_hierarchical_rnn.py,66,Converts class vectors to binary class matrices.,
keras/examples/mnist_hierarchical_rnn.py,72,4D input.,
keras/examples/mnist_hierarchical_rnn.py,75,Encodes a row of pixels using TimeDistributed Wrapper.,
keras/examples/mnist_hierarchical_rnn.py,78,Encodes columns of encoded rows.,
keras/examples/mnist_hierarchical_rnn.py,81,Final predictions and model.,
keras/examples/mnist_hierarchical_rnn.py,88,Training.,
keras/examples/mnist_hierarchical_rnn.py,95,Evaluation.,
keras/examples/variational_autoencoder.py,33,reparameterization trick,
keras/examples/variational_autoencoder.py,34,"instead of sampling from Q(z|X), sample epsilon = N(0,I)",
keras/examples/variational_autoencoder.py,35,z = z_mean + sqrt(var) * epsilon,
keras/examples/variational_autoencoder.py,49,"by default, random_normal has mean = 0 and std = 1.0",
keras/examples/variational_autoencoder.py,72,display a 2D plot of the digit classes in the latent space,
keras/examples/variational_autoencoder.py,84,display a 30x30 2D manifold of digits,
keras/examples/variational_autoencoder.py,88,linearly spaced coordinates corresponding to the 2D plot,
keras/examples/variational_autoencoder.py,89,of digit classes in the latent space,
keras/examples/variational_autoencoder.py,116,MNIST dataset,
keras/examples/variational_autoencoder.py,126,network parameters,
keras/examples/variational_autoencoder.py,133,VAE model = encoder + decoder,
keras/examples/variational_autoencoder.py,134,build encoder model,
keras/examples/variational_autoencoder.py,140,use reparameterization trick to push the sampling out as input,
keras/examples/variational_autoencoder.py,141,"note that ""output_shape"" isn't necessary with the TensorFlow backend",
keras/examples/variational_autoencoder.py,144,instantiate encoder model,
keras/examples/variational_autoencoder.py,149,build decoder model,
keras/examples/variational_autoencoder.py,154,instantiate decoder model,
keras/examples/variational_autoencoder.py,159,instantiate VAE model,
keras/examples/variational_autoencoder.py,175,VAE loss = mse_loss or xent_loss + kl_loss,
keras/examples/variational_autoencoder.py,197,train the autoencoder,
keras/examples/antirectifier.py,52,only valid for 2D tensors,
keras/examples/antirectifier.py,63,global parameters,
keras/examples/antirectifier.py,68,"the data, split between train and test sets",
keras/examples/antirectifier.py,80,convert class vectors to binary class matrices,
keras/examples/antirectifier.py,84,build the model,
keras/examples/antirectifier.py,95,compile the model,
keras/examples/antirectifier.py,100,train the model,
keras/examples/antirectifier.py,107,"next, compare with an equivalent network",
keras/examples/antirectifier.py,108,with2x bigger Dense layers and ReLU,
keras/examples/imdb_cnn_lstm.py,16,Embedding,
keras/examples/imdb_cnn_lstm.py,21,Convolution,
keras/examples/imdb_cnn_lstm.py,26,LSTM,
keras/examples/imdb_cnn_lstm.py,29,Training,
keras/examples/lstm_seq2seq.py,58,Batch size for training.,
keras/examples/lstm_seq2seq.py,59,Number of epochs to train for.,
keras/examples/lstm_seq2seq.py,60,Latent dimensionality of the encoding space.,
keras/examples/lstm_seq2seq.py,61,Number of samples to train on.,
keras/examples/lstm_seq2seq.py,62,Path to the data txt file on disk.,
keras/examples/lstm_seq2seq.py,65,Vectorize the data.,
keras/examples/lstm_seq2seq.py,74,"We use ""tab"" as the ""start sequence"" character",
keras/examples/lstm_seq2seq.py,75,"for the targets, and ""\n"" as ""end sequence"" character.",
keras/examples/lstm_seq2seq.py,119,decoder_target_data is ahead of decoder_input_data by one timestep,
keras/examples/lstm_seq2seq.py,122,decoder_target_data will be ahead by one timestep,
keras/examples/lstm_seq2seq.py,123,and will not include the start character.,
keras/examples/lstm_seq2seq.py,127,Define an input sequence and process it.,
keras/examples/lstm_seq2seq.py,131,We discard `encoder_outputs` and only keep the states.,
keras/examples/lstm_seq2seq.py,134,"Set up the decoder, using `encoder_states` as initial state.",
keras/examples/lstm_seq2seq.py,136,"We set up our decoder to return full output sequences,",
keras/examples/lstm_seq2seq.py,137,and to return internal states as well. We don't use the,
keras/examples/lstm_seq2seq.py,138,"return states in the training model, but we will use them in inference.",
keras/examples/lstm_seq2seq.py,145,Define the model that will turn,
keras/examples/lstm_seq2seq.py,146,`encoder_input_data` & `decoder_input_data` into `decoder_target_data`,
keras/examples/lstm_seq2seq.py,149,Run training,
keras/examples/lstm_seq2seq.py,156,Save model,
keras/examples/lstm_seq2seq.py,159,Next: inference mode (sampling).,
keras/examples/lstm_seq2seq.py,160,Here's the drill:,
keras/examples/lstm_seq2seq.py,161,1) encode input and retrieve initial decoder state,
keras/examples/lstm_seq2seq.py,162,2) run one step of decoder with this initial state,
keras/examples/lstm_seq2seq.py,163,"and a ""start of sequence"" token as target.",
keras/examples/lstm_seq2seq.py,164,Output will be the next target token,
keras/examples/lstm_seq2seq.py,165,3) Repeat with the current target token and current states,
keras/examples/lstm_seq2seq.py,167,Define sampling models,
keras/examples/lstm_seq2seq.py,181,Reverse-lookup token index to decode sequences back to,
keras/examples/lstm_seq2seq.py,182,something readable.,
keras/examples/lstm_seq2seq.py,190,Encode the input as state vectors.,
keras/examples/lstm_seq2seq.py,193,Generate empty target sequence of length 1.,
keras/examples/lstm_seq2seq.py,195,Populate the first character of target sequence with the start character.,
keras/examples/lstm_seq2seq.py,198,Sampling loop for a batch of sequences,
keras/examples/lstm_seq2seq.py,199,"(to simplify, here we assume a batch of size 1).",
keras/examples/lstm_seq2seq.py,206,Sample a token,
keras/examples/lstm_seq2seq.py,211,Exit condition: either hit max length,
keras/examples/lstm_seq2seq.py,212,or find stop character.,
keras/examples/lstm_seq2seq.py,217,Update the target sequence (of length 1).,
keras/examples/lstm_seq2seq.py,221,Update states,
keras/examples/lstm_seq2seq.py,228,Take one sequence (part of the training set),
keras/examples/lstm_seq2seq.py,229,for trying out decoding.,
keras/examples/mnist_siamese.py,96,"the data, split between train and test sets",
keras/examples/mnist_siamese.py,104,create training+test positive and negative pairs,
keras/examples/mnist_siamese.py,111,network definition,
keras/examples/mnist_siamese.py,117,"because we re-use the same instance `base_network`,",
keras/examples/mnist_siamese.py,118,the weights of the network,
keras/examples/mnist_siamese.py,119,will be shared across the two branches,
keras/examples/mnist_siamese.py,128,train,
keras/examples/mnist_siamese.py,136,compute final accuracy on training and test sets,
keras/examples/imdb_cnn.py,17,set parameters:,
keras/examples/imdb_cnn.py,41,we start off with an efficient embedding layer which maps,
keras/examples/imdb_cnn.py,42,our vocab indices into embedding_dims dimensions,
keras/examples/imdb_cnn.py,48,"we add a Convolution1D, which will learn filters",
keras/examples/imdb_cnn.py,49,word group filters of size filter_length:,
keras/examples/imdb_cnn.py,55,we use max pooling:,
keras/examples/imdb_cnn.py,58,We add a vanilla hidden layer:,
keras/examples/imdb_cnn.py,63,"We project onto a single unit output layer, and squash it with a sigmoid:",
keras/examples/mnist_sklearn_wrapper.py,20,input image dimensions,
keras/examples/mnist_sklearn_wrapper.py,23,load training data and do basic data normalization,
keras/examples/mnist_sklearn_wrapper.py,40,convert class vectors to binary class matrices,
keras/examples/mnist_sklearn_wrapper.py,83,epochs is avail for tuning even when not,
keras/examples/mnist_sklearn_wrapper.py,84,an argument to model building function,
keras/examples/mnist_sklearn_wrapper.py,96,validator.best_estimator_ returns sklearn-wrapped version of best model.,
keras/examples/mnist_sklearn_wrapper.py,97,validator.best_estimator_.model returns the (unwrapped) keras model,
keras/examples/mnist_mlp.py,20,"the data, split between train and test sets",
keras/examples/mnist_mlp.py,32,convert class vectors to binary class matrices,
keras/examples/mnist_cnn.py,20,input image dimensions,
keras/examples/mnist_cnn.py,23,"the data, split between train and test sets",
keras/examples/mnist_cnn.py,43,convert class vectors to binary class matrices,
keras/examples/mnist_acgan.py,1,-*- coding: utf-8 -*-,
keras/examples/mnist_acgan.py,50,"we will map a pair of (z, L), where z is a latent vector and L is a",
keras/examples/mnist_acgan.py,51,"label drawn from P_c, to image space (..., 28, 28, 1)",
keras/examples/mnist_acgan.py,57,"upsample to (7, 7, ...)",
keras/examples/mnist_acgan.py,63,"upsample to (14, 14, ...)",
keras/examples/mnist_acgan.py,69,"upsample to (28, 28, ...)",
keras/examples/mnist_acgan.py,74,this is the z space commonly referred to in GAN papers,
keras/examples/mnist_acgan.py,77,this will be our label,
keras/examples/mnist_acgan.py,83,hadamard product between z-space and a class conditional embedding,
keras/examples/mnist_acgan.py,92,"build a relatively standard conv net, with LeakyReLUs as suggested in",
keras/examples/mnist_acgan.py,93,the reference paper,
keras/examples/mnist_acgan.py,119,first output (name=generation) is whether or not the discriminator,
keras/examples/mnist_acgan.py,120,"thinks the image that is being shown is fake, and the second output",
keras/examples/mnist_acgan.py,121,(name=auxiliary) is the class that the discriminator thinks the image,
keras/examples/mnist_acgan.py,122,belongs to.,
keras/examples/mnist_acgan.py,130,batch and latent size taken from the paper,
keras/examples/mnist_acgan.py,135,Adam parameters suggested in https://arxiv.org/abs/1511.06434,
keras/examples/mnist_acgan.py,139,build the discriminator,
keras/examples/mnist_acgan.py,148,build the generator,
keras/examples/mnist_acgan.py,154,get a fake image,
keras/examples/mnist_acgan.py,157,we only want to be able to train generation for the combined model,
keras/examples/mnist_acgan.py,169,"get our mnist data, and force it to be of shape (..., 28, 28, 1) with",
keras/examples/mnist_acgan.py,170,"range [-1, 1]",
keras/examples/mnist_acgan.py,193,get a batch of real images,
keras/examples/mnist_acgan.py,197,generate a new batch of noise,
keras/examples/mnist_acgan.py,200,sample some labels from p_c,
keras/examples/mnist_acgan.py,203,"generate a batch of fake images, using the generated labels as a",
keras/examples/mnist_acgan.py,204,conditioner. We reshape the sampled labels to be,
keras/examples/mnist_acgan.py,205,"(len(image_batch), 1) so that we can feed them into the embedding",
keras/examples/mnist_acgan.py,206,layer as a length one sequence,
keras/examples/mnist_acgan.py,212,use one-sided soft real/fake labels,
keras/examples/mnist_acgan.py,213,"Salimans et al., 2016",
keras/examples/mnist_acgan.py,214,https://arxiv.org/pdf/1606.03498.pdf (Section 3.4),
keras/examples/mnist_acgan.py,220,we don't want the discriminator to also maximize the classification,
keras/examples/mnist_acgan.py,221,"accuracy of the auxiliary classifier on generated images, so we",
keras/examples/mnist_acgan.py,222,don't train discriminator to produce class labels for generated,
keras/examples/mnist_acgan.py,223,images (see https://openreview.net/forum?id=rJXTf9Bxg).,
keras/examples/mnist_acgan.py,224,"To preserve sum of sample weights for the auxiliary classifier,",
keras/examples/mnist_acgan.py,225,we assign sample weight of 2 to the real images.,
keras/examples/mnist_acgan.py,230,see if the discriminator can figure itself out...,
keras/examples/mnist_acgan.py,234,make new noise. we generate 2 * batch size here such that we have,
keras/examples/mnist_acgan.py,235,the generator optimize over an identical number of images as the,
keras/examples/mnist_acgan.py,236,discriminator,
keras/examples/mnist_acgan.py,240,we want to train the generator to trick the discriminator,
keras/examples/mnist_acgan.py,241,"For the generator, we want all the {fake, not-fake} labels to say",
keras/examples/mnist_acgan.py,242,not-fake,
keras/examples/mnist_acgan.py,253,evaluate the testing loss here,
keras/examples/mnist_acgan.py,255,generate a new batch of noise,
keras/examples/mnist_acgan.py,258,sample some labels from p_c and generate images from them,
keras/examples/mnist_acgan.py,267,see if the discriminator can figure itself out...,
keras/examples/mnist_acgan.py,273,make new noise,
keras/examples/mnist_acgan.py,285,generate an epoch report on performance,
keras/examples/mnist_acgan.py,306,save weights every epoch,
keras/examples/mnist_acgan.py,312,generate some digits to display,
keras/examples/mnist_acgan.py,321,get a batch to display,
keras/examples/mnist_acgan.py,325,prepare real images sorted by class label,
keras/examples/mnist_acgan.py,332,"display generated images, white separator, real images",
keras/examples/mnist_acgan.py,338,arrange them into a grid,
keras/examples/cnn_seq2seq.py,54,Batch size for training.,
keras/examples/cnn_seq2seq.py,55,Number of epochs to train for.,
keras/examples/cnn_seq2seq.py,56,Number of samples to train on.,
keras/examples/cnn_seq2seq.py,57,Path to the data txt file on disk.,
keras/examples/cnn_seq2seq.py,60,Vectorize the data.,
keras/examples/cnn_seq2seq.py,69,"We use ""tab"" as the ""start sequence"" character",
keras/examples/cnn_seq2seq.py,70,"for the targets, and ""\n"" as ""end sequence"" character.",
keras/examples/cnn_seq2seq.py,113,decoder_target_data is ahead of decoder_input_data by one timestep,
keras/examples/cnn_seq2seq.py,116,decoder_target_data will be ahead by one timestep,
keras/examples/cnn_seq2seq.py,117,and will not include the start character.,
keras/examples/cnn_seq2seq.py,120,Define an input sequence and process it.,
keras/examples/cnn_seq2seq.py,122,Encoder,
keras/examples/cnn_seq2seq.py,131,Decoder,
keras/examples/cnn_seq2seq.py,138,Attention,
keras/examples/cnn_seq2seq.py,149,Output,
keras/examples/cnn_seq2seq.py,153,Define the model that will turn,
keras/examples/cnn_seq2seq.py,154,`encoder_input_data` & `decoder_input_data` into `decoder_target_data`,
keras/examples/cnn_seq2seq.py,158,Run training,
keras/examples/cnn_seq2seq.py,164,Save model,
keras/examples/cnn_seq2seq.py,167,Next: inference mode (sampling).,
keras/examples/cnn_seq2seq.py,169,Define sampling models,
keras/examples/cnn_seq2seq.py,195,Take one sequence (part of the training set),
keras/examples/cnn_seq2seq.py,196,for trying out decoding.,
keras/examples/cifar10_resnet.py,51,Training parameters,
keras/examples/cifar10_resnet.py,52,orig paper trained all networks with batch_size=128,
keras/examples/cifar10_resnet.py,57,Subtracting pixel mean improves accuracy,
keras/examples/cifar10_resnet.py,60,Model parameter,
keras/examples/cifar10_resnet.py,61,----------------------------------------------------------------------------,
keras/examples/cifar10_resnet.py,62,|      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch,
keras/examples/cifar10_resnet.py,63,Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti,
keras/examples/cifar10_resnet.py,64,|v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2),
keras/examples/cifar10_resnet.py,65,----------------------------------------------------------------------------,
keras/examples/cifar10_resnet.py,66,ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---),
keras/examples/cifar10_resnet.py,67,ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA),
keras/examples/cifar10_resnet.py,68,ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA),
keras/examples/cifar10_resnet.py,69,ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100),
keras/examples/cifar10_resnet.py,70,ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180),
keras/examples/cifar10_resnet.py,71,ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---),
keras/examples/cifar10_resnet.py,72,ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---),
keras/examples/cifar10_resnet.py,73,---------------------------------------------------------------------------,
keras/examples/cifar10_resnet.py,76,Model version,
keras/examples/cifar10_resnet.py,77,"Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)",
keras/examples/cifar10_resnet.py,80,Computed depth from supplied model parameter n,
keras/examples/cifar10_resnet.py,86,"Model name, depth and version",
keras/examples/cifar10_resnet.py,89,Load the CIFAR10 data.,
keras/examples/cifar10_resnet.py,92,Input image dimensions.,
keras/examples/cifar10_resnet.py,95,Normalize data.,
keras/examples/cifar10_resnet.py,99,If subtract pixel mean is enabled,
keras/examples/cifar10_resnet.py,110,Convert class vectors to binary class matrices.,
keras/examples/cifar10_resnet.py,215,Start model definition.,
keras/examples/cifar10_resnet.py,221,Instantiate the stack of residual units,
keras/examples/cifar10_resnet.py,225,first layer but not first stack,
keras/examples/cifar10_resnet.py,226,downsample,
keras/examples/cifar10_resnet.py,233,first layer but not first stack,
keras/examples/cifar10_resnet.py,234,linear projection residual shortcut connection to match,
keras/examples/cifar10_resnet.py,235,changed dims,
keras/examples/cifar10_resnet.py,246,Add classifier on top.,
keras/examples/cifar10_resnet.py,247,v1 does not use BN after last shortcut connection-ReLU,
keras/examples/cifar10_resnet.py,254,Instantiate model.,
keras/examples/cifar10_resnet.py,286,Start model definition.,
keras/examples/cifar10_resnet.py,291,v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths,
keras/examples/cifar10_resnet.py,296,Instantiate the stack of residual units,
keras/examples/cifar10_resnet.py,304,first layer and first stage,
keras/examples/cifar10_resnet.py,309,first layer but not first stage,
keras/examples/cifar10_resnet.py,310,downsample,
keras/examples/cifar10_resnet.py,312,bottleneck residual unit,
keras/examples/cifar10_resnet.py,328,linear projection residual shortcut connection to match,
keras/examples/cifar10_resnet.py,329,changed dims,
keras/examples/cifar10_resnet.py,340,Add classifier on top.,
keras/examples/cifar10_resnet.py,341,v2 has BN-ReLU before Pooling,
keras/examples/cifar10_resnet.py,350,Instantiate model.,
keras/examples/cifar10_resnet.py,366,Prepare model model saving directory.,
keras/examples/cifar10_resnet.py,373,Prepare callbacks for model saving and for learning rate adjustment.,
keras/examples/cifar10_resnet.py,388,"Run training, with or without data augmentation.",
keras/examples/cifar10_resnet.py,399,This will do preprocessing and realtime data augmentation:,
keras/examples/cifar10_resnet.py,401,set input mean to 0 over the dataset,
keras/examples/cifar10_resnet.py,403,set each sample mean to 0,
keras/examples/cifar10_resnet.py,405,divide inputs by std of dataset,
keras/examples/cifar10_resnet.py,407,divide each input by its std,
keras/examples/cifar10_resnet.py,409,apply ZCA whitening,
keras/examples/cifar10_resnet.py,411,epsilon for ZCA whitening,
keras/examples/cifar10_resnet.py,413,randomly rotate images in the range (deg 0 to 180),
keras/examples/cifar10_resnet.py,415,randomly shift images horizontally,
keras/examples/cifar10_resnet.py,417,randomly shift images vertically,
keras/examples/cifar10_resnet.py,419,set range for random shear,
keras/examples/cifar10_resnet.py,421,set range for random zoom,
keras/examples/cifar10_resnet.py,423,set range for random channel shifts,
keras/examples/cifar10_resnet.py,425,set mode for filling points outside the input boundaries,
keras/examples/cifar10_resnet.py,427,"value used for fill_mode = ""constant""",
keras/examples/cifar10_resnet.py,429,randomly flip images,
keras/examples/cifar10_resnet.py,431,randomly flip images,
keras/examples/cifar10_resnet.py,433,set rescaling factor (applied before any other transformation),
keras/examples/cifar10_resnet.py,435,set function that will be applied on each input,
keras/examples/cifar10_resnet.py,437,"image data format, either ""channels_first"" or ""channels_last""",
keras/examples/cifar10_resnet.py,439,fraction of images reserved for validation (strictly between 0 and 1),
keras/examples/cifar10_resnet.py,442,Compute quantities required for featurewise normalization,
keras/examples/cifar10_resnet.py,443,"(std, mean, and principal components if ZCA whitening is applied).",
keras/examples/cifar10_resnet.py,446,Fit the model on the batches generated by datagen.flow().,
keras/examples/cifar10_resnet.py,452,Score trained model.,
keras/examples/variational_autoencoder_deconv.py,35,reparameterization trick,
keras/examples/variational_autoencoder_deconv.py,36,"instead of sampling from Q(z|X), sample eps = N(0,I)",
keras/examples/variational_autoencoder_deconv.py,37,then z = z_mean + sqrt(var)*eps,
keras/examples/variational_autoencoder_deconv.py,51,"by default, random_normal has mean=0 and std=1.0",
keras/examples/variational_autoencoder_deconv.py,74,display a 2D plot of the digit classes in the latent space,
keras/examples/variational_autoencoder_deconv.py,86,display a 30x30 2D manifold of digits,
keras/examples/variational_autoencoder_deconv.py,90,linearly spaced coordinates corresponding to the 2D plot,
keras/examples/variational_autoencoder_deconv.py,91,of digit classes in the latent space,
keras/examples/variational_autoencoder_deconv.py,118,MNIST dataset,
keras/examples/variational_autoencoder_deconv.py,127,network parameters,
keras/examples/variational_autoencoder_deconv.py,135,VAE model = encoder + decoder,
keras/examples/variational_autoencoder_deconv.py,136,build encoder model,
keras/examples/variational_autoencoder_deconv.py,147,shape info needed to build decoder model,
keras/examples/variational_autoencoder_deconv.py,150,generate latent vector Q(z|X),
keras/examples/variational_autoencoder_deconv.py,156,use reparameterization trick to push the sampling out as input,
keras/examples/variational_autoencoder_deconv.py,157,"note that ""output_shape"" isn't necessary with the TensorFlow backend",
keras/examples/variational_autoencoder_deconv.py,160,instantiate encoder model,
keras/examples/variational_autoencoder_deconv.py,165,build decoder model,
keras/examples/variational_autoencoder_deconv.py,184,instantiate decoder model,
keras/examples/variational_autoencoder_deconv.py,189,instantiate VAE model,
keras/examples/variational_autoencoder_deconv.py,203,VAE loss = mse_loss or xent_loss + kl_loss,
keras/examples/variational_autoencoder_deconv.py,223,train the autoencoder,
keras/examples/cifar10_cnn.py,25,"The data, split between train and test sets:",
keras/examples/cifar10_cnn.py,31,Convert class vectors to binary class matrices.,
keras/examples/cifar10_cnn.py,58,initiate RMSprop optimizer,
keras/examples/cifar10_cnn.py,61,Let's train the model using RMSprop,
keras/examples/cifar10_cnn.py,80,This will do preprocessing and realtime data augmentation:,
keras/examples/cifar10_cnn.py,82,set input mean to 0 over the dataset,
keras/examples/cifar10_cnn.py,83,set each sample mean to 0,
keras/examples/cifar10_cnn.py,84,divide inputs by std of the dataset,
keras/examples/cifar10_cnn.py,85,divide each input by its std,
keras/examples/cifar10_cnn.py,86,apply ZCA whitening,
keras/examples/cifar10_cnn.py,87,epsilon for ZCA whitening,
keras/examples/cifar10_cnn.py,88,"randomly rotate images in the range (degrees, 0 to 180)",
keras/examples/cifar10_cnn.py,89,randomly shift images horizontally (fraction of total width),
keras/examples/cifar10_cnn.py,91,randomly shift images vertically (fraction of total height),
keras/examples/cifar10_cnn.py,93,set range for random shear,
keras/examples/cifar10_cnn.py,94,set range for random zoom,
keras/examples/cifar10_cnn.py,95,set range for random channel shifts,
keras/examples/cifar10_cnn.py,96,set mode for filling points outside the input boundaries,
keras/examples/cifar10_cnn.py,98,"value used for fill_mode = ""constant""",
keras/examples/cifar10_cnn.py,99,randomly flip images,
keras/examples/cifar10_cnn.py,100,randomly flip images,
keras/examples/cifar10_cnn.py,101,set rescaling factor (applied before any other transformation),
keras/examples/cifar10_cnn.py,103,set function that will be applied on each input,
keras/examples/cifar10_cnn.py,105,"image data format, either ""channels_first"" or ""channels_last""",
keras/examples/cifar10_cnn.py,107,fraction of images reserved for validation (strictly between 0 and 1),
keras/examples/cifar10_cnn.py,110,Compute quantities required for feature-wise normalization,
keras/examples/cifar10_cnn.py,111,"(std, mean, and principal components if ZCA whitening is applied).",
keras/examples/cifar10_cnn.py,114,Fit the model on the batches generated by datagen.flow().,
keras/examples/cifar10_cnn.py,121,Save model and weights,
keras/examples/cifar10_cnn.py,128,Score trained model.,
keras/examples/imdb_fasttext.py,70,Set parameters:,
keras/examples/imdb_fasttext.py,71,ngram_range = 2 will add bi-grams features,
keras/examples/imdb_fasttext.py,90,Create set of unique n-gram from the training set.,
keras/examples/imdb_fasttext.py,97,Dictionary mapping n-gram token to a unique integer.,
keras/examples/imdb_fasttext.py,98,Integer values are greater than max_features in order,
keras/examples/imdb_fasttext.py,99,to avoid collision with existing features.,
keras/examples/imdb_fasttext.py,104,max_features is the highest integer that could be found in the dataset.,
keras/examples/imdb_fasttext.py,107,Augmenting x_train and x_test with n-grams features,
keras/examples/imdb_fasttext.py,124,we start off with an efficient embedding layer which maps,
keras/examples/imdb_fasttext.py,125,our vocab indices into embedding_dims dimensions,
keras/examples/imdb_fasttext.py,130,"we add a GlobalAveragePooling1D, which will average the embeddings",
keras/examples/imdb_fasttext.py,131,of all words in the document,
keras/examples/imdb_fasttext.py,134,"We project onto a single unit output layer, and squash it with a sigmoid:",
keras/examples/lstm_seq2seq_restore.py,18,Batch size for training.,
keras/examples/lstm_seq2seq_restore.py,19,Number of epochs to train for.,
keras/examples/lstm_seq2seq_restore.py,20,Latent dimensionality of the encoding space.,
keras/examples/lstm_seq2seq_restore.py,21,Number of samples to train on.,
keras/examples/lstm_seq2seq_restore.py,22,Path to the data txt file on disk.,
keras/examples/lstm_seq2seq_restore.py,25,Vectorize the data.  We use the same approach as the training script.,
keras/examples/lstm_seq2seq_restore.py,26,"NOTE: the data must be identical, in order for the character -> integer",
keras/examples/lstm_seq2seq_restore.py,27,mappings to be consistent.,
keras/examples/lstm_seq2seq_restore.py,28,We omit encoding target_texts since they are not needed.,
keras/examples/lstm_seq2seq_restore.py,37,"We use ""tab"" as the ""start sequence"" character",
keras/examples/lstm_seq2seq_restore.py,38,"for the targets, and ""\n"" as ""end sequence"" character.",
keras/examples/lstm_seq2seq_restore.py,75,Restore the model and construct the encoder and decoder.,
keras/examples/lstm_seq2seq_restore.py,78,input_1,
keras/examples/lstm_seq2seq_restore.py,79,lstm_1,
keras/examples/lstm_seq2seq_restore.py,83,input_2,
keras/examples/lstm_seq2seq_restore.py,97,Reverse-lookup token index to decode sequences back to,
keras/examples/lstm_seq2seq_restore.py,98,something readable.,
keras/examples/lstm_seq2seq_restore.py,105,Decodes an input sequence.  Future work should support beam search.,
keras/examples/lstm_seq2seq_restore.py,107,Encode the input as state vectors.,
keras/examples/lstm_seq2seq_restore.py,110,Generate empty target sequence of length 1.,
keras/examples/lstm_seq2seq_restore.py,112,Populate the first character of target sequence with the start character.,
keras/examples/lstm_seq2seq_restore.py,115,Sampling loop for a batch of sequences,
keras/examples/lstm_seq2seq_restore.py,116,"(to simplify, here we assume a batch of size 1).",
keras/examples/lstm_seq2seq_restore.py,123,Sample a token,
keras/examples/lstm_seq2seq_restore.py,128,Exit condition: either hit max length,
keras/examples/lstm_seq2seq_restore.py,129,or find stop character.,
keras/examples/lstm_seq2seq_restore.py,134,Update the target sequence (of length 1).,
keras/examples/lstm_seq2seq_restore.py,138,Update states,
keras/examples/lstm_seq2seq_restore.py,145,Take one sequence (part of the training set),
keras/examples/lstm_seq2seq_restore.py,146,for trying out decoding.,
keras/examples/neural_style_transfer.py,87,these are the weights of the different loss components,
keras/examples/neural_style_transfer.py,92,dimensions of the generated picture.,
keras/examples/neural_style_transfer.py,97,"util function to open, resize and format pictures into appropriate tensors",
keras/examples/neural_style_transfer.py,107,util function to convert a tensor into a valid image,
keras/examples/neural_style_transfer.py,116,Remove zero-center by mean pixel,
keras/examples/neural_style_transfer.py,120,'BGR'->'RGB',
keras/examples/neural_style_transfer.py,125,get tensor representations of our images,
keras/examples/neural_style_transfer.py,129,this will contain our generated image,
keras/examples/neural_style_transfer.py,135,combine the 3 images into a single Keras tensor,
keras/examples/neural_style_transfer.py,140,build the VGG19 network with our 3 images as input,
keras/examples/neural_style_transfer.py,141,the model will be loaded with pre-trained ImageNet weights,
keras/examples/neural_style_transfer.py,146,"get the symbolic outputs of each ""key"" layer (we gave them unique names).",
keras/examples/neural_style_transfer.py,149,compute the neural style loss,
keras/examples/neural_style_transfer.py,150,first we need to define 4 util functions,
keras/examples/neural_style_transfer.py,152,the gram matrix of an image tensor (feature-wise outer product),
keras/examples/neural_style_transfer.py,164,"the ""style loss"" is designed to maintain",
keras/examples/neural_style_transfer.py,165,the style of the reference image in the generated image.,
keras/examples/neural_style_transfer.py,166,It is based on the gram matrices (which capture style) of,
keras/examples/neural_style_transfer.py,167,feature maps from the style reference image,
keras/examples/neural_style_transfer.py,168,and from the generated image,
keras/examples/neural_style_transfer.py,180,an auxiliary loss function,
keras/examples/neural_style_transfer.py,181,"designed to maintain the ""content"" of the",
keras/examples/neural_style_transfer.py,182,base image in the generated image,
keras/examples/neural_style_transfer.py,188,"the 3rd loss function, total variation loss,",
keras/examples/neural_style_transfer.py,189,designed to keep the generated image locally coherent,
keras/examples/neural_style_transfer.py,207,combine these loss functions into a single scalar,
keras/examples/neural_style_transfer.py,226,get the gradients of the generated image wrt the loss,
keras/examples/neural_style_transfer.py,251,this Evaluator class makes it possible,
keras/examples/neural_style_transfer.py,252,to compute loss and gradients in one pass,
keras/examples/neural_style_transfer.py,253,"while retrieving them via two separate functions,",
keras/examples/neural_style_transfer.py,254,"""loss"" and ""grads"". This is done because scipy.optimize",
keras/examples/neural_style_transfer.py,255,"requires separate functions for loss and gradients,",
keras/examples/neural_style_transfer.py,256,but computing them separately would be inefficient.,
keras/examples/neural_style_transfer.py,282,run scipy-based optimization (L-BFGS) over the pixels of the generated image,
keras/examples/neural_style_transfer.py,283,so as to minimize the neural style loss,
keras/examples/neural_style_transfer.py,292,save current generated image,
keras/examples/lstm_stateful.py,46,----------------------------------------------------------,
keras/examples/lstm_stateful.py,47,EDITABLE PARAMETERS,
keras/examples/lstm_stateful.py,48,Read the documentation in the script head for more details,
keras/examples/lstm_stateful.py,49,----------------------------------------------------------,
keras/examples/lstm_stateful.py,51,length of input,
keras/examples/lstm_stateful.py,54,The window length of the moving average used to generate,
keras/examples/lstm_stateful.py,55,the output from the input in the input/output pair used,
keras/examples/lstm_stateful.py,56,to train the LSTM,
keras/examples/lstm_stateful.py,57,"e.g. if tsteps=2 and input=[1, 2, 3, 4, 5],",
keras/examples/lstm_stateful.py,58,"then output=[1.5, 2.5, 3.5, 4.5]",
keras/examples/lstm_stateful.py,61,The input sequence length that the LSTM is trained on for each output point,
keras/examples/lstm_stateful.py,64,"training parameters passed to ""model.fit(...)""",
keras/examples/lstm_stateful.py,68,------------,
keras/examples/lstm_stateful.py,69,MAIN PROGRAM,
keras/examples/lstm_stateful.py,70,------------,
keras/examples/lstm_stateful.py,97,"Since the output is a moving average of the input,",
keras/examples/lstm_stateful.py,98,the first few points of output will be NaN,
keras/examples/lstm_stateful.py,99,and will be dropped from the generated data,
keras/examples/lstm_stateful.py,100,before training the LSTM.,
keras/examples/lstm_stateful.py,101,"Also, when lahead > 1,",
keras/examples/lstm_stateful.py,102,"the preprocessing step later of ""rolling window view""",
keras/examples/lstm_stateful.py,103,will also cause some points to be lost.,
keras/examples/lstm_stateful.py,104,"For aesthetic reasons,",
keras/examples/lstm_stateful.py,105,"in order to maintain generated data length = input_len after pre-processing,",
keras/examples/lstm_stateful.py,106,add a few points to account for the values that will be lost.,
keras/examples/lstm_stateful.py,110,set the target to be a N-point average of the input,
keras/examples/lstm_stateful.py,113,"when lahead > 1, need to convert the input to ""rolling window view""",
keras/examples/lstm_stateful.py,114,https://docs.scipy.org/doc/numpy/reference/generated/numpy.repeat.html,
keras/examples/lstm_stateful.py,121,drop the nan,
keras/examples/lstm_stateful.py,158,split train/test data,
keras/examples/lstm_stateful.py,161,tweak to match with batch_size,
keras/examples/lstm_stateful.py,169,tweak to match with batch_size,
keras/examples/lstm_stateful.py,175,some reshaping,
keras/examples/lstm_stateful.py,196,Note that the last state for sample i in a batch will,
keras/examples/lstm_stateful.py,197,be used as initial state for sample i in the next batch.,
keras/examples/lstm_stateful.py,198,Thus we are simultaneously training on batch_size series with,
keras/examples/lstm_stateful.py,199,lower resolution than the original series contained in data_input.,
keras/examples/lstm_stateful.py,200,Each of these series are offset by one step and can be,
keras/examples/lstm_stateful.py,201,extracted with data_input[i::batch_size].,
keras/examples/lstm_stateful.py,229,----------------------------,
keras/examples/lstm_stateful.py,236,"drop the first ""tsteps-1"" because it is not possible to predict them",
keras/examples/lstm_stateful.py,237,"since the ""previous"" timesteps to use do not exist",
keras/examples/babi_rnn.py,103,Only select the related substory,
keras/examples/babi_rnn.py,107,Provide all the substories,
keras/examples/babi_rnn.py,138,let's not forget that index 0 is reserved,
keras/examples/babi_rnn.py,169,Default QA1 with 1000 samples,
keras/examples/babi_rnn.py,170,challenge = 'tasks_1-20_v1-2/en/qa1_single-supporting-fact_{}.txt',
keras/examples/babi_rnn.py,171,"QA1 with 10,000 samples",
keras/examples/babi_rnn.py,172,challenge = 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt',
keras/examples/babi_rnn.py,173,QA2 with 1000 samples,
keras/examples/babi_rnn.py,175,"QA2 with 10,000 samples",
keras/examples/babi_rnn.py,176,challenge = 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt',
keras/examples/babi_rnn.py,186,Reserve 0 for masking via pad_sequences,
keras/examples/conv_filter_visualization.py,40,"normalize tensor: center on 0., ensure std is 0.25",
keras/examples/conv_filter_visualization.py,45,"clip to [0, 1]",
keras/examples/conv_filter_visualization.py,49,convert to RGB array,
keras/examples/conv_filter_visualization.py,118,we build a loss function that maximizes the activation,
keras/examples/conv_filter_visualization.py,119,of the nth filter of the layer considered,
keras/examples/conv_filter_visualization.py,125,we compute the gradient of the input picture wrt this loss,
keras/examples/conv_filter_visualization.py,128,normalization trick: we normalize the gradient,
keras/examples/conv_filter_visualization.py,131,this function returns the loss and grads given the input picture,
keras/examples/conv_filter_visualization.py,134,we start from a gray image with some random noise,
keras/examples/conv_filter_visualization.py,145,Slowly upscaling towards the original size prevents,
keras/examples/conv_filter_visualization.py,146,a dominating high-frequency of the to visualized structure,
keras/examples/conv_filter_visualization.py,147,as it would occur if we directly compute the 412d-image.,
keras/examples/conv_filter_visualization.py,148,Behaves as a better starting point for each following dimension,
keras/examples/conv_filter_visualization.py,149,and therefore avoids poor local minima,
keras/examples/conv_filter_visualization.py,151,we run gradient ascent for e.g. 20 steps,
keras/examples/conv_filter_visualization.py,156,"some filters get stuck to 0, we can skip them",
keras/examples/conv_filter_visualization.py,160,Calculate upscaled dimension,
keras/examples/conv_filter_visualization.py,163,Upscale,
keras/examples/conv_filter_visualization.py,170,decode the resulting input image,
keras/examples/conv_filter_visualization.py,190,the filters that have the highest loss are assumed to be better-looking.,
keras/examples/conv_filter_visualization.py,191,we will only keep the top n*n filters.,
keras/examples/conv_filter_visualization.py,195,build a black picture with enough space for,
keras/examples/conv_filter_visualization.py,196,"e.g. our 8 x 8 filters of size 412 x 412, with a 5px margin in between",
keras/examples/conv_filter_visualization.py,202,fill the picture with our saved filters,
keras/examples/conv_filter_visualization.py,212,save the result to disk,
keras/examples/conv_filter_visualization.py,215,this is the placeholder for the input images,
keras/examples/conv_filter_visualization.py,219,"get the symbolic outputs of each ""key"" layer (we gave them unique names).",
keras/examples/conv_filter_visualization.py,225,Compute to be processed filter range,
keras/examples/conv_filter_visualization.py,235,iterate through each filter and generate its corresponding image,
keras/examples/conv_filter_visualization.py,244,Finally draw and store the best filters to disk,
keras/examples/conv_filter_visualization.py,249,the name of the layer we want to visualize,
keras/examples/conv_filter_visualization.py,250,(see model definition at keras/applications/vgg16.py),
keras/examples/conv_filter_visualization.py,253,build the VGG16 network with ImageNet weights,
keras/examples/conv_filter_visualization.py,258,example function call,
keras/examples/conv_lstm.py,14,We create a layer which take as input movies of shape,
keras/examples/conv_lstm.py,15,"(n_frames, width, height, channels) and returns a movie",
keras/examples/conv_lstm.py,16,of identical shape.,
keras/examples/conv_lstm.py,42,Artificial data generation:,
keras/examples/conv_lstm.py,43,Generate movies with 3 to 7 moving squares inside.,
keras/examples/conv_lstm.py,44,"The squares are of shape 1x1 or 2x2 pixels,",
keras/examples/conv_lstm.py,45,which move linearly over time.,
keras/examples/conv_lstm.py,46,For convenience we first create movies with bigger width and height (80x80),
keras/examples/conv_lstm.py,47,and at the end we select a 40x40 window.,
keras/examples/conv_lstm.py,57,Add 3 to 7 moving squares,
keras/examples/conv_lstm.py,61,Initial position,
keras/examples/conv_lstm.py,64,Direction of motion,
keras/examples/conv_lstm.py,68,Size of the square,
keras/examples/conv_lstm.py,77,Make it more robust by adding noise.,
keras/examples/conv_lstm.py,78,"The idea is that if during inference,",
keras/examples/conv_lstm.py,79,"the value of the pixel is not exactly one,",
keras/examples/conv_lstm.py,80,we need to train the network to be robust and still,
keras/examples/conv_lstm.py,81,consider it as a pixel belonging to a square.,
keras/examples/conv_lstm.py,89,Shift the ground truth by 1,
keras/examples/conv_lstm.py,95,Cut to a 40x40 window,
keras/examples/conv_lstm.py,102,Train the network,
keras/examples/conv_lstm.py,107,Testing the network on one movie,
keras/examples/conv_lstm.py,108,feed it with the first 7 positions and then,
keras/examples/conv_lstm.py,109,predict the new positions,
keras/examples/conv_lstm.py,119,And then compare the predictions,
keras/examples/conv_lstm.py,120,to the ground truth,
keras/examples/mnist_swwae.py,97,This example assume 'channels_first' data format.,
keras/examples/mnist_swwae.py,100,input image dimensions,
keras/examples/mnist_swwae.py,103,"the data, split between train and test sets",
keras/examples/mnist_swwae.py,116,The size of the kernel used for the MaxPooling2D,
keras/examples/mnist_swwae.py,118,The total number of feature maps at each layer,
keras/examples/mnist_swwae.py,120,The sizes of the pooling kernel at each layer,
keras/examples/mnist_swwae.py,122,The convolution kernel size,
keras/examples/mnist_swwae.py,124,Number of epochs to train for,
keras/examples/mnist_swwae.py,126,Batch size during training,
keras/examples/mnist_swwae.py,130,if using a 5 layer net of pool_size = 2,
keras/examples/mnist_swwae.py,136,if using a 3 layer net of pool_size = 3,
keras/examples/mnist_swwae.py,144,Shape of input to train on (note that model is fully convolutional however),
keras/examples/mnist_swwae.py,146,"The final list of the size of axis=1 for all layers, including input",
keras/examples/mnist_swwae.py,149,"First build the encoder, all the while keeping track of the 'where' masks",
keras/examples/mnist_swwae.py,152,We push the 'where' masks to the following list,
keras/examples/mnist_swwae.py,161,"Now build the decoder, and use the stored 'where' masks to place the features",
keras/examples/mnist_swwae.py,168,Use hard_simgoid to clip range of reconstruction,
keras/examples/mnist_swwae.py,171,"Define the model and it's mean square error loss, and compile it with Adam",
keras/examples/mnist_swwae.py,175,Fit the model,
keras/examples/mnist_swwae.py,181,Plot,
keras/examples/class_activation_maps.py,1,-*- coding: utf-8 -*-,
keras/examples/class_activation_maps.py,13,Set an appropriate image file,
keras/examples/class_activation_maps.py,21,,
keras/examples/class_activation_maps.py,22,The following parameters can be changed to other models,
keras/examples/class_activation_maps.py,23,that use global average pooling.,
keras/examples/class_activation_maps.py,24,e.g.) InceptionResnetV2 / NASNetLarge,
keras/examples/class_activation_maps.py,30,,
keras/examples/class_activation_maps.py,32,number of imagenet classes,
keras/examples/class_activation_maps.py,74,1. load image,
keras/examples/class_activation_maps.py,79,2. prediction,
keras/examples/class_activation_maps.py,86,4. post processing,
keras/examples/class_activation_maps.py,89,5. plot image+cam to original size,
keras/examples/addition_rnn.py,1,-*- coding: utf-8 -*-,
keras/examples/addition_rnn.py,28,noqa,
keras/examples/addition_rnn.py,85,Parameters for the model and dataset.,
keras/examples/addition_rnn.py,90,"Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of",
keras/examples/addition_rnn.py,91,int is DIGITS.,
keras/examples/addition_rnn.py,94,"All the numbers, plus sign and space for padding.",
keras/examples/addition_rnn.py,106,Skip any addition questions we've already seen,
keras/examples/addition_rnn.py,107,Also skip any such that x+Y == Y+x (hence the sorting).,
keras/examples/addition_rnn.py,112,Pad the data with spaces such that it is always MAXLEN.,
keras/examples/addition_rnn.py,116,Answers can be of maximum size DIGITS + 1.,
keras/examples/addition_rnn.py,119,"Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the",
keras/examples/addition_rnn.py,120,space used for padding.),
keras/examples/addition_rnn.py,134,"Shuffle (x, y) in unison as the later parts of x will almost all be larger",
keras/examples/addition_rnn.py,135,digits.,
keras/examples/addition_rnn.py,141,Explicitly set apart 10% for validation data that we never train over.,
keras/examples/addition_rnn.py,154,"Try replacing GRU, or SimpleRNN.",
keras/examples/addition_rnn.py,162,"""Encode"" the input sequence using an RNN, producing an output of HIDDEN_SIZE.",
keras/examples/addition_rnn.py,163,"Note: In a situation where your input sequences have a variable length,",
keras/examples/addition_rnn.py,164,"use input_shape=(None, num_feature).",
keras/examples/addition_rnn.py,166,"As the decoder RNN's input, repeatedly provide with the last output of",
keras/examples/addition_rnn.py,167,RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum,
keras/examples/addition_rnn.py,168,"length of output, e.g., when DIGITS=3, max output is 999+999=1998.",
keras/examples/addition_rnn.py,170,The decoder RNN could be multiple layers stacked or a single layer.,
keras/examples/addition_rnn.py,172,"By setting return_sequences to True, return not only the last output but",
keras/examples/addition_rnn.py,173,"all the outputs so far in the form of (num_samples, timesteps,",
keras/examples/addition_rnn.py,174,output_dim). This is necessary as TimeDistributed in the below expects,
keras/examples/addition_rnn.py,175,the first dimension to be the timesteps.,
keras/examples/addition_rnn.py,178,Apply a dense layer to the every temporal slice of an input. For each of step,
keras/examples/addition_rnn.py,179,"of the output sequence, decide which character should be chosen.",
keras/examples/addition_rnn.py,186,Train the model each generation and show predictions against the validation,
keras/examples/addition_rnn.py,187,dataset.,
keras/examples/addition_rnn.py,196,Select 10 samples from the validation set at random so we can visualize,
keras/examples/addition_rnn.py,197,errors.,
keras/examples/imdb_lstm.py,26,cut texts after this number of words (among top max_features most common words),
keras/examples/imdb_lstm.py,47,try using different optimizers and different optimizer configs,
keras/examples/imdb_bidirectional_lstm.py,18,cut texts after this number of words,
keras/examples/imdb_bidirectional_lstm.py,19,(among top max_features most common words),
keras/examples/imdb_bidirectional_lstm.py,42,try using different optimizers and different optimizer configs,
keras/examples/mnist_denoising_autoencoder.py,35,MNIST dataset,
keras/examples/mnist_denoising_autoencoder.py,44,Generate corrupted MNIST images by adding noise with normal dist,
keras/examples/mnist_denoising_autoencoder.py,45,centered at 0.5 and std=0.5,
keras/examples/mnist_denoising_autoencoder.py,54,Network parameters,
keras/examples/mnist_denoising_autoencoder.py,59,Encoder/Decoder number of CNN layers and filters per layer,
keras/examples/mnist_denoising_autoencoder.py,62,Build the Autoencoder Model,
keras/examples/mnist_denoising_autoencoder.py,63,First build the Encoder Model,
keras/examples/mnist_denoising_autoencoder.py,66,Stack of Conv2D blocks,
keras/examples/mnist_denoising_autoencoder.py,67,Notes:,
keras/examples/mnist_denoising_autoencoder.py,68,1) Use Batch Normalization before ReLU on deep networks,
keras/examples/mnist_denoising_autoencoder.py,69,2) Use MaxPooling2D as alternative to strides>1,
keras/examples/mnist_denoising_autoencoder.py,70,- faster but not as good as strides>1,
keras/examples/mnist_denoising_autoencoder.py,78,Shape info needed to build Decoder Model,
keras/examples/mnist_denoising_autoencoder.py,81,Generate the latent vector,
keras/examples/mnist_denoising_autoencoder.py,85,Instantiate Encoder Model,
keras/examples/mnist_denoising_autoencoder.py,89,Build the Decoder Model,
keras/examples/mnist_denoising_autoencoder.py,94,Stack of Transposed Conv2D blocks,
keras/examples/mnist_denoising_autoencoder.py,95,Notes:,
keras/examples/mnist_denoising_autoencoder.py,96,1) Use Batch Normalization before ReLU on deep networks,
keras/examples/mnist_denoising_autoencoder.py,97,2) Use UpSampling2D as alternative to strides>1,
keras/examples/mnist_denoising_autoencoder.py,98,- faster but not as good as strides>1,
keras/examples/mnist_denoising_autoencoder.py,112,Instantiate Decoder Model,
keras/examples/mnist_denoising_autoencoder.py,116,Autoencoder = Encoder + Decoder,
keras/examples/mnist_denoising_autoencoder.py,117,Instantiate Autoencoder Model,
keras/examples/mnist_denoising_autoencoder.py,123,Train the autoencoder,
keras/examples/mnist_denoising_autoencoder.py,130,Predict the Autoencoder output from corrupted test images,
keras/examples/mnist_denoising_autoencoder.py,133,Display the 1st 8 corrupted and denoised images,
keras/examples/pretrained_word_embeddings.py,36,"first, build index mapping words in the embeddings set",
keras/examples/pretrained_word_embeddings.py,37,to their embedding vector,
keras/examples/pretrained_word_embeddings.py,50,"second, prepare text samples and their labels",
keras/examples/pretrained_word_embeddings.py,53,list of text samples,
keras/examples/pretrained_word_embeddings.py,54,dictionary mapping label name to numeric id,
keras/examples/pretrained_word_embeddings.py,55,list of label ids,
keras/examples/pretrained_word_embeddings.py,67,skip header,
keras/examples/pretrained_word_embeddings.py,75,"finally, vectorize the text samples into a 2D integer tensor",
keras/examples/pretrained_word_embeddings.py,89,split the data into a training set and a validation set,
keras/examples/pretrained_word_embeddings.py,103,prepare embedding matrix,
keras/examples/pretrained_word_embeddings.py,111,words not found in embedding index will be all-zeros.,
keras/examples/pretrained_word_embeddings.py,114,load pre-trained word embeddings into an Embedding layer,
keras/examples/pretrained_word_embeddings.py,115,note that we set trainable = False so as to keep the embeddings fixed,
keras/examples/pretrained_word_embeddings.py,124,train a 1D convnet with global maxpooling,
keras/examples/mnist_transfer_cnn.py,28,input image dimensions,
keras/examples/mnist_transfer_cnn.py,30,number of convolutional filters to use,
keras/examples/mnist_transfer_cnn.py,32,size of pooling area for max pooling,
keras/examples/mnist_transfer_cnn.py,34,convolution kernel size,
keras/examples/mnist_transfer_cnn.py,54,convert class vectors to binary class matrices,
keras/examples/mnist_transfer_cnn.py,74,"the data, split between train and test sets",
keras/examples/mnist_transfer_cnn.py,77,create two datasets one with digits below 5 and one with 5 and above,
keras/examples/mnist_transfer_cnn.py,88,define two groups of layers: feature (convolutions) and classification (dense),
keras/examples/mnist_transfer_cnn.py,109,create complete model,
keras/examples/mnist_transfer_cnn.py,112,train model for 5-digit classification [0..4],
keras/examples/mnist_transfer_cnn.py,117,freeze feature layers and rebuild model,
keras/examples/mnist_transfer_cnn.py,121,transfer: train dense layers for new classification task [5..9],
keras/examples/mnist_net2net.py,76,image shape,
keras/examples/mnist_net2net.py,78,image shape,
keras/examples/mnist_net2net.py,79,number of classes,
keras/examples/mnist_net2net.py,83,load and pre-process data,
keras/examples/mnist_net2net.py,99,knowledge transfer algorithms,
keras/examples/mnist_net2net.py,142,"add small noise to break symmetry, so that student model will have",
keras/examples/mnist_net2net.py,143,full capacity later,
keras/examples/mnist_net2net.py,192,"add small noise to break symmetry, so that student model will have",
keras/examples/mnist_net2net.py,193,full capacity later,
keras/examples/mnist_net2net.py,226,methods to construct teacher_model and student_models,
keras/examples/mnist_net2net.py,263,a wider conv1 compared to teacher_model,
keras/examples/mnist_net2net.py,270,a wider fc1 compared to teacher model,
keras/examples/mnist_net2net.py,274,The weights for other layers need to be copied from teacher_model,
keras/examples/mnist_net2net.py,275,"to student_model, except for widened layers",
keras/examples/mnist_net2net.py,276,"and their immediate downstreams, which will be initialized separately.",
keras/examples/mnist_net2net.py,277,For this example there are no other layers that need to be copied.,
keras/examples/mnist_net2net.py,314,add another conv2d layer to make original conv2 deeper,
keras/examples/mnist_net2net.py,327,add another fc layer to make original fc1 deeper,
keras/examples/mnist_net2net.py,329,"net2deeper for fc layer with relu, is just an identity initializer",
keras/examples/mnist_net2net.py,338,copy weights for other layers,
keras/examples/mnist_net2net.py,351,experiments setup,
keras/examples/mnist_net2net.py,399,run the experiments,
keras/examples/deep_dream.py,33,These are the names of the layers,
keras/examples/deep_dream.py,34,"for which we try to maximize activation,",
keras/examples/deep_dream.py,35,as well as their weight in the final loss,
keras/examples/deep_dream.py,36,we try to maximize.,
keras/examples/deep_dream.py,37,You can tweak these setting to obtain new visual effects.,
keras/examples/deep_dream.py,49,"Util function to open, resize and format pictures",
keras/examples/deep_dream.py,50,into appropriate tensors.,
keras/examples/deep_dream.py,59,Util function to convert a tensor into a valid image.,
keras/examples/deep_dream.py,73,Build the InceptionV3 network with our placeholder.,
keras/examples/deep_dream.py,74,The model will be loaded with pre-trained ImageNet weights.,
keras/examples/deep_dream.py,80,"Get the symbolic outputs of each ""key"" layer (we gave them unique names).",
keras/examples/deep_dream.py,83,Define the loss.,
keras/examples/deep_dream.py,86,Add the L2 norm of the features of a layer to the loss.,
keras/examples/deep_dream.py,91,We avoid border artifacts by only involving non-border pixels in the loss.,
keras/examples/deep_dream.py,98,Compute the gradients of the dream wrt the loss.,
keras/examples/deep_dream.py,100,Normalize gradients.,
keras/examples/deep_dream.py,103,Set up function to retrieve the value,
keras/examples/deep_dream.py,104,of the loss and gradients given an input image.,
keras/examples/deep_dream.py,158,Playing with these hyperparameters will also allow you to achieve new effects,
keras/examples/deep_dream.py,159,Gradient ascent step size,
keras/examples/deep_dream.py,160,Number of scales at which to run gradient ascent,
keras/examples/deep_dream.py,161,Size ratio between scales,
keras/examples/deep_dream.py,162,Number of ascent steps per scale,
keras/examples/image_ocr.py,1,-*- coding: utf-8 -*-,
keras/examples/image_ocr.py,70,character classes and matching regex filter,
keras/examples/image_ocr.py,77,"this creates larger ""blotches"" of noise which look",
keras/examples/image_ocr.py,78,more realistic than just adding gaussian noise,
keras/examples/image_ocr.py,79,assumes greyscale with pixels ranging from 0 to 1,
keras/examples/image_ocr.py,90,paints the string in a random location the bounding box,
keras/examples/image_ocr.py,91,"also uses a random font, a slight random rotation,",
keras/examples/image_ocr.py,92,and a random amount of speckle noise,
keras/examples/image_ocr.py,97,White,
keras/examples/image_ocr.py,99,this font list works in CentOS 7,
keras/examples/image_ocr.py,119,teach the RNN translational invariance by,
keras/examples/image_ocr.py,120,"fitting text box randomly on canvas, with some room to rotate",
keras/examples/image_ocr.py,135,grab single channel,
keras/examples/image_ocr.py,167,Translation of characters to unique integer values,
keras/examples/image_ocr.py,175,Reverse translation of numerical classes back to characters,
keras/examples/image_ocr.py,179,CTC Blank,
keras/examples/image_ocr.py,186,only a-z and space..probably not to difficult,
keras/examples/image_ocr.py,187,to expand to uppercase and symbols,
keras/examples/image_ocr.py,194,Uses generator functions to supply train/test with,
keras/examples/image_ocr.py,195,data. Image renderings and text are created on the fly,
keras/examples/image_ocr.py,196,each time with random perturbations,
keras/examples/image_ocr.py,217,num_words can be independent of the epoch size due to the use of generators,
keras/examples/image_ocr.py,218,"as max_string_len grows, num_words can grow",
keras/examples/image_ocr.py,236,monogram file is sorted by frequency in english speech,
keras/examples/image_ocr.py,245,bigram file contains common word pairings in english speech,
keras/examples/image_ocr.py,258,interlace to mix up the easy and hard words,
keras/examples/image_ocr.py,271,"each time an image is requested from train/val/test, a new random",
keras/examples/image_ocr.py,272,painting of the text is performed,
keras/examples/image_ocr.py,274,width and height are backwards from typical Keras convention,
keras/examples/image_ocr.py,275,because width is the time dimension when it gets fed into the RNN,
keras/examples/image_ocr.py,286,Mix in some blank inputs.  This seems to be important for,
keras/examples/image_ocr.py,287,achieving translational invariance,
keras/examples/image_ocr.py,312,used for visualization only,
keras/examples/image_ocr.py,314,dummy data for dummy loss function,
keras/examples/image_ocr.py,344,rebind the paint function to implement curriculum learning,
keras/examples/image_ocr.py,361,the actual loss calc occurs here despite it not being,
keras/examples/image_ocr.py,362,an internal Keras loss function,
keras/examples/image_ocr.py,366,the 2 is critical here since the first couple outputs of the RNN,
keras/examples/image_ocr.py,367,tend to be garbage:,
keras/examples/image_ocr.py,372,"For a real OCR application, this should be beam search with a dictionary",
keras/examples/image_ocr.py,373,"and language model.  For this example, best path is sufficient.",
keras/examples/image_ocr.py,446,Input Parameters,
keras/examples/image_ocr.py,452,Network parameters,
keras/examples/image_ocr.py,493,cuts down input size going into RNN:,
keras/examples/image_ocr.py,496,Two layers of bidirectional GRUs,
keras/examples/image_ocr.py,497,"GRU seems to work as well, if not better than LSTM:",
keras/examples/image_ocr.py,509,transforms RNN output to character activations:,
keras/examples/image_ocr.py,519,Keras doesn't currently support loss funcs with extra parameters,
keras/examples/image_ocr.py,520,so CTC loss is implemented in a lambda layer,
keras/examples/image_ocr.py,525,clipnorm seems to speeds up convergence,
keras/examples/image_ocr.py,534,"the loss calc occurs elsewhere, so use a dummy lambda func for the loss",
keras/examples/image_ocr.py,541,captures output of softmax so we can decode the output during visualization,
keras/examples/image_ocr.py,559,increase to wider images and start at epoch 20.,
keras/examples/image_ocr.py,560,The learned weights are reloaded,
keras/examples/mnist_irnn.py,34,"the data, split between train and test sets",
keras/examples/mnist_irnn.py,47,convert class vectors to binary class matrices,
keras/examples/lstm_text_generation.py,38,cut the text in semi-redundant sequences of maxlen characters,
keras/examples/lstm_text_generation.py,57,build the model: a single LSTM,
keras/examples/lstm_text_generation.py,68,helper function to sample an index from a probability array,
keras/examples/lstm_text_generation.py,78,Function invoked at end of each epoch. Prints generated text.,
keras/examples/neural_doodle.py,62,Command line arguments,
keras/examples/neural_doodle.py,88,RGB,
keras/examples/neural_doodle.py,89,determine image sizes based on target_mask,
keras/examples/neural_doodle.py,100,"To get better generation qualities, use more conv layers for style features",
keras/examples/neural_doodle.py,105,helper functions for reading/processing images,
keras/examples/neural_doodle.py,120,Remove zero-center by mean pixel,
keras/examples/neural_doodle.py,124,'BGR'->'RGB',
keras/examples/neural_doodle.py,176,Create tensor variables for images,
keras/examples/neural_doodle.py,191,Create tensor variables for masks,
keras/examples/neural_doodle.py,197,index constants for images and tasks variables,
keras/examples/neural_doodle.py,200,"Build image model, mask model and use layer outputs as features",
keras/examples/neural_doodle.py,201,image model as VGG19,
keras/examples/neural_doodle.py,204,mask model as a series of pooling,
keras/examples/neural_doodle.py,216,Collect features from image_model and task_model,
keras/examples/neural_doodle.py,228,Define loss functions,
keras/examples/neural_doodle.py,298,"Overall loss is the weighted sum of content_loss, style_loss and tv_loss",
keras/examples/neural_doodle.py,299,Each individual loss uses features from image/mask models.,
keras/examples/neural_doodle.py,317,Evaluator class for computing efficiency,
keras/examples/neural_doodle.py,364,Generate images by iterative optimization,
keras/examples/neural_doodle.py,376,save current generated image,
keras/docs/structure.py,1,-*- coding: utf-8 -*-,
keras/docs/structure.py,96,"For each class to document, it is possible to:",
keras/docs/structure.py,97,"1) Document only the class: [classA, classB, ...]",
keras/docs/structure.py,98,"2) Document all its methods: [classA, (classB, ""*"")]",
keras/docs/structure.py,99,3) Choose which methods to document (methods listed as strings):,
keras/docs/structure.py,100,"[classA, (classB, [""method1"", ""method2"", ...]), ...]",
keras/docs/structure.py,101,4) Choose which methods to document (methods listed as qualified names):,
keras/docs/structure.py,102,"[classA, (classB, [module.classB.method1, module.classB.method2, ...]), ...]",
keras/docs/autogen.py,1,-*- coding: utf-8 -*-,
keras/docs/autogen.py,69,in case the class inherits from object and does not,
keras/docs/autogen.py,70,define __init__,
keras/docs/autogen.py,131,Place marker for later reinjection.,
keras/docs/autogen.py,138,Remove the computed number of leading white spaces from each line.,
keras/docs/autogen.py,140,Usually lines have at least 4 additional leading spaces.,
keras/docs/autogen.py,141,"These have to be removed, but first the list roots have to be detected.",
keras/docs/autogen.py,146,All the other lines get simply the 4 leading space (if present) removed,
keras/docs/autogen.py,148,Fix text lines after lists,
keras/docs/autogen.py,155,If it is a list element,
keras/docs/autogen.py,173,"First, extract code blocks and process them.",
keras/docs/autogen.py,181,Place marker in docstring for later reinjection.,
keras/docs/autogen.py,185,Remove leading spaces.,
keras/docs/autogen.py,190,Most code snippets have 3 or 4 more leading spaces,
keras/docs/autogen.py,191,"on inner lines, but not all. Remove them.",
keras/docs/autogen.py,211,Format docstring lists.,
keras/docs/autogen.py,232,"`docstring` has changed, so we can't use `next_section_idx` anymore",
keras/docs/autogen.py,233,we have to recompute it,
keras/docs/autogen.py,236,Format docstring section titles.,
keras/docs/autogen.py,241,Strip all remaining leading spaces.,
keras/docs/autogen.py,245,Reinject list blocks.,
keras/docs/autogen.py,249,Reinject code blocks.,
keras/docs/autogen.py,262,"if there is something on the line, add 8 spaces.",
keras/docs/autogen.py,358,skip docstring,
keras/docs/autogen.py,363,next line might be empty.,
keras/docs/autogen.py,368,copy the rest of the file.,
keras/docs/autogen.py,445,Save module page.,
keras/docs/autogen.py,446,"Either insert content into existing page,",
keras/docs/autogen.py,447,or create page otherwise.,
