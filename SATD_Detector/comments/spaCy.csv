file path,line #,comment,satd
spaCy/setup.py,1,!/usr/bin/env python,
spaCy/setup.py,79,"On Mac, use libc++ because Apple deprecated use of",
spaCy/setup.py,80,libstdc,
spaCy/setup.py,83,g++ (used by unix compiler on mac) links to libstdc++ as a default lib.,
spaCy/setup.py,84,See: https://stackoverflow.com/questions/1653047/avoid-linking-to-libstdc,
spaCy/setup.py,88,By subclassing build_extensions we have the actual compiler that will be used which is really known only after finalize_options,
spaCy/setup.py,89,http://stackoverflow.com/questions/724664/python-distutils-how-to-get-a-compiler-that-is-going-to-be-used,
spaCy/setup.py,169,???,
spaCy/setup.py,170,Imported from patch from @mikepb,
spaCy/setup.py,171,See Issue #267. Running blind here...,
spaCy/fabfile.py,1,coding: utf-8,
spaCy/bin/train_word_vectors.py,1,!/usr/bin/env python,
spaCy/bin/train_word_vectors.py,24,This is to keep the input to the blank model (which doesn't,
spaCy/bin/train_word_vectors.py,25,sentencize) from being too long. It works particularly well with,
spaCy/bin/train_word_vectors.py,26,the output of [WikiExtractor](https://github.com/attardi/wikiextractor),
spaCy/bin/load_reddit.py,1,coding: utf8,
spaCy/bin/load_reddit.py,93,Python flushes standard streams on exit; redirect remaining output,
spaCy/bin/load_reddit.py,94,to devnull to avoid another BrokenPipeError at shutdown,
spaCy/bin/load_reddit.py,97,Python exits with error code 1 on EPIPE,
spaCy/bin/cythonize.py,1,!/usr/bin/env python,
spaCy/bin/cythonize.py,61,See Issue #791,
spaCy/bin/cythonize.py,65,There are ways of installing Cython that don't result in a cython,
spaCy/bin/cythonize.py,66,"executable on the path, see gh-2397.",
spaCy/bin/ud/ud_train.py,1,flake8: noqa,
spaCy/bin/ud/ud_train.py,36,,
spaCy/bin/ud/ud_train.py,37,Data reading,
spaCy/bin/ud/ud_train.py,38,,
spaCy/bin/ud/ud_train.py,64,sd is spacy doc; cd is conllu doc,
spaCy/bin/ud/ud_train.py,65,"cs is conllu sent, ct is conllu token",
spaCy/bin/ud/ud_train.py,151,"Flatten the conll annotations, and adjust the head indices",
spaCy/bin/ud/ud_train.py,160,Construct text if necessary,
spaCy/bin/ud/ud_train.py,178,,
spaCy/bin/ud/ud_train.py,179,Data transforms for spaCy,
spaCy/bin/ud/ud_train.py,180,,
spaCy/bin/ud/ud_train.py,195,,
spaCy/bin/ud/ud_train.py,196,Evaluation,
spaCy/bin/ud/ud_train.py,197,,
spaCy/bin/ud/ud_train.py,295,"def get_sent_conllu(sent, sent_id):",
spaCy/bin/ud/ud_train.py,296,"lines = [""# sent_id = {sent_id}"".format(sent_id=sent_id)]",
spaCy/bin/ud/ud_train.py,331,,
spaCy/bin/ud/ud_train.py,332,Initialization,
spaCy/bin/ud/ud_train.py,333,,
spaCy/bin/ud/ud_train.py,391,,
spaCy/bin/ud/ud_train.py,392,Command line helpers,
spaCy/bin/ud/ud_train.py,393,,
spaCy/bin/ud/run_eval.py,15,All languages in spaCy - in UD format (note that Norwegian is 'no' instead of 'nb'),
spaCy/bin/ud/run_eval.py,21,Non-parsing tasks that will be evaluated (works for default models),
spaCy/bin/ud/run_eval.py,24,Tasks that will be evaluated if check_parse=True (does not work for default models),
spaCy/bin/ud/run_eval.py,27,Minimum frequency an error should have to be printed,
spaCy/bin/ud/run_eval.py,30,Maximum number of errors printed per category,
spaCy/bin/ud/run_eval.py,80,assume the corpus is largely blinded when there are less than 1% unique tokens,
spaCy/bin/ud/run_eval.py,100,ignore treebanks where the texts are not publicly available,
spaCy/bin/ud/run_eval.py,104,check the tokens in the gold annotation to keep only the biggest treebank per language,
spaCy/bin/ud/run_eval.py,122,STEP 1: tokenize text,
spaCy/bin/ud/run_eval.py,129,STEP 2: record stats and timings,
spaCy/bin/ud/run_eval.py,136,STEP 3: evaluate predicted tokens and features,
spaCy/bin/ud/run_eval.py,144,STEP 4: format the scoring results,
spaCy/bin/ud/run_eval.py,170,saving to CSV with ; seperator so blinding ; in the example output,
spaCy/bin/ud/run_eval.py,180,STEP 5: print the formatted results to CSV,
spaCy/bin/ud/run_eval.py,199,nested try blocks to ensure the code can continue with the next iteration after a failure,
spaCy/bin/ud/run_eval.py,240,fetching all relevant treebank from the directory,
spaCy/bin/ud/run_eval.py,247,multi-lang model,
spaCy/bin/ud/run_eval.py,252,initialize all models with the multi-lang model,
spaCy/bin/ud/run_eval.py,255,add default models if we don't want to evaluate parsing info,
spaCy/bin/ud/run_eval.py,257,Norwegian is 'nb' in spaCy but 'no' in the UD corpora,
spaCy/bin/ud/run_eval.py,263,language-specific trained models,
spaCy/bin/ud/ud_run_test.py,1,flake8: noqa,
spaCy/bin/ud/ud_run_test.py,21,"from spacy.morphology import Fused_begin, Fused_inside",
spaCy/bin/ud/ud_run_test.py,41,,
spaCy/bin/ud/ud_run_test.py,42,Data reading,
spaCy/bin/ud/ud_run_test.py,43,,
spaCy/bin/ud/ud_run_test.py,52,,
spaCy/bin/ud/ud_run_test.py,53,Evaluation,
spaCy/bin/ud/ud_run_test.py,54,,
spaCy/bin/ud/ud_run_test.py,190,"Happy case: we get a perfect split, with each letter accounted for.",
spaCy/bin/ud/ud_run_test.py,193,"Unideal, but at least lengths match.",
spaCy/bin/ud/ud_run_test.py,203,"Let's say word is 6 long, and there are three subtokens. The orths",
spaCy/bin/ud/ud_run_test.py,204,"*must* equal the original string. Arbitrarily, split [4, 1, 1]",
spaCy/bin/ud/ud_run_test.py,261,,
spaCy/bin/ud/ud_run_test.py,262,Initialization,
spaCy/bin/ud/ud_run_test.py,263,,
spaCy/bin/ud/conll17_ud_eval.py,1,!/usr/bin/env python,
spaCy/bin/ud/conll17_ud_eval.py,2,flake8: noqa,
spaCy/bin/ud/conll17_ud_eval.py,4,CoNLL 2017 UD Parsing evaluation script.,
spaCy/bin/ud/conll17_ud_eval.py,5,,
spaCy/bin/ud/conll17_ud_eval.py,6,"Compatible with Python 2.7 and 3.2+, can be used either as a module",
spaCy/bin/ud/conll17_ud_eval.py,7,or a standalone executable.,
spaCy/bin/ud/conll17_ud_eval.py,8,,
spaCy/bin/ud/conll17_ud_eval.py,9,"Copyright 2017 Institute of Formal and Applied Linguistics (UFAL),",
spaCy/bin/ud/conll17_ud_eval.py,10,"Faculty of Mathematics and Physics, Charles University, Czech Republic.",
spaCy/bin/ud/conll17_ud_eval.py,11,,
spaCy/bin/ud/conll17_ud_eval.py,12,Changelog:,
spaCy/bin/ud/conll17_ud_eval.py,13,- [02 Jan 2017] Version 0.9: Initial release,
spaCy/bin/ud/conll17_ud_eval.py,14,- [25 Jan 2017] Version 0.9.1: Fix bug in LCS alignment computation,
spaCy/bin/ud/conll17_ud_eval.py,15,- [10 Mar 2017] Version 1.0: Add documentation and test,
spaCy/bin/ud/conll17_ud_eval.py,16,Compare HEADs correctly using aligned words,
spaCy/bin/ud/conll17_ud_eval.py,17,Allow evaluation with errorneous spaces in forms,
spaCy/bin/ud/conll17_ud_eval.py,18,Compare forms in LCS case insensitively,
spaCy/bin/ud/conll17_ud_eval.py,19,Detect cycles and multiple root nodes,
spaCy/bin/ud/conll17_ud_eval.py,20,Compute AlignedAccuracy,
spaCy/bin/ud/conll17_ud_eval.py,22,Command line usage,
spaCy/bin/ud/conll17_ud_eval.py,23,------------------,
spaCy/bin/ud/conll17_ud_eval.py,24,conll17_ud_eval.py [-v] [-w weights_file] gold_conllu_file system_conllu_file,
spaCy/bin/ud/conll17_ud_eval.py,25,,
spaCy/bin/ud/conll17_ud_eval.py,26,"- if no -v is given, only the CoNLL17 UD Shared Task evaluation LAS metrics",
spaCy/bin/ud/conll17_ud_eval.py,27,is printed,
spaCy/bin/ud/conll17_ud_eval.py,28,"- if -v is given, several metrics are printed (as precision, recall, F1 score,",
spaCy/bin/ud/conll17_ud_eval.py,29,and in case the metric is computed on aligned words also accuracy on these):,
spaCy/bin/ud/conll17_ud_eval.py,30,- Tokens: how well do the gold tokens match system tokens,
spaCy/bin/ud/conll17_ud_eval.py,31,- Sentences: how well do the gold sentences match system sentences,
spaCy/bin/ud/conll17_ud_eval.py,32,- Words: how well can the gold words be aligned to system words,
spaCy/bin/ud/conll17_ud_eval.py,33,"- UPOS: using aligned words, how well does UPOS match",
spaCy/bin/ud/conll17_ud_eval.py,34,"- XPOS: using aligned words, how well does XPOS match",
spaCy/bin/ud/conll17_ud_eval.py,35,"- Feats: using aligned words, how well does FEATS match",
spaCy/bin/ud/conll17_ud_eval.py,36,"- AllTags: using aligned words, how well does UPOS+XPOS+FEATS match",
spaCy/bin/ud/conll17_ud_eval.py,37,"- Lemmas: using aligned words, how well does LEMMA match",
spaCy/bin/ud/conll17_ud_eval.py,38,"- UAS: using aligned words, how well does HEAD match",
spaCy/bin/ud/conll17_ud_eval.py,39,"- LAS: using aligned words, how well does HEAD+DEPREL(ignoring subtypes) match",
spaCy/bin/ud/conll17_ud_eval.py,40,"- if weights_file is given (with lines containing deprel-weight pairs),",
spaCy/bin/ud/conll17_ud_eval.py,41,one more metric is shown:,
spaCy/bin/ud/conll17_ud_eval.py,42,"- WeightedLAS: as LAS, but each deprel (ignoring subtypes) has different weight",
spaCy/bin/ud/conll17_ud_eval.py,44,API usage,
spaCy/bin/ud/conll17_ud_eval.py,45,---------,
spaCy/bin/ud/conll17_ud_eval.py,46,- load_conllu(file),
spaCy/bin/ud/conll17_ud_eval.py,47,- loads CoNLL-U file from given file object to an internal representation,
spaCy/bin/ud/conll17_ud_eval.py,48,- the file object should return str on both Python 2 and Python 3,
spaCy/bin/ud/conll17_ud_eval.py,49,- raises UDError exception if the given file cannot be loaded,
spaCy/bin/ud/conll17_ud_eval.py,50,"- evaluate(gold_ud, system_ud)",
spaCy/bin/ud/conll17_ud_eval.py,51,- evaluate the given gold and system CoNLL-U files (loaded with load_conllu),
spaCy/bin/ud/conll17_ud_eval.py,52,- raises UDError if the concatenated tokens of gold and system file do not match,
spaCy/bin/ud/conll17_ud_eval.py,53,"- returns a dictionary with the metrics described above, each metrics having",
spaCy/bin/ud/conll17_ud_eval.py,54,"four fields: precision, recall, f1 and aligned_accuracy (when using aligned",
spaCy/bin/ud/conll17_ud_eval.py,55,"words, otherwise this is None)",
spaCy/bin/ud/conll17_ud_eval.py,57,Description of token matching,
spaCy/bin/ud/conll17_ud_eval.py,58,-----------------------------,
spaCy/bin/ud/conll17_ud_eval.py,59,"In order to match tokens of gold file and system file, we consider the text",
spaCy/bin/ud/conll17_ud_eval.py,60,resulting from concatenation of gold tokens and text resulting from,
spaCy/bin/ud/conll17_ud_eval.py,61,"concatenation of system tokens. These texts should match -- if they do not,",
spaCy/bin/ud/conll17_ud_eval.py,62,the evaluation fails.,
spaCy/bin/ud/conll17_ud_eval.py,63,,
spaCy/bin/ud/conll17_ud_eval.py,64,"If the texts do match, every token is represented as a range in this original",
spaCy/bin/ud/conll17_ud_eval.py,65,"text, and tokens are equal only if their range is the same.",
spaCy/bin/ud/conll17_ud_eval.py,67,Description of word matching,
spaCy/bin/ud/conll17_ud_eval.py,68,----------------------------,
spaCy/bin/ud/conll17_ud_eval.py,69,"When matching words of gold file and system file, we first match the tokens.",
spaCy/bin/ud/conll17_ud_eval.py,70,"The words which are also tokens are matched as tokens, but words in multi-word",
spaCy/bin/ud/conll17_ud_eval.py,71,tokens have to be handled differently.,
spaCy/bin/ud/conll17_ud_eval.py,72,,
spaCy/bin/ud/conll17_ud_eval.py,73,"To handle multi-word tokens, we start by finding ""multi-word spans"".",
spaCy/bin/ud/conll17_ud_eval.py,74,Multi-word span is a span in the original text such that,
spaCy/bin/ud/conll17_ud_eval.py,75,- it contains at least one multi-word token,
spaCy/bin/ud/conll17_ud_eval.py,76,- all multi-word tokens in the span (considering both gold and system ones),
spaCy/bin/ud/conll17_ud_eval.py,77,"are completely inside the span (i.e., they do not ""stick out"")",
spaCy/bin/ud/conll17_ud_eval.py,78,- the multi-word span is as small as possible,
spaCy/bin/ud/conll17_ud_eval.py,79,,
spaCy/bin/ud/conll17_ud_eval.py,80,"For every multi-word span, we align the gold and system words completely",
spaCy/bin/ud/conll17_ud_eval.py,81,inside this span using LCS on their FORMs. The words not intersecting,
spaCy/bin/ud/conll17_ud_eval.py,82,(even partially) any multi-word span are then aligned as tokens.,
spaCy/bin/ud/conll17_ud_eval.py,93,CoNLL-U column names,
spaCy/bin/ud/conll17_ud_eval.py,96,UD Error is used when raising exceptions in this module,
spaCy/bin/ud/conll17_ud_eval.py,100,Load given CoNLL-U file into internal representation,
spaCy/bin/ud/conll17_ud_eval.py,102,Internal representation classes,
spaCy/bin/ud/conll17_ud_eval.py,105,Characters of all the tokens in the whole file.,
spaCy/bin/ud/conll17_ud_eval.py,106,Whitespace between tokens is not included.,
spaCy/bin/ud/conll17_ud_eval.py,108,List of UDSpan instances with start&end indices into `characters`.,
spaCy/bin/ud/conll17_ud_eval.py,110,List of UDWord instances.,
spaCy/bin/ud/conll17_ud_eval.py,112,List of UDSpan instances with start&end indices into `characters`.,
spaCy/bin/ud/conll17_ud_eval.py,117,"Note that self.end marks the first position **after the end** of span,",
spaCy/bin/ud/conll17_ud_eval.py,118,"so we can use characters[start:end] or range(start, end).",
spaCy/bin/ud/conll17_ud_eval.py,133,"Span of this word (or MWT, see below) within ud_representation.characters.",
spaCy/bin/ud/conll17_ud_eval.py,135,"10 columns of the CoNLL-U file: ID, FORM, LEMMA,...",
spaCy/bin/ud/conll17_ud_eval.py,137,is_multiword==True means that this word is part of a multi-word token.,
spaCy/bin/ud/conll17_ud_eval.py,138,"In that case, self.span marks the span of the whole multi-word token.",
spaCy/bin/ud/conll17_ud_eval.py,140,Reference to the UDWord instance representing the HEAD (or None if root).,
spaCy/bin/ud/conll17_ud_eval.py,142,Let's ignore language-specific deprel subtypes.,
spaCy/bin/ud/conll17_ud_eval.py,147,Load the CoNLL-U file,
spaCy/bin/ud/conll17_ud_eval.py,157,Handle sentence start boundaries,
spaCy/bin/ud/conll17_ud_eval.py,159,Skip comments,
spaCy/bin/ud/conll17_ud_eval.py,162,Start a new sentence,
spaCy/bin/ud/conll17_ud_eval.py,166,Add parent UDWord links and check there are no cycles,
spaCy/bin/ud/conll17_ud_eval.py,184,Check there is a single root node,
spaCy/bin/ud/conll17_ud_eval.py,189,End the sentence,
spaCy/bin/ud/conll17_ud_eval.py,194,Read next token/word,
spaCy/bin/ud/conll17_ud_eval.py,199,Skip empty nodes,
spaCy/bin/ud/conll17_ud_eval.py,203,Delete spaces from FORM so gold.characters == system.characters,
spaCy/bin/ud/conll17_ud_eval.py,204,even if one of them tokenizes the space.,
spaCy/bin/ud/conll17_ud_eval.py,209,Save token,
spaCy/bin/ud/conll17_ud_eval.py,214,Handle multi-word tokens to save word(s),
spaCy/bin/ud/conll17_ud_eval.py,228,Basic tokens/words,
spaCy/bin/ud/conll17_ud_eval.py,251,Evaluate the gold and system treebanks (loaded using load_conllu).,
spaCy/bin/ud/conll17_ud_eval.py,279,We represent root parents in both gold and system data by '0'.,
spaCy/bin/ud/conll17_ud_eval.py,280,"For gold data, we represent non-root parent by corresponding gold word.",
spaCy/bin/ud/conll17_ud_eval.py,281,"For system data, we represent non-root parent by either gold word aligned",
spaCy/bin/ud/conll17_ud_eval.py,282,"to parent system nodes, or by None if no gold words is aligned to the parent.",
spaCy/bin/ud/conll17_ud_eval.py,304,avoid counting the same mistake twice,
spaCy/bin/ud/conll17_ud_eval.py,310,avoid counting the same mistake twice,
spaCy/bin/ud/conll17_ud_eval.py,346,Return score for whole aligned words,
spaCy/bin/ud/conll17_ud_eval.py,368,We know gold_words[gi].is_multiword or system_words[si].is_multiword.,
spaCy/bin/ud/conll17_ud_eval.py,369,"Find the start of the multiword span (gs, ss), so the multiword span is minimal.",
spaCy/bin/ud/conll17_ud_eval.py,370,Initialize multiword_span_end characters index.,
spaCy/bin/ud/conll17_ud_eval.py,375,if system_words[si].is_multiword,
spaCy/bin/ud/conll17_ud_eval.py,381,Find the end of the multiword span,
spaCy/bin/ud/conll17_ud_eval.py,382,(so both gi and si are pointing to the word following the multiword span end).,
spaCy/bin/ud/conll17_ud_eval.py,410,"A: Multi-word tokens => align via LCS within the whole ""multiword span"".",
spaCy/bin/ud/conll17_ud_eval.py,416,Store aligned words,
spaCy/bin/ud/conll17_ud_eval.py,428,B: No multi-word token => align according to spans.,
spaCy/bin/ud/conll17_ud_eval.py,442,Check that underlying character sequences do match,
spaCy/bin/ud/conll17_ud_eval.py,456,Align words,
spaCy/bin/ud/conll17_ud_eval.py,459,Compute the F1-scores,
spaCy/bin/ud/conll17_ud_eval.py,483,Add WeightedLAS if weights are given,
spaCy/bin/ud/conll17_ud_eval.py,497,Ignore comments and empty lines,
spaCy/bin/ud/conll17_ud_eval.py,514,Load CoNLL-U files,
spaCy/bin/ud/conll17_ud_eval.py,518,Load weights if requested,
spaCy/bin/ud/conll17_ud_eval.py,524,Parse arguments,
spaCy/bin/ud/conll17_ud_eval.py,537,Use verbose if weights are supplied,
spaCy/bin/ud/conll17_ud_eval.py,541,Evaluate,
spaCy/bin/ud/conll17_ud_eval.py,544,Print the evaluation,
spaCy/bin/ud/conll17_ud_eval.py,566,"Tests, which can be executed with `python -m unittest conll17_ud_eval`.",
spaCy/bin/ud/__init__.py,1,noqa: F401,
spaCy/bin/ud/__init__.py,2,noqa: F401,
spaCy/bin/wiki_entity_linking/train_descriptions.py,1,coding: utf-8,
spaCy/bin/wiki_entity_linking/train_descriptions.py,26,Set min. acceptable loss to avoid a 'mean of empty slice' warning by numpy,
spaCy/bin/wiki_entity_linking/train_descriptions.py,29,Reasonable default to stop training when things are not improving,
spaCy/bin/wiki_entity_linking/train_descriptions.py,77,copy this list so that shuffling does not affect other functions,
spaCy/bin/wiki_entity_linking/train_descriptions.py,100,"in general, continue training if we haven't reached our ideal min yet",
spaCy/bin/wiki_entity_linking/train_descriptions.py,103,store the best loss and track how long it's been,
spaCy/bin/wiki_entity_linking/train_descriptions.py,110,stop learning if we haven't seen improvement since the last few iterations,
spaCy/bin/wiki_entity_linking/train_descriptions.py,134,very simple encoder-decoder model,
spaCy/bin/wiki_entity_linking/kb_creator.py,1,coding: utf-8,
spaCy/bin/wiki_entity_linking/kb_creator.py,27,Create the knowledge base from Wikidata entries,
spaCy/bin/wiki_entity_linking/kb_creator.py,35,read the mappings from file,
spaCy/bin/wiki_entity_linking/kb_creator.py,39,check the length of the nlp vectors,
spaCy/bin/wiki_entity_linking/kb_creator.py,51,"filter the entities for in the KB by frequency, because there's just too much data (8M entities) otherwise",
spaCy/bin/wiki_entity_linking/kb_creator.py,106,adding aliases with prior probabilities,
spaCy/bin/wiki_entity_linking/kb_creator.py,107,"we can read this file sequentially, it's sorted by alias, and then by count",
spaCy/bin/wiki_entity_linking/kb_creator.py,110,skip header,
spaCy/bin/wiki_entity_linking/kb_creator.py,124,done reading the previous alias --> output,
spaCy/bin/wiki_entity_linking/wiki_namespaces.py,1,coding: utf8,
spaCy/bin/wiki_entity_linking/wiki_namespaces.py,4,"List of meta pages in Wikidata, should be kept out of the Knowledge base",
spaCy/bin/wiki_entity_linking/wiki_namespaces.py,27,TODO: add more cases from non-English WP's,
spaCy/bin/wiki_entity_linking/wiki_namespaces.py,29,"List of prefixes that refer to Wikipedia ""file"" pages",
spaCy/bin/wiki_entity_linking/wiki_namespaces.py,32,"List of prefixes that refer to Wikipedia ""category"" pages",
spaCy/bin/wiki_entity_linking/wiki_namespaces.py,35,"List of prefixes that refer to Wikipedia ""meta"" pages",
spaCy/bin/wiki_entity_linking/wiki_namespaces.py,36,these will/should be matched ignoring case,
spaCy/bin/wiki_entity_linking/wikidata_pretrain_kb.py,1,coding: utf-8,
spaCy/bin/wiki_entity_linking/wikidata_pretrain_kb.py,78,STEP 0: set up IO,
spaCy/bin/wiki_entity_linking/wikidata_pretrain_kb.py,82,STEP 1: Load the NLP object,
spaCy/bin/wiki_entity_linking/wikidata_pretrain_kb.py,86,check the length of the nlp vectors,
spaCy/bin/wiki_entity_linking/wikidata_pretrain_kb.py,93,STEP 2: create prior probabilities from WP,
spaCy/bin/wiki_entity_linking/wikidata_pretrain_kb.py,95,It takes about 2h to process 1000M lines of Wikipedia XML dump,
spaCy/bin/wiki_entity_linking/wikidata_pretrain_kb.py,103,STEP 3: calculate entity frequencies,
spaCy/bin/wiki_entity_linking/wikidata_pretrain_kb.py,110,STEP 4: reading definitions and (possibly) descriptions from WikiData or from file,
spaCy/bin/wiki_entity_linking/wikidata_pretrain_kb.py,112,It takes about 10h to process 55M lines of Wikidata JSON dump,
spaCy/bin/wiki_entity_linking/wikidata_pretrain_kb.py,137,STEP 5: Getting gold entities from Wikipedia,
spaCy/bin/wiki_entity_linking/wikidata_pretrain_kb.py,151,STEP 6: creating the actual KB,
spaCy/bin/wiki_entity_linking/wikidata_pretrain_kb.py,152,It takes ca. 30 minutes to pretrain the entity embeddings,
spaCy/bin/wiki_entity_linking/wiki_io.py,1,coding: utf-8,
spaCy/bin/wiki_entity_linking/wiki_io.py,7,"min() needed to prevent error on windows, cf https://stackoverflow.com/questions/52404416/",
spaCy/bin/wiki_entity_linking/wiki_io.py,13,Entity definition: WP title -> WD ID,
spaCy/bin/wiki_entity_linking/wiki_io.py,25,skip header,
spaCy/bin/wiki_entity_linking/wiki_io.py,32,Entity aliases from WD: WD ID -> WD alias,
spaCy/bin/wiki_entity_linking/wiki_io.py,45,skip header,
spaCy/bin/wiki_entity_linking/wiki_io.py,61,skip header,
spaCy/bin/wiki_entity_linking/wiki_io.py,69,Entity descriptions from WD: WD ID -> WD alias,
spaCy/bin/wiki_entity_linking/wiki_io.py,81,skip header,
spaCy/bin/wiki_entity_linking/wiki_io.py,88,Entity counts from WP: WP title -> count,
spaCy/bin/wiki_entity_linking/wiki_io.py,90,Write entity counts for quick access later,
spaCy/bin/wiki_entity_linking/wiki_io.py,95,skip header,
spaCy/bin/wiki_entity_linking/wiki_io.py,101,alias = splits[0],
spaCy/bin/wiki_entity_linking/wiki_io.py,122,skip header,
spaCy/bin/wiki_entity_linking/wikidata_processor.py,1,coding: utf-8,
spaCy/bin/wiki_entity_linking/wikidata_processor.py,14,Read the JSON wiki data and parse out the entities. Takes about 7-10h to parse 55M lines.,
spaCy/bin/wiki_entity_linking/wikidata_processor.py,15,get latest-all.json.bz2 from https://dumps.wikimedia.org/wikidatawiki/entities/,
spaCy/bin/wiki_entity_linking/wikidata_processor.py,19,filter: currently defined as OR: one hit suffices to be removed from further processing,
spaCy/bin/wiki_entity_linking/wikidata_processor.py,22,punctuation,
spaCy/bin/wiki_entity_linking/wikidata_processor.py,25,letters etc,
spaCy/bin/wiki_entity_linking/wikidata_processor.py,29,instance of,
spaCy/bin/wiki_entity_linking/wikidata_processor.py,30,subclass,
spaCy/bin/wiki_entity_linking/wikidata_processor.py,37,parse appropriate fields - depending on what we need in the KB,
spaCy/bin/wiki_entity_linking/wikidata_processor.py,83,parsing all properties that refer to other entities,
spaCy/bin/wiki_entity_linking/wikidata_processor.py,150,log final number of lines processed,
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,1,coding: utf-8,
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,36,non-greedy,
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,37,non-greedy,
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,39,find the links,
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,42,"match on interwiki links, e.g. `en:` or `:fr:`",
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,44,match on Namespace: optionally preceded by a :,
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,79,we attempt at reading the article's ID (but not the revision or contributor ID),
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,90,only processing prior probabilities from true training (non-dev) articles,
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,103,write all aliases and their entities and count occurrences to file,
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,116,remove everything after # as this is not part of the title but refers to a specific paragraph,
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,118,wikipedia titles are always capitalized,
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,140,"ignore the entity if it points to a ""meta"" page",
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,142,"this is a simple [[link]], with the alias the same as the mention",
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,148,"in wiki format, the link is written as [[entity|alias]]",
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,153,specific wiki format  [[alias (specification)|]],
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,216,Start reading new page,
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,221,finished reading this page,
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,252,start reading text within a page,
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,259,stop reading text within a page (we assume a new page doesn't start on the same line),
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,263,read the ID of this article (outside the revision portion of the document),
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,271,This should never happen ...,
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,274,read the title of this article (outside the revision portion of the document),
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,283,ignore meta Wikipedia pages,
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,287,remove the text tags,
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,293,stop processing if this is a redirect page,
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,297,"get the raw text without markup etc, keeping only interwiki links",
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,305,remove bolding & italic markup,
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,309,remove nested {{info}} statements by removing the inner/smallest ones first and iterating,
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,315,non-greedy match excluding a nested {,
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,322,remove HTML comments,
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,325,remove Category and File statements,
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,329,remove multiple =,
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,338,remove refs (non-greedy match),
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,342,remove additional wikiformatting,
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,346,change special characters back to normal ones,
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,353,remove multiple spaces,
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,361,read the text char by char to get the right offsets for the interwiki links,
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,379,switch from reading entity to mention in the [[entity|mention]] pattern,
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,404,we just finished reading an entity,
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,408,Ignore cases with nested structures like File: handles etc,
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,526,TODO: check that alias == doc.text[start:end],
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,550,custom length cut-off,
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,556,custom length cut-off,
spaCy/bin/wiki_entity_linking/wikipedia_processor.py,560,remove 'enumeration' sentences (occurs often on Wikipedia),
spaCy/bin/wiki_entity_linking/wikidata_train_entity_linker.py,1,coding: utf-8,
spaCy/bin/wiki_entity_linking/wikidata_train_entity_linker.py,64,STEP 0: set up IO,
spaCy/bin/wiki_entity_linking/wikidata_train_entity_linker.py,68,STEP 1 : load the NLP object,
spaCy/bin/wiki_entity_linking/wikidata_train_entity_linker.py,73,check that there is a NER component in the pipeline,
spaCy/bin/wiki_entity_linking/wikidata_train_entity_linker.py,80,STEP 2: read the training dataset previously created from WP,
spaCy/bin/wiki_entity_linking/wikidata_train_entity_linker.py,90,STEP 3: create and train an entity linking pipe,
spaCy/bin/wiki_entity_linking/wikidata_train_entity_linker.py,106,only train Entity Linking,
spaCy/bin/wiki_entity_linking/wikidata_train_entity_linker.py,125,"we either process the whole training file, or just a part each epoch",
spaCy/bin/wiki_entity_linking/wikidata_train_entity_linker.py,155,re-read the dev_data (data is returned as a generator),
spaCy/bin/wiki_entity_linking/wikidata_train_entity_linker.py,162,STEP 4: write the NLP pipeline (now including an EL model) to file,
spaCy/bin/wiki_entity_linking/entity_linker_evaluation.py,1,coding: utf-8,
spaCy/bin/wiki_entity_linking/entity_linker_evaluation.py,20,"Assume that we have no labeled negatives in the data (i.e. cases where true_entity is ""NIL"")",
spaCy/bin/wiki_entity_linking/entity_linker_evaluation.py,21,"Therefore, if candidate_is_correct then we have a true positive and never a true negative.",
spaCy/bin/wiki_entity_linking/entity_linker_evaluation.py,25,A wrong prediction (e.g. Q42 != Q3) counts both as a FP as well as a FN.,
spaCy/bin/wiki_entity_linking/entity_linker_evaluation.py,110,only evaluating on positive examples,
spaCy/bin/wiki_entity_linking/entity_linker_evaluation.py,118,using only context,
spaCy/bin/wiki_entity_linking/entity_linker_evaluation.py,123,measuring combined accuracy (prior + context),
spaCy/bin/wiki_entity_linking/entity_linker_evaluation.py,152,the gold annotations are not complete so we can't evaluate missing annotations as 'wrong',
spaCy/bin/wiki_entity_linking/entity_linker_evaluation.py,173,the gold annotations are not complete so we can't evaluate missing annotations as 'wrong',
spaCy/spacy/compat.py,1,coding: utf8,
spaCy/spacy/compat.py,40,noqa: F401,
spaCy/spacy/compat.py,42,noqa: F401,
spaCy/spacy/compat.py,55,See: https://github.com/benjaminp/six/blob/master/six.py,
spaCy/spacy/compat.py,62,noqa: F821,
spaCy/spacy/compat.py,63,noqa: F821,
spaCy/spacy/compat.py,64,noqa: F821,
spaCy/spacy/compat.py,85,"Important: if no encoding is set, string becomes ""b'...'""",
spaCy/spacy/compat.py,110,https://stackoverflow.com/q/26554135/6400719,
spaCy/spacy/compat.py,112,this should only be on Py2.7 and windows,
spaCy/spacy/compat.py,173,"We only want to unescape the unicode, so we first must protect the other",
spaCy/spacy/compat.py,174,backslashes.,
spaCy/spacy/compat.py,176,Now we remove that protection for the unicode.,
spaCy/spacy/compat.py,179,Now we unescape by evaling the string with the AST. This can't execute,
spaCy/spacy/compat.py,180,code -- it only does the representational level.,
spaCy/spacy/scorer.py,1,coding: utf8,
spaCy/spacy/scorer.py,61,catch ValueError: Only one class present in y_true.,
spaCy/spacy/scorer.py,62,ROC AUC score is not defined in that case.,
spaCy/spacy/scorer.py,171,binary multiclass,
spaCy/spacy/scorer.py,174,other multiclass,
spaCy/spacy/scorer.py,180,multilabel,
spaCy/spacy/scorer.py,263,"None is indistinct, so we can't just add it to the set",
spaCy/spacy/scorer.py,264,"Multiple (None, None) deps are possible",
spaCy/spacy/scorer.py,278,Find all NER labels in gold and doc,
spaCy/spacy/scorer.py,280,Set up all labels for per type scoring and prepare gold per type,
spaCy/spacy/scorer.py,288,"Find all candidate labels, for all and per type",
spaCy/spacy/scorer.py,300,Scores per ent,
spaCy/spacy/scorer.py,304,Score for all ents,
spaCy/spacy/scorer.py,350,,
spaCy/spacy/scorer.py,351,,
spaCy/spacy/scorer.py,352,The following implementation of roc_auc_score() is adapted from,
spaCy/spacy/scorer.py,353,"scikit-learn, which is distributed under the following license:",
spaCy/spacy/scorer.py,354,,
spaCy/spacy/scorer.py,355,New BSD License,
spaCy/spacy/scorer.py,356,,
spaCy/spacy/scorer.py,357,Copyright (c) 2007–2019 The scikit-learn developers.,
spaCy/spacy/scorer.py,358,All rights reserved.,
spaCy/spacy/scorer.py,359,,
spaCy/spacy/scorer.py,360,,
spaCy/spacy/scorer.py,361,"Redistribution and use in source and binary forms, with or without",
spaCy/spacy/scorer.py,362,"modification, are permitted provided that the following conditions are met:",
spaCy/spacy/scorer.py,363,,
spaCy/spacy/scorer.py,364,"a. Redistributions of source code must retain the above copyright notice,",
spaCy/spacy/scorer.py,365,this list of conditions and the following disclaimer.,
spaCy/spacy/scorer.py,366,b. Redistributions in binary form must reproduce the above copyright,
spaCy/spacy/scorer.py,367,"notice, this list of conditions and the following disclaimer in the",
spaCy/spacy/scorer.py,368,documentation and/or other materials provided with the distribution.,
spaCy/spacy/scorer.py,369,c. Neither the name of the Scikit-learn Developers  nor the names of,
spaCy/spacy/scorer.py,370,its contributors may be used to endorse or promote products,
spaCy/spacy/scorer.py,371,derived from this software without specific prior written,
spaCy/spacy/scorer.py,372,permission.,
spaCy/spacy/scorer.py,373,,
spaCy/spacy/scorer.py,374,,
spaCy/spacy/scorer.py,375,"THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""",
spaCy/spacy/scorer.py,376,"AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE",
spaCy/spacy/scorer.py,377,IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE,
spaCy/spacy/scorer.py,378,ARE DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE FOR,
spaCy/spacy/scorer.py,379,"ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL",
spaCy/spacy/scorer.py,380,"DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR",
spaCy/spacy/scorer.py,381,"SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER",
spaCy/spacy/scorer.py,382,"CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT",
spaCy/spacy/scorer.py,383,"LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY",
spaCy/spacy/scorer.py,384,"OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH",
spaCy/spacy/scorer.py,385,DAMAGE.,
spaCy/spacy/scorer.py,478,Add an extra threshold position,
spaCy/spacy/scorer.py,479,"to make sure that the curve starts at (0, 0)",
spaCy/spacy/scorer.py,530,make y_true a boolean vector,
spaCy/spacy/scorer.py,533,sort scores and corresponding truth values,
spaCy/spacy/scorer.py,539,y_score typically has many tied values. Here we extract,
spaCy/spacy/scorer.py,540,the indices associated with the distinct values. We also,
spaCy/spacy/scorer.py,541,concatenate a value for the end of the curve.,
spaCy/spacy/scorer.py,545,accumulate the true positives with decreasing threshold,
spaCy/spacy/scorer.py,608,Reductions such as .sum used internally in np.trapz do not return a,
spaCy/spacy/scorer.py,609,scalar by default for numpy.memmap instances contrary to,
spaCy/spacy/scorer.py,610,regular numpy.ndarray instances.,
spaCy/spacy/__main__.py,1,coding: utf8,
spaCy/spacy/__main__.py,4,NB! This breaks in plac on Python 2!!,
spaCy/spacy/__main__.py,5,from __future__ import unicode_literals,
spaCy/spacy/util.py,1,coding: utf8,
spaCy/spacy/util.py,69,Check if language is registered / entry point is available,
spaCy/spacy/util.py,160,in data dir / shortcut,
spaCy/spacy/util.py,163,installed as package,
spaCy/spacy/util.py,165,path to model data directory,
spaCy/spacy/util.py,167,Path or Path-like to model data,
spaCy/spacy/util.py,193,Support language factories registered via entry points (e.g. custom,
spaCy/spacy/util.py,194,"language subclass) while keeping top-level language identifier ""lang""",
spaCy/spacy/util.py,258,compare package name against lowercase name,
spaCy/spacy/util.py,272,use lowercase version to be safe,
spaCy/spacy/util.py,273,Here we're importing the module just to find it. This is worryingly,
spaCy/spacy/util.py,274,"indirect, but it's otherwise very difficult to find the package.",
spaCy/spacy/util.py,284,https://stackoverflow.com/a/39662359/6400719,
spaCy/spacy/util.py,288,Jupyter notebook or qtconsole,
spaCy/spacy/util.py,290,Probably standard Python interpreter,
spaCy/spacy/util.py,360,Handle deprecated data,
spaCy/spacy/util.py,398,"This is implemented as functools.partial instead of a closure, to allow",
spaCy/spacy/util.py,399,pickle to work.,
spaCy/spacy/util.py,612,Check for end - 1 here because boundaries are inclusive,
spaCy/spacy/util.py,623,Split to support file names like meta.json,
spaCy/spacy/util.py,632,Split to support file names like meta.json,
spaCy/spacy/util.py,643,Split to support file names like meta.json,
spaCy/spacy/util.py,652,Split to support file names like meta.json,
spaCy/spacy/util.py,705,We're using a helper function here to make it easier to change the,
spaCy/spacy/util.py,706,"validator that's used (e.g. different draft implementation), without",
spaCy/spacy/util.py,707,having to change it all across the codebase.,
spaCy/spacy/util.py,708,"TODO: replace with (stable) Draft6Validator, if available",
spaCy/spacy/util.py,734,"Error has suberrors, e.g. if schema uses anyOf",
spaCy/spacy/util.py,746,Split to support file names like meta.json,
spaCy/spacy/util.py,754,TODO: user warning?,
spaCy/spacy/util.py,764,normalize words to remove all whitespace tokens,
spaCy/spacy/util.py,766,align words with text,
spaCy/spacy/util.py,805,"add dummy methods for to_bytes, from_bytes, to_disk and from_disk to",
spaCy/spacy/util.py,806,allow serialization (see #1557),
spaCy/spacy/language.py,1,coding: utf8,
spaCy/spacy/language.py,71,"This is messy, but it's the minimal working fix to Issue #639.",
spaCy/spacy/language.py,216,Conveniences to access pipeline components,
spaCy/spacy/language.py,217,Shouldn't be used anymore!,
spaCy/spacy/language.py,453,support list of names instead of spread,
spaCy/spacy/language.py,499,"Allow dict of args to GoldParse, instead of GoldParse objects.",
spaCy/spacy/language.py,543,TODO: document,
spaCy/spacy/language.py,601,Populate vocab,
spaCy/spacy/language.py,607,noqa: F841,
spaCy/spacy/language.py,717,TODO: Having trouble with contextlib,
spaCy/spacy/language.py,718,Workaround: these aren't actually context managers atm.,
spaCy/spacy/language.py,786,contains functools.partial objects to easily create multiprocess worker.,
spaCy/spacy/language.py,791,Allow component_cfg to overwrite the top-level kwargs.,
spaCy/spacy/language.py,796,"Apply the function, but yield the doc",
spaCy/spacy/language.py,803,"if n_process == 1, no processes are forked.",
spaCy/spacy/language.py,808,"Track weakrefs of ""recent"" documents, so that we can see when they",
spaCy/spacy/language.py,809,"expire from memory. When they do, we know we don't need old strings.",
spaCy/spacy/language.py,810,"This way, we avoid maintaining an unbounded growth in string entries",
spaCy/spacy/language.py,811,in the string store.,
spaCy/spacy/language.py,814,"Keep track of the original string data, so that if we flush old strings,",
spaCy/spacy/language.py,815,"we can recover the original ones. However, we only want to do this if we're",
spaCy/spacy/language.py,816,"really adding strings, to save up-front costs.",
spaCy/spacy/language.py,839,raw_texts is used later to stop iteration.,
spaCy/spacy/language.py,841,for sending texts to worker,
spaCy/spacy/language.py,843,for receiving byte-encoded docs from worker,
spaCy/spacy/language.py,849,Sender sends texts to the workers.,
spaCy/spacy/language.py,850,This is necessary to properly handle infinite length of texts.,
spaCy/spacy/language.py,851,"(In this case, all data cannot be sent to the workers at once)",
spaCy/spacy/language.py,853,send twice to make process busy,
spaCy/spacy/language.py,874,Cycle channels not to break the order of docs.,
spaCy/spacy/language.py,875,"The received object is a batch of byte-encoded docs, so flatten them with chain.from_iterable.",
spaCy/spacy/language.py,882,tell `sender` that one batch was consumed.,
spaCy/spacy/language.py,952,Convert to list here in case exclude is (default) tuple,
spaCy/spacy/language.py,1024,"NB: This decorator needs to live here, because it needs to write to",
spaCy/spacy/language.py,1025,Language.factories. All other solutions would cause circular import.,
spaCy/spacy/language.py,1062,TODO: Replace this once we handle vectors consistently as static,
spaCy/spacy/language.py,1063,data,
spaCy/spacy/language.py,1088,Important! Not deep copy -- we just want the container (but we also,
spaCy/spacy/language.py,1089,want to support people providing arbitrarily typed nlp.pipeline,
spaCy/spacy/language.py,1090,objects.),
spaCy/spacy/language.py,1106,Don't change the pipeline if we're raising an error.,
spaCy/spacy/language.py,1113,We added some args for pipe that __call__ doesn't expect.,
spaCy/spacy/language.py,1140,"Connection does not accept unpickable objects, so send list.",
spaCy/spacy/language.py,1158,cycle channels so that distribute the texts evenly,
spaCy/spacy/glossary.py,1,coding: utf8,
spaCy/spacy/glossary.py,21,POS tags,
spaCy/spacy/glossary.py,22,Universal POS Tags,
spaCy/spacy/glossary.py,23,http://universaldependencies.org/u/pos/,
spaCy/spacy/glossary.py,44,POS tags (English),
spaCy/spacy/glossary.py,45,OntoNotes 5 / Penn Treebank,
spaCy/spacy/glossary.py,46,https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html,
spaCy/spacy/glossary.py,102,POS Tags (German),
spaCy/spacy/glossary.py,103,TIGER Treebank,
spaCy/spacy/glossary.py,104,http://www.ims.uni-stuttgart.de/forschung/ressourcen/korpora/TIGERCorpus/annotation/tiger_introduction.pdf,
spaCy/spacy/glossary.py,159,Noun chunks,
spaCy/spacy/glossary.py,168,Dependency Labels (English),
spaCy/spacy/glossary.py,169,ClearNLP / Universal Dependencies,
spaCy/spacy/glossary.py,170,https://github.com/clir/clearnlp-guidelines/blob/master/md/specifications/dependency_labels.md,
spaCy/spacy/glossary.py,241,Dependency labels (German),
spaCy/spacy/glossary.py,242,TIGER Treebank,
spaCy/spacy/glossary.py,243,http://www.ims.uni-stuttgart.de/forschung/ressourcen/korpora/TIGERCorpus/annotation/tiger_introduction.pdf,
spaCy/spacy/glossary.py,244,currently missing: 'cc' (comparative complement) because of conflict,
spaCy/spacy/glossary.py,245,with English labels,
spaCy/spacy/glossary.py,288,Named Entity Recognition,
spaCy/spacy/glossary.py,289,OntoNotes 5,
spaCy/spacy/glossary.py,290,https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf,
spaCy/spacy/glossary.py,310,Named Entity Recognition,
spaCy/spacy/glossary.py,311,Wikipedia,
spaCy/spacy/glossary.py,312,http://www.sciencedirect.com/science/article/pii/S0004370212000276,
spaCy/spacy/glossary.py,313,https://pdfs.semanticscholar.org/5744/578cc243d92287f47448870bb426c66cc941.pdf,
spaCy/spacy/glossary.py,316,https://github.com/ltgoslo/norne,
spaCy/spacy/lemmatizer.py,1,coding: utf8,
spaCy/spacy/lemmatizer.py,58,See Issue #435 for example of where this logic is requied.,
spaCy/spacy/lemmatizer.py,87,This maps 'VBP' to base form -- probably just need 'IS_BASE',
spaCy/spacy/lemmatizer.py,88,morphology,
spaCy/spacy/lemmatizer.py,160,"Remove duplicates but preserve the ordering of applied ""rules""",
spaCy/spacy/lemmatizer.py,162,"Put exceptions at the front of the list, so they get priority.",
spaCy/spacy/lemmatizer.py,163,This is a dodgy heuristic -- but it's the best we can do until we get,
spaCy/spacy/lemmatizer.py,164,"frequencies on this. We can at least prune out problematic exceptions,",
spaCy/spacy/lemmatizer.py,165,if they shadow more frequent analyses.,
spaCy/spacy/about.py,1,fmt: off,
spaCy/spacy/lookups.py,1,coding: utf-8,
spaCy/spacy/lookups.py,193,Assume a default size of 1M items,
spaCy/spacy/lookups.py,245,"This can give a false positive, so we need to check it after",
spaCy/spacy/errors.py,1,coding: utf8,
spaCy/spacy/errors.py,20,fmt: off,
spaCy/spacy/errors.py,573,fmt: on,
spaCy/spacy/errors.py,607,don't show any warnings,
spaCy/spacy/errors.py,609,show all available warnings,
spaCy/spacy/errors.py,643,get ID from string,
spaCy/spacy/analysis.py,1,coding: utf8,
spaCy/spacy/analysis.py,84,Support Span only for custom extension attributes,
spaCy/spacy/analysis.py,89,first element is not doc/token/span,
spaCy/spacy/analysis.py,92,"attr is something like ""doc""",
spaCy/spacy/analysis.py,96,"attr is something like ""doc._""",
spaCy/spacy/analysis.py,99,We don't check whether the attribute actually exists,
spaCy/spacy/analysis.py,100,attr is something like doc._.x.y,
spaCy/spacy/analysis.py,104,we can't validate those further,
spaCy/spacy/analysis.py,105,"attr is something like ""token.pos_""",
spaCy/spacy/analysis.py,107,attr is something like doc.x.y,
spaCy/spacy/__init__.py,1,coding: utf8,
spaCy/spacy/__init__.py,9,These are imported as part of the API,
spaCy/spacy/_ml.py,1,coding: utf8,
spaCy/spacy/_ml.py,32,Backwards compatibility with <2.2.2,
spaCy/spacy/_ml.py,139,The dtype here matches what thinc is expecting -- which differs per,
spaCy/spacy/_ml.py,140,platform (by int definition). This should be fixed once the problem,
spaCy/spacy/_ml.py,141,is fixed on Thinc's side.,
spaCy/spacy/_ml.py,198,Reuse the buffer,
spaCy/spacy/_ml.py,203,"(o, p, f, i) --> (f, o, p, i)",
spaCy/spacy/_ml.py,217,"(1, nF, nO, nP) += (nN, nF, nO, nP) where IDs (nN, nF) < 0",
spaCy/spacy/_ml.py,249,nS ids. nW tokvecs. Exclude the padding array.,
spaCy/spacy/_ml.py,250,"(nW, f, o, p)",
spaCy/spacy/_ml.py,252,need nS vectors,
spaCy/spacy/_ml.py,294,"Set an entry here, so that vectors are accessed by StaticVectors",
spaCy/spacy/_ml.py,295,"(unideal, I know)",
spaCy/spacy/_ml.py,299,This is a hack to avoid the problem in #3853.,
spaCy/spacy/_ml.py,321,"Preserve prior tok2vec for backwards compat, in v2.2.2",
spaCy/spacy/_ml.py,502,"Clip to range (-10, 10)",
spaCy/spacy/_ml.py,674,TODO Make concatenate support lists,
spaCy/spacy/_ml.py,766,context encoder,
spaCy/spacy/_ml.py,802,pragma: no cover,
spaCy/spacy/_ml.py,868,This needs to be here to avoid circular imports,
spaCy/spacy/_ml.py,886,NB: If you change this implementation to instead modify,
spaCy/spacy/_ml.py,887,"the docs in place, take care that the IDs reflect the original",
spaCy/spacy/_ml.py,888,words. Currently we use the original docs to make the vectors,
spaCy/spacy/_ml.py,889,"for the target, so we don't lose the original tokens. But if",
spaCy/spacy/_ml.py,890,"you modified the docs in place here, you would.",
spaCy/spacy/_ml.py,940,This assists in indexing; it's like looping over this dimension.,
spaCy/spacy/_ml.py,941,Still consider this weird witch craft...But thanks to Mark Neumann,
spaCy/spacy/_ml.py,942,for the tip.,
spaCy/spacy/_ml.py,947,"Let's say I have a 2d array of indices, and a 3d table of data. What numpy",
spaCy/spacy/_ml.py,948,incantation do I chant to get,
spaCy/spacy/_ml.py,949,"output[i, j, k] == data[j, ids[i, j], k]?",
spaCy/spacy/_ml.py,968,Find the zero vectors,
spaCy/spacy/_ml.py,971,Add a small constant to avoid 0 vectors,
spaCy/spacy/_ml.py,974,https://math.stackexchange.com/questions/1923613/partial-derivative-of-cosine-similarity,
spaCy/spacy/_ml.py,982,"If the target was a zero vector, don't count it in the loss.",
spaCy/spacy/displacy/templates.py,1,coding: utf8,
spaCy/spacy/displacy/templates.py,5,Setting explicit height and max-width: none on the SVG is required for,
spaCy/spacy/displacy/templates.py,6,Jupyter to render it properly in a cell,
spaCy/spacy/displacy/__init__.py,1,coding: utf8,
spaCy/spacy/displacy/__init__.py,57,return HTML rendered by IPython display(),
spaCy/spacy/displacy/__init__.py,58,See #4840 for details on span wrapper to disable mathjax,
spaCy/spacy/displacy/__init__.py,107,"Headers and status need to be bytes in Python 2, see #1227",
spaCy/spacy/displacy/render.py,1,coding: utf8,
spaCy/spacy/displacy/render.py,55,Create a random ID prefix to make sure parses don't receive the,
spaCy/spacy/displacy/render.py,56,"same ID, even if they're identical",
spaCy/spacy/tokens/underscore.py,1,coding: utf8,
spaCy/spacy/tokens/underscore.py,19,"Assumption is that for doc values, _start and _end will both be None",
spaCy/spacy/tokens/underscore.py,20,Span will set non-None values for _start and _end,
spaCy/spacy/tokens/underscore.py,21,"Token will have _start be non-None, _end be None",
spaCy/spacy/tokens/underscore.py,22,"This lets us key everything into the doc.user_data dictionary,",
spaCy/spacy/tokens/underscore.py,23,"(see _get_key), and lets us use a single Underscore class.",
spaCy/spacy/tokens/underscore.py,29,Hack to enable autocomplete on custom extensions,
spaCy/spacy/tokens/underscore.py,41,Hack to port over docstrings of the original function,
spaCy/spacy/tokens/underscore.py,42,See https://stackoverflow.com/q/27362727/6400719,
spaCy/spacy/tokens/underscore.py,55,Handle mutable default arguments (see #2581),
spaCy/spacy/tokens/underscore.py,119,"Extension is writable if it has a setter (getter + setter), if it has a",
spaCy/spacy/tokens/underscore.py,120,"default value (or, if its default value is none, none of the other values",
spaCy/spacy/tokens/underscore.py,121,should be set).,
spaCy/spacy/tokens/_serialize.py,1,coding: utf8,
spaCy/spacy/tokens/_serialize.py,58,Ensure ORTH is always attrs[0],
spaCy/spacy/tokens/_serialize.py,82,this should never happen,
spaCy/spacy/tokens/_serialize.py,139,this should never happen,
spaCy/spacy/tokens/_serialize.py,176,this should never happen,
spaCy/spacy/tokens/_serialize.py,204,"Compatibility, as we had named it this previously.",
spaCy/spacy/tokens/__init__.py,1,coding: utf8,
spaCy/spacy/matcher/_schemas.py,1,coding: utf8,
spaCy/spacy/matcher/__init__.py,1,coding: utf8,
spaCy/spacy/tests/test_lemmatizer.py,1,coding: utf8,
spaCy/spacy/tests/test_lemmatizer.py,20,The update to the table should be reflected in the lemmatizer,
spaCy/spacy/tests/test_lemmatizer.py,28,Make sure we have the previously saved lookup table,
spaCy/spacy/tests/test_align.py,1,coding: utf-8,
spaCy/spacy/tests/test_gold.py,1,coding: utf-8,
spaCy/spacy/tests/test_gold.py,87,noqa: F841,
spaCy/spacy/tests/test_gold.py,118,roundtrip to JSON,
spaCy/spacy/tests/test_gold.py,137,roundtrip to JSONL train dicts,
spaCy/spacy/tests/test_gold.py,156,roundtrip to JSONL tuples,
spaCy/spacy/tests/test_gold.py,159,write to JSONL train dicts,
spaCy/spacy/tests/test_gold.py,162,load and rewrite as JSONL tuples,
spaCy/spacy/tests/test_gold.py,207,check symmetry,
spaCy/spacy/tests/util.py,1,coding: utf-8,
spaCy/spacy/tests/util.py,53,"if there are any other annotations, set them",
spaCy/spacy/tests/util.py,75,"finally, set the entities",
spaCy/spacy/tests/conftest.py,1,coding: utf-8,
spaCy/spacy/tests/conftest.py,14,When using 'pytest --pyargs spacy' to test an installed copy of,
spaCy/spacy/tests/conftest.py,15,"spacy, pytest skips running our pytest_addoption() hook. Later, when",
spaCy/spacy/tests/conftest.py,16,"we call getoption(), pytest raises an error, because it doesn't",
spaCy/spacy/tests/conftest.py,17,"recognize the option we're asking about. To avoid this, we need to",
spaCy/spacy/tests/conftest.py,18,"pass a default value. We default to False, i.e., we act like all the",
spaCy/spacy/tests/conftest.py,19,options weren't given.,
spaCy/spacy/tests/conftest.py,27,Fixtures for language tokenizers (languages sorted alphabetically),
spaCy/spacy/tests/test_pickles.py,1,coding: utf-8,
spaCy/spacy/tests/test_language.py,1,coding: utf-8,
spaCy/spacy/tests/test_language.py,33,Update with doc and gold objects,
spaCy/spacy/tests/test_language.py,35,Update with text and dict,
spaCy/spacy/tests/test_language.py,37,Update with doc object and dict,
spaCy/spacy/tests/test_language.py,39,Update with text and gold object,
spaCy/spacy/tests/test_language.py,41,Update badly,
spaCy/spacy/tests/test_language.py,55,Evaluate with doc and gold objects,
spaCy/spacy/tests/test_language.py,57,Evaluate with text and dict,
spaCy/spacy/tests/test_language.py,59,Evaluate with doc object and dict,
spaCy/spacy/tests/test_language.py,61,Evaluate with text and gold object,
spaCy/spacy/tests/test_language.py,63,Evaluate badly,
spaCy/spacy/tests/test_language.py,142,check if nlp.pipe can handle infinite length iterator properly.,
spaCy/spacy/tests/test_tok2vec.py,1,coding: utf-8,
spaCy/spacy/tests/test_tok2vec.py,17,"Make the words numbers, so that they're distnct",
spaCy/spacy/tests/test_tok2vec.py,18,"across the batch, and easy to track.",
spaCy/spacy/tests/test_tok2vec.py,25,This fails in Thinc v7.3.1. Need to push patch,
spaCy/spacy/tests/test_scorer.py,1,coding: utf-8,
spaCy/spacy/tests/test_scorer.py,47,Gold and Doc are identical,
spaCy/spacy/tests/test_scorer.py,69,One dep is incorrect in Doc,
spaCy/spacy/tests/test_scorer.py,94,Gold and Doc are identical,
spaCy/spacy/tests/test_scorer.py,113,Doc has one missing and one extra entity,
spaCy/spacy/tests/test_scorer.py,114,Entity type MONEY is not present in Doc,
spaCy/spacy/tests/test_scorer.py,144,"Binary classification, toy tests from scikit-learn test suite",
spaCy/spacy/tests/test_scorer.py,185,same result as above with ROCAUCScore wrapper,
spaCy/spacy/tests/test_scorer.py,191,check that errors are raised in undefined cases and score is -inf,
spaCy/spacy/tests/test_cli.py,1,coding: utf-8,
spaCy/spacy/tests/test_cli.py,12,https://raw.githubusercontent.com/ohenrik/nb_news_ud_sm/master/original_data/no-ud-dev-ner.conllu,
spaCy/spacy/tests/test_cli.py,52,fmt: off,
spaCy/spacy/tests/test_cli.py,55,fmt: on,
spaCy/spacy/tests/test_cli.py,118,fmt: off,
spaCy/spacy/tests/test_cli.py,121,fmt: on,
spaCy/spacy/tests/test_architectures.py,1,coding: utf8,
spaCy/spacy/tests/test_json_schemas.py,1,coding: utf-8,
spaCy/spacy/tests/test_displacy.py,1,coding: utf-8,
spaCy/spacy/tests/test_displacy.py,71,Source: http://www.sobhe.ir/hazm/ – is this correct?,
spaCy/spacy/tests/test_displacy.py,73,"These are (likely) wrong, but it's just for testing",
spaCy/spacy/tests/test_displacy.py,74,needs to match lang.fa.tag_map,
spaCy/spacy/tests/test_displacy.py,101,Restore,
spaCy/spacy/tests/test_misc.py,1,coding: utf-8,
spaCy/spacy/tests/test_misc.py,29,yield -- need to cleanup even if assertion fails,
spaCy/spacy/tests/test_misc.py,30,https://github.com/pytest-dev/pytest/issues/2508#issuecomment-309934240,
spaCy/spacy/tests/test_misc.py,33,Remove symlink only if it was created,
spaCy/spacy/tests/test_misc.py,99,noqa: F401,
spaCy/spacy/tests/test_misc.py,106,noqa: F401,
spaCy/spacy/tests/regression/test_issue3555.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue4054.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue2001-2500.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue2001-2500.py,21,"Problem: The dot is now properly split off, but the prefix/suffix rules",
spaCy/spacy/tests/regression/test_issue2001-2500.py,22,are not applied again afterwards. This means that the quote will still be,
spaCy/spacy/tests/regression/test_issue2001-2500.py,23,attached to the remaining token.,
spaCy/spacy/tests/regression/test_issue2001-2500.py,51,Work around lemma corruption problem and set lemmas after tags,
spaCy/spacy/tests/regression/test_issue2001-2500.py,56,"We need to serialize both tag and lemma, since this is what causes the bug",
spaCy/spacy/tests/regression/test_issue2001-2500.py,83,fix bug in labels with a 'b' character,
spaCy/spacy/tests/regression/test_issue2001-2500.py,86,maintain support for iob1 format,
spaCy/spacy/tests/regression/test_issue2001-2500.py,89,maintain support for iob2 format,
spaCy/spacy/tests/regression/test_issue3972.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue3972.py,19,We should have a match for each of the two rules,
spaCy/spacy/tests/regression/test_issue4707.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue3962.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue3962.py,33,"""jests at scars ,""",
spaCy/spacy/tests/regression/test_issue3962.py,40,"head set to itself, being the new artificial root",
spaCy/spacy/tests/regression/test_issue3962.py,46,head set to the new artificial root,
spaCy/spacy/tests/regression/test_issue3962.py,49,We should still have 1 sentence,
spaCy/spacy/tests/regression/test_issue3962.py,52,"""never felt a""",
spaCy/spacy/tests/regression/test_issue3962.py,61,head set to ancestor,
spaCy/spacy/tests/regression/test_issue3962.py,64,"We should still have 1 sentence as ""a"" can be attached to ""felt"" instead of ""wound""",
spaCy/spacy/tests/regression/test_issue3962.py,92,"""jests at scars. They never""",
spaCy/spacy/tests/regression/test_issue3962.py,99,"head set to itself, being the new artificial root (in sentence 1)",
spaCy/spacy/tests/regression/test_issue3962.py,109,"head set to itself, being the new artificial root (in sentence 2)",
spaCy/spacy/tests/regression/test_issue3962.py,113,head set to the new artificial head (in sentence 2),
spaCy/spacy/tests/regression/test_issue3962.py,116,We should still have 2 sentences,
spaCy/spacy/tests/regression/test_issue3839.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue3869.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue4725.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue4725.py,11,ensures that this runs correctly and doesn't hang or crash because of the global vectors,
spaCy/spacy/tests/regression/test_issue4002.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue3611.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue3611.py,18,preparing the data,
spaCy/spacy/tests/regression/test_issue3611.py,24,set up the spacy model with a text categorizer component,
spaCy/spacy/tests/regression/test_issue3611.py,36,training the network,
spaCy/spacy/tests/regression/test_issue3625.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue4529.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue4674.py,1,coding: utf-8,
spaCy/spacy/tests/regression/test_issue4674.py,28,dumping to file & loading back in,
spaCy/spacy/tests/regression/test_issue3879.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue3879.py,14,fails because of a FP match 'is a test',
spaCy/spacy/tests/regression/test_issue3803.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue4373.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue4924.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue3531.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue4348.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue4651.py,1,coding: utf-8,
spaCy/spacy/tests/regression/test_issue4030.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue4030.py,18,preparing the data,
spaCy/spacy/tests/regression/test_issue4030.py,24,set up the spacy model with a text categorizer component,
spaCy/spacy/tests/regression/test_issue4030.py,36,training the network,
spaCy/spacy/tests/regression/test_issue4030.py,53,processing of an empty doc should result in 0.0 for all categories,
spaCy/spacy/tests/regression/test_issue3521.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue3521.py,10,"'not' and 'would' should be stopwords, also in their abbreviated forms",
spaCy/spacy/tests/regression/test_issue1001-1500.py,1,coding: utf-8,
spaCy/spacy/tests/regression/test_issue1001-1500.py,30,"For sanity, check it works when pipeline is clean.",
spaCy/spacy/tests/regression/test_issue3001-3500.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue3001-3500.py,79,Serializing then deserializing,
spaCy/spacy/tests/regression/test_issue3001-3500.py,201,Add the OUT action. I wouldn't have thought this would be necessary...,
spaCy/spacy/tests/regression/test_issue3001-3500.py,205,"Get into the state just before ""New""",
spaCy/spacy/tests/regression/test_issue3001-3500.py,210,Check that B-GPE is valid.,
spaCy/spacy/tests/regression/test_issue3001-3500.py,215,"If we have this test in Python 3, pytest chokes, as it can't print the",
spaCy/spacy/tests/regression/test_issue3001-3500.py,216,string above in the xpass message.,
spaCy/spacy/tests/regression/test_issue3001-3500.py,332,this crashed because of a padding error in layer.ops.unflatten in thinc,
spaCy/spacy/tests/regression/test_issue4042.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue4042.py,17,add ner pipe,
spaCy/spacy/tests/regression/test_issue4042.py,23,Add entity ruler,
spaCy/spacy/tests/regression/test_issue4042.py,30,"works fine with ""after""",
spaCy/spacy/tests/regression/test_issue4042.py,53,add ner pipe,
spaCy/spacy/tests/regression/test_issue4042.py,59,add a new label to the doc,
spaCy/spacy/tests/regression/test_issue4042.py,66,reapply the NER - at this point it should resize itself,
spaCy/spacy/tests/regression/test_issue4042.py,73,assert IO goes fine,
spaCy/spacy/tests/regression/test_issue5082.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue5082.py,10,Ensure the 'merge_entities' pipeline does something sensible for the vectors of the merged tokens,
spaCy/spacy/tests/regression/test_issue5048.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue4402.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue4402.py,21,assert that the data got split into 4 sentences,
spaCy/spacy/tests/regression/test_issue4120.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue4120.py,13,works,
spaCy/spacy/tests/regression/test_issue4120.py,16,fixed,
spaCy/spacy/tests/regression/test_issue4120.py,21,works,
spaCy/spacy/tests/regression/test_issue4120.py,26,fixed,
spaCy/spacy/tests/regression/test_issue3526.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue4903.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue4903.py,32,ensures that this runs correctly and doesn't hang or crash on Windows / macOS,
spaCy/spacy/tests/regression/test_issue4590.py,1,coding: utf-8,
spaCy/spacy/tests/regression/test_issue1-1000.py,1,coding: utf-8,
spaCy/spacy/tests/regression/test_issue1-1000.py,89,"One token can only be part of one entity, so test that the matches",
spaCy/spacy/tests/regression/test_issue1-1000.py,90,can't be added as entities,
spaCy/spacy/tests/regression/test_issue1-1000.py,413,Skip test if pytest-timeout is not installed,
spaCy/spacy/tests/regression/test_issue3882.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue3549.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue4190.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue4190.py,13,Load default language,
spaCy/spacy/tests/regression/test_issue4190.py,16,noqa: F841,
spaCy/spacy/tests/regression/test_issue4190.py,17,Modify tokenizer,
spaCy/spacy/tests/regression/test_issue4190.py,21,Save and Reload,
spaCy/spacy/tests/regression/test_issue4190.py,25,This should be the modified tokenizer,
spaCy/spacy/tests/regression/test_issue4190.py,35,Remove all exceptions where a single letter is followed by a period (e.g. 'h.'),
spaCy/spacy/tests/regression/test_issue4272.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue3880.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue4367.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue3951.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue3540.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue4313.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue4313.py,22,add a new label to the doc,
spaCy/spacy/tests/regression/test_issue4313.py,29,ensure the beam_parse still works with the new label,
spaCy/spacy/tests/regression/test_issue3959.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue3959.py,19,usually this is already True when starting from proper models instead of blank English,
spaCy/spacy/tests/regression/test_issue4278.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue4267.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue4267.py,18,assert that we have correct IOB annotations,
spaCy/spacy/tests/regression/test_issue4267.py,24,add entity ruler and run again,
spaCy/spacy/tests/regression/test_issue4267.py,33,assert that we still have correct IOB annotations,
spaCy/spacy/tests/regression/test_issue4133.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue4133.py,18,usually this is already True when starting from proper models instead of blank English,
spaCy/spacy/tests/regression/test_issue4849.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue4849.py,27,USING 1 PROCESS,
spaCy/spacy/tests/regression/test_issue4849.py,33,USING 2 PROCESSES,
spaCy/spacy/tests/regression/test_issue2501-3000.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue2501-3000.py,26,initialise weights,
spaCy/spacy/tests/regression/test_issue2501-3000.py,125,A tree with a non-projective (i.e. crossing) arc,
spaCy/spacy/tests/regression/test_issue2501-3000.py,126,"The arcs (0, 4) and (2, 9) cross.",
spaCy/spacy/tests/regression/test_issue2501-3000.py,192,noqa: F841,
spaCy/spacy/tests/regression/test_issue1501-2000.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue1501-2000.py,37,We should run cleanup more than one time to actually cleanup data.,
spaCy/spacy/tests/regression/test_issue1501-2000.py,38,In first run — clean up only mark strings as «not hitted».,
spaCy/spacy/tests/regression/test_issue1501-2000.py,69,"TODO: Currently segfaulting, due to l_edge and r_edge misalignment",
spaCy/spacy/tests/regression/test_issue1501-2000.py,70,def test_issue1537_model():,
spaCy/spacy/tests/regression/test_issue1501-2000.py,71,nlp = load_spacy('en'),
spaCy/spacy/tests/regression/test_issue1501-2000.py,72,doc = nlp('The sky is blue. The man is pink. The dog is purple.'),
spaCy/spacy/tests/regression/test_issue1501-2000.py,73,sents = [s.as_doc() for s in doc.sents],
spaCy/spacy/tests/regression/test_issue1501-2000.py,74,print(list(sents[0].noun_chunks)),
spaCy/spacy/tests/regression/test_issue1501-2000.py,75,print(list(sents[1].noun_chunks)),
spaCy/spacy/tests/regression/test_issue1501-2000.py,241,should error out,
spaCy/spacy/tests/regression/test_issue1501-2000.py,254,we should see two overlapping matches here,
spaCy/spacy/tests/regression/test_issue1501-2000.py,279,Possibly related to #2675 and #2671?,
spaCy/spacy/tests/regression/test_issue1501-2000.py,290,"We could also assert length 1 here, but this is more conclusive, because",
spaCy/spacy/tests/regression/test_issue1501-2000.py,291,the real problem here is that it returns a duplicate match for a match_id,
spaCy/spacy/tests/regression/test_issue1501-2000.py,292,that's not actually in the vocab!,
spaCy/spacy/tests/regression/test_issue1501-2000.py,300,"{""IN"": [""EUR""]}}]",
spaCy/spacy/tests/regression/test_issue1501-2000.py,331,Uncommenting this caused a segmentation fault,
spaCy/spacy/tests/regression/test_issue4528.py,1,coding: utf8,
spaCy/spacy/tests/regression/test_issue4528.py,11,This is how extension attribute values are stored in the user data,
spaCy/spacy/tests/morphology/test_morph_features.py,1,coding: utf-8,
spaCy/spacy/tests/serialize/test_serialize_language.py,1,coding: utf-8,
spaCy/spacy/tests/serialize/test_serialize_kb.py,1,coding: utf-8,
spaCy/spacy/tests/serialize/test_serialize_kb.py,11,baseline assertions,
spaCy/spacy/tests/serialize/test_serialize_kb.py,15,dumping to file & loading back in,
spaCy/spacy/tests/serialize/test_serialize_kb.py,26,final assertions,
spaCy/spacy/tests/serialize/test_serialize_kb.py,50,check entities,
spaCy/spacy/tests/serialize/test_serialize_kb.py,57,check aliases,
spaCy/spacy/tests/serialize/test_serialize_kb.py,64,check candidates & probabilities,
spaCy/spacy/tests/serialize/test_serialize_tokenizer.py,1,coding: utf-8,
spaCy/spacy/tests/serialize/test_serialize_vocab_strings.py,1,coding: utf-8,
spaCy/spacy/tests/serialize/test_serialize_vocab_strings.py,75,Reported in #2153,
spaCy/spacy/tests/serialize/test_serialize_extension_attrs.py,1,coding: utf-8,
spaCy/spacy/tests/serialize/test_serialize_doc.py,1,coding: utf-8,
spaCy/spacy/tests/serialize/test_serialize_doc.py,77,"Deserialize later, e.g. in a new process",
spaCy/spacy/tests/serialize/test_serialize_pipeline.py,1,coding: utf-8,
spaCy/spacy/tests/serialize/test_serialize_pipeline.py,118,See issue #1105,
spaCy/spacy/tests/vocab_vectors/test_vocab_api.py,1,coding: utf-8,
spaCy/spacy/tests/vocab_vectors/test_vocab_api.py,45,noqa: F841,
spaCy/spacy/tests/vocab_vectors/test_vectors.py,1,coding: utf-8,
spaCy/spacy/tests/vocab_vectors/test_vectors.py,95,decrease vector dimension (truncate),
spaCy/spacy/tests/vocab_vectors/test_vectors.py,105,increase vector dimension (pad with zeros),
spaCy/spacy/tests/vocab_vectors/test_vectors.py,160,not 1.0000002,
spaCy/spacy/tests/vocab_vectors/test_vectors.py,164,not 0.9999999,
spaCy/spacy/tests/vocab_vectors/test_vectors.py,313,noqa: F841,
spaCy/spacy/tests/vocab_vectors/test_vectors.py,314,noqa: F841,
spaCy/spacy/tests/vocab_vectors/test_vectors.py,315,noqa: F841,
spaCy/spacy/tests/vocab_vectors/test_lexeme.py,1,coding: utf-8,
spaCy/spacy/tests/vocab_vectors/test_lookups.py,1,coding: utf-8,
spaCy/spacy/tests/vocab_vectors/test_similarity.py,1,coding: utf-8,
spaCy/spacy/tests/vocab_vectors/test_stringstore.py,1,coding: utf-8,
spaCy/spacy/tests/matcher/test_matcher_logic.py,1,coding: utf-8,
spaCy/spacy/tests/matcher/test_matcher_logic.py,160,should give two matches,
spaCy/spacy/tests/matcher/test_matcher_logic.py,164,removing once should work,
spaCy/spacy/tests/matcher/test_matcher_logic.py,167,should not return any maches anymore,
spaCy/spacy/tests/matcher/test_matcher_logic.py,171,removing again should throw an error,
spaCy/spacy/tests/matcher/test_matcher_api.py,1,coding: utf-8,
spaCy/spacy/tests/matcher/test_matcher_api.py,10,noqa: F401,
spaCy/spacy/tests/matcher/test_matcher_api.py,81,"New API: add(key: str, patterns: List[List[dict]], on_match: Callable)",
spaCy/spacy/tests/matcher/test_matcher_api.py,375,"def test_dependency_matcher(dependency_matcher, text, heads, deps):",
spaCy/spacy/tests/matcher/test_matcher_api.py,376,"doc = get_doc(dependency_matcher.vocab, text.split(), heads=heads, deps=deps)",
spaCy/spacy/tests/matcher/test_matcher_api.py,377,matches = dependency_matcher(doc),
spaCy/spacy/tests/matcher/test_matcher_api.py,378,"assert matches[0][1] == [[3, 1, 2]]",
spaCy/spacy/tests/matcher/test_matcher_api.py,379,"assert matches[1][1] == [[4, 3, 3]]",
spaCy/spacy/tests/matcher/test_matcher_api.py,380,"assert matches[2][1] == [[4, 3, 2]]",
spaCy/spacy/tests/matcher/test_matcher_api.py,385,Potential mistake: pass in pattern instead of list of patterns,
spaCy/spacy/tests/matcher/test_matcher_api.py,397,DEP requires is_parsed,
spaCy/spacy/tests/matcher/test_matcher_api.py,405,"TAG, POS, LEMMA require is_tagged",
spaCy/spacy/tests/matcher/test_matcher_api.py,414,TEXT/ORTH only require tokens,
spaCy/spacy/tests/matcher/test_pattern_validation.py,1,coding: utf-8,
spaCy/spacy/tests/matcher/test_pattern_validation.py,10,"(pattern, num errors with validation, num errors identified with minimal",
spaCy/spacy/tests/matcher/test_pattern_validation.py,11,checks),
spaCy/spacy/tests/matcher/test_pattern_validation.py,13,Bad patterns flagged in all cases,
spaCy/spacy/tests/matcher/test_pattern_validation.py,20,Bad patterns flagged outside of Matcher,
spaCy/spacy/tests/matcher/test_pattern_validation.py,22,Bad patterns not flagged with minimal checks,
spaCy/spacy/tests/matcher/test_pattern_validation.py,29,Good patterns,
spaCy/spacy/tests/matcher/test_phrase_matcher.py,1,coding: utf-8,
spaCy/spacy/tests/matcher/test_phrase_matcher.py,13,intermediate phrase,
spaCy/spacy/tests/matcher/test_phrase_matcher.py,18,initial token,
spaCy/spacy/tests/matcher/test_phrase_matcher.py,23,initial phrase,
spaCy/spacy/tests/matcher/test_phrase_matcher.py,28,final token,
spaCy/spacy/tests/matcher/test_phrase_matcher.py,33,final phrase,
spaCy/spacy/tests/matcher/test_phrase_matcher.py,67,"New API: add(key: str, patterns: List[List[dict]], on_match: Callable)",
spaCy/spacy/tests/matcher/test_phrase_matcher.py,80,match ID only gets added once,
spaCy/spacy/tests/matcher/test_phrase_matcher.py,121,TEST2 is added alongside TEST,
spaCy/spacy/tests/matcher/test_phrase_matcher.py,127,removing TEST does not remove the entry for TEST2,
spaCy/spacy/tests/matcher/test_phrase_matcher.py,133,removing TEST2 removes all,
spaCy/spacy/tests/matcher/test_phrase_matcher.py,221,DEP requires is_parsed,
spaCy/spacy/tests/matcher/test_phrase_matcher.py,228,"TAG, POS, LEMMA require is_tagged",
spaCy/spacy/tests/matcher/test_phrase_matcher.py,236,TEXT/ORTH only require tokens,
spaCy/spacy/tests/matcher/test_phrase_matcher.py,265,Potential mistake: pass in pattern instead of list of patterns,
spaCy/spacy/tests/doc/test_add_entities.py,1,coding: utf-8,
spaCy/spacy/tests/doc/test_token_api.py,1,coding: utf-8,
spaCy/spacy/tests/doc/test_token_api.py,16,fmt: off,
spaCy/spacy/tests/doc/test_token_api.py,21,fmt: on,
spaCy/spacy/tests/doc/test_token_api.py,57,"TODO: Test more of these, esp. if a bug is found",
spaCy/spacy/tests/doc/test_token_api.py,104,the structure of this sentence depends on the English annotation scheme,
spaCy/spacy/tests/doc/test_token_api.py,118,the structure of this sentence depends on the English annotation scheme,
spaCy/spacy/tests/doc/test_token_api.py,170,head token must be from the same document,
spaCy/spacy/tests/doc/test_underscore.py,1,coding: utf-8,
spaCy/spacy/tests/doc/test_underscore.py,12,"reset the Underscore object after the test, to avoid having state copied across tests",
spaCy/spacy/tests/doc/test_retokenize_split.py,1,coding: utf-8,
spaCy/spacy/tests/doc/test_retokenize_split.py,61,Not enough heads,
spaCy/spacy/tests/doc/test_retokenize_split.py,66,Too many heads,
spaCy/spacy/tests/doc/test_retokenize_split.py,73,Test entity IOB stays consistent after merging,
spaCy/spacy/tests/doc/test_retokenize_split.py,88,fmt: off,
spaCy/spacy/tests/doc/test_retokenize_split.py,94,fmt: on,
spaCy/spacy/tests/doc/test_retokenize_split.py,150,Overwriting getter without setter,
spaCy/spacy/tests/doc/test_retokenize_split.py,151,Overwriting method,
spaCy/spacy/tests/doc/test_retokenize_split.py,152,Overwriting nonexistent attribute,
spaCy/spacy/tests/doc/test_retokenize_split.py,153,Combination,
spaCy/spacy/tests/doc/test_retokenize_split.py,154,Combination,
spaCy/spacy/tests/doc/test_retokenize_split.py,155,Not a list of dicts,
spaCy/spacy/tests/doc/test_pickle_doc.py,1,coding: utf-8,
spaCy/spacy/tests/doc/test_pickle_doc.py,19,noqa: F841,
spaCy/spacy/tests/doc/test_span.py,1,coding: utf-8,
spaCy/spacy/tests/doc/test_span.py,16,fmt: off,
spaCy/spacy/tests/doc/test_span.py,21,fmt: on,
spaCy/spacy/tests/doc/test_span.py,91,test on manual sbd,
spaCy/spacy/tests/doc/test_span.py,104,the & the -> the,
spaCy/spacy/tests/doc/test_span.py,105,the & lazy -> dog (out of span),
spaCy/spacy/tests/doc/test_span.py,106,lazy & the -> dog (out of span),
spaCy/spacy/tests/doc/test_span.py,107,lazy & lazy -> lazy,
spaCy/spacy/tests/doc/test_span.py,111,lazy & lazy -> lazy,
spaCy/spacy/tests/doc/test_span.py,112,lazy & dog -> dog,
spaCy/spacy/tests/doc/test_span.py,113,lazy & slept -> slept,
spaCy/spacy/tests/doc/test_span.py,117,dog & dog -> dog,
spaCy/spacy/tests/doc/test_span.py,118,dog & slept -> slept,
spaCy/spacy/tests/doc/test_span.py,119,slept & dog -> slept,
spaCy/spacy/tests/doc/test_span.py,120,slept & slept -> slept,
spaCy/spacy/tests/doc/test_span.py,240,"First sentence, also tests start of sentence",
spaCy/spacy/tests/doc/test_span.py,245,Second sentence,
spaCy/spacy/tests/doc/test_span.py,251,"Third sentence ents, Also tests end of sentence",
spaCy/spacy/tests/doc/test_span.py,259,Test filtering duplicates,
spaCy/spacy/tests/doc/test_span.py,266,Test filtering overlaps with longest preference,
spaCy/spacy/tests/doc/test_span.py,274,Test filtering overlaps with earlier preference for identical length,
spaCy/spacy/tests/doc/test_morphanalysis.py,1,coding: utf-8,
spaCy/spacy/tests/doc/test_to_json.py,1,coding: utf-8,
spaCy/spacy/tests/doc/test_to_json.py,32,character offset!,
spaCy/spacy/tests/doc/test_to_json.py,33,character offset!,
spaCy/spacy/tests/doc/test_doc_api.py,1,coding: utf-8,
spaCy/spacy/tests/doc/test_doc_api.py,18,"Get the tokens in this order, so their ID ordering doesn't match the idx",
spaCy/spacy/tests/doc/test_doc_api.py,150,Example that caused run-time error while parsing Reddit,
spaCy/spacy/tests/doc/test_doc_api.py,151,fmt: off,
spaCy/spacy/tests/doc/test_doc_api.py,156,fmt: on,
spaCy/spacy/tests/doc/test_doc_api.py,177,fmt: off,
spaCy/spacy/tests/doc/test_doc_api.py,181,fmt: on,
spaCy/spacy/tests/doc/test_doc_api.py,267,Test creating doc from array with unknown values,
spaCy/spacy/tests/doc/test_doc_api.py,271,Test serialization,
spaCy/spacy/tests/doc/test_doc_api.py,279,fmt: off,
spaCy/spacy/tests/doc/test_doc_api.py,281,fmt: on,
spaCy/spacy/tests/doc/test_array.py,1,coding: utf-8,
spaCy/spacy/tests/doc/test_array.py,87,correct,
spaCy/spacy/tests/doc/test_array.py,92,head before start,
spaCy/spacy/tests/doc/test_array.py,99,head after end,
spaCy/spacy/tests/doc/test_creation.py,1,coding: utf-8,
spaCy/spacy/tests/doc/test_creation.py,45,no whitespace in words,
spaCy/spacy/tests/doc/test_creation.py,55,partial whitespace in words,
spaCy/spacy/tests/doc/test_creation.py,65,non-standard whitespace tokens,
spaCy/spacy/tests/doc/test_creation.py,75,mismatch between words and text,
spaCy/spacy/tests/doc/test_retokenize_merge.py,1,coding: utf-8,
spaCy/spacy/tests/doc/test_retokenize_merge.py,62,test both string and integer attributes and values,
spaCy/spacy/tests/doc/test_retokenize_merge.py,212,fmt: off,
spaCy/spacy/tests/doc/test_retokenize_merge.py,217,fmt: on,
spaCy/spacy/tests/doc/test_retokenize_merge.py,228,check looping is ok,
spaCy/spacy/tests/doc/test_retokenize_merge.py,233,Test entity IOB stays consistent after merging,
spaCy/spacy/tests/doc/test_retokenize_merge.py,250,Test that IOB stays consistent with provided IOB,
spaCy/spacy/tests/doc/test_retokenize_merge.py,260,"if no parse/heads, the first word in the span is the root and provides",
spaCy/spacy/tests/doc/test_retokenize_merge.py,261,default values,
spaCy/spacy/tests/doc/test_retokenize_merge.py,282,"if there is a parse, span.root provides default values",
spaCy/spacy/tests/doc/test_retokenize_merge.py,291,root of 'c d' is d,
spaCy/spacy/tests/doc/test_retokenize_merge.py,292,root is 'e f' is e,
spaCy/spacy/tests/doc/test_retokenize_merge.py,305,check that B is preserved if span[start] is B,
spaCy/spacy/tests/doc/test_retokenize_merge.py,322,fmt: off,
spaCy/spacy/tests/doc/test_retokenize_merge.py,328,fmt: on,
spaCy/spacy/tests/doc/test_retokenize_merge.py,343,fmt: off,
spaCy/spacy/tests/doc/test_retokenize_merge.py,349,fmt: on,
spaCy/spacy/tests/doc/test_retokenize_merge.py,364,Test regular merging,
spaCy/spacy/tests/doc/test_retokenize_merge.py,371,Test bulk merging,
spaCy/spacy/tests/doc/test_retokenize_merge.py,399,Test regular merging,
spaCy/spacy/tests/doc/test_retokenize_merge.py,406,Test bulk merging,
spaCy/spacy/tests/parser/test_ner.py,1,coding: utf-8,
spaCy/spacy/tests/parser/test_ner.py,132,1. test normal behaviour,
spaCy/spacy/tests/parser/test_ner.py,139,Add the OUT action,
spaCy/spacy/tests/parser/test_ner.py,142,"Get into the state just before ""New""",
spaCy/spacy/tests/parser/test_ner.py,147,Check that B-GPE is valid.,
spaCy/spacy/tests/parser/test_ner.py,150,2. test blocking behaviour,
spaCy/spacy/tests/parser/test_ner.py,155,"set ""New York"" to a blocked entity",
spaCy/spacy/tests/parser/test_ner.py,160,Check that B-GPE is now invalid.,
spaCy/spacy/tests/parser/test_ner.py,168,"we can only use U- for ""New""",
spaCy/spacy/tests/parser/test_ner.py,172,"we can only use U- for ""York""",
spaCy/spacy/tests/parser/test_ner.py,183,The untrained NER will predict O for each token,
spaCy/spacy/tests/parser/test_ner.py,188,Check that a new ner can overwrite O,
spaCy/spacy/tests/parser/test_ner.py,204,"1 : Entity Ruler - should set ""this"" to B and everything else to empty",
spaCy/spacy/tests/parser/test_ner.py,210,2: untrained NER - should set everything else to O,
spaCy/spacy/tests/parser/test_ner.py,227,1: untrained NER - should set everything to O,
spaCy/spacy/tests/parser/test_ner.py,233,"2 : Entity Ruler - should set ""this"" to B and keep everything else O",
spaCy/spacy/tests/parser/test_ner.py,248,"block ""Antti L Korhonen"" from being a named entity",
spaCy/spacy/tests/parser/test_ner.py,263,Test the default number features,
spaCy/spacy/tests/parser/test_ner.py,270,Test we can change it,
spaCy/spacy/tests/parser/test_ner.py,279,Test the model runs,
spaCy/spacy/tests/parser/test_parse_navigate.py,1,coding: utf-8,
spaCy/spacy/tests/parser/test_parse_navigate.py,36,fmt: off,
spaCy/spacy/tests/parser/test_parse_navigate.py,54,fmt: on,
spaCy/spacy/tests/parser/test_parse.py,1,coding: utf-8,
spaCy/spacy/tests/parser/test_parse.py,28,noqa: F841,
spaCy/spacy/tests/parser/test_parse.py,36,"heads = [1, 0, 1, -2, -3, -1, -5]",
spaCy/spacy/tests/parser/test_parse.py,82,right branching,
spaCy/spacy/tests/parser/test_parse.py,117,left branching,
spaCy/spacy/tests/parser/test_parse.py,154,fmt: off,
spaCy/spacy/tests/parser/test_parse.py,158,fmt: on,
spaCy/spacy/tests/parser/test_add_label.py,1,coding: utf8,
spaCy/spacy/tests/parser/test_neural_parser.py,1,coding: utf8,
spaCy/spacy/tests/parser/test_nonproj.py,1,coding: utf-8,
spaCy/spacy/tests/parser/test_nonproj.py,93,fmt: off,
spaCy/spacy/tests/parser/test_nonproj.py,99,fmt: on,
spaCy/spacy/tests/parser/test_nonproj.py,112,fmt: off,
spaCy/spacy/tests/parser/test_nonproj.py,132,if decoration is wrong such that there is no head with the desired label,
spaCy/spacy/tests/parser/test_nonproj.py,133,the structure is kept and the label is undecorated,
spaCy/spacy/tests/parser/test_nonproj.py,143,"if there are two potential new heads, the first one is chosen even if",
spaCy/spacy/tests/parser/test_nonproj.py,144,"it""s wrong",
spaCy/spacy/tests/parser/test_nonproj.py,155,fmt: on,
spaCy/spacy/tests/parser/test_preset_sbd.py,1,coding: utf8,
spaCy/spacy/tests/parser/test_preset_sbd.py,24,parser.add_label('right'),
spaCy/spacy/tests/parser/test_arc_eager_oracle.py,1,coding: utf8,
spaCy/spacy/tests/parser/test_arc_eager_oracle.py,72,Check gold moves is 0 cost,
spaCy/spacy/tests/parser/test_space_attachment.py,1,coding: utf-8,
spaCy/spacy/tests/parser/test_space_attachment.py,22,fmt: off,
spaCy/spacy/tests/parser/test_space_attachment.py,28,fmt: on,
spaCy/spacy/tests/parser/test_space_attachment.py,74,noqa: F841,
spaCy/spacy/tests/parser/test_nn_beam.py,1,coding: utf8,
spaCy/spacy/tests/pipeline/test_factories.py,1,coding: utf8,
spaCy/spacy/tests/pipeline/test_tagger.py,1,coding: utf8,
spaCy/spacy/tests/pipeline/test_entity_ruler.py,1,coding: utf8,
spaCy/spacy/tests/pipeline/test_entity_ruler.py,141,invalid pattern raises error without validate,
spaCy/spacy/tests/pipeline/test_entity_ruler.py,145,valid pattern is added without errors with validate,
spaCy/spacy/tests/pipeline/test_entity_ruler.py,148,invalid pattern raises error with validate,
spaCy/spacy/tests/pipeline/test_textcat.py,1,coding: utf8,
spaCy/spacy/tests/pipeline/test_entity_linker.py,1,coding: utf-8,
spaCy/spacy/tests/pipeline/test_entity_linker.py,26,adding entities,
spaCy/spacy/tests/pipeline/test_entity_linker.py,31,adding aliases,
spaCy/spacy/tests/pipeline/test_entity_linker.py,35,test the size of the corresponding KB,
spaCy/spacy/tests/pipeline/test_entity_linker.py,39,test retrieval of the entity vectors,
spaCy/spacy/tests/pipeline/test_entity_linker.py,44,test retrieval of prior probabilities,
spaCy/spacy/tests/pipeline/test_entity_linker.py,55,adding entities,
spaCy/spacy/tests/pipeline/test_entity_linker.py,60,adding aliases - should fail because one of the given IDs is not valid,
spaCy/spacy/tests/pipeline/test_entity_linker.py,71,adding entities,
spaCy/spacy/tests/pipeline/test_entity_linker.py,76,adding aliases - should fail because the sum of the probabilities exceeds 1,
spaCy/spacy/tests/pipeline/test_entity_linker.py,85,adding entities,
spaCy/spacy/tests/pipeline/test_entity_linker.py,90,adding aliases - should fail because the entities and probabilities vectors are not of equal length,
spaCy/spacy/tests/pipeline/test_entity_linker.py,101,adding entities,
spaCy/spacy/tests/pipeline/test_entity_linker.py,104,this should fail because the kb's expected entity vector length is 3,
spaCy/spacy/tests/pipeline/test_entity_linker.py,113,adding entities,
spaCy/spacy/tests/pipeline/test_entity_linker.py,118,adding aliases,
spaCy/spacy/tests/pipeline/test_entity_linker.py,122,test the size of the relevant candidates,
spaCy/spacy/tests/pipeline/test_entity_linker.py,127,test the content of the candidates,
spaCy/spacy/tests/pipeline/test_entity_linker.py,138,adding entities,
spaCy/spacy/tests/pipeline/test_entity_linker.py,143,adding aliases,
spaCy/spacy/tests/pipeline/test_entity_linker.py,147,test the size of the relevant candidates,
spaCy/spacy/tests/pipeline/test_entity_linker.py,150,append an alias,
spaCy/spacy/tests/pipeline/test_entity_linker.py,153,test the size of the relevant candidates has been incremented,
spaCy/spacy/tests/pipeline/test_entity_linker.py,156,append the same alias-entity pair again should not work (will throw a warning),
spaCy/spacy/tests/pipeline/test_entity_linker.py,160,test the size of the relevant candidates remained unchanged,
spaCy/spacy/tests/pipeline/test_entity_linker.py,168,adding entities,
spaCy/spacy/tests/pipeline/test_entity_linker.py,173,adding aliases,
spaCy/spacy/tests/pipeline/test_entity_linker.py,177,append an alias - should fail because the entities and probabilities vectors are not of equal length,
spaCy/spacy/tests/pipeline/test_entity_linker.py,186,adding entities,
spaCy/spacy/tests/pipeline/test_entity_linker.py,190,adding aliases,
spaCy/spacy/tests/pipeline/test_entity_linker.py,194,"set up pipeline with NER (Entity Ruler) and NEL (prior probability only, model not trained)",
spaCy/spacy/tests/pipeline/test_entity_linker.py,213,test whether the entity links are preserved by the `as_doc()` function,
spaCy/spacy/tests/pipeline/test_analysis.py,1,coding: utf8,
spaCy/spacy/tests/pipeline/test_analysis.py,119,"The first argument here is the class itself, so we're accepting any here",
spaCy/spacy/tests/pipeline/test_functions.py,1,coding: utf-8,
spaCy/spacy/tests/pipeline/test_functions.py,11,fmt: off,
spaCy/spacy/tests/pipeline/test_functions.py,16,fmt: on,
spaCy/spacy/tests/pipeline/test_functions.py,23,"get_doc() doesn't set spaces, so the result is ""And a third .""",
spaCy/spacy/tests/pipeline/test_sentencizer.py,1,coding: utf8,
spaCy/spacy/tests/pipeline/test_sentencizer.py,51,The expected result here is that the duplicate punctuation gets merged,
spaCy/spacy/tests/pipeline/test_sentencizer.py,52,onto the same sentence and no one-token sentence is created for them.,
spaCy/spacy/tests/pipeline/test_sentencizer.py,58,We also want to make sure ¡ and ¿ aren't treated as sentence end,
spaCy/spacy/tests/pipeline/test_sentencizer.py,59,"markers, even though they're punctuation",
spaCy/spacy/tests/pipeline/test_sentencizer.py,65,The Token.is_punct check ensures that quotes are handled as well,
spaCy/spacy/tests/pipeline/test_sentencizer.py,91,"Even thought it's not common, the punct_chars should be able to",
spaCy/spacy/tests/pipeline/test_sentencizer.py,92,handle any tokens,
spaCy/spacy/tests/pipeline/test_sentencizer.py,120,fmt: off,
spaCy/spacy/tests/pipeline/test_sentencizer.py,132,fmt: on,
spaCy/spacy/tests/pipeline/test_pipe_methods.py,1,coding: utf8,
spaCy/spacy/tests/pipeline/test_pipe_methods.py,137,Tagger always has the default coarse-grained label scheme,
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,1,coding: utf-8,
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,6,"Examples taken from the ""Big List of Naughty Strings""",
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,7,https://github.com/minimaxir/big-list-of-naughty-strings,
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,11,ASCII punctuation,
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,15,"Unicode additional control characters, byte order marks",
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,18,Unicode Symbols,
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,30,Unicode Subscript/Superscript/Accents,
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,35,Two-Byte Characters,
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,45,Japanese Emoticons,
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,56,Emoji,
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,65,Regional Indicator Symbols,
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,69,Unicode Numbers,
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,72,Right-To-Left Strings,
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,80,Trick Unicode,
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,86,Zalgo Text,
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,92,Unicode Upsidedown,
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,95,Unicode font,
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,104,File paths,
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,107,iOS Vulnerabilities,
spaCy/spacy/tests/tokenizer/test_urls.py,1,coding: utf-8,
spaCy/spacy/tests/tokenizer/test_urls.py,20,URL SHOULD_MATCH and SHOULD_NOT_MATCH patterns courtesy of https://mathiasbynens.be/demo/url-regex,
spaCy/spacy/tests/tokenizer/test_urls.py,49,this is a legit domain name see: https://gist.github.com/dperini/729294 comment on 9/9/2014,
spaCy/spacy/tests/tokenizer/test_urls.py,115,Punctuation we want to check is split away before the URL,
spaCy/spacy/tests/tokenizer/test_urls.py,119,Punctuation we want to check is split away after the URL,
spaCy/spacy/tests/tokenizer/test_tokenizer.py,1,coding: utf-8,
spaCy/spacy/tests/tokenizer/test_whitespace.py,1,coding: utf-8,
spaCy/spacy/tests/tokenizer/test_exceptions.py,1,coding: utf-8,
spaCy/spacy/tests/tokenizer/test_exceptions.py,9,Tweebo challenge (CMU),
spaCy/spacy/tests/tokenizer/test_exceptions.py,46,"These break on narrow unicode builds, e.g. Windows",
spaCy/spacy/tests/tokenizer/test_explain.py,1,coding: utf-8,
spaCy/spacy/tests/tokenizer/test_explain.py,7,Only include languages with no external dependencies,
spaCy/spacy/tests/tokenizer/test_explain.py,8,"""is"" seems to confuse importlib, so we're also excluding it for now",
spaCy/spacy/tests/tokenizer/test_explain.py,9,"excluded: ja, ru, th, uk, vi, zh, is",
spaCy/spacy/tests/lang/test_initialize.py,1,coding: utf-8,
spaCy/spacy/tests/lang/test_initialize.py,8,fmt: off,
spaCy/spacy/tests/lang/test_initialize.py,9,Only include languages with no external dependencies,
spaCy/spacy/tests/lang/test_initialize.py,10,"excluded: ja, ru, th, uk, vi, zh",
spaCy/spacy/tests/lang/test_initialize.py,15,fmt: on,
spaCy/spacy/tests/lang/test_initialize.py,22,Check for stray print statements (see #3342),
spaCy/spacy/tests/lang/test_initialize.py,23,noqa: F841,
spaCy/spacy/tests/lang/test_attrs.py,1,coding: utf-8,
spaCy/spacy/tests/lang/ar/test_text.py,1,coding: utf8,
spaCy/spacy/tests/lang/ar/test_exceptions.py,1,coding: utf-8,
spaCy/spacy/tests/lang/pl/test_tokenizer.py,1,coding: utf8,
spaCy/spacy/tests/lang/pl/test_text.py,1,coding: utf-8,
spaCy/spacy/tests/lang/da/test_text.py,1,coding: utf-8,
spaCy/spacy/tests/lang/da/test_exceptions.py,1,coding: utf-8,
spaCy/spacy/tests/lang/da/test_exceptions.py,61,note: skipping due to weirdness in UD_Danish-DDT,
spaCy/spacy/tests/lang/da/test_exceptions.py,62,"(""Rotorhastigheden er 3400 o/m."", 5),",
spaCy/spacy/tests/lang/da/test_prefix_suffix_infix.py,1,coding: utf-8,
spaCy/spacy/tests/lang/lt/test_text.py,1,coding: utf-8,
spaCy/spacy/tests/lang/hy/test_tokenizer.py,6,TODO add test cases with valid punctuation signs.,
spaCy/spacy/tests/lang/uk/test_tokenizer.py,1,coding: utf-8,
spaCy/spacy/tests/lang/uk/test_tokenizer_exc.py,1,coding: utf-8,
spaCy/spacy/tests/lang/tt/test_tokenizer.py,1,coding: utf8,
spaCy/spacy/tests/lang/tt/test_tokenizer.py,42,"""?)"" => ""?)"" or ""? )""",
spaCy/spacy/tests/lang/it/test_prefix_suffix_infix.py,1,coding: utf-8,
spaCy/spacy/tests/lang/lb/test_text.py,1,coding: utf-8,
spaCy/spacy/tests/lang/lb/test_exceptions.py,1,coding: utf-8,
spaCy/spacy/tests/lang/lb/test_prefix_suffix_infix.py,1,coding: utf-8,
spaCy/spacy/tests/lang/fi/test_tokenizer.py,1,coding: utf8,
spaCy/spacy/tests/lang/fi/test_text.py,1,coding: utf-8,
spaCy/spacy/tests/lang/nb/test_tokenizer.py,1,coding: utf8,
spaCy/spacy/tests/lang/ga/test_tokenizer.py,1,coding: utf8,
spaCy/spacy/tests/lang/ga/test_tokenizer.py,7,fmt: off,
spaCy/spacy/tests/lang/ga/test_tokenizer.py,12,fmt: on,
spaCy/spacy/tests/lang/de/test_text.py,1,coding: utf-8,
spaCy/spacy/tests/lang/de/test_parser.py,1,coding: utf-8,
spaCy/spacy/tests/lang/de/test_exceptions.py,1,coding: utf-8,
spaCy/spacy/tests/lang/de/test_prefix_suffix_infix.py,1,coding: utf-8,
spaCy/spacy/tests/lang/hu/test_tokenizer.py,1,coding: utf8,
spaCy/spacy/tests/lang/hu/test_tokenizer.py,303,normal: default tests + 10% of extra tests,
spaCy/spacy/tests/lang/hu/test_tokenizer.py,307,slow: remaining 90% of extra tests,
spaCy/spacy/tests/lang/ru/test_lemmatizer.py,1,coding: utf-8,
spaCy/spacy/tests/lang/ru/test_tokenizer.py,1,coding: utf-8,
spaCy/spacy/tests/lang/ru/test_text.py,1,coding: utf-8,
spaCy/spacy/tests/lang/ru/test_exceptions.py,1,coding: utf-8,
spaCy/spacy/tests/lang/ro/test_tokenizer.py,1,coding: utf8,
spaCy/spacy/tests/lang/ro/test_tokenizer.py,15,number tests,
spaCy/spacy/tests/lang/es/test_text.py,1,coding: utf-8,
spaCy/spacy/tests/lang/es/test_exception.py,1,coding: utf-8,
spaCy/spacy/tests/lang/zh/test_tokenizer.py,1,coding: utf-8,
spaCy/spacy/tests/lang/zh/test_tokenizer.py,7,fmt: off,
spaCy/spacy/tests/lang/zh/test_tokenizer.py,14,fmt: on,
spaCy/spacy/tests/lang/zh/test_tokenizer.py,29,"note: three spaces after ""I""",
spaCy/spacy/tests/lang/zh/test_text.py,1,coding: utf-8,
spaCy/spacy/tests/lang/sv/test_noun_chunks.py,1,coding: utf-8,
spaCy/spacy/tests/lang/sv/test_noun_chunks.py,10,A student read a book,
spaCy/spacy/tests/lang/sv/test_noun_chunks.py,17,The student read the best book,
spaCy/spacy/tests/lang/sv/test_noun_chunks.py,24,The remorseless crooks had stolen the largest jewels that sunday,
spaCy/spacy/tests/lang/sv/test_tokenizer.py,1,coding: utf8,
spaCy/spacy/tests/lang/sv/test_text.py,1,coding: utf-8,
spaCy/spacy/tests/lang/sv/test_exceptions.py,1,coding: utf8,
spaCy/spacy/tests/lang/sv/test_prefix_suffix_infix.py,1,coding: utf-8,
spaCy/spacy/tests/lang/pt/test_text.py,1,coding: utf-8,
spaCy/spacy/tests/lang/sr/test_tokenizer.py,1,coding: utf-8,
spaCy/spacy/tests/lang/sr/test_exceptions.py,1,coding: utf-8,
spaCy/spacy/tests/lang/id/test_text.py,1,coding: utf-8,
spaCy/spacy/tests/lang/id/test_prefix_suffix_infix.py,1,coding: utf-8,
spaCy/spacy/tests/lang/ca/test_text.py,1,coding: utf-8,
spaCy/spacy/tests/lang/ca/test_exception.py,1,coding: utf-8,
spaCy/spacy/tests/lang/ca/test_prefix_suffix_infix.py,1,coding: utf-8,
spaCy/spacy/tests/lang/yo/test_text.py,1,coding: utf8,
spaCy/spacy/tests/lang/th/test_tokenizer.py,1,coding: utf8,
spaCy/spacy/tests/lang/en/test_noun_chunks.py,1,coding: utf-8,
spaCy/spacy/tests/lang/en/test_tagger.py,1,coding: utf-8,
spaCy/spacy/tests/lang/en/test_text.py,1,coding: utf-8,
spaCy/spacy/tests/lang/en/test_indices.py,1,coding: utf-8,
spaCy/spacy/tests/lang/en/test_parser.py,1,coding: utf-8,
spaCy/spacy/tests/lang/en/test_parser.py,22,fmt: off,
spaCy/spacy/tests/lang/en/test_parser.py,27,fmt: on,
spaCy/spacy/tests/lang/en/test_parser.py,54,fmt: off,
spaCy/spacy/tests/lang/en/test_parser.py,59,fmt: on,
spaCy/spacy/tests/lang/en/test_sbd.py,1,coding: utf-8,
spaCy/spacy/tests/lang/en/test_sbd.py,22,fmt: off,
spaCy/spacy/tests/lang/en/test_sbd.py,29,fmt: on,
spaCy/spacy/tests/lang/en/test_punct.py,1,coding: utf-8,
spaCy/spacy/tests/lang/en/test_customized_tokenizer.py,1,coding: utf-8,
spaCy/spacy/tests/lang/en/test_customized_tokenizer.py,56,the trailing '-' may cause Assertion Error,
spaCy/spacy/tests/lang/en/test_exceptions.py,1,coding: utf-8,
spaCy/spacy/tests/lang/en/test_prefix_suffix_infix.py,1,coding: utf-8,
spaCy/spacy/tests/lang/en/test_prefix_suffix_infix.py,128,Re Issue #225,
spaCy/spacy/tests/lang/fr/test_text.py,1,coding: utf8,
spaCy/spacy/tests/lang/fr/test_exceptions.py,1,coding: utf-8,
spaCy/spacy/tests/lang/fr/test_exceptions.py,25,"u""K-POP"",",
spaCy/spacy/tests/lang/fr/test_exceptions.py,26,"u""K-Pop"",",
spaCy/spacy/tests/lang/fr/test_exceptions.py,27,"u""K-pop"",",
spaCy/spacy/tests/lang/fr/test_prefix_suffix_infix.py,1,coding: utf-8,
spaCy/spacy/tests/lang/el/test_text.py,1,coding: utf8,
spaCy/spacy/tests/lang/el/test_exception.py,1,coding: utf8,
spaCy/spacy/tests/lang/ja/test_tokenizer.py,1,coding: utf-8,
spaCy/spacy/tests/lang/ja/test_tokenizer.py,7,fmt: off,
spaCy/spacy/tests/lang/ja/test_tokenizer.py,31,fmt: on,
spaCy/spacy/tests/lang/ja/test_tokenizer.py,53,"note: three spaces after ""I""",
spaCy/spacy/tests/lang/ja/test_lemmatization.py,1,coding: utf-8,
spaCy/spacy/tests/lang/ko/test_tokenizer.py,1,coding: utf-8,
spaCy/spacy/tests/lang/ko/test_tokenizer.py,6,fmt: off,
spaCy/spacy/tests/lang/ko/test_tokenizer.py,23,fmt: on,
spaCy/spacy/tests/lang/ko/test_lemmatization.py,1,coding: utf-8,
spaCy/spacy/tests/lang/he/test_tokenizer.py,1,encoding: utf8,
spaCy/spacy/tests/lang/eu/test_text.py,1,coding: utf-8,
spaCy/spacy/tests/lang/nl/test_text.py,1,coding: utf-8,
spaCy/spacy/tests/lang/ur/test_text.py,1,coding: utf-8,
spaCy/spacy/tests/lang/ur/test_prefix_suffix_infix.py,1,coding: utf-8,
spaCy/spacy/tests/lang/bn/test_tokenizer.py,1,coding: utf8,
spaCy/spacy/tests/lang/bn/test_tokenizer.py,7,fmt: off,
spaCy/spacy/tests/lang/bn/test_tokenizer.py,9,Punctuation tests,
spaCy/spacy/tests/lang/bn/test_tokenizer.py,18,Abbreviations,
spaCy/spacy/tests/lang/bn/test_tokenizer.py,21,fmt: on,
spaCy/spacy/cli/_schemas.py,1,coding: utf-8,
spaCy/spacy/cli/_schemas.py,5,"NB: This schema describes the new format of the training data, see #2928",
spaCy/spacy/cli/validate.py,1,coding: utf8,
spaCy/spacy/cli/convert.py,1,coding: utf8,
spaCy/spacy/cli/convert.py,14,"Converters are matched by file extension except for ner/iob, which are",
spaCy/spacy/cli/convert.py,15,"matched by file extension and content. To add a converter, add a new",
spaCy/spacy/cli/convert.py,16,entry to this dict with the file extension mapped to the converter function,
spaCy/spacy/cli/convert.py,17,imported from /converters.,
spaCy/spacy/cli/convert.py,27,File types,
spaCy/spacy/cli/convert.py,70,TODO: support msgpack via stdout in srsly?,
spaCy/spacy/cli/convert.py,97,Use converter function to convert data,
spaCy/spacy/cli/convert.py,109,Export data to a file,
spaCy/spacy/cli/convert.py,122,Print to stdout,
spaCy/spacy/cli/convert.py,130,guess format from the first 20 lines,
spaCy/spacy/cli/profile.py,1,coding: utf8,
spaCy/spacy/cli/pretrain.py,1,coding: utf8,
spaCy/spacy/cli/pretrain.py,147,Load texts from file or stdin,
spaCy/spacy/cli/pretrain.py,148,reading from a file,
spaCy/spacy/cli/pretrain.py,158,reading from stdin,
spaCy/spacy/cli/pretrain.py,173,Requires PyTorch. Experimental.,
spaCy/spacy/cli/pretrain.py,174,Set to False for Chinese etc,
spaCy/spacy/cli/pretrain.py,175,"If set to 1, use Mish activation.",
spaCy/spacy/cli/pretrain.py,178,Load in pretrained weights,
spaCy/spacy/cli/pretrain.py,182,Parse the epoch number from the given weight file,
spaCy/spacy/cli/pretrain.py,185,Default weight file name so read epoch_start from it by cutting off 'model' and '.bin',
spaCy/spacy/cli/pretrain.py,201,Without '--init-tok2vec' the '--epoch-start' argument is ignored,
spaCy/spacy/cli/pretrain.py,251,Reshuffle the texts if texts were loaded from a file,
spaCy/spacy/cli/pretrain.py,269,Don't want to return a cupy object here,
spaCy/spacy/cli/pretrain.py,270,"The gradients are modified in-place by the BERT MLM,",
spaCy/spacy/cli/pretrain.py,271,so we get an accurate loss,
spaCy/spacy/cli/pretrain.py,313,The simplest way to implement this would be to vstack the,
spaCy/spacy/cli/pretrain.py,314,"token.vector values, but that's a bit inefficient, especially on GPU.",
spaCy/spacy/cli/pretrain.py,315,"Instead we fetch the index into the vectors table for each of our tokens,",
spaCy/spacy/cli/pretrain.py,316,and look them up all at once. This prevents data copying.,
spaCy/spacy/cli/pretrain.py,339,"This is annoying, but the parser etc have the flatten step after",
spaCy/spacy/cli/pretrain.py,340,"the tok2vec. To load the weights in cleanly, we need to match",
spaCy/spacy/cli/pretrain.py,341,the shape of the models' components exactly. So what we cann,
spaCy/spacy/cli/pretrain.py,342,"""tok2vec"" has to be the same set of processes as what the components do.",
spaCy/spacy/cli/download.py,1,coding: utf8,
spaCy/spacy/cli/download.py,49,"if download subprocess doesn't return 0, exit",
spaCy/spacy/cli/download.py,55,Only create symlink if the model is installed via a shortcut like 'en'.,
spaCy/spacy/cli/download.py,56,There's no real advantage over an additional symlink for en_core_web_sm,
spaCy/spacy/cli/download.py,57,"and if anything, it's more error prone and causes more confusion.",
spaCy/spacy/cli/download.py,60,Get package path here because link uses,
spaCy/spacy/cli/download.py,61,pip.get_installed_distributions() to check if model is a,
spaCy/spacy/cli/download.py,62,"package, which fails if model was just installed via",
spaCy/spacy/cli/download.py,63,subprocess,
spaCy/spacy/cli/download.py,66,noqa: E722,
spaCy/spacy/cli/download.py,67,"Dirty, but since spacy.download and the auto-linking is",
spaCy/spacy/cli/download.py,68,"mostly a convenience wrapper, it's best to show a success",
spaCy/spacy/cli/download.py,69,"message and loading instructions, even if linking fails.",
spaCy/spacy/cli/download.py,77,"If a model is downloaded and then loaded within the same process, our",
spaCy/spacy/cli/download.py,78,"is_package check currently fails, because pkg_resources.working_set",
spaCy/spacy/cli/download.py,79,is not refreshed automatically (see #3923). We're trying to work,
spaCy/spacy/cli/download.py,80,around this here be requiring the package explicitly.,
spaCy/spacy/cli/download.py,90,noqa: E722,
spaCy/spacy/cli/info.py,1,coding: utf8,
spaCy/spacy/cli/info.py,68,exclude common cache directories and hidden directories,
spaCy/spacy/cli/debug_data.py,1,coding: utf8,
spaCy/spacy/cli/debug_data.py,16,Minimum number of expected occurrences of NER label in data to train new label,
spaCy/spacy/cli/debug_data.py,18,Minimum number of expected occurrences of dependency labels,
spaCy/spacy/cli/debug_data.py,20,Minimum number of expected examples to train a blank model,
spaCy/spacy/cli/debug_data.py,26,fmt: off,
spaCy/spacy/cli/debug_data.py,36,fmt: on,
spaCy/spacy/cli/debug_data.py,56,Make sure all files and paths exists if they are needed,
spaCy/spacy/cli/debug_data.py,66,Initialize the model and pipeline,
spaCy/spacy/cli/debug_data.py,73,Update tag map with provided mapping,
spaCy/spacy/cli/debug_data.py,78,TODO: Validate data format using the JSON schema,
spaCy/spacy/cli/debug_data.py,79,TODO: update once the new format is ready,
spaCy/spacy/cli/debug_data.py,80,TODO: move validation to GoldCorpus in order to be able to load from dir,
spaCy/spacy/cli/debug_data.py,82,Create the gold corpus to be able to better analyze data,
spaCy/spacy/cli/debug_data.py,110,Create all gold data here to avoid iterating over the train_docs constantly,
spaCy/spacy/cli/debug_data.py,189,Get all unique NER labels present in the data,
spaCy/spacy/cli/debug_data.py,383,profile sentence length,
spaCy/spacy/cli/debug_data.py,392,check for documents with multiple sentences,
spaCy/spacy/cli/debug_data.py,402,profile labels,
spaCy/spacy/cli/debug_data.py,441,rare labels in train,
spaCy/spacy/cli/debug_data.py,451,rare labels in projectivized train,
spaCy/spacy/cli/debug_data.py,476,labels only in train,
spaCy/spacy/cli/debug_data.py,484,labels only in dev,
spaCy/spacy/cli/debug_data.py,499,multiple root labels,
spaCy/spacy/cli/debug_data.py,509,"these should not happen, but just in case",
spaCy/spacy/cli/debug_data.py,594,"""Illegal"" whitespace entity",
spaCy/spacy/cli/debug_data.py,603,"punctuation entity: could be replaced by whitespace when training with noise,",
spaCy/spacy/cli/debug_data.py,604,so add a warning to alert the user to this unexpected side effect.,
spaCy/spacy/cli/package.py,1,coding: utf8,
spaCy/spacy/cli/package.py,43,only print if user doesn't want to overwrite,
spaCy/spacy/cli/link.py,1,coding: utf8,
spaCy/spacy/cli/link.py,50,does a symlink exist?,
spaCy/spacy/cli/link.py,51,"NB: It's important to check for is_symlink here and not for exists,",
spaCy/spacy/cli/link.py,52,because invalid/outdated symlinks would return False otherwise.,
spaCy/spacy/cli/link.py,54,does it exist otherwise?,
spaCy/spacy/cli/link.py,55,"NB: Check this last because valid symlinks also ""exist"".",
spaCy/spacy/cli/link.py,65,noqa: E722,
spaCy/spacy/cli/link.py,66,"This is quite dirty, but just making sure other errors are caught.",
spaCy/spacy/cli/init_model.py,1,coding: utf8,
spaCy/spacy/cli/init_model.py,125,noqa: F841,
spaCy/spacy/cli/init_model.py,137,"Decode as a little-endian string, so that we can do & 15 to get",
spaCy/spacy/cli/init_model.py,138,the first 4 bits. See _parse_features.pyx,
spaCy/spacy/cli/init_model.py,238,Take odd strings literally.,
spaCy/spacy/cli/init_model.py,258,"If the clusterer has only seen the word a few times, its",
spaCy/spacy/cli/init_model.py,259,cluster is unreliable.,
spaCy/spacy/cli/init_model.py,264,Expand clusters with re-casing,
spaCy/spacy/cli/train.py,1,coding: utf8,
spaCy/spacy/cli/train.py,26,fmt: off,
spaCy/spacy/cli/train.py,63,fmt: on,
spaCy/spacy/cli/train.py,111,Make sure all files and paths exists if they are needed,
spaCy/spacy/cli/train.py,140,Take dropout and batch size as generators of values -- dropout,
spaCy/spacy/cli/train.py,141,"starts high and decays sharply, to force the optimizer to explore.",
spaCy/spacy/cli/train.py,142,"Batch size starts at 1 and grows, so that we make updates quickly",
spaCy/spacy/cli/train.py,143,at the beginning of training.,
spaCy/spacy/cli/train.py,164,"Set up the base model and pipeline. If a base model is specified, load",
spaCy/spacy/cli/train.py,165,the model and make sure the pipeline matches the pipeline setting. If,
spaCy/spacy/cli/train.py,166,"training starts from a blank model, intitalize the language class.",
spaCy/spacy/cli/train.py,248,Update tag map with provided mapping,
spaCy/spacy/cli/train.py,255,Multitask objectives,
spaCy/spacy/cli/train.py,268,Prepare training corpus,
spaCy/spacy/cli/train.py,274,"Start with an existing model, use default optimizer",
spaCy/spacy/cli/train.py,277,"Start with a blank model, call begin_training",
spaCy/spacy/cli/train.py,290,Load in pretrained weights,
spaCy/spacy/cli/train.py,295,Verify textcat config,
spaCy/spacy/cli/train.py,379,fmt: off,
spaCy/spacy/cli/train.py,383,fmt: on,
spaCy/spacy/cli/train.py,425,"If raw text is available, perform 'rehearsal' updates,",
spaCy/spacy/cli/train.py,426,which use unlabelled data to reduce overfitting.,
spaCy/spacy/cli/train.py,476,Update model meta.json,
spaCy/spacy/cli/train.py,532,Early stopping,
spaCy/spacy/cli/train.py,572,combine cpu and gpu speeds with the base model speeds,
spaCy/spacy/cli/train.py,583,"if there were no speeds to update, overwrite with meta",
spaCy/spacy/cli/train.py,589,note: beam speeds are not combined with the base model,
spaCy/spacy/cli/train.py,632,These attrs are expected to be set by data. Others should,
spaCy/spacy/cli/train.py,633,be set by calling the language functions.,
spaCy/spacy/cli/train.py,679,remove per_type dicts from score list for max() comparison,
spaCy/spacy/cli/__init__.py,1,noqa: F401,
spaCy/spacy/cli/__init__.py,2,noqa: F401,
spaCy/spacy/cli/__init__.py,3,noqa: F401,
spaCy/spacy/cli/__init__.py,4,noqa: F401,
spaCy/spacy/cli/__init__.py,5,noqa: F401,
spaCy/spacy/cli/__init__.py,6,noqa: F401,
spaCy/spacy/cli/__init__.py,7,noqa: F401,
spaCy/spacy/cli/__init__.py,8,noqa: F401,
spaCy/spacy/cli/__init__.py,9,noqa: F401,
spaCy/spacy/cli/__init__.py,10,noqa: F401,
spaCy/spacy/cli/__init__.py,11,noqa: F401,
spaCy/spacy/cli/__init__.py,12,noqa: F401,
spaCy/spacy/cli/evaluate.py,1,coding: utf8,
spaCy/spacy/cli/converters/conllu2json.py,1,coding: utf8,
spaCy/spacy/cli/converters/conllu2json.py,18,"by @dvsrepo, via #11 explosion/spacy-dev-resources",
spaCy/spacy/cli/converters/conllu2json.py,19,by @katarkor,
spaCy/spacy/cli/converters/conllu2json.py,31,Real-sized documents could be extracted using the comments on the,
spaCy/spacy/cli/converters/conllu2json.py,32,conluu document,
spaCy/spacy/cli/converters/conllu2json.py,79,noqa: E722,
spaCy/spacy/cli/converters/conll_ner2json.py,1,coding: utf8,
spaCy/spacy/cli/converters/conll_ner2json.py,41,"check for existing delimiters, which should be preserved",
spaCy/spacy/cli/converters/conll_ner2json.py,54,do document segmentation with existing sentences,
spaCy/spacy/cli/converters/conll_ner2json.py,58,do sentence segmentation with existing documents,
spaCy/spacy/cli/converters/conll_ner2json.py,61,do both sentence segmentation and document segmentation according,
spaCy/spacy/cli/converters/conll_ner2json.py,62,to options,
spaCy/spacy/cli/converters/conll_ner2json.py,64,sentence segmentation required for document segmentation,
spaCy/spacy/cli/converters/conll_ner2json.py,76,provide warnings for problematic data,
spaCy/spacy/cli/converters/jsonl2json.py,1,coding: utf8,
spaCy/spacy/cli/converters/jsonl2json.py,40,Trim whitespace,
spaCy/spacy/cli/converters/iob2json.py,1,coding: utf8,
spaCy/spacy/cli/converters/__init__.py,1,noqa: F401,
spaCy/spacy/cli/converters/__init__.py,2,noqa: F401,
spaCy/spacy/cli/converters/__init__.py,3,noqa: F401,
spaCy/spacy/cli/converters/__init__.py,4,noqa: F401,
spaCy/spacy/ml/_wire.py,6,pragma: no cover,
spaCy/spacy/ml/tok2vec.py,35,"For backwards compatibility with models before the architecture registry,",
spaCy/spacy/ml/tok2vec.py,36,we have to be careful to get exactly the same model structure. One subtle,
spaCy/spacy/ml/tok2vec.py,37,"trick is that when we define concatenation with the operator, the operator",
spaCy/spacy/ml/tok2vec.py,38,"is actually binary associative. So when we write (a | b | c), we're actually",
spaCy/spacy/ml/tok2vec.py,39,"getting concatenate(concatenate(a, b), c). That's why the implementation",
spaCy/spacy/ml/tok2vec.py,40,is a bit ugly here.,
spaCy/spacy/ml/__init__.py,1,coding: utf8,
spaCy/spacy/ml/__init__.py,4,noqa: F401,
spaCy/spacy/ml/__init__.py,5,noqa: F401,
spaCy/spacy/ml/_legacy_tok2vec.py,1,coding: utf8,
spaCy/spacy/ml/_legacy_tok2vec.py,16,Circular imports :(,
spaCy/spacy/ml/_legacy_tok2vec.py,89,Work around thinc API limitations :(. TODO: Revise in Thinc 7,
spaCy/spacy/ml/_legacy_tok2vec.py,107,pragma: no cover,
spaCy/spacy/pipeline/hooks.py,1,coding: utf8,
spaCy/spacy/pipeline/entityruler.py,1,coding: utf8,
spaCy/spacy/pipeline/entityruler.py,106,check for end - 1 here because boundaries are inclusive,
spaCy/spacy/pipeline/entityruler.py,199,disable the nlp components after this one in case they hadn't been initialized / deserialised yet,
spaCy/spacy/pipeline/entityruler.py,378,user wants to save only JSONL,
spaCy/spacy/pipeline/functions.py,1,coding: utf8,
spaCy/spacy/pipeline/__init__.py,1,coding: utf8,
spaCy/spacy/lang/norm_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/norm_exceptions.py,5,These exceptions are used to add NORM values based on a token's ORTH value.,
spaCy/spacy/lang/norm_exceptions.py,6,Individual languages can also add their own exceptions and overwrite them -,
spaCy/spacy/lang/norm_exceptions.py,7,"for example, British vs. American spelling in English.",
spaCy/spacy/lang/norm_exceptions.py,9,Norms are only set if no alternative is provided in the tokenizer exceptions.,
spaCy/spacy/lang/norm_exceptions.py,10,Note that this does not change any other token attributes. Its main purpose,
spaCy/spacy/lang/norm_exceptions.py,11,is to normalise the word representations so that equivalent tokens receive,
spaCy/spacy/lang/norm_exceptions.py,12,"similar representations. For example: $ and € are very different, but they're",
spaCy/spacy/lang/norm_exceptions.py,13,"both currency symbols. By normalising currency symbols to $, all symbols are",
spaCy/spacy/lang/norm_exceptions.py,14,"seen as similar, no matter how common they are in the training data.",
spaCy/spacy/lang/char_classes.py,1,coding: utf8,
spaCy/spacy/lang/char_classes.py,20,from the final table in: https://en.wikipedia.org/wiki/CJK_Unified_Ideographs,
spaCy/spacy/lang/char_classes.py,31,Latin standard,
spaCy/spacy/lang/char_classes.py,40,"letters with diacritics - French, German, Icelandic, Spanish",
spaCy/spacy/lang/char_classes.py,49,"letters with diacritics - Catalan, Czech, Latin, Latvian, Lithuanian, Polish, Slovak, Turkish, Welsh",
spaCy/spacy/lang/char_classes.py,66,"special characters - Khoisan, Pan-Nigerian, Pinyin, Romanian",
spaCy/spacy/lang/char_classes.py,67,those that are a combination of both upper and lower letters are only included in the group _latin_extendedB,
spaCy/spacy/lang/char_classes.py,88,"special characters - Uighur, Uralic Phonetic",
spaCy/spacy/lang/char_classes.py,97,"special characters - phonetic, Mayan, Medieval",
spaCy/spacy/lang/char_classes.py,114,special characters - phonetic Teuthonista and Sakha,
spaCy/spacy/lang/char_classes.py,118,"phonetic letters - Greek, Latin, Cyrillic",
spaCy/spacy/lang/char_classes.py,122,letters with multiple diacritics - Vietnamese,
spaCy/spacy/lang/char_classes.py,147,all lower latin classes,
spaCy/spacy/lang/char_classes.py,164,all upper latin classes,
spaCy/spacy/lang/char_classes.py,179,all latin classes,
spaCy/spacy/lang/char_classes.py,245,"These expressions contain various unicode variations, including characters",
spaCy/spacy/lang/char_classes.py,246,"used in Chinese (see #1333, #1340, #1351) – unless there are cross-language",
spaCy/spacy/lang/char_classes.py,247,"conflicts, spaCy's base tokenizer should handle all of those by default",
spaCy/spacy/lang/char_classes.py,254,"Various symbols like dingbats, but also emoji",
spaCy/spacy/lang/char_classes.py,255,Details: https://www.compart.com/en/unicode/category/So,
spaCy/spacy/lang/tokenizer_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/tokenizer_exceptions.py,10,URL validation regex courtesy of: https://mathiasbynens.be/demo/url-regex,
spaCy/spacy/lang/tokenizer_exceptions.py,11,"and https://gist.github.com/dperini/729294 (Diego Perini, MIT License)",
spaCy/spacy/lang/tokenizer_exceptions.py,12,A few mods to this regex to account for use cases represented in test_urls,
spaCy/spacy/lang/tokenizer_exceptions.py,14,fmt: off,
spaCy/spacy/lang/tokenizer_exceptions.py,16,protocol identifier (mods: make optional and expand schemes),
spaCy/spacy/lang/tokenizer_exceptions.py,17,(see: https://www.iana.org/assignments/uri-schemes/uri-schemes.xhtml),
spaCy/spacy/lang/tokenizer_exceptions.py,19,mailto:user or user:pass authentication,
spaCy/spacy/lang/tokenizer_exceptions.py,22,IP address exclusion,
spaCy/spacy/lang/tokenizer_exceptions.py,23,private & local networks,
spaCy/spacy/lang/tokenizer_exceptions.py,27,IP address dotted notation octets,
spaCy/spacy/lang/tokenizer_exceptions.py,28,excludes loopback network 0.0.0.0,
spaCy/spacy/lang/tokenizer_exceptions.py,29,excludes reserved space >= 224.0.0.0,
spaCy/spacy/lang/tokenizer_exceptions.py,30,excludes network & broadcast addresses,
spaCy/spacy/lang/tokenizer_exceptions.py,31,(first & last IP address of each class),
spaCy/spacy/lang/tokenizer_exceptions.py,32,"MH: Do we really need this? Seems excessive, and seems to have caused",
spaCy/spacy/lang/tokenizer_exceptions.py,33,Issue #957,
spaCy/spacy/lang/tokenizer_exceptions.py,38,host & domain names,
spaCy/spacy/lang/tokenizer_exceptions.py,39,"mods: match is case-sensitive, so include [A-Z]",
spaCy/spacy/lang/tokenizer_exceptions.py,40,noqa,
spaCy/spacy/lang/tokenizer_exceptions.py,47,TLD identifier,
spaCy/spacy/lang/tokenizer_exceptions.py,48,mods: use ALPHA_LOWER instead of a wider range so that this doesn't match,
spaCy/spacy/lang/tokenizer_exceptions.py,49,"strings like ""lower.Upper"", which can be split on ""."" by infixes in some",
spaCy/spacy/lang/tokenizer_exceptions.py,50,languages,
spaCy/spacy/lang/tokenizer_exceptions.py,53,port number,
spaCy/spacy/lang/tokenizer_exceptions.py,55,resource path,
spaCy/spacy/lang/tokenizer_exceptions.py,58,fmt: on,
spaCy/spacy/lang/punctuation.py,1,coding: utf8,
spaCy/spacy/lang/lex_attrs.py,1,coding: utf8,
spaCy/spacy/lang/lex_attrs.py,44,can be overwritten by lang with list of number words,
spaCy/spacy/lang/lex_attrs.py,113,"can be overwritten by lang with list of currency words, e.g. dollar, euro",
spaCy/spacy/lang/lex_attrs.py,125,"We're looking for things that function in text like URLs. So, valid URL",
spaCy/spacy/lang/lex_attrs.py,126,"or not, anything they say http:// is going to be good.",
spaCy/spacy/lang/lex_attrs.py,134,prevent matches on e-mail addresses – check after splitting the text,
spaCy/spacy/lang/lex_attrs.py,135,to still allow URLs containing an '@' character (see #1715),
spaCy/spacy/lang/tag_map.py,1,coding: utf8,
spaCy/spacy/lang/bg/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/bg/stop_words.py,5,Source: https://github.com/Alir3z4/stop-words,
spaCy/spacy/lang/bg/__init__.py,1,coding: utf8,
spaCy/spacy/lang/bg/examples.py,1,coding: utf8,
spaCy/spacy/lang/ar/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/ar/tokenizer_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/ar/tokenizer_exceptions.py,10,Time,
spaCy/spacy/lang/ar/tokenizer_exceptions.py,20,Scientific abv.,
spaCy/spacy/lang/ar/tokenizer_exceptions.py,37,Other abv.,
spaCy/spacy/lang/ar/punctuation.py,1,coding: utf8,
spaCy/spacy/lang/ar/punctuation.py,13,Arabic is written from Right-To-Left,
spaCy/spacy/lang/ar/lex_attrs.py,1,coding: utf8,
spaCy/spacy/lang/ar/__init__.py,1,coding: utf8,
spaCy/spacy/lang/ar/examples.py,1,coding: utf8,
spaCy/spacy/lang/sq/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/sq/stop_words.py,5,Source: https://github.com/andrixh/index-albanian,
spaCy/spacy/lang/sq/__init__.py,1,coding: utf8,
spaCy/spacy/lang/sq/examples.py,1,coding: utf8,
spaCy/spacy/lang/pl/_tokenizer_exceptions_list.py,1,-*- coding: utf-8 -*-,
spaCy/spacy/lang/pl/_tokenizer_exceptions_list.py,5,The following list consists of:,
spaCy/spacy/lang/pl/_tokenizer_exceptions_list.py,6,- exceptions generated from polish_srx_rules [1],
spaCy/spacy/lang/pl/_tokenizer_exceptions_list.py,7,(https://github.com/milekpl/polish_srx_rules),
spaCy/spacy/lang/pl/_tokenizer_exceptions_list.py,8,- abbreviations parsed from Wikipedia,
spaCy/spacy/lang/pl/_tokenizer_exceptions_list.py,9,- some manually added exceptions,
spaCy/spacy/lang/pl/_tokenizer_exceptions_list.py,10,,
spaCy/spacy/lang/pl/_tokenizer_exceptions_list.py,11,"[1] M. Miłkowski and J. Lipski,",
spaCy/spacy/lang/pl/_tokenizer_exceptions_list.py,12,"""Using SRX Standard for Sentence Segmentation,"" in LTC 2009,",
spaCy/spacy/lang/pl/_tokenizer_exceptions_list.py,13,"Lecture Notes in Artificial Intelligence 6562,",
spaCy/spacy/lang/pl/_tokenizer_exceptions_list.py,14,"Z. Vetulani, Ed. Berlin Heidelberg: Springer-Verlag, 2011, pp. 172–182.",
spaCy/spacy/lang/pl/stop_words.py,1,encoding: utf8,
spaCy/spacy/lang/pl/stop_words.py,5,sources: https://github.com/bieli/stopwords/blob/master/polish.stopwords.txt and https://github.com/stopwords-iso/stopwords-pl,
spaCy/spacy/lang/pl/tokenizer_exceptions.py,1,encoding: utf8,
spaCy/spacy/lang/pl/punctuation.py,1,coding: utf8,
spaCy/spacy/lang/pl/lex_attrs.py,1,coding: utf8,
spaCy/spacy/lang/pl/tag_map.py,1,coding: utf8,
spaCy/spacy/lang/pl/tag_map.py,24,fmt: off,
spaCy/spacy/lang/pl/tag_map.py,600,UD,
spaCy/spacy/lang/pl/tag_map.py,1649,fmt: on,
spaCy/spacy/lang/pl/__init__.py,1,coding: utf8,
spaCy/spacy/lang/pl/examples.py,1,coding: utf8,
spaCy/spacy/lang/mr/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/mr/stop_words.py,5,"Source: https://github.com/stopwords-iso/stopwords-mr/blob/master/stopwords-mr.txt, https://github.com/6/stopwords-json/edit/master/dist/mr.json",
spaCy/spacy/lang/mr/__init__.py,1,coding: utf8,
spaCy/spacy/lang/cs/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/cs/stop_words.py,5,Source: https://github.com/Alir3z4/stop-words,
spaCy/spacy/lang/cs/__init__.py,1,coding: utf8,
spaCy/spacy/lang/kn/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/kn/__init__.py,1,coding: utf8,
spaCy/spacy/lang/kn/examples.py,1,coding: utf8,
spaCy/spacy/lang/sk/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/sk/stop_words.py,5,Source: https://github.com/Ardevop-sk/stopwords-sk,
spaCy/spacy/lang/sk/lex_attrs.py,1,coding: utf8,
spaCy/spacy/lang/sk/tag_map.py,1,coding: utf8,
spaCy/spacy/lang/sk/tag_map.py,7,Source https://universaldependencies.org/tagset-conversion/sk-snk-uposf.html,
spaCy/spacy/lang/sk/tag_map.py,8,fmt: off,
spaCy/spacy/lang/sk/__init__.py,1,coding: utf8,
spaCy/spacy/lang/sk/examples.py,1,coding: utf8,
spaCy/spacy/lang/da/norm_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/da/norm_exceptions.py,9,Sources:,
spaCy/spacy/lang/da/norm_exceptions.py,10,1: https://dsn.dk/retskrivning/om-retskrivningsordbogen/mere-om-retskrivningsordbogen-2012/endrede-stave-og-ordformer/,
spaCy/spacy/lang/da/norm_exceptions.py,11,2: http://www.tjerry-korrektur.dk/ord-med-flere-stavemaader/,
spaCy/spacy/lang/da/norm_exceptions.py,14,Alternative spelling,
spaCy/spacy/lang/da/norm_exceptions.py,15,1,
spaCy/spacy/lang/da/norm_exceptions.py,16,2,
spaCy/spacy/lang/da/norm_exceptions.py,18,1,
spaCy/spacy/lang/da/norm_exceptions.py,19,1,
spaCy/spacy/lang/da/norm_exceptions.py,20,1,
spaCy/spacy/lang/da/norm_exceptions.py,21,1,
spaCy/spacy/lang/da/norm_exceptions.py,22,1,
spaCy/spacy/lang/da/norm_exceptions.py,23,1,
spaCy/spacy/lang/da/norm_exceptions.py,24,1,
spaCy/spacy/lang/da/norm_exceptions.py,25,1,
spaCy/spacy/lang/da/norm_exceptions.py,26,1,
spaCy/spacy/lang/da/norm_exceptions.py,27,1,
spaCy/spacy/lang/da/norm_exceptions.py,28,2,
spaCy/spacy/lang/da/norm_exceptions.py,29,1,
spaCy/spacy/lang/da/norm_exceptions.py,30,2,
spaCy/spacy/lang/da/norm_exceptions.py,31,1,
spaCy/spacy/lang/da/norm_exceptions.py,32,1,
spaCy/spacy/lang/da/norm_exceptions.py,33,1,
spaCy/spacy/lang/da/norm_exceptions.py,34,2,
spaCy/spacy/lang/da/norm_exceptions.py,35,2,
spaCy/spacy/lang/da/norm_exceptions.py,36,1,
spaCy/spacy/lang/da/norm_exceptions.py,37,1,
spaCy/spacy/lang/da/norm_exceptions.py,38,1,
spaCy/spacy/lang/da/norm_exceptions.py,39,2,
spaCy/spacy/lang/da/norm_exceptions.py,40,1,
spaCy/spacy/lang/da/norm_exceptions.py,41,1,
spaCy/spacy/lang/da/norm_exceptions.py,42,1,
spaCy/spacy/lang/da/norm_exceptions.py,43,1,
spaCy/spacy/lang/da/norm_exceptions.py,44,1,
spaCy/spacy/lang/da/norm_exceptions.py,45,1,
spaCy/spacy/lang/da/norm_exceptions.py,46,1,
spaCy/spacy/lang/da/norm_exceptions.py,47,1,
spaCy/spacy/lang/da/norm_exceptions.py,48,1,
spaCy/spacy/lang/da/norm_exceptions.py,49,1,
spaCy/spacy/lang/da/norm_exceptions.py,50,1,
spaCy/spacy/lang/da/norm_exceptions.py,51,1,
spaCy/spacy/lang/da/norm_exceptions.py,52,1,
spaCy/spacy/lang/da/norm_exceptions.py,53,2,
spaCy/spacy/lang/da/norm_exceptions.py,54,2,
spaCy/spacy/lang/da/norm_exceptions.py,55,1,
spaCy/spacy/lang/da/norm_exceptions.py,56,1,
spaCy/spacy/lang/da/norm_exceptions.py,57,2,
spaCy/spacy/lang/da/norm_exceptions.py,58,2,
spaCy/spacy/lang/da/norm_exceptions.py,59,1,
spaCy/spacy/lang/da/norm_exceptions.py,60,1,
spaCy/spacy/lang/da/norm_exceptions.py,61,1,
spaCy/spacy/lang/da/norm_exceptions.py,62,1,
spaCy/spacy/lang/da/norm_exceptions.py,63,1,
spaCy/spacy/lang/da/norm_exceptions.py,64,1,
spaCy/spacy/lang/da/norm_exceptions.py,65,1,
spaCy/spacy/lang/da/norm_exceptions.py,66,1,
spaCy/spacy/lang/da/norm_exceptions.py,67,2,
spaCy/spacy/lang/da/norm_exceptions.py,68,1,
spaCy/spacy/lang/da/norm_exceptions.py,69,2,
spaCy/spacy/lang/da/norm_exceptions.py,70,1,
spaCy/spacy/lang/da/norm_exceptions.py,71,1,
spaCy/spacy/lang/da/norm_exceptions.py,72,1,
spaCy/spacy/lang/da/norm_exceptions.py,73,1,
spaCy/spacy/lang/da/norm_exceptions.py,74,1,
spaCy/spacy/lang/da/norm_exceptions.py,75,1,
spaCy/spacy/lang/da/norm_exceptions.py,76,1,
spaCy/spacy/lang/da/norm_exceptions.py,77,2,
spaCy/spacy/lang/da/norm_exceptions.py,78,1,
spaCy/spacy/lang/da/norm_exceptions.py,79,1,
spaCy/spacy/lang/da/norm_exceptions.py,80,1,
spaCy/spacy/lang/da/norm_exceptions.py,81,1,
spaCy/spacy/lang/da/norm_exceptions.py,82,1,
spaCy/spacy/lang/da/norm_exceptions.py,83,1,
spaCy/spacy/lang/da/norm_exceptions.py,84,1,
spaCy/spacy/lang/da/norm_exceptions.py,85,1,
spaCy/spacy/lang/da/norm_exceptions.py,86,1,
spaCy/spacy/lang/da/norm_exceptions.py,87,2,
spaCy/spacy/lang/da/norm_exceptions.py,88,1,
spaCy/spacy/lang/da/norm_exceptions.py,89,1,
spaCy/spacy/lang/da/norm_exceptions.py,90,1,
spaCy/spacy/lang/da/norm_exceptions.py,91,1,
spaCy/spacy/lang/da/norm_exceptions.py,92,1,
spaCy/spacy/lang/da/norm_exceptions.py,93,1,
spaCy/spacy/lang/da/norm_exceptions.py,94,1,
spaCy/spacy/lang/da/norm_exceptions.py,95,1,
spaCy/spacy/lang/da/norm_exceptions.py,96,1,
spaCy/spacy/lang/da/norm_exceptions.py,97,1,
spaCy/spacy/lang/da/norm_exceptions.py,98,2,
spaCy/spacy/lang/da/norm_exceptions.py,99,1,
spaCy/spacy/lang/da/norm_exceptions.py,100,1,
spaCy/spacy/lang/da/norm_exceptions.py,101,1,
spaCy/spacy/lang/da/norm_exceptions.py,102,1,
spaCy/spacy/lang/da/norm_exceptions.py,103,1,
spaCy/spacy/lang/da/norm_exceptions.py,104,1,
spaCy/spacy/lang/da/norm_exceptions.py,105,1,
spaCy/spacy/lang/da/norm_exceptions.py,106,1,
spaCy/spacy/lang/da/norm_exceptions.py,107,1,
spaCy/spacy/lang/da/norm_exceptions.py,108,1,
spaCy/spacy/lang/da/norm_exceptions.py,109,1,
spaCy/spacy/lang/da/norm_exceptions.py,110,1,
spaCy/spacy/lang/da/norm_exceptions.py,111,1,
spaCy/spacy/lang/da/norm_exceptions.py,112,1,
spaCy/spacy/lang/da/norm_exceptions.py,113,2,
spaCy/spacy/lang/da/norm_exceptions.py,114,1,
spaCy/spacy/lang/da/norm_exceptions.py,115,1,
spaCy/spacy/lang/da/norm_exceptions.py,116,1,
spaCy/spacy/lang/da/norm_exceptions.py,117,1,
spaCy/spacy/lang/da/norm_exceptions.py,118,2,
spaCy/spacy/lang/da/norm_exceptions.py,119,1,
spaCy/spacy/lang/da/norm_exceptions.py,120,1,
spaCy/spacy/lang/da/norm_exceptions.py,121,1,
spaCy/spacy/lang/da/norm_exceptions.py,122,1,
spaCy/spacy/lang/da/norm_exceptions.py,123,1,
spaCy/spacy/lang/da/norm_exceptions.py,124,1,
spaCy/spacy/lang/da/norm_exceptions.py,125,1,
spaCy/spacy/lang/da/norm_exceptions.py,126,1,
spaCy/spacy/lang/da/norm_exceptions.py,127,1,
spaCy/spacy/lang/da/norm_exceptions.py,128,1,
spaCy/spacy/lang/da/norm_exceptions.py,129,1,
spaCy/spacy/lang/da/norm_exceptions.py,130,2,
spaCy/spacy/lang/da/norm_exceptions.py,131,2,
spaCy/spacy/lang/da/norm_exceptions.py,132,2,
spaCy/spacy/lang/da/norm_exceptions.py,133,1,
spaCy/spacy/lang/da/norm_exceptions.py,134,1,
spaCy/spacy/lang/da/norm_exceptions.py,135,2,
spaCy/spacy/lang/da/norm_exceptions.py,136,1,
spaCy/spacy/lang/da/norm_exceptions.py,137,1,
spaCy/spacy/lang/da/norm_exceptions.py,138,1,
spaCy/spacy/lang/da/norm_exceptions.py,139,2,
spaCy/spacy/lang/da/norm_exceptions.py,140,1,
spaCy/spacy/lang/da/norm_exceptions.py,141,1,
spaCy/spacy/lang/da/norm_exceptions.py,142,1,
spaCy/spacy/lang/da/norm_exceptions.py,143,1,
spaCy/spacy/lang/da/norm_exceptions.py,144,1,
spaCy/spacy/lang/da/norm_exceptions.py,145,1,
spaCy/spacy/lang/da/norm_exceptions.py,146,1,
spaCy/spacy/lang/da/norm_exceptions.py,147,1,
spaCy/spacy/lang/da/norm_exceptions.py,148,1,
spaCy/spacy/lang/da/norm_exceptions.py,149,1,
spaCy/spacy/lang/da/norm_exceptions.py,150,1,
spaCy/spacy/lang/da/norm_exceptions.py,151,1,
spaCy/spacy/lang/da/norm_exceptions.py,152,1,
spaCy/spacy/lang/da/norm_exceptions.py,153,1,
spaCy/spacy/lang/da/norm_exceptions.py,154,1,
spaCy/spacy/lang/da/norm_exceptions.py,155,1,
spaCy/spacy/lang/da/norm_exceptions.py,156,1,
spaCy/spacy/lang/da/norm_exceptions.py,157,1,
spaCy/spacy/lang/da/norm_exceptions.py,158,1,
spaCy/spacy/lang/da/norm_exceptions.py,159,2,
spaCy/spacy/lang/da/norm_exceptions.py,160,1,
spaCy/spacy/lang/da/norm_exceptions.py,161,1,
spaCy/spacy/lang/da/norm_exceptions.py,162,1,
spaCy/spacy/lang/da/norm_exceptions.py,163,1,
spaCy/spacy/lang/da/norm_exceptions.py,164,1,
spaCy/spacy/lang/da/norm_exceptions.py,165,2,
spaCy/spacy/lang/da/norm_exceptions.py,166,2,
spaCy/spacy/lang/da/norm_exceptions.py,167,1,
spaCy/spacy/lang/da/norm_exceptions.py,168,1,
spaCy/spacy/lang/da/norm_exceptions.py,169,1,
spaCy/spacy/lang/da/norm_exceptions.py,170,1,
spaCy/spacy/lang/da/norm_exceptions.py,171,1,
spaCy/spacy/lang/da/norm_exceptions.py,172,1,
spaCy/spacy/lang/da/norm_exceptions.py,173,1,
spaCy/spacy/lang/da/norm_exceptions.py,174,1,
spaCy/spacy/lang/da/norm_exceptions.py,175,1,
spaCy/spacy/lang/da/norm_exceptions.py,176,1,
spaCy/spacy/lang/da/norm_exceptions.py,177,1,
spaCy/spacy/lang/da/norm_exceptions.py,178,1,
spaCy/spacy/lang/da/norm_exceptions.py,179,1,
spaCy/spacy/lang/da/norm_exceptions.py,180,1,
spaCy/spacy/lang/da/norm_exceptions.py,181,1,
spaCy/spacy/lang/da/norm_exceptions.py,182,1,
spaCy/spacy/lang/da/norm_exceptions.py,183,1,
spaCy/spacy/lang/da/norm_exceptions.py,184,1,
spaCy/spacy/lang/da/norm_exceptions.py,185,1,
spaCy/spacy/lang/da/norm_exceptions.py,186,1,
spaCy/spacy/lang/da/norm_exceptions.py,187,1,
spaCy/spacy/lang/da/norm_exceptions.py,188,1,
spaCy/spacy/lang/da/norm_exceptions.py,189,2,
spaCy/spacy/lang/da/norm_exceptions.py,190,2,
spaCy/spacy/lang/da/norm_exceptions.py,191,1,
spaCy/spacy/lang/da/norm_exceptions.py,192,1,
spaCy/spacy/lang/da/norm_exceptions.py,193,1,
spaCy/spacy/lang/da/norm_exceptions.py,194,2,
spaCy/spacy/lang/da/norm_exceptions.py,195,1,
spaCy/spacy/lang/da/norm_exceptions.py,196,2,
spaCy/spacy/lang/da/norm_exceptions.py,197,1,
spaCy/spacy/lang/da/norm_exceptions.py,198,1,
spaCy/spacy/lang/da/norm_exceptions.py,199,2,
spaCy/spacy/lang/da/norm_exceptions.py,200,1,
spaCy/spacy/lang/da/norm_exceptions.py,201,1,
spaCy/spacy/lang/da/norm_exceptions.py,202,1,
spaCy/spacy/lang/da/norm_exceptions.py,203,1,
spaCy/spacy/lang/da/norm_exceptions.py,204,2,
spaCy/spacy/lang/da/norm_exceptions.py,205,1,
spaCy/spacy/lang/da/norm_exceptions.py,206,1,
spaCy/spacy/lang/da/norm_exceptions.py,207,1,
spaCy/spacy/lang/da/norm_exceptions.py,208,1,
spaCy/spacy/lang/da/norm_exceptions.py,209,1,
spaCy/spacy/lang/da/norm_exceptions.py,210,1,
spaCy/spacy/lang/da/norm_exceptions.py,211,1,
spaCy/spacy/lang/da/norm_exceptions.py,212,1,
spaCy/spacy/lang/da/norm_exceptions.py,213,1,
spaCy/spacy/lang/da/norm_exceptions.py,214,2,
spaCy/spacy/lang/da/norm_exceptions.py,215,1,
spaCy/spacy/lang/da/norm_exceptions.py,216,1,
spaCy/spacy/lang/da/norm_exceptions.py,217,1,
spaCy/spacy/lang/da/norm_exceptions.py,218,1,
spaCy/spacy/lang/da/norm_exceptions.py,219,1,
spaCy/spacy/lang/da/norm_exceptions.py,220,2,
spaCy/spacy/lang/da/norm_exceptions.py,221,1,
spaCy/spacy/lang/da/norm_exceptions.py,222,1,
spaCy/spacy/lang/da/norm_exceptions.py,223,1,
spaCy/spacy/lang/da/norm_exceptions.py,224,1,
spaCy/spacy/lang/da/norm_exceptions.py,225,1,
spaCy/spacy/lang/da/norm_exceptions.py,226,1,
spaCy/spacy/lang/da/norm_exceptions.py,227,1,
spaCy/spacy/lang/da/norm_exceptions.py,228,1,
spaCy/spacy/lang/da/norm_exceptions.py,229,1,
spaCy/spacy/lang/da/norm_exceptions.py,230,1,
spaCy/spacy/lang/da/norm_exceptions.py,231,1,
spaCy/spacy/lang/da/norm_exceptions.py,232,2,
spaCy/spacy/lang/da/norm_exceptions.py,233,1,
spaCy/spacy/lang/da/norm_exceptions.py,234,1,
spaCy/spacy/lang/da/norm_exceptions.py,235,1,
spaCy/spacy/lang/da/norm_exceptions.py,236,1,
spaCy/spacy/lang/da/norm_exceptions.py,237,1,
spaCy/spacy/lang/da/norm_exceptions.py,238,2,
spaCy/spacy/lang/da/norm_exceptions.py,239,1,
spaCy/spacy/lang/da/norm_exceptions.py,240,2,
spaCy/spacy/lang/da/norm_exceptions.py,241,1,
spaCy/spacy/lang/da/norm_exceptions.py,242,2,
spaCy/spacy/lang/da/norm_exceptions.py,243,1,
spaCy/spacy/lang/da/norm_exceptions.py,244,1,
spaCy/spacy/lang/da/norm_exceptions.py,245,1,
spaCy/spacy/lang/da/norm_exceptions.py,246,1,
spaCy/spacy/lang/da/norm_exceptions.py,247,1,
spaCy/spacy/lang/da/norm_exceptions.py,248,1,
spaCy/spacy/lang/da/norm_exceptions.py,249,1,
spaCy/spacy/lang/da/norm_exceptions.py,250,1,
spaCy/spacy/lang/da/norm_exceptions.py,251,1,
spaCy/spacy/lang/da/norm_exceptions.py,252,1,
spaCy/spacy/lang/da/norm_exceptions.py,253,1,
spaCy/spacy/lang/da/norm_exceptions.py,254,1,
spaCy/spacy/lang/da/norm_exceptions.py,255,1,
spaCy/spacy/lang/da/norm_exceptions.py,256,1,
spaCy/spacy/lang/da/norm_exceptions.py,257,1,
spaCy/spacy/lang/da/norm_exceptions.py,258,1,
spaCy/spacy/lang/da/norm_exceptions.py,259,1,
spaCy/spacy/lang/da/norm_exceptions.py,260,1,
spaCy/spacy/lang/da/norm_exceptions.py,261,1,
spaCy/spacy/lang/da/norm_exceptions.py,262,1,
spaCy/spacy/lang/da/norm_exceptions.py,263,2,
spaCy/spacy/lang/da/norm_exceptions.py,264,1,
spaCy/spacy/lang/da/norm_exceptions.py,265,2,
spaCy/spacy/lang/da/norm_exceptions.py,266,1,
spaCy/spacy/lang/da/norm_exceptions.py,267,2,
spaCy/spacy/lang/da/norm_exceptions.py,268,1,
spaCy/spacy/lang/da/norm_exceptions.py,269,1,
spaCy/spacy/lang/da/norm_exceptions.py,270,1,
spaCy/spacy/lang/da/norm_exceptions.py,271,1,
spaCy/spacy/lang/da/norm_exceptions.py,272,1,
spaCy/spacy/lang/da/norm_exceptions.py,273,1,
spaCy/spacy/lang/da/norm_exceptions.py,274,1,
spaCy/spacy/lang/da/norm_exceptions.py,275,2,
spaCy/spacy/lang/da/norm_exceptions.py,276,1,
spaCy/spacy/lang/da/norm_exceptions.py,277,1,
spaCy/spacy/lang/da/norm_exceptions.py,278,2,
spaCy/spacy/lang/da/norm_exceptions.py,279,1,
spaCy/spacy/lang/da/norm_exceptions.py,280,1,
spaCy/spacy/lang/da/norm_exceptions.py,281,1,
spaCy/spacy/lang/da/norm_exceptions.py,282,1,
spaCy/spacy/lang/da/norm_exceptions.py,283,1,
spaCy/spacy/lang/da/norm_exceptions.py,284,1,
spaCy/spacy/lang/da/norm_exceptions.py,285,1,
spaCy/spacy/lang/da/norm_exceptions.py,286,1,
spaCy/spacy/lang/da/norm_exceptions.py,287,1,
spaCy/spacy/lang/da/norm_exceptions.py,288,1,
spaCy/spacy/lang/da/norm_exceptions.py,289,1,
spaCy/spacy/lang/da/norm_exceptions.py,290,1,
spaCy/spacy/lang/da/norm_exceptions.py,291,1,
spaCy/spacy/lang/da/norm_exceptions.py,292,2,
spaCy/spacy/lang/da/norm_exceptions.py,293,1,
spaCy/spacy/lang/da/norm_exceptions.py,294,1,
spaCy/spacy/lang/da/norm_exceptions.py,295,1,
spaCy/spacy/lang/da/norm_exceptions.py,296,1,
spaCy/spacy/lang/da/norm_exceptions.py,297,1,
spaCy/spacy/lang/da/norm_exceptions.py,298,1,
spaCy/spacy/lang/da/norm_exceptions.py,299,1,
spaCy/spacy/lang/da/norm_exceptions.py,300,1,
spaCy/spacy/lang/da/norm_exceptions.py,301,1,
spaCy/spacy/lang/da/norm_exceptions.py,302,1,
spaCy/spacy/lang/da/norm_exceptions.py,303,1,
spaCy/spacy/lang/da/norm_exceptions.py,304,1,
spaCy/spacy/lang/da/norm_exceptions.py,305,1,
spaCy/spacy/lang/da/norm_exceptions.py,306,1,
spaCy/spacy/lang/da/norm_exceptions.py,307,1,
spaCy/spacy/lang/da/norm_exceptions.py,308,1,
spaCy/spacy/lang/da/norm_exceptions.py,309,1,
spaCy/spacy/lang/da/norm_exceptions.py,310,1,
spaCy/spacy/lang/da/norm_exceptions.py,311,1,
spaCy/spacy/lang/da/norm_exceptions.py,312,1,
spaCy/spacy/lang/da/norm_exceptions.py,313,1,
spaCy/spacy/lang/da/norm_exceptions.py,314,1,
spaCy/spacy/lang/da/norm_exceptions.py,315,1,
spaCy/spacy/lang/da/norm_exceptions.py,316,1,
spaCy/spacy/lang/da/norm_exceptions.py,317,1,
spaCy/spacy/lang/da/norm_exceptions.py,318,1,
spaCy/spacy/lang/da/norm_exceptions.py,319,1,
spaCy/spacy/lang/da/norm_exceptions.py,320,1,
spaCy/spacy/lang/da/norm_exceptions.py,321,1,
spaCy/spacy/lang/da/norm_exceptions.py,322,2,
spaCy/spacy/lang/da/norm_exceptions.py,323,2,
spaCy/spacy/lang/da/norm_exceptions.py,324,1,
spaCy/spacy/lang/da/norm_exceptions.py,325,1,
spaCy/spacy/lang/da/norm_exceptions.py,326,1,
spaCy/spacy/lang/da/norm_exceptions.py,327,1,
spaCy/spacy/lang/da/norm_exceptions.py,328,1,
spaCy/spacy/lang/da/norm_exceptions.py,329,1,
spaCy/spacy/lang/da/norm_exceptions.py,330,1,
spaCy/spacy/lang/da/norm_exceptions.py,331,2,
spaCy/spacy/lang/da/norm_exceptions.py,332,2,
spaCy/spacy/lang/da/norm_exceptions.py,333,1,
spaCy/spacy/lang/da/norm_exceptions.py,334,1,
spaCy/spacy/lang/da/norm_exceptions.py,335,1,
spaCy/spacy/lang/da/norm_exceptions.py,336,2,
spaCy/spacy/lang/da/norm_exceptions.py,337,2,
spaCy/spacy/lang/da/norm_exceptions.py,338,2,
spaCy/spacy/lang/da/norm_exceptions.py,339,1,
spaCy/spacy/lang/da/norm_exceptions.py,340,1,
spaCy/spacy/lang/da/norm_exceptions.py,341,1,
spaCy/spacy/lang/da/norm_exceptions.py,342,1,
spaCy/spacy/lang/da/norm_exceptions.py,343,1,
spaCy/spacy/lang/da/norm_exceptions.py,344,1,
spaCy/spacy/lang/da/norm_exceptions.py,345,1,
spaCy/spacy/lang/da/norm_exceptions.py,346,1,
spaCy/spacy/lang/da/norm_exceptions.py,347,2,
spaCy/spacy/lang/da/norm_exceptions.py,348,1,
spaCy/spacy/lang/da/norm_exceptions.py,349,1,
spaCy/spacy/lang/da/norm_exceptions.py,350,1,
spaCy/spacy/lang/da/norm_exceptions.py,351,1,
spaCy/spacy/lang/da/norm_exceptions.py,352,2,
spaCy/spacy/lang/da/norm_exceptions.py,353,1,
spaCy/spacy/lang/da/norm_exceptions.py,354,1,
spaCy/spacy/lang/da/norm_exceptions.py,355,1,
spaCy/spacy/lang/da/norm_exceptions.py,356,1,
spaCy/spacy/lang/da/norm_exceptions.py,357,2,
spaCy/spacy/lang/da/norm_exceptions.py,358,1,
spaCy/spacy/lang/da/norm_exceptions.py,359,1,
spaCy/spacy/lang/da/norm_exceptions.py,360,1,
spaCy/spacy/lang/da/norm_exceptions.py,361,1,
spaCy/spacy/lang/da/norm_exceptions.py,362,1,
spaCy/spacy/lang/da/norm_exceptions.py,363,2,
spaCy/spacy/lang/da/norm_exceptions.py,364,2,
spaCy/spacy/lang/da/norm_exceptions.py,365,1,
spaCy/spacy/lang/da/norm_exceptions.py,366,1,
spaCy/spacy/lang/da/norm_exceptions.py,367,2,
spaCy/spacy/lang/da/norm_exceptions.py,368,1,
spaCy/spacy/lang/da/norm_exceptions.py,369,1,
spaCy/spacy/lang/da/norm_exceptions.py,370,1,
spaCy/spacy/lang/da/norm_exceptions.py,371,1,
spaCy/spacy/lang/da/norm_exceptions.py,372,1,
spaCy/spacy/lang/da/norm_exceptions.py,373,1,
spaCy/spacy/lang/da/norm_exceptions.py,374,1,
spaCy/spacy/lang/da/norm_exceptions.py,375,1,
spaCy/spacy/lang/da/norm_exceptions.py,376,1,
spaCy/spacy/lang/da/norm_exceptions.py,377,2,
spaCy/spacy/lang/da/norm_exceptions.py,378,1,
spaCy/spacy/lang/da/norm_exceptions.py,379,1,
spaCy/spacy/lang/da/norm_exceptions.py,380,2,
spaCy/spacy/lang/da/norm_exceptions.py,381,2,
spaCy/spacy/lang/da/norm_exceptions.py,382,1,
spaCy/spacy/lang/da/norm_exceptions.py,383,1,
spaCy/spacy/lang/da/norm_exceptions.py,384,1,
spaCy/spacy/lang/da/norm_exceptions.py,385,1,
spaCy/spacy/lang/da/norm_exceptions.py,386,2,
spaCy/spacy/lang/da/norm_exceptions.py,387,1,
spaCy/spacy/lang/da/norm_exceptions.py,388,1,
spaCy/spacy/lang/da/norm_exceptions.py,389,1,
spaCy/spacy/lang/da/norm_exceptions.py,390,1,
spaCy/spacy/lang/da/norm_exceptions.py,391,1,
spaCy/spacy/lang/da/norm_exceptions.py,392,1,
spaCy/spacy/lang/da/norm_exceptions.py,393,1,
spaCy/spacy/lang/da/norm_exceptions.py,394,1,
spaCy/spacy/lang/da/norm_exceptions.py,395,1,
spaCy/spacy/lang/da/norm_exceptions.py,396,1,
spaCy/spacy/lang/da/norm_exceptions.py,397,2,
spaCy/spacy/lang/da/norm_exceptions.py,398,1,
spaCy/spacy/lang/da/norm_exceptions.py,399,1,
spaCy/spacy/lang/da/norm_exceptions.py,400,1,
spaCy/spacy/lang/da/norm_exceptions.py,401,1,
spaCy/spacy/lang/da/norm_exceptions.py,402,1,
spaCy/spacy/lang/da/norm_exceptions.py,403,1,
spaCy/spacy/lang/da/norm_exceptions.py,404,1,
spaCy/spacy/lang/da/norm_exceptions.py,405,1,
spaCy/spacy/lang/da/norm_exceptions.py,406,1,
spaCy/spacy/lang/da/norm_exceptions.py,407,1,
spaCy/spacy/lang/da/norm_exceptions.py,408,1,
spaCy/spacy/lang/da/norm_exceptions.py,409,1,
spaCy/spacy/lang/da/norm_exceptions.py,410,1,
spaCy/spacy/lang/da/norm_exceptions.py,411,1,
spaCy/spacy/lang/da/norm_exceptions.py,412,1,
spaCy/spacy/lang/da/norm_exceptions.py,413,1,
spaCy/spacy/lang/da/norm_exceptions.py,414,1,
spaCy/spacy/lang/da/norm_exceptions.py,415,1,
spaCy/spacy/lang/da/norm_exceptions.py,416,1,
spaCy/spacy/lang/da/norm_exceptions.py,417,1,
spaCy/spacy/lang/da/norm_exceptions.py,418,1,
spaCy/spacy/lang/da/norm_exceptions.py,419,1,
spaCy/spacy/lang/da/norm_exceptions.py,420,1,
spaCy/spacy/lang/da/norm_exceptions.py,421,1,
spaCy/spacy/lang/da/norm_exceptions.py,422,1,
spaCy/spacy/lang/da/norm_exceptions.py,423,1,
spaCy/spacy/lang/da/norm_exceptions.py,424,1,
spaCy/spacy/lang/da/norm_exceptions.py,425,1,
spaCy/spacy/lang/da/norm_exceptions.py,426,1,
spaCy/spacy/lang/da/norm_exceptions.py,427,2,
spaCy/spacy/lang/da/norm_exceptions.py,428,1,
spaCy/spacy/lang/da/norm_exceptions.py,429,1,
spaCy/spacy/lang/da/norm_exceptions.py,430,1,
spaCy/spacy/lang/da/norm_exceptions.py,431,1,
spaCy/spacy/lang/da/norm_exceptions.py,432,1,
spaCy/spacy/lang/da/norm_exceptions.py,433,2,
spaCy/spacy/lang/da/norm_exceptions.py,434,2,
spaCy/spacy/lang/da/norm_exceptions.py,435,1,
spaCy/spacy/lang/da/norm_exceptions.py,436,1,
spaCy/spacy/lang/da/norm_exceptions.py,437,2,
spaCy/spacy/lang/da/norm_exceptions.py,438,1,
spaCy/spacy/lang/da/norm_exceptions.py,439,1,
spaCy/spacy/lang/da/norm_exceptions.py,440,1,
spaCy/spacy/lang/da/norm_exceptions.py,441,1,
spaCy/spacy/lang/da/norm_exceptions.py,442,1,
spaCy/spacy/lang/da/norm_exceptions.py,443,1,
spaCy/spacy/lang/da/norm_exceptions.py,444,1,
spaCy/spacy/lang/da/norm_exceptions.py,445,1,
spaCy/spacy/lang/da/norm_exceptions.py,446,2,
spaCy/spacy/lang/da/norm_exceptions.py,447,1,
spaCy/spacy/lang/da/norm_exceptions.py,448,1,
spaCy/spacy/lang/da/norm_exceptions.py,449,1,
spaCy/spacy/lang/da/norm_exceptions.py,450,1,
spaCy/spacy/lang/da/norm_exceptions.py,451,1,
spaCy/spacy/lang/da/norm_exceptions.py,452,1,
spaCy/spacy/lang/da/norm_exceptions.py,453,1,
spaCy/spacy/lang/da/norm_exceptions.py,454,1,
spaCy/spacy/lang/da/norm_exceptions.py,455,1,
spaCy/spacy/lang/da/norm_exceptions.py,456,1,
spaCy/spacy/lang/da/norm_exceptions.py,457,1,
spaCy/spacy/lang/da/norm_exceptions.py,458,1,
spaCy/spacy/lang/da/norm_exceptions.py,459,1,
spaCy/spacy/lang/da/norm_exceptions.py,460,1,
spaCy/spacy/lang/da/norm_exceptions.py,461,1,
spaCy/spacy/lang/da/norm_exceptions.py,462,1,
spaCy/spacy/lang/da/norm_exceptions.py,463,1,
spaCy/spacy/lang/da/norm_exceptions.py,464,1,
spaCy/spacy/lang/da/norm_exceptions.py,465,1,
spaCy/spacy/lang/da/norm_exceptions.py,466,1,
spaCy/spacy/lang/da/norm_exceptions.py,467,2,
spaCy/spacy/lang/da/norm_exceptions.py,468,2,
spaCy/spacy/lang/da/norm_exceptions.py,469,1,
spaCy/spacy/lang/da/norm_exceptions.py,470,1,
spaCy/spacy/lang/da/norm_exceptions.py,471,1,
spaCy/spacy/lang/da/norm_exceptions.py,472,2,
spaCy/spacy/lang/da/norm_exceptions.py,473,2,
spaCy/spacy/lang/da/norm_exceptions.py,474,1,
spaCy/spacy/lang/da/norm_exceptions.py,475,1,
spaCy/spacy/lang/da/norm_exceptions.py,476,1,
spaCy/spacy/lang/da/norm_exceptions.py,477,1,
spaCy/spacy/lang/da/norm_exceptions.py,478,1,
spaCy/spacy/lang/da/norm_exceptions.py,479,1,
spaCy/spacy/lang/da/norm_exceptions.py,480,1,
spaCy/spacy/lang/da/norm_exceptions.py,481,1,
spaCy/spacy/lang/da/norm_exceptions.py,482,1,
spaCy/spacy/lang/da/norm_exceptions.py,483,1,
spaCy/spacy/lang/da/norm_exceptions.py,484,1,
spaCy/spacy/lang/da/norm_exceptions.py,485,1,
spaCy/spacy/lang/da/norm_exceptions.py,486,1,
spaCy/spacy/lang/da/norm_exceptions.py,487,1,
spaCy/spacy/lang/da/norm_exceptions.py,488,1,
spaCy/spacy/lang/da/norm_exceptions.py,489,1,
spaCy/spacy/lang/da/norm_exceptions.py,490,2,
spaCy/spacy/lang/da/norm_exceptions.py,491,1,
spaCy/spacy/lang/da/norm_exceptions.py,492,1,
spaCy/spacy/lang/da/norm_exceptions.py,493,1,
spaCy/spacy/lang/da/norm_exceptions.py,494,1,
spaCy/spacy/lang/da/norm_exceptions.py,495,1,
spaCy/spacy/lang/da/norm_exceptions.py,496,1,
spaCy/spacy/lang/da/norm_exceptions.py,497,1,
spaCy/spacy/lang/da/norm_exceptions.py,498,2,
spaCy/spacy/lang/da/norm_exceptions.py,499,1,
spaCy/spacy/lang/da/norm_exceptions.py,500,1,
spaCy/spacy/lang/da/norm_exceptions.py,501,1,
spaCy/spacy/lang/da/norm_exceptions.py,502,1,
spaCy/spacy/lang/da/norm_exceptions.py,503,1,
spaCy/spacy/lang/da/norm_exceptions.py,504,1,
spaCy/spacy/lang/da/norm_exceptions.py,505,1,
spaCy/spacy/lang/da/norm_exceptions.py,506,1,
spaCy/spacy/lang/da/norm_exceptions.py,507,2,
spaCy/spacy/lang/da/norm_exceptions.py,508,1,
spaCy/spacy/lang/da/norm_exceptions.py,509,1,
spaCy/spacy/lang/da/norm_exceptions.py,510,1,
spaCy/spacy/lang/da/norm_exceptions.py,511,1,
spaCy/spacy/lang/da/norm_exceptions.py,512,2,
spaCy/spacy/lang/da/norm_exceptions.py,513,1,
spaCy/spacy/lang/da/norm_exceptions.py,514,1,
spaCy/spacy/lang/da/norm_exceptions.py,515,1,
spaCy/spacy/lang/da/norm_exceptions.py,516,1,
spaCy/spacy/lang/da/norm_exceptions.py,517,1,
spaCy/spacy/lang/da/norm_exceptions.py,518,2,
spaCy/spacy/lang/da/norm_exceptions.py,519,1,
spaCy/spacy/lang/da/morph_rules.py,1,coding: utf8,
spaCy/spacy/lang/da/morph_rules.py,6,Source: Danish Universal Dependencies and http://fjern-uv.dk/pronom.php,
spaCy/spacy/lang/da/morph_rules.py,8,Note: The Danish Universal Dependencies specify Case=Acc for all instances,
spaCy/spacy/lang/da/morph_rules.py,9,"of ""den""/""det"" even when the case is in fact ""Nom"". In the rules below, Case",
spaCy/spacy/lang/da/morph_rules.py,10,"is left unspecified for ""den"" and ""det"".",
spaCy/spacy/lang/da/morph_rules.py,21,Case=Nom|Gender=Com|Number=Sing|Person=1|PronType=Prs,
spaCy/spacy/lang/da/morph_rules.py,29,Case=Acc|Gender=Com|Number=Sing|Person=1|PronType=Prs,
spaCy/spacy/lang/da/morph_rules.py,37,Gender=Com|Number=Sing|Number[psor]=Sing|Person=1|Poss=Yes|PronType=Prs,
spaCy/spacy/lang/da/morph_rules.py,45,Gender=Neut|Number=Sing|Number[psor]=Sing|Person=1|Poss=Yes|PronType=Prs,
spaCy/spacy/lang/da/morph_rules.py,53,Gender=Com|Number=Sing|Number[psor]=Plur|Person=1|Poss=Yes|PronType=Prs|Style=Form,
spaCy/spacy/lang/da/morph_rules.py,61,Gender=Neut|Number=Sing|Number[psor]=Plur|Person=1|Poss=Yes|PronType=Prs|Style=Form,
spaCy/spacy/lang/da/morph_rules.py,69,Case=Nom|Gender=Com|Number=Sing|Person=2|PronType=Prs,
spaCy/spacy/lang/da/morph_rules.py,77,Case=Acc|Gender=Com|Number=Sing|Person=2|PronType=Prs,
spaCy/spacy/lang/da/morph_rules.py,85,Gender=Com|Number=Sing|Number[psor]=Sing|Person=2|Poss=Yes|PronType=Prs,
spaCy/spacy/lang/da/morph_rules.py,93,Gender=Neut|Number=Sing|Number[psor]=Sing|Person=2|Poss=Yes|PronType=Prs,
spaCy/spacy/lang/da/morph_rules.py,101,Case=Nom|Gender=Com|Number=Sing|Person=3|PronType=Prs,
spaCy/spacy/lang/da/morph_rules.py,109,Case=Nom|Gender=Com|Number=Sing|Person=3|PronType=Prs,
spaCy/spacy/lang/da/morph_rules.py,116,"Case=Acc|Gender=Com|Number=Sing|Person=3|PronType=Prs, See note above.",
spaCy/spacy/lang/da/morph_rules.py,123,Case=Acc|Gender=Neut|Number=Sing|Person=3|PronType=Prs See note above.,
spaCy/spacy/lang/da/morph_rules.py,131,Case=Acc|Gender=Com|Number=Sing|Person=3|PronType=Prs,
spaCy/spacy/lang/da/morph_rules.py,139,Case=Acc|Gender=Com|Number=Sing|Person=3|PronType=Prs,
spaCy/spacy/lang/da/morph_rules.py,148,Gender=Com|Number=Sing|Number[psor]=Sing|Person=3|Poss=Yes|PronType=Prs|Reflex=Yes,
spaCy/spacy/lang/da/morph_rules.py,157,Gender=Neut|Number=Sing|Number[psor]=Sing|Person=3|Poss=Yes|PronType=Prs|Reflex=Yes,
spaCy/spacy/lang/da/morph_rules.py,165,Case=Nom|Gender=Com|Number=Plur|Person=1|PronType=Prs,
spaCy/spacy/lang/da/morph_rules.py,173,Case=Acc|Gender=Com|Number=Plur|Person=1|PronType=Prs,
spaCy/spacy/lang/da/morph_rules.py,180,Number=Plur|Number[psor]=Sing|Person=1|Poss=Yes|PronType=Prs,
spaCy/spacy/lang/da/morph_rules.py,187,Number=Plur|Number[psor]=Plur|Person=1|Poss=Yes|PronType=Prs|Style=Form,
spaCy/spacy/lang/da/morph_rules.py,195,Case=Nom|Gender=Com|Number=Plur|Person=2|PronType=Prs,
spaCy/spacy/lang/da/morph_rules.py,203,Case=Acc|Gender=Com|Number=Plur|Person=2|PronType=Prs,
spaCy/spacy/lang/da/morph_rules.py,210,Number=Plur|Number[psor]=Sing|Person=2|Poss=Yes|PronType=Prs,
spaCy/spacy/lang/da/morph_rules.py,217,Case=Nom|Number=Plur|Person=3|PronType=Prs,
spaCy/spacy/lang/da/morph_rules.py,224,Case=Acc|Number=Plur|Person=3|PronType=Prs,
spaCy/spacy/lang/da/morph_rules.py,232,Number=Plur|Number[psor]=Sing|Person=3|Poss=Yes|PronType=Prs|Reflex=Yes,
spaCy/spacy/lang/da/morph_rules.py,238,Number[psor]=Plur|Person=1|Poss=Yes|PronType=Prs,
spaCy/spacy/lang/da/morph_rules.py,245,Case=Nom|Gender=Com|Person=2|Polite=Form|PronType=Prs,
spaCy/spacy/lang/da/morph_rules.py,252,Case=Acc|Gender=Com|Person=2|Polite=Form|PronType=Prs,
spaCy/spacy/lang/da/morph_rules.py,258,Person=2|Polite=Form|Poss=Yes|PronType=Prs,
spaCy/spacy/lang/da/morph_rules.py,264,Number[psor]=Plur|Person=2|Poss=Yes|PronType=Prs,
spaCy/spacy/lang/da/morph_rules.py,271,Case=Acc|Person=3|PronType=Prs|Reflex=Yes,
spaCy/spacy/lang/da/morph_rules.py,277,Number[psor]=Sing|Person=3|Poss=Yes|PronType=Prs,
spaCy/spacy/lang/da/morph_rules.py,283,Number[psor]=Sing|Person=3|Poss=Yes|PronType=Prs,
spaCy/spacy/lang/da/morph_rules.py,289,Number[psor]=Sing|Person=3|Poss=Yes|PronType=Prs,
spaCy/spacy/lang/da/morph_rules.py,295,Number[psor]=Sing|Person=3|Poss=Yes|PronType=Prs,
spaCy/spacy/lang/da/morph_rules.py,301,Number[psor]=Plur|Person=3|Poss=Yes|PronType=Prs,
spaCy/spacy/lang/da/stop_words.py,1,encoding: utf8,
spaCy/spacy/lang/da/stop_words.py,4,Source: Handpicked by Jens Dahl Møllerhøj.,
spaCy/spacy/lang/da/tokenizer_exceptions.py,1,encoding: utf8,
spaCy/spacy/lang/da/tokenizer_exceptions.py,14,"Abbreviations for weekdays ""søn."" (for ""søndag"") as well as ""Tor."" and ""Tors.""",
spaCy/spacy/lang/da/tokenizer_exceptions.py,15,"(for ""torsdag"") are left out because they are ambiguous. The same is the case",
spaCy/spacy/lang/da/tokenizer_exceptions.py,16,"for abbreviations ""jul."" and ""Jul."" (""juli"").",
spaCy/spacy/lang/da/tokenizer_exceptions.py,60,Specified case only,
spaCy/spacy/lang/da/tokenizer_exceptions.py,575,Dates,
spaCy/spacy/lang/da/punctuation.py,1,coding: utf8,
spaCy/spacy/lang/da/lex_attrs.py,1,coding: utf8,
spaCy/spacy/lang/da/lex_attrs.py,7,Source http://fjern-uv.dk/tal.php,
spaCy/spacy/lang/da/lex_attrs.py,23,Source: http://www.duda.dk/video/dansk/grammatik/talord/talord.html,
spaCy/spacy/lang/da/__init__.py,1,coding: utf8,
spaCy/spacy/lang/da/examples.py,1,coding: utf8,
spaCy/spacy/lang/lt/morph_rules.py,1,coding: utf8,
spaCy/spacy/lang/lt/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/lt/tokenizer_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/lt/tokenizer_exceptions.py,11,"""G."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,12,"""J. E."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,13,"""J. Em."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,14,"""J.E."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,15,"""J.Em."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,16,"""K."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,17,"""N."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,18,"""V."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,19,"""Vt."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,20,"""a."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,21,"""a.k."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,22,"""a.s."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,23,"""adv."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,24,"""akad."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,25,"""aklg."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,26,"""akt."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,27,"""al."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,28,"""ang."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,29,"""angl."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,30,"""aps."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,31,"""apskr."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,32,"""apyg."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,33,"""arbat."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,34,"""asist."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,35,"""asm."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,36,"""asm.k."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,37,"""asmv."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,38,"""atk."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,39,"""atsak."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,40,"""atsisk."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,41,"""atsisk.sąsk."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,42,"""atv."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,43,"""aut."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,44,"""avd."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,45,"""b.k."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,46,"""baud."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,47,"""biol."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,48,"""bkl."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,49,"""bot."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,50,"""bt."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,51,"""buv."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,52,"""ch."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,53,"""chem."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,54,"""corp."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,55,"""d."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,56,"""dab."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,57,"""dail."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,58,"""dek."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,59,"""deš."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,60,"""dir."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,61,"""dirig."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,62,"""doc."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,63,"""dol."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,64,"""dr."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,65,"""drp."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,66,"""dvit."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,67,"""dėst."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,68,"""dš."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,69,"""dž."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,70,"""e.b."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,71,"""e.bankas"",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,72,"""e.p."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,73,"""e.parašas"",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,74,"""e.paštas"",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,75,"""e.v."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,76,"""e.valdžia"",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,77,"""egz."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,78,"""eil."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,79,"""ekon."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,80,"""el."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,81,"""el.bankas"",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,82,"""el.p."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,83,"""el.parašas"",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,84,"""el.paštas"",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,85,"""el.valdžia"",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,86,"""etc."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,87,"""ež."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,88,"""fak."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,89,"""faks."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,90,"""feat."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,91,"""filol."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,92,"""filos."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,93,"""g."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,94,"""gen."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,95,"""geol."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,96,"""gerb."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,97,"""gim."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,98,"""gr."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,99,"""gv."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,100,"""gyd."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,101,"""gyv."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,102,"""habil."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,103,"""inc."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,104,"""insp."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,105,"""inž."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,106,"""ir pan."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,107,"""ir t. t."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,108,"""isp."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,109,"""istor."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,110,"""it."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,111,"""just."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,112,"""k."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,113,"""k. a."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,114,"""k.a."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,115,"""kab."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,116,"""kand."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,117,"""kart."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,118,"""kat."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,119,"""ketv."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,120,"""kh."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,121,"""kl."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,122,"""kln."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,123,"""km."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,124,"""kn."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,125,"""koresp."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,126,"""kpt."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,127,"""kr."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,128,"""kt."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,129,"""kub."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,130,"""kun."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,131,"""kv."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,132,"""kyš."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,133,"""l. e. p."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,134,"""l.e.p."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,135,"""lenk."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,136,"""liet."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,137,"""lot."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,138,"""lt."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,139,"""ltd."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,140,"""ltn."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,141,"""m."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,142,"""m.e.."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,143,"""m.m."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,144,"""mat."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,145,"""med."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,146,"""mgnt."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,147,"""mgr."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,148,"""min."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,149,"""mjr."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,150,"""ml."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,151,"""mln."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,152,"""mlrd."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,153,"""mob."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,154,"""mok."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,155,"""moksl."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,156,"""mokyt."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,157,"""mot."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,158,"""mr."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,159,"""mst."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,160,"""mstl."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,161,"""mėn."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,162,"""nkt."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,163,"""no."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,164,"""nr."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,165,"""ntk."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,166,"""nuotr."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,167,"""op."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,168,"""org."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,169,"""orig."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,170,"""p."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,171,"""p.d."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,172,"""p.m.e."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,173,"""p.s."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,174,"""pab."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,175,"""pan."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,176,"""past."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,177,"""pav."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,178,"""pavad."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,179,"""per."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,180,"""perd."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,181,"""pirm."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,182,"""pl."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,183,"""plg."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,184,"""plk."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,185,"""pr."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,186,"""pr.Kr."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,187,"""pranc."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,188,"""proc."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,189,"""prof."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,190,"""prom."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,191,"""prot."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,192,"""psl."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,193,"""pss."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,194,"""pvz."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,195,"""pšt."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,196,"""r."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,197,"""raj."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,198,"""red."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,199,"""rez."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,200,"""rež."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,201,"""rus."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,202,"""rš."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,203,"""s."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,204,"""sav."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,205,"""saviv."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,206,"""sek."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,207,"""sekr."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,208,"""sen."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,209,"""sh."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,210,"""sk."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,211,"""skg."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,212,"""skv."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,213,"""skyr."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,214,"""sp."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,215,"""spec."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,216,"""sr."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,217,"""st."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,218,"""str."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,219,"""stud."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,220,"""sąs."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,221,"""t."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,222,"""t. p."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,223,"""t. y."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,224,"""t.p."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,225,"""t.t."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,226,"""t.y."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,227,"""techn."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,228,"""tel."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,229,"""teol."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,230,"""th."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,231,"""tir."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,232,"""trit."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,233,"""trln."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,234,"""tšk."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,235,"""tūks."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,236,"""tūkst."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,237,"""up."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,238,"""upl."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,239,"""v.s."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,240,"""vad."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,241,"""val."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,242,"""valg."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,243,"""ved."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,244,"""vert."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,245,"""vet."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,246,"""vid."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,247,"""virš."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,248,"""vlsč."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,249,"""vnt."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,250,"""vok."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,251,"""vs."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,252,"""vtv."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,253,"""vv."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,254,"""vyr."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,255,"""vyresn."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,256,"""zool."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,257,"""Įn"",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,258,"""įl."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,259,"""š.m."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,260,"""šnek."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,261,"""šv."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,262,"""švč."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,263,"""ž.ū."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,264,"""žin."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,265,"""žml."",",
spaCy/spacy/lang/lt/tokenizer_exceptions.py,266,"""žr."",",
spaCy/spacy/lang/lt/punctuation.py,1,coding: utf8,
spaCy/spacy/lang/lt/lex_attrs.py,1,coding: utf8,
spaCy/spacy/lang/lt/tag_map.py,1,coding: utf8,
spaCy/spacy/lang/lt/__init__.py,1,coding: utf8,
spaCy/spacy/lang/lt/examples.py,1,coding: utf8,
spaCy/spacy/lang/vi/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/vi/stop_words.py,4,Source: https://github.com/stopwords/vietnamese-stopwords,
spaCy/spacy/lang/vi/lex_attrs.py,1,coding: utf8,
spaCy/spacy/lang/vi/tag_map.py,1,coding: utf8,
spaCy/spacy/lang/vi/__init__.py,1,coding: utf8,
spaCy/spacy/lang/vi/__init__.py,15,for pickling,
spaCy/spacy/lang/vi/__init__.py,26,override defaults,
spaCy/spacy/lang/hy/tag_map.py,1,coding: utf8,
spaCy/spacy/lang/af/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/af/stop_words.py,5,Source: https://github.com/stopwords-iso/stopwords-af,
spaCy/spacy/lang/af/__init__.py,1,coding: utf8,
spaCy/spacy/lang/uk/stop_words.py,1,encoding: utf8,
spaCy/spacy/lang/uk/tokenizer_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/uk/lemmatizer.py,1,coding: utf8,
spaCy/spacy/lang/uk/lemmatizer.py,29,Skip unchangeable pos,
spaCy/spacy/lang/uk/lemmatizer.py,36,Skip suggested parse variant for unknown word for pymorphy,
spaCy/spacy/lang/uk/lemmatizer.py,55,VERB,
spaCy/spacy/lang/uk/lemmatizer.py,116,Can also be an ADV - unchangeable,
spaCy/spacy/lang/uk/lemmatizer.py,117,Can also be a SCONJ - both unchangeable ones,
spaCy/spacy/lang/uk/lex_attrs.py,1,coding: utf8,
spaCy/spacy/lang/uk/tag_map.py,1,coding: utf8,
spaCy/spacy/lang/uk/__init__.py,1,coding: utf8,
spaCy/spacy/lang/uk/examples.py,1,coding: utf8,
spaCy/spacy/lang/uk/examples.py,15,Serhiy Zhadan,
spaCy/spacy/lang/uk/examples.py,17,wikipedia,
spaCy/spacy/lang/uk/examples.py,19,blyznets_viktor_semenovych/zemlia_svitliachkiv,
spaCy/spacy/lang/uk/examples.py,21,Hryhorij Czubaj,
spaCy/spacy/lang/uk/examples.py,22,homographs,
spaCy/spacy/lang/tt/stop_words.py,1,encoding: utf8,
spaCy/spacy/lang/tt/stop_words.py,4,Tatar stopwords are from https://github.com/aliiae/stopwords-tt,
spaCy/spacy/lang/tt/tokenizer_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/tt/tokenizer_exceptions.py,9,Weekdays abbreviations,
spaCy/spacy/lang/tt/tokenizer_exceptions.py,17,Months abbreviations,
spaCy/spacy/lang/tt/tokenizer_exceptions.py,30,Number abbreviations,
spaCy/spacy/lang/tt/tokenizer_exceptions.py,40,"""etc."" abbreviations",
spaCy/spacy/lang/tt/punctuation.py,1,coding: utf8,
spaCy/spacy/lang/tt/lex_attrs.py,1,coding: utf8,
spaCy/spacy/lang/tt/__init__.py,1,coding: utf8,
spaCy/spacy/lang/tt/examples.py,1,coding: utf8,
spaCy/spacy/lang/it/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/it/tokenizer_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/it/punctuation.py,1,coding: utf8,
spaCy/spacy/lang/it/tag_map.py,1,coding: utf8,
spaCy/spacy/lang/it/__init__.py,1,coding: utf8,
spaCy/spacy/lang/it/examples.py,1,coding: utf8,
spaCy/spacy/lang/ta/norm_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/ta/norm_exceptions.py,5,Regional words normal,
spaCy/spacy/lang/ta/norm_exceptions.py,6,Sri Lanka - wikipeadia,
spaCy/spacy/lang/ta/norm_exceptions.py,41,Srilankan and indian,
spaCy/spacy/lang/ta/norm_exceptions.py,52,relationship,
spaCy/spacy/lang/ta/norm_exceptions.py,62,difference words,
spaCy/spacy/lang/ta/norm_exceptions.py,71,regular spokens,
spaCy/spacy/lang/ta/norm_exceptions.py,78,Portugeese formal words,
spaCy/spacy/lang/ta/norm_exceptions.py,93,Dutch formal words,
spaCy/spacy/lang/ta/norm_exceptions.py,98,English formal words,
spaCy/spacy/lang/ta/norm_exceptions.py,105,Arabic formal words,
spaCy/spacy/lang/ta/norm_exceptions.py,112,"Persian, Hindustanian and hindi formal words",
spaCy/spacy/lang/ta/norm_exceptions.py,122,English words used in text conversations,
spaCy/spacy/lang/ta/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/ta/stop_words.py,5,Stop words,
spaCy/spacy/lang/ta/lex_attrs.py,1,coding: utf8,
spaCy/spacy/lang/ta/lex_attrs.py,57,"20-89 ,90-899,900-99999 and above have different suffixes",
spaCy/spacy/lang/ta/lex_attrs.py,59,text without numeral suffixes,
spaCy/spacy/lang/ta/__init__.py,1,coding: utf8,
spaCy/spacy/lang/ta/examples.py,1,coding: utf8,
spaCy/spacy/lang/lv/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/lv/stop_words.py,5,Source: https://github.com/stopwords-iso/stopwords-lv,
spaCy/spacy/lang/lv/__init__.py,1,coding: utf8,
spaCy/spacy/lang/tr/stop_words.py,1,encoding: utf8,
spaCy/spacy/lang/tr/stop_words.py,5,Source: https://github.com/stopwords-iso/stopwords-tr,
spaCy/spacy/lang/tr/tokenizer_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/tr/lex_attrs.py,1,coding: utf8,
spaCy/spacy/lang/tr/lex_attrs.py,7,"Thirteen, fifteen etc. are written separate: on üç",
spaCy/spacy/lang/tr/__init__.py,1,coding: utf8,
spaCy/spacy/lang/tr/examples.py,1,coding: utf8,
spaCy/spacy/lang/tl/stop_words.py,1,encoding: utf8,
spaCy/spacy/lang/tl/tokenizer_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/tl/lex_attrs.py,1,coding: utf8,
spaCy/spacy/lang/tl/__init__.py,1,coding: utf8,
spaCy/spacy/lang/lb/norm_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/lb/norm_exceptions.py,4,TODO,
spaCy/spacy/lang/lb/norm_exceptions.py,5,norm execptions: find a possibility to deal with the zillions of spelling,
spaCy/spacy/lang/lb/norm_exceptions.py,6,"variants (vläicht = vlaicht, vleicht, viläicht, viläischt, etc. etc.)",
spaCy/spacy/lang/lb/norm_exceptions.py,7,here one could include the most common spelling mistakes,
spaCy/spacy/lang/lb/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/lb/tokenizer_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/lb/tokenizer_exceptions.py,6,TODO,
spaCy/spacy/lang/lb/tokenizer_exceptions.py,7,"treat other apostrophes within words as part of the word: [op d'mannst], [fir d'éischt] (= exceptions)",
spaCy/spacy/lang/lb/tokenizer_exceptions.py,11,translate / delete what is not necessary,
spaCy/spacy/lang/lb/tokenizer_exceptions.py,30,to be extended,
spaCy/spacy/lang/lb/punctuation.py,1,coding: utf8,
spaCy/spacy/lang/lb/lex_attrs.py,1,coding: utf8,
spaCy/spacy/lang/lb/tag_map.py,1,coding: utf8,
spaCy/spacy/lang/lb/tag_map.py,7,TODO: tag map is still using POS tags from an internal training set.,
spaCy/spacy/lang/lb/tag_map.py,8,These POS tags have to be modified to match those from Universal Dependencies,
spaCy/spacy/lang/lb/__init__.py,1,coding: utf8,
spaCy/spacy/lang/lb/examples.py,1,coding: utf8,
spaCy/spacy/lang/fi/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/fi/stop_words.py,5,Source https://github.com/stopwords-iso/stopwords-fi/blob/master/stopwords-fi.txt,
spaCy/spacy/lang/fi/stop_words.py,6,Reformatted with some minor corrections,
spaCy/spacy/lang/fi/tokenizer_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/fi/tokenizer_exceptions.py,10,Source https://www.cs.tut.fi/~jkorpela/kielenopas/5.5.html,
spaCy/spacy/lang/fi/punctuation.py,1,coding: utf8,
spaCy/spacy/lang/fi/lex_attrs.py,1,coding: utf8,
spaCy/spacy/lang/fi/__init__.py,1,coding: utf8,
spaCy/spacy/lang/fi/examples.py,1,coding: utf8,
spaCy/spacy/lang/nb/morph_rules.py,1,encoding: utf8,
spaCy/spacy/lang/nb/morph_rules.py,6,This dict includes all the PRON and DET tag combinations found in the,
spaCy/spacy/lang/nb/morph_rules.py,7,"dataset developed by Schibsted, Nasjonalbiblioteket and LTG (to be published",
spaCy/spacy/lang/nb/morph_rules.py,8,autumn 2018) and the rarely used polite form.,
spaCy/spacy/lang/nb/morph_rules.py,28,"polite form, not sure about the tag",
spaCy/spacy/lang/nb/morph_rules.py,132,"polite form, not sure about the tag",
spaCy/spacy/lang/nb/morph_rules.py,259,"polite form, not sure about the tag",
spaCy/spacy/lang/nb/morph_rules.py,320,"polite form, not sure about the tag",
spaCy/spacy/lang/nb/morph_rules.py,381,"polite form, not sure about the tag",
spaCy/spacy/lang/nb/morph_rules.py,656,"same wordform and pos (verb), have to specify the exact features in order to not mix them up",
spaCy/spacy/lang/nb/morph_rules.py,665,copied from the English morph_rules.py,
spaCy/spacy/lang/nb/syntax_iterators.py,1,coding: utf8,
spaCy/spacy/lang/nb/syntax_iterators.py,21,Ensure works on both Doc and Span.,
spaCy/spacy/lang/nb/syntax_iterators.py,29,Prevent nested chunks from being produced,
spaCy/spacy/lang/nb/syntax_iterators.py,41,"If the head is an NP, and we're coordinated to it, we're an NP",
spaCy/spacy/lang/nb/stop_words.py,1,encoding: utf8,
spaCy/spacy/lang/nb/tokenizer_exceptions.py,1,encoding: utf8,
spaCy/spacy/lang/nb/punctuation.py,1,coding: utf8,
spaCy/spacy/lang/nb/punctuation.py,9,Punctuation adapted from Danish,
spaCy/spacy/lang/nb/tag_map.py,1,coding: utf8,
spaCy/spacy/lang/nb/tag_map.py,8,Tags are a combination of POS and morphological features from a,
spaCy/spacy/lang/nb/tag_map.py,9,"https://github.com/ltgoslo/norne developed by Schibsted, Nasjonalbiblioteket and LTG. The",
spaCy/spacy/lang/nb/tag_map.py,10,data format is .conllu and follows the Universal Dependencies annotation.,
spaCy/spacy/lang/nb/tag_map.py,11,(There are some annotation differences compared to this dataset:,
spaCy/spacy/lang/nb/tag_map.py,12,https://github.com/UniversalDependencies/UD_Norwegian-Bokmaal,
spaCy/spacy/lang/nb/tag_map.py,13,mainly in the way determiners and pronouns are tagged).,
spaCy/spacy/lang/nb/__init__.py,1,coding: utf8,
spaCy/spacy/lang/nb/examples.py,1,coding: utf8,
spaCy/spacy/lang/ga/irish_morphology_helpers.py,1,coding: utf8,
spaCy/spacy/lang/ga/irish_morphology_helpers.py,5,fmt: off,
spaCy/spacy/lang/ga/irish_morphology_helpers.py,10,fmt: on,
spaCy/spacy/lang/ga/stop_words.py,1,encoding: utf8,
spaCy/spacy/lang/ga/tokenizer_exceptions.py,1,encoding: utf8,
spaCy/spacy/lang/ga/tag_map.py,1,coding: utf8,
spaCy/spacy/lang/ga/tag_map.py,4,fmt: off,
spaCy/spacy/lang/ga/tag_map.py,369,fmt: on,
spaCy/spacy/lang/ga/__init__.py,1,coding: utf8,
spaCy/spacy/lang/de/norm_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/de/norm_exceptions.py,4,"Here we only want to include the absolute most common words. Otherwise,",
spaCy/spacy/lang/de/norm_exceptions.py,5,this list would get impossibly long for German – especially considering the,
spaCy/spacy/lang/de/norm_exceptions.py,6,"old vs. new spelling rules, and all possible cases.",
spaCy/spacy/lang/de/syntax_iterators.py,1,coding: utf8,
spaCy/spacy/lang/de/syntax_iterators.py,11,this iterator extracts spans headed by NOUNs starting from the left-most,
spaCy/spacy/lang/de/syntax_iterators.py,12,syntactic dependent until the NOUN itself for close apposition and,
spaCy/spacy/lang/de/syntax_iterators.py,13,"measurement construction, the span is sometimes extended to the right of",
spaCy/spacy/lang/de/syntax_iterators.py,14,"the NOUN. Example: ""eine Tasse Tee"" (a cup (of) tea) returns ""eine Tasse Tee""",
spaCy/spacy/lang/de/syntax_iterators.py,15,"and not just ""eine Tasse"", same for ""das Thema Familie"".",
spaCy/spacy/lang/de/syntax_iterators.py,30,Ensure works on both Doc and Span.,
spaCy/spacy/lang/de/syntax_iterators.py,41,try to extend the span to the right,
spaCy/spacy/lang/de/syntax_iterators.py,42,to capture close apposition/measurement constructions,
spaCy/spacy/lang/de/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/de/tokenizer_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/de/punctuation.py,1,coding: utf8,
spaCy/spacy/lang/de/tag_map.py,1,coding: utf8,
spaCy/spacy/lang/de/__init__.py,1,coding: utf8,
spaCy/spacy/lang/de/examples.py,1,coding: utf8,
spaCy/spacy/lang/is/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/is/stop_words.py,5,Source: https://github.com/Xangis/extra-stopwords,
spaCy/spacy/lang/is/__init__.py,1,coding: utf8,
spaCy/spacy/lang/hu/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/hu/tokenizer_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/hu/punctuation.py,1,coding: utf8,
spaCy/spacy/lang/hu/punctuation.py,8,removing ° from the special icons to keep e.g. 99° as one token,
spaCy/spacy/lang/hu/__init__.py,1,coding: utf8,
spaCy/spacy/lang/hu/examples.py,1,coding: utf8,
spaCy/spacy/lang/hr/stop_words.py,1,encoding: utf8,
spaCy/spacy/lang/hr/stop_words.py,5,Source: https://github.com/stopwords-iso/stopwords-hr,
spaCy/spacy/lang/hr/__init__.py,1,coding: utf8,
spaCy/spacy/lang/hr/examples.py,1,coding: utf8,
spaCy/spacy/lang/ru/norm_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/ru/norm_exceptions.py,6,Slang,
spaCy/spacy/lang/ru/stop_words.py,1,encoding: utf8,
spaCy/spacy/lang/ru/tokenizer_exceptions.py,1,encoding: utf8,
spaCy/spacy/lang/ru/tokenizer_exceptions.py,10,Weekdays abbreviations,
spaCy/spacy/lang/ru/tokenizer_exceptions.py,22,Months abbreviations,
spaCy/spacy/lang/ru/tokenizer_exceptions.py,27,"{ORTH: ""март"", LEMMA: ""март"", NORM: ""март""},",
spaCy/spacy/lang/ru/tokenizer_exceptions.py,30,"{ORTH: ""май"", LEMMA: ""май"", NORM: ""май""},",
spaCy/spacy/lang/ru/tokenizer_exceptions.py,32,"{ORTH: ""июнь"", LEMMA: ""июнь"", NORM: ""июнь""},",
spaCy/spacy/lang/ru/tokenizer_exceptions.py,34,"{ORTH: ""июль"", LEMMA: ""июль"", NORM: ""июль""},",
spaCy/spacy/lang/ru/lemmatizer.py,1,coding: utf8,
spaCy/spacy/lang/ru/lemmatizer.py,32,Skip unchangeable pos,
spaCy/spacy/lang/ru/lemmatizer.py,39,Skip suggested parse variant for unknown word for pymorphy,
spaCy/spacy/lang/ru/lemmatizer.py,58,VERB,
spaCy/spacy/lang/ru/lemmatizer.py,119,Can also be an ADV - unchangeable,
spaCy/spacy/lang/ru/lemmatizer.py,120,Can also be a SCONJ - both unchangeable ones,
spaCy/spacy/lang/ru/lex_attrs.py,1,coding: utf8,
spaCy/spacy/lang/ru/tag_map.py,1,coding: utf8,
spaCy/spacy/lang/ru/tag_map.py,7,fmt: off,
spaCy/spacy/lang/ru/tag_map.py,746,fmt: on,
spaCy/spacy/lang/ru/__init__.py,1,encoding: utf8,
spaCy/spacy/lang/ru/examples.py,1,coding: utf8,
spaCy/spacy/lang/ru/examples.py,14,Translations from English:,
spaCy/spacy/lang/ru/examples.py,19,Native Russian sentences:,
spaCy/spacy/lang/ru/examples.py,20,Colloquial:,
spaCy/spacy/lang/ru/examples.py,21,Typical polite refusal,
spaCy/spacy/lang/ru/examples.py,22,From a tour guide speech,
spaCy/spacy/lang/ru/examples.py,23,Examples of Bookish Russian:,
spaCy/spacy/lang/ru/examples.py,24,"Quote from ""The Golden Calf""",
spaCy/spacy/lang/ru/examples.py,26,"Quotes from ""Ivan Vasilievich changes his occupation""",
spaCy/spacy/lang/ru/examples.py,29,Quotes from Dostoevsky:,
spaCy/spacy/lang/ru/examples.py,33,Quotes from Chekhov:,
spaCy/spacy/lang/ru/examples.py,35,Quotes from Turgenev:,
spaCy/spacy/lang/ru/examples.py,38,Quotes from newspapers:,
spaCy/spacy/lang/ru/examples.py,39,Komsomolskaya Pravda:,
spaCy/spacy/lang/ru/examples.py,42,Argumenty i Facty:,
spaCy/spacy/lang/ro/stop_words.py,1,encoding: utf8,
spaCy/spacy/lang/ro/stop_words.py,5,Source: https://github.com/stopwords-iso/stopwords-ro,
spaCy/spacy/lang/ro/tokenizer_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/ro/tokenizer_exceptions.py,11,Source: https://en.wiktionary.org/wiki/Category:Romanian_abbreviations,
spaCy/spacy/lang/ro/tokenizer_exceptions.py,49,below: from UD_Romanian-RRT:,
spaCy/spacy/lang/ro/tokenizer_exceptions.py,92,note: does not distinguish capitalized-only exceptions from others,
spaCy/spacy/lang/ro/punctuation.py,1,coding: utf8,
spaCy/spacy/lang/ro/punctuation.py,35,UD_Romanian-RRT closed class prefixes,
spaCy/spacy/lang/ro/punctuation.py,36,POS: ADP|AUX|CCONJ|DET|NUM|PART|PRON|SCONJ,
spaCy/spacy/lang/ro/punctuation.py,70,UD_Romanian-RRT closed class suffixes without NUM,
spaCy/spacy/lang/ro/punctuation.py,71,POS: ADP|AUX|CCONJ|DET|PART|PRON|SCONJ,
spaCy/spacy/lang/ro/lex_attrs.py,1,coding: utf8,
spaCy/spacy/lang/ro/__init__.py,1,coding: utf8,
spaCy/spacy/lang/ro/__init__.py,16,Lemma data note:,
spaCy/spacy/lang/ro/__init__.py,17,Original pairs downloaded from http://www.lexiconista.com/datasets/lemmatization/,
spaCy/spacy/lang/ro/__init__.py,18,Replaced characters using cedillas with the correct ones (ș and ț),
spaCy/spacy/lang/ro/examples.py,1,coding: utf8,
spaCy/spacy/lang/te/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/te/stop_words.py,4,Source: https://github.com/Xangis/extra-stopwords (MIT License),
spaCy/spacy/lang/te/lex_attrs.py,1,coding: utf8,
spaCy/spacy/lang/te/__init__.py,1,coding: utf8,
spaCy/spacy/lang/te/examples.py,1,coding: utf8,
spaCy/spacy/lang/es/syntax_iterators.py,1,coding: utf8,
spaCy/spacy/lang/es/syntax_iterators.py,12,"['nunmod', 'det', 'appos', 'fixed']",
spaCy/spacy/lang/es/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/es/tokenizer_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/es/tokenizer_exceptions.py,27,Times,
spaCy/spacy/lang/es/punctuation.py,1,coding: utf8,
spaCy/spacy/lang/es/lex_attrs.py,1,coding: utf8,
spaCy/spacy/lang/es/tag_map.py,1,coding: utf8,
spaCy/spacy/lang/es/tag_map.py,7,fmt: off,
spaCy/spacy/lang/es/tag_map.py,313,fmt: on,
spaCy/spacy/lang/es/__init__.py,1,coding: utf8,
spaCy/spacy/lang/es/examples.py,1,coding: utf8,
spaCy/spacy/lang/zh/stop_words.py,1,encoding: utf8,
spaCy/spacy/lang/zh/stop_words.py,5,stop words as whitespace-separated list,
spaCy/spacy/lang/zh/stop_words.py,6,"Chinese stop words,maybe not enough",
spaCy/spacy/lang/zh/lex_attrs.py,1,coding: utf8,
spaCy/spacy/lang/zh/lex_attrs.py,89,fmt: off,
spaCy/spacy/lang/zh/lex_attrs.py,94,fmt: on,
spaCy/spacy/lang/zh/tag_map.py,1,coding: utf8,
spaCy/spacy/lang/zh/tag_map.py,7,The Chinese part-of-speech tagger uses the OntoNotes 5 version of the Penn,
spaCy/spacy/lang/zh/tag_map.py,8,Treebank tag set. We also map the tags to the simpler Universal Dependencies,
spaCy/spacy/lang/zh/tag_map.py,9,v2 tag set.,
spaCy/spacy/lang/zh/__init__.py,1,coding: utf8,
spaCy/spacy/lang/zh/__init__.py,36,use jieba,
spaCy/spacy/lang/zh/__init__.py,46,second token in adjacent whitespace following a,
spaCy/spacy/lang/zh/__init__.py,47,non-space token,
spaCy/spacy/lang/zh/__init__.py,51,first space token following non-space token,
spaCy/spacy/lang/zh/__init__.py,54,token is non-space whitespace or any whitespace following,
spaCy/spacy/lang/zh/__init__.py,55,a whitespace token,
spaCy/spacy/lang/zh/__init__.py,57,extend previous whitespace token with more whitespace,
spaCy/spacy/lang/zh/__init__.py,60,otherwise it's a new whitespace token,
spaCy/spacy/lang/zh/__init__.py,69,split into individual characters,
spaCy/spacy/lang/zh/__init__.py,100,override defaults,
spaCy/spacy/lang/zh/examples.py,1,coding: utf8,
spaCy/spacy/lang/zh/examples.py,12,from https://zh.wikipedia.org/wiki/汉语,
spaCy/spacy/lang/fa/syntax_iterators.py,1,coding: utf8,
spaCy/spacy/lang/fa/syntax_iterators.py,22,Ensure works on both Doc and Span.,
spaCy/spacy/lang/fa/syntax_iterators.py,30,Prevent nested chunks from being produced,
spaCy/spacy/lang/fa/syntax_iterators.py,42,"If the head is an NP, and we're coordinated to it, we're an NP",
spaCy/spacy/lang/fa/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/fa/stop_words.py,5,Stop words from HAZM package,
spaCy/spacy/lang/fa/tokenizer_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/fa/punctuation.py,1,coding: utf8,
spaCy/spacy/lang/fa/punctuation.py,13,"4% -> [""4"", ""%""]",
spaCy/spacy/lang/fa/punctuation.py,14,Persian is written from Right-To-Left,
spaCy/spacy/lang/fa/lex_attrs.py,1,coding: utf8,
spaCy/spacy/lang/fa/tag_map.py,1,coding: utf8,
spaCy/spacy/lang/fa/__init__.py,1,coding: utf8,
spaCy/spacy/lang/fa/examples.py,1,coding: utf8,
spaCy/spacy/lang/fa/generate_verbs_exc.py,1,coding: utf8,
spaCy/spacy/lang/fa/generate_verbs_exc.py,609,"Below code is a modified version of HAZM package's verb conjugator,",
spaCy/spacy/lang/fa/generate_verbs_exc.py,610,with soem extra verbs (Anything in hazm and not in here? compare needed!),
spaCy/spacy/lang/fa/generate_verbs_exc.py,617,special case of '#هست':,
spaCy/spacy/lang/lij/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/lij/tokenizer_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/lij/tokenizer_exceptions.py,28,"Prefix + prepositions with à (e.g. ""sott'a-o"")",
spaCy/spacy/lang/lij/punctuation.py,1,coding: utf8,
spaCy/spacy/lang/lij/__init__.py,1,coding: utf8,
spaCy/spacy/lang/lij/examples.py,1,coding: utf8,
spaCy/spacy/lang/si/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/si/lex_attrs.py,1,coding: utf8,
spaCy/spacy/lang/si/__init__.py,1,coding: utf8,
spaCy/spacy/lang/si/examples.py,1,coding: utf8,
spaCy/spacy/lang/sv/morph_rules.py,1,coding: utf8,
spaCy/spacy/lang/sv/morph_rules.py,7,Used the table of pronouns at https://sv.wiktionary.org/wiki/deras,
spaCy/spacy/lang/sv/syntax_iterators.py,1,coding: utf8,
spaCy/spacy/lang/sv/syntax_iterators.py,22,Ensure works on both Doc and Span.,
spaCy/spacy/lang/sv/syntax_iterators.py,30,Prevent nested chunks from being produced,
spaCy/spacy/lang/sv/syntax_iterators.py,42,"If the head is an NP, and we're coordinated to it, we're an NP",
spaCy/spacy/lang/sv/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/sv/tokenizer_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/sv/tokenizer_exceptions.py,9,Verbs,
spaCy/spacy/lang/sv/tokenizer_exceptions.py,29,"Abbreviations for weekdays ""sön."" (for ""söndag"" / ""söner"")",
spaCy/spacy/lang/sv/tokenizer_exceptions.py,30,are left out because they are ambiguous. The same is the case,
spaCy/spacy/lang/sv/tokenizer_exceptions.py,31,"for abbreviations ""jul."" and ""Jul."" (""juli"" / ""jul"").",
spaCy/spacy/lang/sv/tokenizer_exceptions.py,73,Specific case abbreviations only,
spaCy/spacy/lang/sv/tokenizer_exceptions.py,145,Add abbreviation for trailing punctuation too. If the abbreviation already has a trailing punctuation - skip it.,
spaCy/spacy/lang/sv/tokenizer_exceptions.py,155,"Sentences ending in ""i."" (as in ""... peka i.""), ""m."" (as in ""...än 2000 m.""),",
spaCy/spacy/lang/sv/tokenizer_exceptions.py,156,should be tokenized as two separate tokens.,
spaCy/spacy/lang/sv/tag_map.py,1,coding: utf8,
spaCy/spacy/lang/sv/tag_map.py,8,Tag mappings according to https://universaldependencies.org/tagset-conversion/sv-suc-uposf.html,
spaCy/spacy/lang/sv/tag_map.py,9,for https://github.com/UniversalDependencies/UD_Swedish-Talbanken,
spaCy/spacy/lang/sv/tag_map.py,12,"inte, också, så, bara, nu",
spaCy/spacy/lang/sv/tag_map.py,13,"t.ex., ca, t_ex, bl.a., s_k",
spaCy/spacy/lang/sv/tag_map.py,14,"mer, tidigare, mindre, vidare, mera",
spaCy/spacy/lang/sv/tag_map.py,15,"mycket, helt, ofta, länge, långt",
spaCy/spacy/lang/sv/tag_map.py,16,"över-, in-",
spaCy/spacy/lang/sv/tag_map.py,17,"minst, mest, högst, främst, helst",
spaCy/spacy/lang/sv/tag_map.py,20,"det, detta",
spaCy/spacy/lang/sv/tag_map.py,21,"ett, något, inget, vart, vartannat",
spaCy/spacy/lang/sv/tag_map.py,22,allt,
spaCy/spacy/lang/sv/tag_map.py,23,"de, dessa, bägge, dom",
spaCy/spacy/lang/sv/tag_map.py,24,"några, inga",
spaCy/spacy/lang/sv/tag_map.py,25,alla,
spaCy/spacy/lang/sv/tag_map.py,26,samma,
spaCy/spacy/lang/sv/tag_map.py,27,vardera,
spaCy/spacy/lang/sv/tag_map.py,28,"varje, varenda",
spaCy/spacy/lang/sv/tag_map.py,29,"den, denna",
spaCy/spacy/lang/sv/tag_map.py,30,"en, någon, ingen, var, varannan",
spaCy/spacy/lang/sv/tag_map.py,31,all,
spaCy/spacy/lang/sv/tag_map.py,32,"när, där, hur, som, då",
spaCy/spacy/lang/sv/tag_map.py,33,vilket,
spaCy/spacy/lang/sv/tag_map.py,34,vilka,
spaCy/spacy/lang/sv/tag_map.py,35,vilken,
spaCy/spacy/lang/sv/tag_map.py,36,som,
spaCy/spacy/lang/sv/tag_map.py,37,"vad, vilket",
spaCy/spacy/lang/sv/tag_map.py,39,vilka,
spaCy/spacy/lang/sv/tag_map.py,40,"vilken, vem",
spaCy/spacy/lang/sv/tag_map.py,41,"vars, vilkas, Vems",
spaCy/spacy/lang/sv/tag_map.py,42,att,
spaCy/spacy/lang/sv/tag_map.py,43,"Jo, ja, nej, fan, visst",
spaCy/spacy/lang/sv/tag_map.py,44,"ev, S:t, Kungl, Kungl., Teol",
spaCy/spacy/lang/sv/tag_map.py,45,äldres,
spaCy/spacy/lang/sv/tag_map.py,48,"större, högre, mindre, bättre, äldre",
spaCy/spacy/lang/sv/tag_map.py,50,"enskildes, sjukes, andres",
spaCy/spacy/lang/sv/tag_map.py,51,"enskilde, sjuke, andre, unge, ene",
spaCy/spacy/lang/sv/tag_map.py,52,eget,
spaCy/spacy/lang/sv/tag_map.py,54,"annat, svårt, möjligt, nytt, sådant",
spaCy/spacy/lang/sv/tag_map.py,57,"ogiftas, ungas, frånskildas, efterkommandes, färgblindas",
spaCy/spacy/lang/sv/tag_map.py,58,"olika, andra, många, stora, vissa",
spaCy/spacy/lang/sv/tag_map.py,59,"flera, sådana, fler, få, samtliga",
spaCy/spacy/lang/sv/tag_map.py,61,"bra, ena, enda, nästa, ringa",
spaCy/spacy/lang/sv/tag_map.py,63,"hela, nya, andra, svenska, ekonomiska",
spaCy/spacy/lang/sv/tag_map.py,64,"fri-, låg-, sexual-",
spaCy/spacy/lang/sv/tag_map.py,65,egen,
spaCy/spacy/lang/sv/tag_map.py,66,enskilds,
spaCy/spacy/lang/sv/tag_map.py,67,"stor, annan, själv, sådan, viss",
spaCy/spacy/lang/sv/tag_map.py,69,"störste, främste, äldste, minste",
spaCy/spacy/lang/sv/tag_map.py,70,flesta,
spaCy/spacy/lang/sv/tag_map.py,74,"bästa, största, närmaste, viktigaste, högsta",
spaCy/spacy/lang/sv/tag_map.py,77,"störst, bäst, tidigast, högst, fattigast",
spaCy/spacy/lang/sv/tag_map.py,78,"och, eller, som, än, men",
spaCy/spacy/lang/sv/tag_map.py,80,"., ?, :, !, ...",
spaCy/spacy/lang/sv/tag_map.py,81,",, -, :, *, ;",
spaCy/spacy/lang/sv/tag_map.py,82,"godo, fjol, fullo, somras, måtto",
spaCy/spacy/lang/sv/tag_map.py,83,"kr, %, s., dr, kap.",
spaCy/spacy/lang/sv/tag_map.py,85,"yrkes-, barn-, hem-, fack-, vatten-",
spaCy/spacy/lang/sv/tag_map.py,88,"barnens, årens, u-ländernas, företagens, århundradenas",
spaCy/spacy/lang/sv/tag_map.py,89,"barnen, u-länderna, åren, länderna, könen",
spaCy/spacy/lang/sv/tag_map.py,90,"slags, års, barns, länders, tusentals",
spaCy/spacy/lang/sv/tag_map.py,91,"barn, år, fall, länder, problem",
spaCy/spacy/lang/sv/tag_map.py,94,"äktenskapets, samhällets, barnets, 1800-talets, 1960-talets",
spaCy/spacy/lang/sv/tag_map.py,97,"äktenskapet, samhället, barnet, stället, hemmet",
spaCy/spacy/lang/sv/tag_map.py,98,"års, slags, lands, havs, företags",
spaCy/spacy/lang/sv/tag_map.py,99,"år, arbete, barn, sätt, äktenskap",
spaCy/spacy/lang/sv/tag_map.py,100,"PCB-, Syd-",
spaCy/spacy/lang/sv/tag_map.py,101,"dags, rätta",
spaCy/spacy/lang/sv/tag_map.py,102,"far-, kibbutz-, röntgen-, barna-, hälso-",
spaCy/spacy/lang/sv/tag_map.py,105,"föräldrarnas, kvinnornas, elevernas, kibbutzernas, makarnas",
spaCy/spacy/lang/sv/tag_map.py,108,"kvinnorna, föräldrarna, makarna, männen, hyrorna",
spaCy/spacy/lang/sv/tag_map.py,109,"människors, kvinnors, dagars, tiders, månaders",
spaCy/spacy/lang/sv/tag_map.py,110,"procent, människor, kvinnor, miljoner, kronor",
spaCy/spacy/lang/sv/tag_map.py,111,"kvinnans, världens, familjens, dagens, jordens",
spaCy/spacy/lang/sv/tag_map.py,112,"familjen, kvinnan, mannen, världen, skolan",
spaCy/spacy/lang/sv/tag_map.py,113,"sorts, medelålders, makes, kvinnas, veckas",
spaCy/spacy/lang/sv/tag_map.py,114,"del, tid, dag, fråga, man",
spaCy/spacy/lang/sv/tag_map.py,115,", ), (",
spaCy/spacy/lang/sv/tag_map.py,117,avlidnes,
spaCy/spacy/lang/sv/tag_map.py,119,"taget, sett, särskilt, förbjudet, ökat",
spaCy/spacy/lang/sv/tag_map.py,120,"försäkrades, anställdas",
spaCy/spacy/lang/sv/tag_map.py,123,"särskilda, gifta, ökade, handikappade, skilda",
spaCy/spacy/lang/sv/tag_map.py,125,"ökade, gifta, nämnda, nedärvda, dolda",
spaCy/spacy/lang/sv/tag_map.py,127,"särskild, ökad, beredd, gift, oförändrad",
spaCy/spacy/lang/sv/tag_map.py,130,"studerandes, sammanboendes, dubbelarbetandes",
spaCy/spacy/lang/sv/tag_map.py,133,"följande, beroende, nuvarande, motsvarande, liknande",
spaCy/spacy/lang/sv/tag_map.py,134,"ut, upp, in, till, med",
spaCy/spacy/lang/sv/tag_map.py,136,"F, N, Liechtenstein, Danmark, DK",
spaCy/spacy/lang/sv/tag_map.py,137,"Sveriges, EEC:s, Guds, Stockholms, Kristi",
spaCy/spacy/lang/sv/tag_map.py,138,"Sverige, EEC, Stockholm, USA, ATP",
spaCy/spacy/lang/sv/tag_map.py,139,"Göteborgs-, Nord-, Väst-",
spaCy/spacy/lang/sv/tag_map.py,140,denne,
spaCy/spacy/lang/sv/tag_map.py,141,"det, detta, detsamma",
spaCy/spacy/lang/sv/tag_map.py,142,"något, allt, mycket, annat, ingenting",
spaCy/spacy/lang/sv/tag_map.py,143,"dem, varandra, varann",
spaCy/spacy/lang/sv/tag_map.py,144,"de, bägge",
spaCy/spacy/lang/sv/tag_map.py,145,"dessa, dom, båda, den, bådadera",
spaCy/spacy/lang/sv/tag_map.py,146,"andra, alla, många, sådana, några",
spaCy/spacy/lang/sv/tag_map.py,147,"sig, sej",
spaCy/spacy/lang/sv/tag_map.py,148,"oss, er, eder",
spaCy/spacy/lang/sv/tag_map.py,149,vi,
spaCy/spacy/lang/sv/tag_map.py,150,"dig, mig, henne, honom, Er",
spaCy/spacy/lang/sv/tag_map.py,151,"du, han, hon, jag, ni",
spaCy/spacy/lang/sv/tag_map.py,152,"den, denna, densamma",
spaCy/spacy/lang/sv/tag_map.py,153,man,
spaCy/spacy/lang/sv/tag_map.py,154,"en, var, någon, ingen, Varannan",
spaCy/spacy/lang/sv/tag_map.py,155,"i, av, på, för, till",
spaCy/spacy/lang/sv/tag_map.py,156,f,
spaCy/spacy/lang/sv/tag_map.py,158,"sitt, vårt, ditt, mitt, ert",
spaCy/spacy/lang/sv/tag_map.py,159,"sina, våra, dina, mina",
spaCy/spacy/lang/sv/tag_map.py,160,"deras, dess, hans, hennes, varandras",
spaCy/spacy/lang/sv/tag_map.py,161,"sin, vår, din, min, er",
spaCy/spacy/lang/sv/tag_map.py,162,"2, 17, 20, 1, 18",
spaCy/spacy/lang/sv/tag_map.py,165,ett,
spaCy/spacy/lang/sv/tag_map.py,166,"två, tre, 1, 20, 2",
spaCy/spacy/lang/sv/tag_map.py,167,"ett-, 1950-, två-, tre-, 1700-",
spaCy/spacy/lang/sv/tag_map.py,169,en,
spaCy/spacy/lang/sv/tag_map.py,171,förste,
spaCy/spacy/lang/sv/tag_map.py,173,"första, andra, tredje, fjärde, femte",
spaCy/spacy/lang/sv/tag_map.py,174,"att, om, innan, eftersom, medan",
spaCy/spacy/lang/sv/tag_map.py,175,"companionship, vice, versa, family, capita",
spaCy/spacy/lang/sv/tag_map.py,176,jfr,
spaCy/spacy/lang/sv/tag_map.py,177,"se, Diskutera, låt, Läs, Gå",
spaCy/spacy/lang/sv/tag_map.py,178,tas,
spaCy/spacy/lang/sv/tag_map.py,179,"vara, få, ha, bli, kunna",
spaCy/spacy/lang/sv/tag_map.py,180,"användas, finnas, göras, tas, ses",
spaCy/spacy/lang/sv/tag_map.py,181,"vare, Gånge",
spaCy/spacy/lang/sv/tag_map.py,182,"vore, finge",
spaCy/spacy/lang/sv/tag_map.py,184,"är, har, kan, får, måste",
spaCy/spacy/lang/sv/tag_map.py,185,"finns, kallas, behövs, beräknas, används",
spaCy/spacy/lang/sv/tag_map.py,186,"skulle, var, hade, kunde, fick",
spaCy/spacy/lang/sv/tag_map.py,187,"fanns, gjordes, höjdes, användes, infördes",
spaCy/spacy/lang/sv/tag_map.py,188,läs-,
spaCy/spacy/lang/sv/tag_map.py,189,"varit, fått, blivit, haft, kommit",
spaCy/spacy/lang/sv/tag_map.py,190,"nämnts, gjorts, förändrats, sagts, framhållits",
spaCy/spacy/lang/sv/__init__.py,1,coding: utf8,
spaCy/spacy/lang/sv/__init__.py,9,Punctuation stolen from Danish,
spaCy/spacy/lang/sv/examples.py,1,coding: utf8,
spaCy/spacy/lang/pt/norm_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/pt/norm_exceptions.py,4,These exceptions are used to add NORM values based on a token's ORTH value.,
spaCy/spacy/lang/pt/norm_exceptions.py,5,Individual languages can also add their own exceptions and overwrite them -,
spaCy/spacy/lang/pt/norm_exceptions.py,6,"for example, British vs. American spelling in English.",
spaCy/spacy/lang/pt/norm_exceptions.py,8,Norms are only set if no alternative is provided in the tokenizer exceptions.,
spaCy/spacy/lang/pt/norm_exceptions.py,9,Note that this does not change any other token attributes. Its main purpose,
spaCy/spacy/lang/pt/norm_exceptions.py,10,is to normalise the word representations so that equivalent tokens receive,
spaCy/spacy/lang/pt/norm_exceptions.py,11,"similar representations. For example: $ and € are very different, but they're",
spaCy/spacy/lang/pt/norm_exceptions.py,12,"both currency symbols. By normalising currency symbols to $, all symbols are",
spaCy/spacy/lang/pt/norm_exceptions.py,13,"seen as similar, no matter how common they are in the training data.",
spaCy/spacy/lang/pt/norm_exceptions.py,17,Real,
spaCy/spacy/lang/pt/norm_exceptions.py,18,Real,
spaCy/spacy/lang/pt/norm_exceptions.py,19,Cruzado,
spaCy/spacy/lang/pt/norm_exceptions.py,20,Cruzado,
spaCy/spacy/lang/pt/norm_exceptions.py,21,Cruzado Novo,
spaCy/spacy/lang/pt/norm_exceptions.py,22,Cruzado Novo,
spaCy/spacy/lang/pt/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/pt/tokenizer_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/pt/punctuation.py,1,coding: utf8,
spaCy/spacy/lang/pt/lex_attrs.py,1,coding: utf8,
spaCy/spacy/lang/pt/tag_map.py,1,coding: utf8,
spaCy/spacy/lang/pt/__init__.py,1,coding: utf8,
spaCy/spacy/lang/pt/examples.py,1,coding: utf8,
spaCy/spacy/lang/sl/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/sl/stop_words.py,5,Source: https://github.com/stopwords-iso/stopwords-sl,
spaCy/spacy/lang/sl/stop_words.py,6,TODO: probably needs to be tidied up – the list seems to have month names in,
spaCy/spacy/lang/sl/stop_words.py,7,"it, which shouldn't be considered stop words.",
spaCy/spacy/lang/sl/__init__.py,1,coding: utf8,
spaCy/spacy/lang/sr/norm_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/sr/norm_exceptions.py,6,Slang,
spaCy/spacy/lang/sr/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/sr/tokenizer_exceptions.py,1,encoding: utf8,
spaCy/spacy/lang/sr/tokenizer_exceptions.py,10,Weekdays abbreviations,
spaCy/spacy/lang/sr/tokenizer_exceptions.py,18,Months abbreviations,
spaCy/spacy/lang/sr/tokenizer_exceptions.py,43,common abbreviations,
spaCy/spacy/lang/sr/tokenizer_exceptions.py,45,without dot,
spaCy/spacy/lang/sr/tokenizer_exceptions.py,59,with dot,
spaCy/spacy/lang/sr/tokenizer_exceptions.py,84,with qoute,
spaCy/spacy/lang/sr/lex_attrs.py,1,coding: utf8,
spaCy/spacy/lang/sr/__init__.py,1,coding: utf8,
spaCy/spacy/lang/sr/examples.py,1,coding: utf8,
spaCy/spacy/lang/sr/examples.py,14,Translations from English,
spaCy/spacy/lang/sr/examples.py,20,Serbian common and slang,
spaCy/spacy/lang/id/norm_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/id/norm_exceptions.py,4,Daftar kosakata yang sering salah dieja,
spaCy/spacy/lang/id/norm_exceptions.py,5,https://id.wikipedia.org/wiki/Wikipedia:Daftar_kosakata_bahasa_Indonesia_yang_sering_salah_dieja,
spaCy/spacy/lang/id/norm_exceptions.py,7,Slang and abbreviations,
spaCy/spacy/lang/id/norm_exceptions.py,35,Daftar kosakata yang sering salah dieja,
spaCy/spacy/lang/id/syntax_iterators.py,1,coding: utf8,
spaCy/spacy/lang/id/syntax_iterators.py,21,Ensure works on both Doc and Span.,
spaCy/spacy/lang/id/syntax_iterators.py,29,Prevent nested chunks from being produced,
spaCy/spacy/lang/id/syntax_iterators.py,41,"If the head is an NP, and we're coordinated to it, we're an NP",
spaCy/spacy/lang/id/_tokenizer_exceptions_list.py,1,coding: utf8,
spaCy/spacy/lang/id/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/id/tokenizer_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/id/tokenizer_exceptions.py,7,Daftar singkatan dan Akronim dari:,
spaCy/spacy/lang/id/tokenizer_exceptions.py,8,https://id.wiktionary.org/wiki/Wiktionary:Daftar_singkatan_dan_akronim_bahasa_Indonesia#A,
spaCy/spacy/lang/id/punctuation.py,1,coding: utf8,
spaCy/spacy/lang/id/punctuation.py,31,hashtag,
spaCy/spacy/lang/id/punctuation.py,38,disabled: variable width currency variable,
spaCy/spacy/lang/id/punctuation.py,39,"r""(?<={c})(?:[0-9]+)"".format(c=CURRENCY),",
spaCy/spacy/lang/id/punctuation.py,42,disabled: variable width HTML_SUFFIX variable,
spaCy/spacy/lang/id/punctuation.py,43,"r""(?<=[0-9{a}]{h})(?:[\.,:-])"".format(a=ALPHA, h=HTML_SUFFIX),",
spaCy/spacy/lang/id/punctuation.py,51,disabled: variable width units variable,
spaCy/spacy/lang/id/punctuation.py,52,"r""(?<={u})[\/-](?=[0-9])"".format(u=UNITS),",
spaCy/spacy/lang/id/punctuation.py,53,disabled: variable width months variable,
spaCy/spacy/lang/id/punctuation.py,54,"r""(?<={m})[\/-](?=[0-9])"".format(m=MONTHS),",
spaCy/spacy/lang/id/lex_attrs.py,1,coding: utf8,
spaCy/spacy/lang/id/tag_map.py,1,coding: utf8,
spaCy/spacy/lang/id/tag_map.py,8,POS explanations for indonesian available from https://www.aclweb.org/anthology/Y12-1014,
spaCy/spacy/lang/id/__init__.py,1,coding: utf8,
spaCy/spacy/lang/id/examples.py,1,coding: utf8,
spaCy/spacy/lang/et/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/et/stop_words.py,5,Source: https://github.com/stopwords-iso/stopwords-et,
spaCy/spacy/lang/et/__init__.py,1,coding: utf8,
spaCy/spacy/lang/xx/__init__.py,1,coding: utf8,
spaCy/spacy/lang/xx/examples.py,1,coding: utf8,
spaCy/spacy/lang/xx/examples.py,12,combined examples from de/en/es/fr/it/nl/pl/pt/ru,
spaCy/spacy/lang/xx/examples.py,69,Translations from English:,
spaCy/spacy/lang/xx/examples.py,74,Native Russian sentences:,
spaCy/spacy/lang/xx/examples.py,75,Colloquial:,
spaCy/spacy/lang/xx/examples.py,76,Typical polite refusal,
spaCy/spacy/lang/xx/examples.py,77,From a tour guide speech,
spaCy/spacy/lang/xx/examples.py,78,Examples of Bookish Russian:,
spaCy/spacy/lang/xx/examples.py,79,"Quote from ""The Golden Calf""",
spaCy/spacy/lang/xx/examples.py,81,"Quotes from ""Ivan Vasilievich changes his occupation""",
spaCy/spacy/lang/xx/examples.py,84,Quotes from Dostoevsky:,
spaCy/spacy/lang/xx/examples.py,88,Quotes from Chekhov:,
spaCy/spacy/lang/xx/examples.py,90,Quotes from Turgenev:,
spaCy/spacy/lang/xx/examples.py,93,Quotes from newspapers:,
spaCy/spacy/lang/xx/examples.py,94,Komsomolskaya Pravda:,
spaCy/spacy/lang/xx/examples.py,97,Argumenty i Facty:,
spaCy/spacy/lang/ca/stop_words.py,1,encoding: utf8,
spaCy/spacy/lang/ca/tokenizer_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/ca/tokenizer_exceptions.py,31,Times,
spaCy/spacy/lang/ca/punctuation.py,1,coding: utf8,
spaCy/spacy/lang/ca/lex_attrs.py,1,coding: utf8,
spaCy/spacy/lang/ca/tag_map.py,1,coding: utf8,
spaCy/spacy/lang/ca/__init__.py,1,coding: utf8,
spaCy/spacy/lang/ca/examples.py,1,coding: utf8,
spaCy/spacy/lang/yo/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/yo/stop_words.py,4,stop words as whitespace-separated list.,
spaCy/spacy/lang/yo/stop_words.py,5,Source: https://raw.githubusercontent.com/dohliam/more-stoplists/master/yo/yo.txt,
spaCy/spacy/lang/yo/lex_attrs.py,1,coding: utf8,
spaCy/spacy/lang/yo/__init__.py,1,coding: utf8,
spaCy/spacy/lang/yo/examples.py,1,coding: utf8,
spaCy/spacy/lang/yo/examples.py,12,1. https://yo.wikipedia.org/wiki/Wikipedia:%C3%80y%E1%BB%8Dk%C3%A0_p%C3%A0t%C3%A0k%C3%AC,
spaCy/spacy/lang/yo/examples.py,13,2.https://yo.wikipedia.org/wiki/Oj%C3%BAew%C3%A9_%C3%80k%E1%BB%8D%CC%81k%E1%BB%8D%CC%81,
spaCy/spacy/lang/yo/examples.py,14,3. https://www.bbc.com/yoruba,
spaCy/spacy/lang/th/norm_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/th/norm_exceptions.py,6,Conjugation and Diversion invalid to Tonal form (ผันอักษรและเสียงไม่ตรงกับรูปวรรณยุกต์),
spaCy/spacy/lang/th/norm_exceptions.py,9,Misspelled because of being lazy or hustle (สะกดผิดเพราะขี้เกียจพิมพ์ หรือเร่งรีบ),
spaCy/spacy/lang/th/norm_exceptions.py,12,Strange (ให้ดูแปลกตา),
spaCy/spacy/lang/th/norm_exceptions.py,48,Misspelled to express emotions (คำที่สะกดผิดเพื่อแสดงอารมณ์),
spaCy/spacy/lang/th/norm_exceptions.py,72,Reduce rough words or Avoid to software filter (คำที่สะกดผิดเพื่อลดความหยาบของคำ หรืออาจใช้หลีกเลี่ยงการกรองคำหยาบของซอฟต์แวร์),
spaCy/spacy/lang/th/norm_exceptions.py,93,Imitate words (คำเลียนเสียง โดยส่วนใหญ่จะเพิ่มทัณฑฆาต หรือซ้ำตัวอักษร),
spaCy/spacy/lang/th/norm_exceptions.py,98,Acronym (แบบคำย่อ),
spaCy/spacy/lang/th/tokenizer_exceptions.py,1,encoding: utf8,
spaCy/spacy/lang/th/tokenizer_exceptions.py,8,หน่วยงานรัฐ / government agency,
spaCy/spacy/lang/th/tokenizer_exceptions.py,102,มหาวิทยาลัย / สถานศึกษา / university / college,
spaCy/spacy/lang/th/tokenizer_exceptions.py,110,ยศ / rank,
spaCy/spacy/lang/th/tokenizer_exceptions.py,229,วุฒิ / bachelor degree,
spaCy/spacy/lang/th/tokenizer_exceptions.py,303,ปี / เวลา / year / time,
spaCy/spacy/lang/th/tokenizer_exceptions.py,309,ระยะทาง / distance,
spaCy/spacy/lang/th/tokenizer_exceptions.py,316,น้ำหนัก / weight,
spaCy/spacy/lang/th/tokenizer_exceptions.py,325,ปริมาตร / volume,
spaCy/spacy/lang/th/tokenizer_exceptions.py,333,พื้นที่ / area,
spaCy/spacy/lang/th/tokenizer_exceptions.py,338,เดือน / month,
spaCy/spacy/lang/th/tokenizer_exceptions.py,351,เพศ / gender,
spaCy/spacy/lang/th/tokenizer_exceptions.py,356,ที่อยู่ / address,
spaCy/spacy/lang/th/tokenizer_exceptions.py,361,สรรพนาม / pronoun,
spaCy/spacy/lang/th/tokenizer_exceptions.py,366,การเมือง / politic,
spaCy/spacy/lang/th/tokenizer_exceptions.py,383,ทั่วไป / general,
spaCy/spacy/lang/th/lex_attrs.py,1,coding: utf8,
spaCy/spacy/lang/th/tag_map.py,1,encoding: utf8,
spaCy/spacy/lang/th/tag_map.py,7,Source: Korakot Chaovavanich,
spaCy/spacy/lang/th/tag_map.py,8,https://www.facebook.com/photo.php?fbid=390564854695031&set=p.390564854695031&type=3&permPage=1&ifg=1,
spaCy/spacy/lang/th/tag_map.py,10,NOUN,
spaCy/spacy/lang/th/tag_map.py,19,VERB,
spaCy/spacy/lang/th/tag_map.py,22,PRON,
spaCy/spacy/lang/th/tag_map.py,25,ADJ,
spaCy/spacy/lang/th/tag_map.py,30,ADV,
spaCy/spacy/lang/th/tag_map.py,36,INT,
spaCy/spacy/lang/th/tag_map.py,38,PRON,
spaCy/spacy/lang/th/tag_map.py,43,DET,
spaCy/spacy/lang/th/tag_map.py,52,TODO: resolve duplicate (see below),
spaCy/spacy/lang/th/tag_map.py,53,"""DCNM"": {POS: DET},",
spaCy/spacy/lang/th/tag_map.py,54,NUM,
spaCy/spacy/lang/th/tag_map.py,59,AUX,
spaCy/spacy/lang/th/tag_map.py,66,ADP,
spaCy/spacy/lang/th/tag_map.py,69,CCONJ,
spaCy/spacy/lang/th/tag_map.py,72,SCONJ,
spaCy/spacy/lang/th/tag_map.py,77,PART,
spaCy/spacy/lang/th/tag_map.py,85,PUNCT,
spaCy/spacy/lang/th/__init__.py,1,coding: utf8,
spaCy/spacy/lang/en/norm_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/en/norm_exceptions.py,6,Slang and abbreviations,
spaCy/spacy/lang/en/norm_exceptions.py,15,US vs. UK spelling,
spaCy/spacy/lang/en/morph_rules.py,1,coding: utf8,
spaCy/spacy/lang/en/morph_rules.py,6,Several entries here look pretty suspicious. These will get the POS SCONJ,
spaCy/spacy/lang/en/morph_rules.py,7,"given the tag IN, when an adpositional reading seems much more likely for",
spaCy/spacy/lang/en/morph_rules.py,8,a lot of these prepositions. I'm not sure what I was running in 04395ffa4,
spaCy/spacy/lang/en/morph_rules.py,9,when I did this? It doesn't seem right.,
spaCy/spacy/lang/en/morph_rules.py,15,"""of"",",
spaCy/spacy/lang/en/morph_rules.py,16,"""for"",",
spaCy/spacy/lang/en/morph_rules.py,17,"""before"",",
spaCy/spacy/lang/en/morph_rules.py,18,"""in"",",
spaCy/spacy/lang/en/morph_rules.py,20,"""after"",",
spaCy/spacy/lang/en/morph_rules.py,23,"""with"",",
spaCy/spacy/lang/en/morph_rules.py,25,"""to"",",
spaCy/spacy/lang/en/morph_rules.py,26,"""by"",",
spaCy/spacy/lang/en/morph_rules.py,27,"""on"",",
spaCy/spacy/lang/en/morph_rules.py,28,"""about"",",
spaCy/spacy/lang/en/morph_rules.py,32,"""from"",",
spaCy/spacy/lang/en/morph_rules.py,34,"""until"",",
spaCy/spacy/lang/en/morph_rules.py,37,"""without"",",
spaCy/spacy/lang/en/morph_rules.py,38,"""at"",",
spaCy/spacy/lang/en/morph_rules.py,39,"""into"",",
spaCy/spacy/lang/en/morph_rules.py,41,"""over"",",
spaCy/spacy/lang/en/morph_rules.py,45,"""beyond"",",
spaCy/spacy/lang/en/morph_rules.py,50,"""then"",",
spaCy/spacy/lang/en/morph_rules.py,54,"""below"",",
spaCy/spacy/lang/en/morph_rules.py,55,"""against"",",
spaCy/spacy/lang/en/morph_rules.py,58,"""toward"",",
spaCy/spacy/lang/en/morph_rules.py,70,"""towards"",",
spaCy/spacy/lang/en/morph_rules.py,76,This seems kind of wrong too?,
spaCy/spacy/lang/en/morph_rules.py,77,"_relative_pronouns = [""this"", ""that"", ""those"", ""these""]",
spaCy/spacy/lang/en/morph_rules.py,80,"""DT"": {word: {""POS"": ""PRON""} for word in _relative_pronouns},",
spaCy/spacy/lang/en/syntax_iterators.py,1,coding: utf8,
spaCy/spacy/lang/en/syntax_iterators.py,22,Ensure works on both Doc and Span.,
spaCy/spacy/lang/en/syntax_iterators.py,30,Prevent nested chunks from being produced,
spaCy/spacy/lang/en/syntax_iterators.py,42,"If the head is an NP, and we're coordinated to it, we're an NP",
spaCy/spacy/lang/en/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/en/stop_words.py,5,Stop words,
spaCy/spacy/lang/en/tokenizer_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/en/tokenizer_exceptions.py,28,Pronouns,
spaCy/spacy/lang/en/tokenizer_exceptions.py,140,"W-words, relative pronouns, prepositions etc.",
spaCy/spacy/lang/en/tokenizer_exceptions.py,213,Verbs,
spaCy/spacy/lang/en/tokenizer_exceptions.py,297,Other contractions with trailing apostrophe,
spaCy/spacy/lang/en/tokenizer_exceptions.py,316,Other contractions with leading apostrophe,
spaCy/spacy/lang/en/tokenizer_exceptions.py,330,Times,
spaCy/spacy/lang/en/tokenizer_exceptions.py,345,Rest,
spaCy/spacy/lang/en/lex_attrs.py,1,coding: utf8,
spaCy/spacy/lang/en/tag_map.py,1,coding: utf8,
spaCy/spacy/lang/en/__init__.py,1,coding: utf8,
spaCy/spacy/lang/en/examples.py,1,coding: utf8,
spaCy/spacy/lang/fr/syntax_iterators.py,1,coding: utf8,
spaCy/spacy/lang/fr/syntax_iterators.py,21,Ensure works on both Doc and Span.,
spaCy/spacy/lang/fr/syntax_iterators.py,29,Prevent nested chunks from being produced,
spaCy/spacy/lang/fr/syntax_iterators.py,41,"If the head is an NP, and we're coordinated to it, we're an NP",
spaCy/spacy/lang/fr/_tokenizer_exceptions_list.py,1,coding: utf8,
spaCy/spacy/lang/fr/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/fr/tokenizer_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/fr/tokenizer_exceptions.py,11,not using the large _tokenizer_exceptions_list by default as it slows down the tokenizer,
spaCy/spacy/lang/fr/tokenizer_exceptions.py,12,from ._tokenizer_exceptions_list import FR_BASE_EXCEPTIONS,
spaCy/spacy/lang/fr/tokenizer_exceptions.py,96,", TAG: ""VERB""},",
spaCy/spacy/lang/fr/tokenizer_exceptions.py,105,", TAG: ""VERB""},",
spaCy/spacy/lang/fr/tokenizer_exceptions.py,140,"loop through the elison and hyphen characters, and try to substitute the ones that weren't used in the original list",
spaCy/spacy/lang/fr/tokenizer_exceptions.py,416,catching cases like faux-vampire,
spaCy/spacy/lang/fr/tokenizer_exceptions.py,420,putting the - first in the [] range avoids having to use a backslash,
spaCy/spacy/lang/fr/tokenizer_exceptions.py,427,catching cases like entr'abat,
spaCy/spacy/lang/fr/tokenizer_exceptions.py,436,"catching cases like saut-de-ski, pet-en-l'air",
spaCy/spacy/lang/fr/tokenizer_exceptions.py,458,URLs,
spaCy/spacy/lang/fr/punctuation.py,1,coding: utf8,
spaCy/spacy/lang/fr/punctuation.py,30,"°C. -> [""°C"", "".""]",
spaCy/spacy/lang/fr/punctuation.py,31,"4% -> [""4"", ""%""]",
spaCy/spacy/lang/fr/lemmatizer.py,1,coding: utf8,
spaCy/spacy/lang/fr/lemmatizer.py,48,See Issue #435 for example of where this logic is requied.,
spaCy/spacy/lang/fr/lemmatizer.py,77,This maps 'VBP' to base form -- probably just need 'IS_BASE',
spaCy/spacy/lang/fr/lemmatizer.py,78,morphology,
spaCy/spacy/lang/fr/lex_attrs.py,1,coding: utf8,
spaCy/spacy/lang/fr/lex_attrs.py,29,Might require more work?,
spaCy/spacy/lang/fr/lex_attrs.py,30,See this discussion: https://github.com/explosion/spaCy/pull/1161,
spaCy/spacy/lang/fr/tag_map.py,1,coding: utf8,
spaCy/spacy/lang/fr/__init__.py,1,coding: utf8,
spaCy/spacy/lang/fr/examples.py,1,coding: utf8,
spaCy/spacy/lang/el/norm_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/el/norm_exceptions.py,5,These exceptions are used to add NORM values based on a token's ORTH value.,
spaCy/spacy/lang/el/norm_exceptions.py,6,Norms are only set if no alternative is provided in the tokenizer exceptions.,
spaCy/spacy/lang/el/syntax_iterators.py,1,coding: utf8,
spaCy/spacy/lang/el/syntax_iterators.py,11,"It follows the logic of the noun chunks finder of English language,",
spaCy/spacy/lang/el/syntax_iterators.py,12,adjusted to some Greek language special characteristics.,
spaCy/spacy/lang/el/syntax_iterators.py,13,obj tag corrects some DEP tagger mistakes.,
spaCy/spacy/lang/el/syntax_iterators.py,14,Further improvement of the models will eliminate the need for this tag.,
spaCy/spacy/lang/el/syntax_iterators.py,16,Ensure works on both Doc and Span.,
spaCy/spacy/lang/el/syntax_iterators.py,25,Prevent nested chunks from being produced,
spaCy/spacy/lang/el/syntax_iterators.py,33,check for patterns such as γραμμή παραγωγής,
spaCy/spacy/lang/el/syntax_iterators.py,46,covers the case: έχει όμορφα και έξυπνα παιδιά,
spaCy/spacy/lang/el/syntax_iterators.py,50,"If the head is an NP, and we're coordinated to it, we're an NP",
spaCy/spacy/lang/el/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/el/stop_words.py,5,Stop words,
spaCy/spacy/lang/el/stop_words.py,6,Link to greek stop words: https://www.translatum.gr/forum/index.php?topic=3550.0?topic=3550.0,
spaCy/spacy/lang/el/tokenizer_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/el/punctuation.py,1,-*- coding: utf-8 -*-,
spaCy/spacy/lang/el/punctuation.py,29,90%,
spaCy/spacy/lang/el/punctuation.py,30,'12'-13,
spaCy/spacy/lang/el/punctuation.py,31,-12.13,
spaCy/spacy/lang/el/punctuation.py,32,'αβγ',
spaCy/spacy/lang/el/punctuation.py,33,αβγ',
spaCy/spacy/lang/el/punctuation.py,35,όνομα*,
spaCy/spacy/lang/el/punctuation.py,51,12+,
spaCy/spacy/lang/el/punctuation.py,52,12',
spaCy/spacy/lang/el/punctuation.py,53,a',
spaCy/spacy/lang/el/punctuation.py,54,12.,
spaCy/spacy/lang/el/punctuation.py,55,12.,
spaCy/spacy/lang/el/punctuation.py,56,12),
spaCy/spacy/lang/el/punctuation.py,57,12),
spaCy/spacy/lang/el/punctuation.py,59,12&,
spaCy/spacy/lang/el/punctuation.py,66,όνομα-,
spaCy/spacy/lang/el/punctuation.py,70,"πρώτος-δεύτερος , πρώτος-δεύτερος-τρίτος",
spaCy/spacy/lang/el/punctuation.py,72,13mg,
spaCy/spacy/lang/el/punctuation.py,73,1.2m,
spaCy/spacy/lang/el/punctuation.py,81,"1/2 , 1-2 , 1*2",
spaCy/spacy/lang/el/punctuation.py,82,name1/name2/name3,
spaCy/spacy/lang/el/punctuation.py,83,"10.9 , 10.9.9 , 10.9-6",
spaCy/spacy/lang/el/punctuation.py,84,"10,11,12",
spaCy/spacy/lang/el/punctuation.py,85,1ης-2,
spaCy/spacy/lang/el/punctuation.py,86,"15/2 , 15/2/17 , 2017/2/15",
spaCy/spacy/lang/el/punctuation.py,88,abc@cde-fgh.a,
spaCy/spacy/lang/el/punctuation.py,89,abc-abc,
spaCy/spacy/lang/el/lemmatizer.py,1,coding: utf8,
spaCy/spacy/lang/el/lex_attrs.py,1,-*- coding: utf-8 -*-,
spaCy/spacy/lang/el/tag_map_fine.py,1,coding: utf8,
spaCy/spacy/lang/el/get_pos_from_wiktionary.py,1,coding: utf8,
spaCy/spacy/lang/el/get_pos_from_wiktionary.py,12,get words based on the Wiktionary dump,
spaCy/spacy/lang/el/get_pos_from_wiktionary.py,13,check only for specific parts,
spaCy/spacy/lang/el/get_pos_from_wiktionary.py,15,==={{κύριο όνομα|el}}===,
spaCy/spacy/lang/el/__init__.py,1,-*- coding: utf-8 -*-,
spaCy/spacy/lang/el/examples.py,1,-*- coding: utf-8 -*-,
spaCy/spacy/lang/ja/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/ja/stop_words.py,4,This list was created by taking the top 2000 words from a Wikipedia dump and,
spaCy/spacy/lang/ja/stop_words.py,5,filtering out everything that wasn't hiragana. ー (one) was also added.,
spaCy/spacy/lang/ja/stop_words.py,6,Considered keeping some non-hiragana words but too many place names were,
spaCy/spacy/lang/ja/stop_words.py,7,present.,
spaCy/spacy/lang/ja/tag_map.py,1,encoding: utf8,
spaCy/spacy/lang/ja/tag_map.py,9,Explanation of Unidic tags:,
spaCy/spacy/lang/ja/tag_map.py,10,https://www.gavo.t.u-tokyo.ac.jp/~mine/japanese/nlp+slp/UNIDIC_manual.pdf,
spaCy/spacy/lang/ja/tag_map.py,11,Universal Dependencies Mapping:,
spaCy/spacy/lang/ja/tag_map.py,12,http://universaldependencies.org/ja/overview/morphology.html,
spaCy/spacy/lang/ja/tag_map.py,13,http://universaldependencies.org/ja/pos/all.html,
spaCy/spacy/lang/ja/tag_map.py,16,this includes characters used to represent sounds like ドレミ,
spaCy/spacy/lang/ja/tag_map.py,19,"this is for Greek and Latin characters used as sumbols, as in math",
spaCy/spacy/lang/ja/tag_map.py,22,this is specifically for unicode full-width space,
spaCy/spacy/lang/ja/tag_map.py,24,This is used when sequential half-width spaces are present,
spaCy/spacy/lang/ja/tag_map.py,30,"XXX ADJ if alone, AUX otherwise",
spaCy/spacy/lang/ja/tag_map.py,34,の as in 走るのが速い,
spaCy/spacy/lang/ja/tag_map.py,35,verb ending て,
spaCy/spacy/lang/ja/tag_map.py,36,"ばかり, つつ after a verb",
spaCy/spacy/lang/ja/tag_map.py,38,XXX: might need refinement,
spaCy/spacy/lang/ja/tag_map.py,40,"がち, チック",
spaCy/spacy/lang/ja/tag_map.py,41,-らしい,
spaCy/spacy/lang/ja/tag_map.py,42,-じみ,
spaCy/spacy/lang/ja/tag_map.py,43,"XXX see 名詞,普通名詞,サ変可能,*",
spaCy/spacy/lang/ja/tag_map.py,46,"-後, -過ぎ",
spaCy/spacy/lang/ja/tag_map.py,49,"XXX VERB if alone, AUX otherwise",
spaCy/spacy/lang/ja/tag_map.py,53,text art,
spaCy/spacy/lang/ja/tag_map.py,54,kaomoji,
spaCy/spacy/lang/ja/tag_map.py,56,open bracket,
spaCy/spacy/lang/ja/tag_map.py,57,close bracket,
spaCy/spacy/lang/ja/tag_map.py,58,period or other EOS marker,
spaCy/spacy/lang/ja/tag_map.py,59,comma,
spaCy/spacy/lang/ja/tag_map.py,60,general proper noun,
spaCy/spacy/lang/ja/tag_map.py,61,person's name,
spaCy/spacy/lang/ja/tag_map.py,62,surname,
spaCy/spacy/lang/ja/tag_map.py,63,first name,
spaCy/spacy/lang/ja/tag_map.py,64,place name,
spaCy/spacy/lang/ja/tag_map.py,65,country name,
spaCy/spacy/lang/ja/tag_map.py,67,includes Chinese numerals,
spaCy/spacy/lang/ja/tag_map.py,68,XXX: sometimes VERB in UDv2; suru-verb noun,
spaCy/spacy/lang/ja/tag_map.py,71,ex: 下手,
spaCy/spacy/lang/ja/tag_map.py,73,XXX: sometimes ADJ in UDv2,
spaCy/spacy/lang/ja/tag_map.py,76,counter / unit,
spaCy/spacy/lang/ja/tag_map.py,78,XXX this has exceptions based on literal token,
spaCy/spacy/lang/ja/__init__.py,1,encoding: utf8,
spaCy/spacy/lang/ja/__init__.py,15,"Handling for multiple spaces in a row is somewhat awkward, this simplifies",
spaCy/spacy/lang/ja/__init__.py,16,the flow by creating a dummy with the same interface.,
spaCy/spacy/lang/ja/__init__.py,43,this is only used for consecutive ascii spaces,
spaCy/spacy/lang/ja/__init__.py,47,TODO: This is a first take. The rules here are crude approximations.,
spaCy/spacy/lang/ja/__init__.py,48,"For many of these, full dependencies are needed to properly resolve",
spaCy/spacy/lang/ja/__init__.py,49,PoS mappings.,
spaCy/spacy/lang/ja/__init__.py,71,"If there's more than one space, spaces after the first become tokens",
spaCy/spacy/lang/ja/__init__.py,85,see #2901,
spaCy/spacy/lang/ja/__init__.py,96,if there's no lemma info (it's an unk) just use the surface,
spaCy/spacy/lang/ja/examples.py,1,coding: utf8,
spaCy/spacy/lang/ko/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/ko/lex_attrs.py,1,coding: utf8,
spaCy/spacy/lang/ko/lex_attrs.py,10,Native Korean number system,
spaCy/spacy/lang/ko/lex_attrs.py,29,Sino-Korean number system,
spaCy/spacy/lang/ko/tag_map.py,1,encoding: utf8,
spaCy/spacy/lang/ko/tag_map.py,7,은전한닢(mecab-ko-dic)의 품사 태그를 universal pos tag로 대응시킴,
spaCy/spacy/lang/ko/tag_map.py,8,https://docs.google.com/spreadsheets/d/1-9blXKjtjeKZqsf4NzHeYJCrr49-nXeRF6D80udfcwY/edit#gid=589544265,
spaCy/spacy/lang/ko/tag_map.py,9,https://universaldependencies.org/u/pos/,
spaCy/spacy/lang/ko/tag_map.py,11,"J.{1,2} 조사",
spaCy/spacy/lang/ko/tag_map.py,19,보조사,
spaCy/spacy/lang/ko/tag_map.py,20,접속 조사,
spaCy/spacy/lang/ko/tag_map.py,21,접속 부사,
spaCy/spacy/lang/ko/tag_map.py,22,일반 부사,
spaCy/spacy/lang/ko/tag_map.py,23,관형사,
spaCy/spacy/lang/ko/tag_map.py,24,접두사,
spaCy/spacy/lang/ko/tag_map.py,25,XS. 접미사,
spaCy/spacy/lang/ko/tag_map.py,29,어근,
spaCy/spacy/lang/ko/tag_map.py,30,"E.{1,2} 어미",
spaCy/spacy/lang/ko/tag_map.py,36,감탄사,
spaCy/spacy/lang/ko/tag_map.py,37,동사,
spaCy/spacy/lang/ko/tag_map.py,38,형용사,
spaCy/spacy/lang/ko/tag_map.py,39,보조 용언,
spaCy/spacy/lang/ko/tag_map.py,40,긍정 지정사(이다),
spaCy/spacy/lang/ko/tag_map.py,41,부정 지정사(아니다),
spaCy/spacy/lang/ko/tag_map.py,42,일반 명사(general noun),
spaCy/spacy/lang/ko/tag_map.py,43,의존 명사,
spaCy/spacy/lang/ko/tag_map.py,44,의존 명사(단위: unit),
spaCy/spacy/lang/ko/tag_map.py,45,고유 명사(proper noun),
spaCy/spacy/lang/ko/tag_map.py,46,대명사,
spaCy/spacy/lang/ko/tag_map.py,47,수사(numerals),
spaCy/spacy/lang/ko/tag_map.py,48,숫자,
spaCy/spacy/lang/ko/tag_map.py,49,"S.{1,2} 부호",
spaCy/spacy/lang/ko/tag_map.py,50,문장 부호,
spaCy/spacy/lang/ko/tag_map.py,51,period or other EOS marker,
spaCy/spacy/lang/ko/tag_map.py,53,"comma, etc.",
spaCy/spacy/lang/ko/tag_map.py,54,open bracket,
spaCy/spacy/lang/ko/tag_map.py,55,close bracket,
spaCy/spacy/lang/ko/tag_map.py,56,기타 기호,
spaCy/spacy/lang/ko/tag_map.py,57,외국어,
spaCy/spacy/lang/ko/tag_map.py,58,한자,
spaCy/spacy/lang/ko/__init__.py,1,encoding: utf8,
spaCy/spacy/lang/ko/__init__.py,26,fmt: on,
spaCy/spacy/lang/ko/__init__.py,53,stem(어간) or pre-final(선어말 어미),
spaCy/spacy/lang/ko/__init__.py,59,"품사 태그(POS)[0], 의미 부류(semantic class)[1],	종성 유무(jongseong)[2], 읽기(reading)[3],",
spaCy/spacy/lang/ko/__init__.py,60,"타입(type)[4], 첫번째 품사(start pos)[5],	마지막 품사(end pos)[6], 표현(expression)[7], *",
spaCy/spacy/lang/ko/examples.py,1,coding: utf8,
spaCy/spacy/lang/hi/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/hi/stop_words.py,5,"Source: https://github.com/taranjeet/hindi-tokenizer/blob/master/stopwords.txt, https://data.mendeley.com/datasets/bsr3frvvjc/1#file-a21d5092-99d7-45d8-b044-3ae9edd391c6",
spaCy/spacy/lang/hi/lex_attrs.py,1,coding: utf8,
spaCy/spacy/lang/hi/lex_attrs.py,8,fmt: off,
spaCy/spacy/lang/hi/lex_attrs.py,16,fmt: on,
spaCy/spacy/lang/hi/lex_attrs.py,18,reference 1:https://en.wikipedia.org/wiki/Indian_numbering_system,
spaCy/spacy/lang/hi/lex_attrs.py,19,reference 2: https://blogs.transparent.com/hindi/hindi-numbers-1-100/,
spaCy/spacy/lang/hi/lex_attrs.py,60,"normalise base exceptions,  e.g. punctuation or currency symbols",
spaCy/spacy/lang/hi/lex_attrs.py,63,"set stem word as norm,  if available,  adapted from:",
spaCy/spacy/lang/hi/lex_attrs.py,64,http://computing.open.ac.uk/Sites/EACLSouthAsia/Papers/p6-Ramanathan.pdf,
spaCy/spacy/lang/hi/lex_attrs.py,65,http://research.variancia.com/hindi_stemmer/,
spaCy/spacy/lang/hi/lex_attrs.py,66,https://github.com/taranjeet/hindi-tokenizer/blob/master/HindiTokenizer.py#L142,
spaCy/spacy/lang/hi/__init__.py,1,coding: utf8,
spaCy/spacy/lang/hi/examples.py,1,coding: utf8,
spaCy/spacy/lang/he/stop_words.py,1,encoding: utf8,
spaCy/spacy/lang/he/__init__.py,1,coding: utf8,
spaCy/spacy/lang/he/examples.py,1,coding: utf8,
spaCy/spacy/lang/eu/stop_words.py,1,encoding: utf8,
spaCy/spacy/lang/eu/stop_words.py,4,Source: https://github.com/stopwords-iso/stopwords-eu,
spaCy/spacy/lang/eu/stop_words.py,5,https://www.ranks.nl/stopwords/basque,
spaCy/spacy/lang/eu/stop_words.py,6,https://www.mustgo.com/worldlanguages/basque/,
spaCy/spacy/lang/eu/punctuation.py,1,coding: utf8,
spaCy/spacy/lang/eu/lex_attrs.py,1,coding: utf8,
spaCy/spacy/lang/eu/lex_attrs.py,6,Source http://mylanguages.org/basque_numbers.php,
spaCy/spacy/lang/eu/lex_attrs.py,35,source https://www.google.com/intl/ur/inputtools/try/,
spaCy/spacy/lang/eu/tag_map.py,1,coding: utf8,
spaCy/spacy/lang/eu/__init__.py,1,coding: utf8,
spaCy/spacy/lang/eu/examples.py,1,coding: utf8,
spaCy/spacy/lang/nl/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/nl/stop_words.py,4,The original stop words list (added in f46ffe3) was taken from,
spaCy/spacy/lang/nl/stop_words.py,5,http://www.damienvanholten.com/downloads/dutch-stop-words.txt,
spaCy/spacy/lang/nl/stop_words.py,6,and consisted of about 100 tokens.,
spaCy/spacy/lang/nl/stop_words.py,7,In order to achieve parity with some of the better-supported,
spaCy/spacy/lang/nl/stop_words.py,8,"languages, e.g., English, French, and German, this original list has been",
spaCy/spacy/lang/nl/stop_words.py,9,extended with 200 additional tokens. The main source of inspiration was,
spaCy/spacy/lang/nl/stop_words.py,10,https://raw.githubusercontent.com/stopwords-iso/stopwords-nl/master/stopwords-nl.txt.,
spaCy/spacy/lang/nl/stop_words.py,11,"However, quite a bit of manual editing has taken place as well.",
spaCy/spacy/lang/nl/stop_words.py,12,Tokens whose status as a stop word is not entirely clear were admitted or,
spaCy/spacy/lang/nl/stop_words.py,13,rejected by deferring to their counterparts in the stop words lists for English,
spaCy/spacy/lang/nl/stop_words.py,14,"and French. Similarly, those lists were used to identify and fill in gaps so",
spaCy/spacy/lang/nl/stop_words.py,15,that -- in principle -- each token contained in the English stop words list,
spaCy/spacy/lang/nl/stop_words.py,16,should have a Dutch counterpart here.,
spaCy/spacy/lang/nl/tokenizer_exceptions.py,1,coding: utf8,
spaCy/spacy/lang/nl/tokenizer_exceptions.py,6,Extensive list of both common and uncommon dutch abbreviations copied from,
spaCy/spacy/lang/nl/tokenizer_exceptions.py,7,"github.com/diasks2/pragmatic_segmenter, a Ruby library for rule-based",
spaCy/spacy/lang/nl/tokenizer_exceptions.py,8,"sentence boundary detection (MIT, Copyright 2015 Kevin S. Dias).",
spaCy/spacy/lang/nl/tokenizer_exceptions.py,9,Source file: https://github.com/diasks2/pragmatic_segmenter/blob/master/lib/pragmatic_segmenter/languages/dutch.rb,
spaCy/spacy/lang/nl/tokenizer_exceptions.py,10,(Last commit: 4d1477b),
spaCy/spacy/lang/nl/tokenizer_exceptions.py,12,Main purpose of such an extensive list: considerably improved sentence,
spaCy/spacy/lang/nl/tokenizer_exceptions.py,13,segmentation.,
spaCy/spacy/lang/nl/tokenizer_exceptions.py,15,Note: This list has been copied over largely as-is. Some of the abbreviations,
spaCy/spacy/lang/nl/tokenizer_exceptions.py,16,are extremely domain-specific. Tokenizer performance may benefit from some,
spaCy/spacy/lang/nl/tokenizer_exceptions.py,17,"slight pruning, although no performance regression has been observed so far.",
spaCy/spacy/lang/nl/punctuation.py,1,coding: utf8,
spaCy/spacy/lang/nl/punctuation.py,14,Copied from `de` package. Main purpose is to ensure that hyphens are not,
spaCy/spacy/lang/nl/punctuation.py,15,split on.,
spaCy/spacy/lang/nl/lemmatizer.py,1,coding: utf8,
spaCy/spacy/lang/nl/lemmatizer.py,9,"Note: CGN does not distinguish AUX verbs, so we treat AUX as VERB.",
spaCy/spacy/lang/nl/lemmatizer.py,41,"Difference 1: self.rules is assumed to be non-None, so no",
spaCy/spacy/lang/nl/lemmatizer.py,42,'is None' check required.,
spaCy/spacy/lang/nl/lemmatizer.py,43,String lowercased from the get-go. All lemmatization results in,
spaCy/spacy/lang/nl/lemmatizer.py,44,"lowercased strings. For most applications, this shouldn't pose",
spaCy/spacy/lang/nl/lemmatizer.py,45,"any problems, and it keeps the exceptions indexes small. If this",
spaCy/spacy/lang/nl/lemmatizer.py,46,"creates problems for proper nouns, we can introduce a check for",
spaCy/spacy/lang/nl/lemmatizer.py,47,"univ_pos == ""PROPN"".",
spaCy/spacy/lang/nl/lemmatizer.py,52,"Because PROPN not in self.univ_pos_name_variants, proper names",
spaCy/spacy/lang/nl/lemmatizer.py,53,"are not lemmatized. They are lowercased, however.",
spaCy/spacy/lang/nl/lemmatizer.py,55,if string in self.lemma_index.get(univ_pos),
spaCy/spacy/lang/nl/lemmatizer.py,58,string is already lemma,
spaCy/spacy/lang/nl/lemmatizer.py,63,string is irregular token contained in exceptions index.,
spaCy/spacy/lang/nl/lemmatizer.py,69,string corresponds to key in lookup table,
spaCy/spacy/lang/nl/lemmatizer.py,78,Back-off through remaining return value candidates.,
spaCy/spacy/lang/nl/lemmatizer.py,95,Overrides parent method so that a lowercased version of the string is,
spaCy/spacy/lang/nl/lemmatizer.py,96,used to search the lookup table. This is necessary because our lookup,
spaCy/spacy/lang/nl/lemmatizer.py,97,table consists entirely of lowercase keys.,
spaCy/spacy/lang/nl/lemmatizer.py,106,Reimplemented to focus more on application of suffix rules and to return,
spaCy/spacy/lang/nl/lemmatizer.py,107,as early as possible.,
spaCy/spacy/lang/nl/lemmatizer.py,109,"returns (forms, is_known: bool)",
spaCy/spacy/lang/nl/lemmatizer.py,117,True = Is known (is lemma),
spaCy/spacy/lang/nl/lex_attrs.py,1,coding: utf8,
spaCy/spacy/lang/nl/lex_attrs.py,26,This only does the most basic check for whether a token is a digit,
spaCy/spacy/lang/nl/lex_attrs.py,27,or matches one of the number words. In order to handle numbers like,
spaCy/spacy/lang/nl/lex_attrs.py,28,"""drieëntwintig"", more work is required.",
spaCy/spacy/lang/nl/lex_attrs.py,29,See this discussion: https://github.com/explosion/spaCy/pull/1177,
spaCy/spacy/lang/nl/tag_map.py,1,coding: utf8,
spaCy/spacy/lang/nl/__init__.py,1,coding: utf8,
spaCy/spacy/lang/nl/examples.py,1,coding: utf8,
spaCy/spacy/lang/ur/stop_words.py,1,encoding: utf8,
spaCy/spacy/lang/ur/stop_words.py,4,Source: collected from different resource on internet,
spaCy/spacy/lang/ur/punctuation.py,1,coding: utf8,
spaCy/spacy/lang/ur/lex_attrs.py,1,coding: utf8,
spaCy/spacy/lang/ur/lex_attrs.py,6,Source https://quizlet.com/4271889/1-100-urdu-number-wordsurdu-numerals-flash-cards/,
spaCy/spacy/lang/ur/lex_attrs.py,7,http://www.urduword.com/lessons.php?lesson=numbers,
spaCy/spacy/lang/ur/lex_attrs.py,8,https://en.wikibooks.org/wiki/Urdu/Vocabulary/Numbers,
spaCy/spacy/lang/ur/lex_attrs.py,9,https://www.urdu-english.com/lessons/beginner/numbers,
spaCy/spacy/lang/ur/lex_attrs.py,24,source https://www.google.com/intl/ur/inputtools/try/,
spaCy/spacy/lang/ur/tag_map.py,1,coding: utf8,
spaCy/spacy/lang/ur/__init__.py,1,coding: utf8,
spaCy/spacy/lang/ur/examples.py,1,coding: utf8,
spaCy/spacy/lang/bn/morph_rules.py,1,coding: utf8,
spaCy/spacy/lang/bn/stop_words.py,1,coding: utf8,
spaCy/spacy/lang/bn/tokenizer_exceptions.py,1,coding=utf-8,
spaCy/spacy/lang/bn/punctuation.py,1,coding: utf8,
spaCy/spacy/lang/bn/tag_map.py,1,coding: utf8,
spaCy/spacy/lang/bn/__init__.py,1,coding: utf8,
spaCy/spacy/lang/bn/examples.py,1,coding: utf8,
spaCy/examples/deep_learning_keras.py,67,Sentiment has a native slot for a single float.,
spaCy/examples/deep_learning_keras.py,68,"For arbitrary data storage, there's:",
spaCy/examples/deep_learning_keras.py,69,doc.user_data['my_data'] = y,
spaCy/examples/deep_learning_keras.py,195,Unzips into two lists,
spaCy/examples/deep_learning_keras.py,217,Shape,
spaCy/examples/deep_learning_keras.py,219,General NN config,
spaCy/examples/deep_learning_keras.py,223,Training params,
spaCy/examples/vectors_tensorboard.py,1,!/usr/bin/env python,
spaCy/examples/vectors_tensorboard.py,2,coding: utf8,
spaCy/examples/vectors_tensorboard.py,57,Store vector data in a tensorflow variable,
spaCy/examples/vectors_tensorboard.py,60,Write a tab-separated file that contains information about the vectors for visualization,
spaCy/examples/vectors_tensorboard.py,61,,
spaCy/examples/vectors_tensorboard.py,62,Reference: https://www.tensorflow.org/programmers_guide/embedding#metadata,
spaCy/examples/vectors_tensorboard.py,64,Define columns in the first row,
spaCy/examples/vectors_tensorboard.py,66,Write out a row for each vector that we add to the tensorflow variable we created,
spaCy/examples/vectors_tensorboard.py,69,https://github.com/tensorflow/tensorflow/issues/9094,
spaCy/examples/vectors_tensorboard.py,73,Store vector data and metadata,
spaCy/examples/vectors_tensorboard.py,89,Link the embeddings into the config,
spaCy/examples/vectors_tensorboard.py,95,Tell the projector about the configured embeddings and metadata file,
spaCy/examples/vectors_tensorboard.py,98,Save session and print run command to the output,
spaCy/examples/vectors_fast_text.py,1,!/usr/bin/env python,
spaCy/examples/vectors_fast_text.py,2,coding: utf8,
spaCy/examples/vectors_fast_text.py,28,create empty language class – this is required if you're planning to,
spaCy/examples/vectors_fast_text.py,29,save the model to disk and load it back later (models always need a,
spaCy/examples/vectors_fast_text.py,30,"""lang"" setting). Use 'xx' for blank multi-language class.",
spaCy/examples/vectors_fast_text.py,41,add the vectors to the vocab,
spaCy/examples/vectors_fast_text.py,42,test the vectors and similarity,
spaCy/examples/load_from_docbin.py,1,coding: utf-8,
spaCy/examples/streamlit_spacy.py,1,coding: utf-8,
spaCy/examples/streamlit_spacy.py,72,Double newlines seem to mess with the rendering,
spaCy/examples/streamlit_spacy.py,86,Newlines seem to mess with the rendering,
spaCy/examples/keras_parikh_entailment/spacy_hook.py,50,the extra +1 is for a zero vector representing sentence-final padding,
spaCy/examples/keras_parikh_entailment/spacy_hook.py,53,create random vectors for OOV tokens,
spaCy/examples/keras_parikh_entailment/__main__.py,17,workaround for keras/tensorflow bug,
spaCy/examples/keras_parikh_entailment/__main__.py,18,see https://github.com/tensorflow/tensorflow/issues/3388,
spaCy/examples/keras_parikh_entailment/__main__.py,66,remove the embedding matrix.  We can reconstruct it.,
spaCy/examples/keras_parikh_entailment/__main__.py,115,"per Parikh, ignore - SNLI entries",
spaCy/examples/keras_parikh_entailment/__main__.py,130,skip odd spaces from tokenizer,
spaCy/examples/keras_parikh_entailment/__main__.py,140,"if we don't have a vector, pick an OOV entry",
spaCy/examples/keras_parikh_entailment/__main__.py,143,there must be a simpler way of generating padded arrays from lists...,
spaCy/examples/keras_parikh_entailment/keras_decomposable_attention.py,1,Semantic entailment/similarity with decomposable attention (using spaCy and Keras),
spaCy/examples/keras_parikh_entailment/keras_decomposable_attention.py,2,Practical state-of-the-art textual entailment with spaCy and Keras,
spaCy/examples/keras_parikh_entailment/keras_decomposable_attention.py,15,embeddings (projected),
spaCy/examples/keras_parikh_entailment/keras_decomposable_attention.py,21,step 1: attend,
spaCy/examples/keras_parikh_entailment/keras_decomposable_attention.py,33,step 2: compare,
spaCy/examples/keras_parikh_entailment/keras_decomposable_attention.py,39,step 3: aggregate,
spaCy/examples/information_extraction/parse_subtrees.py,1,!/usr/bin/env python,
spaCy/examples/information_extraction/parse_subtrees.py,2,coding: utf8,
spaCy/examples/information_extraction/parse_subtrees.py,38,"The easiest way is to find the head of the subtree you want, and then use",
spaCy/examples/information_extraction/parse_subtrees.py,39,"the `.subtree`, `.children`, `.lefts` and `.rights` iterators. `.subtree`",
spaCy/examples/information_extraction/parse_subtrees.py,40,is the one that does what you're asking for most directly:,
spaCy/examples/information_extraction/parse_subtrees.py,45,It'd probably be better for `word.subtree` to return a `Span` object,
spaCy/examples/information_extraction/parse_subtrees.py,46,instead of a generator over the tokens. If you want the `Span` you can,
spaCy/examples/information_extraction/parse_subtrees.py,47,get it via the `.right_edge` and `.left_edge` properties. The `Span`,
spaCy/examples/information_extraction/parse_subtrees.py,48,"object is nice because you can easily get a vector, merge it, etc.",
spaCy/examples/information_extraction/parse_subtrees.py,54,"You might also want to select a head, and then select a start and end",
spaCy/examples/information_extraction/parse_subtrees.py,55,position by walking along its children. You could then take the,
spaCy/examples/information_extraction/parse_subtrees.py,56,"`.left_edge` and `.right_edge` of those tokens, and use it to calculate",
spaCy/examples/information_extraction/parse_subtrees.py,57,a span.,
spaCy/examples/information_extraction/parse_subtrees.py,63,Expected output:,
spaCy/examples/information_extraction/parse_subtrees.py,64,to show you how computers understand language,
spaCy/examples/information_extraction/parse_subtrees.py,65,how computers understand language,
spaCy/examples/information_extraction/parse_subtrees.py,66,to show you how computers understand language | show,
spaCy/examples/information_extraction/parse_subtrees.py,67,how computers understand language | understand,
spaCy/examples/information_extraction/entity_relations.py,1,!/usr/bin/env python,
spaCy/examples/information_extraction/entity_relations.py,2,coding: utf8,
spaCy/examples/information_extraction/entity_relations.py,40,Filter a sequence of spans so they don't contain overlaps,
spaCy/examples/information_extraction/entity_relations.py,41,For spaCy 2.1.4+: this function is available as spacy.util.filter_spans(),
spaCy/examples/information_extraction/entity_relations.py,47,Check for end - 1 here because boundaries are inclusive,
spaCy/examples/information_extraction/entity_relations.py,56,Merge entities and noun chunks into one token,
spaCy/examples/information_extraction/entity_relations.py,78,Expected output:,
spaCy/examples/information_extraction/entity_relations.py,79,Net income      MONEY   $9.4 million,
spaCy/examples/information_extraction/entity_relations.py,80,the prior year  MONEY   $2.7 million,
spaCy/examples/information_extraction/entity_relations.py,81,Revenue         MONEY   twelve billion dollars,
spaCy/examples/information_extraction/entity_relations.py,82,a loss          MONEY   1b,
spaCy/examples/information_extraction/phrase_matcher.py,1,!/usr/bin/env python,
spaCy/examples/information_extraction/phrase_matcher.py,2,coding: utf8,
spaCy/examples/training/pretrain_textcat.py,43,Partition off part of the train data for evaluation,
spaCy/examples/training/pretrain_textcat.py,133,get names of other pipes to disable them during training,
spaCy/examples/training/pretrain_textcat.py,136,only train textcat,
spaCy/examples/training/pretrain_textcat.py,143,batch up the examples using spaCy's minibatch,
spaCy/examples/training/pretrain_textcat.py,149,evaluate on the dev data split off in load_data(),
spaCy/examples/training/pretrain_textcat.py,152,print a simple table,
spaCy/examples/training/pretrain_kb.py,1,!/usr/bin/env python,
spaCy/examples/training/pretrain_kb.py,2,coding: utf8,
spaCy/examples/training/pretrain_kb.py,26,Q2146908 (Russ Cochran): American golfer,
spaCy/examples/training/pretrain_kb.py,27,Q7381115 (Russ Cochran): publisher,
spaCy/examples/training/pretrain_kb.py,30,dimension of pretrained input vectors,
spaCy/examples/training/pretrain_kb.py,31,dimension of output entity vectors,
spaCy/examples/training/pretrain_kb.py,44,load existing spaCy model,
spaCy/examples/training/pretrain_kb.py,47,check the length of the nlp vectors,
spaCy/examples/training/pretrain_kb.py,56,set up the data,
spaCy/examples/training/pretrain_kb.py,66,training entity description encodings,
spaCy/examples/training/pretrain_kb.py,67,this part can easily be replaced with a custom entity encoder,
spaCy/examples/training/pretrain_kb.py,76,get the pretrained entity vectors,
spaCy/examples/training/pretrain_kb.py,79,"set the entities, can also be done by calling `kb.add_entity` for each entity",
spaCy/examples/training/pretrain_kb.py,82,"adding aliases, the entities need to be defined in the KB beforehand",
spaCy/examples/training/pretrain_kb.py,86,the sum of these probabilities should not exceed 1,
spaCy/examples/training/pretrain_kb.py,89,test the trained model,
spaCy/examples/training/pretrain_kb.py,93,save model to output directory,
spaCy/examples/training/pretrain_kb.py,109,test the saved model,
spaCy/examples/training/pretrain_kb.py,110,always reload a knowledge base with the same vocab instance!,
spaCy/examples/training/pretrain_kb.py,128,Expected output:,
spaCy/examples/training/pretrain_kb.py,130,"2 kb entities: ['Q2146908', 'Q7381115']",
spaCy/examples/training/pretrain_kb.py,131,1 kb aliases: ['Russ Cochran'],
spaCy/examples/training/rehearsal.py,57,Avoid use of Adam when resuming training. I don't understand this well,
spaCy/examples/training/rehearsal.py,58,"yet, but I'm getting weird results from Adam. Try commenting out the",
spaCy/examples/training/rehearsal.py,59,"nlp.update(), and using Adam -- you'll find the models drift apart.",
spaCy/examples/training/rehearsal.py,60,"I guess Adam is losing precision, introducing gradient noise?",
spaCy/examples/training/rehearsal.py,65,get names of other pipes to disable them during training,
spaCy/examples/training/rehearsal.py,75,batch up the examples using spaCy's minibatch,
spaCy/examples/training/conllu.py,61,,
spaCy/examples/training/conllu.py,62,Data reading,
spaCy/examples/training/conllu.py,63,,
spaCy/examples/training/conllu.py,89,sd is spacy doc; cd is conllu doc,
spaCy/examples/training/conllu.py,90,"cs is conllu sent, ct is conllu token",
spaCy/examples/training/conllu.py,161,"Flatten the conll annotations, and adjust the head indices",
spaCy/examples/training/conllu.py,167,Construct text if necessary,
spaCy/examples/training/conllu.py,179,,
spaCy/examples/training/conllu.py,180,Data transforms for spaCy,
spaCy/examples/training/conllu.py,181,,
spaCy/examples/training/conllu.py,196,,
spaCy/examples/training/conllu.py,197,Evaluation,
spaCy/examples/training/conllu.py,198,,
spaCy/examples/training/conllu.py,260,"def get_sent_conllu(sent, sent_id):",
spaCy/examples/training/conllu.py,261,"lines = [""# sent_id = {sent_id}"".format(sent_id=sent_id)]",
spaCy/examples/training/conllu.py,293,,
spaCy/examples/training/conllu.py,294,Initialization,
spaCy/examples/training/conllu.py,295,,
spaCy/examples/training/conllu.py,318,Replace labels that didn't make the frequency cutoff,
spaCy/examples/training/conllu.py,328,,
spaCy/examples/training/conllu.py,329,Command line helpers,
spaCy/examples/training/conllu.py,330,,
spaCy/examples/training/train_ner.py,1,!/usr/bin/env python,
spaCy/examples/training/train_ner.py,2,coding: utf8,
spaCy/examples/training/train_ner.py,22,training data,
spaCy/examples/training/train_ner.py,37,load existing spaCy model,
spaCy/examples/training/train_ner.py,40,create blank Language class,
spaCy/examples/training/train_ner.py,43,create the built-in pipeline components and add them to the pipeline,
spaCy/examples/training/train_ner.py,44,nlp.create_pipe works for built-ins that are registered with spaCy,
spaCy/examples/training/train_ner.py,48,"otherwise, get it so we can add labels",
spaCy/examples/training/train_ner.py,52,add labels,
spaCy/examples/training/train_ner.py,57,get names of other pipes to disable them during training,
spaCy/examples/training/train_ner.py,60,only train NER,
spaCy/examples/training/train_ner.py,61,reset and initialize the weights randomly – but only if we're,
spaCy/examples/training/train_ner.py,62,training a new model,
spaCy/examples/training/train_ner.py,68,batch up the examples using spaCy's minibatch,
spaCy/examples/training/train_ner.py,73,batch of texts,
spaCy/examples/training/train_ner.py,74,batch of annotations,
spaCy/examples/training/train_ner.py,75,dropout - make it harder to memorise data,
spaCy/examples/training/train_ner.py,80,test the trained model,
spaCy/examples/training/train_ner.py,86,save model to output directory,
spaCy/examples/training/train_ner.py,94,test the saved model,
spaCy/examples/training/train_ner.py,106,Expected output:,
spaCy/examples/training/train_ner.py,107,"Entities [('Shaka Khan', 'PERSON')]",
spaCy/examples/training/train_ner.py,108,"Tokens [('Who', '', 2), ('is', '', 2), ('Shaka', 'PERSON', 3),",
spaCy/examples/training/train_ner.py,109,"('Khan', 'PERSON', 1), ('?', '', 2)]",
spaCy/examples/training/train_ner.py,110,"Entities [('London', 'LOC'), ('Berlin', 'LOC')]",
spaCy/examples/training/train_ner.py,111,"Tokens [('I', '', 2), ('like', '', 2), ('London', 'LOC', 3),",
spaCy/examples/training/train_ner.py,112,"('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]",
spaCy/examples/training/train_new_entity_type.py,1,!/usr/bin/env python,
spaCy/examples/training/train_new_entity_type.py,2,coding: utf8,
spaCy/examples/training/train_new_entity_type.py,38,new entity label,
spaCy/examples/training/train_new_entity_type.py,41,training data,
spaCy/examples/training/train_new_entity_type.py,42,"Note: If you're using an existing model, make sure to mix in examples of",
spaCy/examples/training/train_new_entity_type.py,43,"other entity types that spaCy correctly recognized before. Otherwise, your",
spaCy/examples/training/train_new_entity_type.py,44,"model might learn the new type, but ""forget"" what it previously knew.",
spaCy/examples/training/train_new_entity_type.py,45,https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting,
spaCy/examples/training/train_new_entity_type.py,75,load existing spaCy model,
spaCy/examples/training/train_new_entity_type.py,78,create blank Language class,
spaCy/examples/training/train_new_entity_type.py,80,Add entity recognizer to model if it's not in the pipeline,
spaCy/examples/training/train_new_entity_type.py,81,nlp.create_pipe works for built-ins that are registered with spaCy,
spaCy/examples/training/train_new_entity_type.py,85,"otherwise, get it, so we can add labels to it",
spaCy/examples/training/train_new_entity_type.py,89,add new entity label to entity recognizer,
spaCy/examples/training/train_new_entity_type.py,90,Adding extraneous labels shouldn't mess anything up,
spaCy/examples/training/train_new_entity_type.py,97,get names of other pipes to disable them during training,
spaCy/examples/training/train_new_entity_type.py,100,only train NER,
spaCy/examples/training/train_new_entity_type.py,102,batch up the examples using spaCy's minibatch,
spaCy/examples/training/train_new_entity_type.py,112,test the trained model,
spaCy/examples/training/train_new_entity_type.py,119,save model to output directory,
spaCy/examples/training/train_new_entity_type.py,124,rename model,
spaCy/examples/training/train_new_entity_type.py,128,test the saved model,
spaCy/examples/training/train_new_entity_type.py,131,Check the classes have loaded back consistently,
spaCy/examples/training/train_textcat.py,1,!/usr/bin/env python,
spaCy/examples/training/train_textcat.py,2,coding: utf8,
spaCy/examples/training/train_textcat.py,36,load existing spaCy model,
spaCy/examples/training/train_textcat.py,39,create blank Language class,
spaCy/examples/training/train_textcat.py,42,add the text classifier to the pipeline if it doesn't exist,
spaCy/examples/training/train_textcat.py,43,nlp.create_pipe works for built-ins that are registered with spaCy,
spaCy/examples/training/train_textcat.py,49,"otherwise, get it, so we can add labels to it",
spaCy/examples/training/train_textcat.py,53,add label to text classifier,
spaCy/examples/training/train_textcat.py,57,load the IMDB dataset,
spaCy/examples/training/train_textcat.py,69,get names of other pipes to disable them during training,
spaCy/examples/training/train_textcat.py,72,only train textcat,
spaCy/examples/training/train_textcat.py,82,batch up the examples using spaCy's minibatch,
spaCy/examples/training/train_textcat.py,89,evaluate on the dev data split off in load_data(),
spaCy/examples/training/train_textcat.py,92,print a simple table,
spaCy/examples/training/train_textcat.py,100,test the trained model,
spaCy/examples/training/train_textcat.py,110,test the saved model,
spaCy/examples/training/train_textcat.py,119,Partition off part of the train data for evaluation,
spaCy/examples/training/train_textcat.py,131,True positives,
spaCy/examples/training/train_textcat.py,132,False positives,
spaCy/examples/training/train_textcat.py,133,False negatives,
spaCy/examples/training/train_textcat.py,134,True negatives,
spaCy/examples/training/train_parser.py,1,!/usr/bin/env python,
spaCy/examples/training/train_parser.py,2,coding: utf8,
spaCy/examples/training/train_parser.py,20,training data,
spaCy/examples/training/train_parser.py,47,load existing spaCy model,
spaCy/examples/training/train_parser.py,50,create blank Language class,
spaCy/examples/training/train_parser.py,53,add the parser to the pipeline if it doesn't exist,
spaCy/examples/training/train_parser.py,54,nlp.create_pipe works for built-ins that are registered with spaCy,
spaCy/examples/training/train_parser.py,58,"otherwise, get it, so we can add labels to it",
spaCy/examples/training/train_parser.py,62,add labels to the parser,
spaCy/examples/training/train_parser.py,67,get names of other pipes to disable them during training,
spaCy/examples/training/train_parser.py,70,only train parser,
spaCy/examples/training/train_parser.py,75,batch up the examples using spaCy's minibatch,
spaCy/examples/training/train_parser.py,82,test the trained model,
spaCy/examples/training/train_parser.py,87,save model to output directory,
spaCy/examples/training/train_parser.py,95,test the saved model,
spaCy/examples/training/train_parser.py,105,expected result:,
spaCy/examples/training/train_parser.py,106,[,
spaCy/examples/training/train_parser.py,107,"('I', 'nsubj', 'like'),",
spaCy/examples/training/train_parser.py,108,"('like', 'ROOT', 'like'),",
spaCy/examples/training/train_parser.py,109,"('securities', 'dobj', 'like'),",
spaCy/examples/training/train_parser.py,110,"('.', 'punct', 'like')",
spaCy/examples/training/train_parser.py,111,],
spaCy/examples/training/train_entity_linker.py,1,!/usr/bin/env python,
spaCy/examples/training/train_entity_linker.py,2,coding: utf8,
spaCy/examples/training/train_entity_linker.py,33,Q2146908 (Russ Cochran): American golfer,
spaCy/examples/training/train_entity_linker.py,34,Q7381115 (Russ Cochran): publisher,
spaCy/examples/training/train_entity_linker.py,55,training data,
spaCy/examples/training/train_entity_linker.py,69,create blank Language class with correct vocab,
spaCy/examples/training/train_entity_linker.py,74,"Add a sentencizer component. Alternatively, add a dependency parser for higher accuracy.",
spaCy/examples/training/train_entity_linker.py,77,"Add a custom component to recognize ""Russ Cochran"" as an entity for the example training data.",
spaCy/examples/training/train_entity_linker.py,78,"Note that in a realistic application, an actual NER algorithm should be used instead.",
spaCy/examples/training/train_entity_linker.py,84,Create the Entity Linker component and add it to the pipeline.,
spaCy/examples/training/train_entity_linker.py,86,use only the predicted EL score and not the prior probability (for demo purposes),
spaCy/examples/training/train_entity_linker.py,95,Convert the texts to docs to make sure we have doc.ents set for the training examples.,
spaCy/examples/training/train_entity_linker.py,96,Also ensure that the annotated examples correspond to known identifiers in the knowlege base.,
spaCy/examples/training/train_entity_linker.py,115,get names of other pipes to disable them during training,
spaCy/examples/training/train_entity_linker.py,118,only train entity linker,
spaCy/examples/training/train_entity_linker.py,119,reset and initialize the weights randomly,
spaCy/examples/training/train_entity_linker.py,124,batch up the examples using spaCy's minibatch,
spaCy/examples/training/train_entity_linker.py,129,batch of texts,
spaCy/examples/training/train_entity_linker.py,130,batch of annotations,
spaCy/examples/training/train_entity_linker.py,131,dropout - make it harder to memorise data,
spaCy/examples/training/train_entity_linker.py,137,test the trained model,
spaCy/examples/training/train_entity_linker.py,140,save model to output directory,
spaCy/examples/training/train_entity_linker.py,149,test the saved model,
spaCy/examples/training/train_entity_linker.py,157,apply the entity linker which will now make predictions for the 'Russ Cochran' entities,
spaCy/examples/training/train_entity_linker.py,167,Expected output (can be shuffled):,
spaCy/examples/training/train_entity_linker.py,169,"Entities[('Russ Cochran', 'PERSON', 'Q7381115')]",
spaCy/examples/training/train_entity_linker.py,170,"Tokens[('Russ', 'PERSON', 'Q7381115'), ('Cochran', 'PERSON', 'Q7381115'), (""his"", '', ''), ('reprints', '', ''), ('include', '', ''), ('The', '', ''), ('Complete', '', ''), ('EC', '', ''), ('Library', '', ''), ('.', '', '')]",
spaCy/examples/training/train_entity_linker.py,172,"Entities[('Russ Cochran', 'PERSON', 'Q7381115')]",
spaCy/examples/training/train_entity_linker.py,173,"Tokens[('Russ', 'PERSON', 'Q7381115'), ('Cochran', 'PERSON', 'Q7381115'), ('has', '', ''), ('been', '', ''), ('publishing', '', ''), ('comic', '', ''), ('art', '', ''), ('.', '', '')]",
spaCy/examples/training/train_entity_linker.py,175,"Entities[('Russ Cochran', 'PERSON', 'Q2146908')]",
spaCy/examples/training/train_entity_linker.py,176,"Tokens[('Russ', 'PERSON', 'Q2146908'), ('Cochran', 'PERSON', 'Q2146908'), ('captured', '', ''), ('his', '', ''), ('first', '', ''), ('major', '', ''), ('title', '', ''), ('with', '', ''), ('his', '', ''), ('son', '', ''), ('as', '', ''), ('caddie', '', ''), ('.', '', '')]",
spaCy/examples/training/train_entity_linker.py,178,"Entities[('Russ Cochran', 'PERSON', 'Q2146908')]",
spaCy/examples/training/train_entity_linker.py,179,"Tokens[('Russ', 'PERSON', 'Q2146908'), ('Cochran', 'PERSON', 'Q2146908'), ('was', '', ''), ('a', '', ''), ('member', '', ''), ('of', '', ''), ('University', '', ''), ('of', '', ''), ('Kentucky', '', ''), (""'s"", '', ''), ('golf', '', ''), ('team', '', ''), ('.', '', '')]",
spaCy/examples/training/train_tagger.py,1,!/usr/bin/env python,
spaCy/examples/training/train_tagger.py,2,coding: utf8,
spaCy/examples/training/train_tagger.py,23,You need to define a mapping from your data's part-of-speech tag names to the,
spaCy/examples/training/train_tagger.py,24,"Universal Part-of-Speech tag set, as spaCy includes an enum of these tags.",
spaCy/examples/training/train_tagger.py,25,See here for the Universal Tag Set:,
spaCy/examples/training/train_tagger.py,26,http://universaldependencies.github.io/docs/u/pos/index.html,
spaCy/examples/training/train_tagger.py,27,"You may also specify morphological features for your tags, from the universal",
spaCy/examples/training/train_tagger.py,28,scheme.,
spaCy/examples/training/train_tagger.py,31,"Usually you'll read this in, of course. Data formats vary. Ensure your",
spaCy/examples/training/train_tagger.py,32,strings are unicode and that the number of tags assigned matches spaCy's,
spaCy/examples/training/train_tagger.py,33,"tokenization. If not, you can always add a 'words' key to the annotations",
spaCy/examples/training/train_tagger.py,34,"that specifies the gold-standard tokenization, e.g.:",
spaCy/examples/training/train_tagger.py,35,"(""Eatblueham"", {'words': ['Eat', 'blue', 'ham'], 'tags': ['V', 'J', 'N']})",
spaCy/examples/training/train_tagger.py,53,add the tagger to the pipeline,
spaCy/examples/training/train_tagger.py,54,nlp.create_pipe works for built-ins that are registered with spaCy,
spaCy/examples/training/train_tagger.py,56,Add the tags. This needs to be done before you start training.,
spaCy/examples/training/train_tagger.py,65,batch up the examples using spaCy's minibatch,
spaCy/examples/training/train_tagger.py,72,test the trained model,
spaCy/examples/training/train_tagger.py,77,save model to output directory,
spaCy/examples/training/train_tagger.py,85,test the save model,
spaCy/examples/training/train_tagger.py,95,Expected output:,
spaCy/examples/training/train_tagger.py,96,[,
spaCy/examples/training/train_tagger.py,97,"('I', 'N', 'NOUN'),",
spaCy/examples/training/train_tagger.py,98,"('like', 'V', 'VERB'),",
spaCy/examples/training/train_tagger.py,99,"('blue', 'J', 'ADJ'),",
spaCy/examples/training/train_tagger.py,100,"('eggs', 'N', 'NOUN')",
spaCy/examples/training/train_tagger.py,101,],
spaCy/examples/training/ner_multitask_objective.py,72,batch of texts,
spaCy/examples/training/ner_multitask_objective.py,73,batch of annotations,
spaCy/examples/training/ner_multitask_objective.py,74,dropout - make it harder to memorise data,
spaCy/examples/training/ner_multitask_objective.py,75,callable to update weights,
spaCy/examples/training/ner_multitask_objective.py,80,test the trained model,
spaCy/examples/training/train_intent_parser.py,1,!/usr/bin/env python,
spaCy/examples/training/train_intent_parser.py,2,coding: utf-8,
spaCy/examples/training/train_intent_parser.py,29,"training data: texts, heads and dependency labels",
spaCy/examples/training/train_intent_parser.py,30,"for no relation, we simply chose an arbitrary dependency label, e.g. '-'",
spaCy/examples/training/train_intent_parser.py,35,index of token head,
spaCy/examples/training/train_intent_parser.py,66,"attach ""flowers"" to store!",
spaCy/examples/training/train_intent_parser.py,110,load existing spaCy model,
spaCy/examples/training/train_intent_parser.py,113,create blank Language class,
spaCy/examples/training/train_intent_parser.py,116,"We'll use the built-in dependency parser class, but we want to create a",
spaCy/examples/training/train_intent_parser.py,117,fresh instance – just in case.,
spaCy/examples/training/train_intent_parser.py,129,only train parser,
spaCy/examples/training/train_intent_parser.py,134,batch up the examples using spaCy's minibatch,
spaCy/examples/training/train_intent_parser.py,141,test the trained model,
spaCy/examples/training/train_intent_parser.py,144,save model to output directory,
spaCy/examples/training/train_intent_parser.py,152,test the saved model,
spaCy/examples/training/train_intent_parser.py,173,Expected output:,
spaCy/examples/training/train_intent_parser.py,174,find a hotel with good wifi,
spaCy/examples/training/train_intent_parser.py,175,[,
spaCy/examples/training/train_intent_parser.py,176,"('find', 'ROOT', 'find'),",
spaCy/examples/training/train_intent_parser.py,177,"('hotel', 'PLACE', 'find'),",
spaCy/examples/training/train_intent_parser.py,178,"('good', 'QUALITY', 'wifi'),",
spaCy/examples/training/train_intent_parser.py,179,"('wifi', 'ATTRIBUTE', 'hotel')",
spaCy/examples/training/train_intent_parser.py,180,],
spaCy/examples/training/train_intent_parser.py,181,find me the cheapest gym near work,
spaCy/examples/training/train_intent_parser.py,182,[,
spaCy/examples/training/train_intent_parser.py,183,"('find', 'ROOT', 'find'),",
spaCy/examples/training/train_intent_parser.py,184,"('cheapest', 'QUALITY', 'gym'),",
spaCy/examples/training/train_intent_parser.py,185,"('gym', 'PLACE', 'find'),",
spaCy/examples/training/train_intent_parser.py,186,"('near', 'ATTRIBUTE', 'gym'),",
spaCy/examples/training/train_intent_parser.py,187,"('work', 'LOCATION', 'near')",
spaCy/examples/training/train_intent_parser.py,188,],
spaCy/examples/training/train_intent_parser.py,189,show me the best hotel in berlin,
spaCy/examples/training/train_intent_parser.py,190,[,
spaCy/examples/training/train_intent_parser.py,191,"('show', 'ROOT', 'show'),",
spaCy/examples/training/train_intent_parser.py,192,"('best', 'QUALITY', 'hotel'),",
spaCy/examples/training/train_intent_parser.py,193,"('hotel', 'PLACE', 'show'),",
spaCy/examples/training/train_intent_parser.py,194,"('berlin', 'LOCATION', 'hotel')",
spaCy/examples/training/train_intent_parser.py,195,],
spaCy/examples/training/textcat_example_data/textcatjsonl_to_trainjson.py,15,Load model with tokenizer + sentencizer only,
spaCy/examples/pipeline/custom_attr_methods.py,1,!/usr/bin/env python,
spaCy/examples/pipeline/custom_attr_methods.py,2,coding: utf-8,
spaCy/examples/pipeline/custom_attr_methods.py,26,start off with blank English class,
spaCy/examples/pipeline/custom_attr_methods.py,37,"add entity manually for demo purposes, to make it work without a model",
spaCy/examples/pipeline/custom_attr_methods.py,47,generate filename from first six non-punct tokens,
spaCy/examples/pipeline/custom_attr_methods.py,49,render markup,
spaCy/examples/pipeline/custom_attr_methods.py,55,save to file,
spaCy/examples/pipeline/custom_attr_methods.py,75,Expected output:,
spaCy/examples/pipeline/custom_attr_methods.py,76,Text 1: Peach emoji is where it has always been.,
spaCy/examples/pipeline/custom_attr_methods.py,77,Text 2: Peach is the superior emoji.,
spaCy/examples/pipeline/custom_attr_methods.py,78,"Overlapping tokens: [Peach, emoji, is, .]",
spaCy/examples/pipeline/fix_space_entities.py,1,!/usr/bin/env python,
spaCy/examples/pipeline/fix_space_entities.py,2,coding: utf8,
spaCy/examples/pipeline/fix_space_entities.py,20,"Sets 'O' tag (0 is None, so I is 1, O is 2)",
spaCy/examples/pipeline/custom_component_countries_api.py,1,!/usr/bin/env python,
spaCy/examples/pipeline/custom_component_countries_api.py,2,coding: utf8,
spaCy/examples/pipeline/custom_component_countries_api.py,25,"For simplicity, we start off with only the blank English Language class",
spaCy/examples/pipeline/custom_component_countries_api.py,26,and no model or pre-defined pipeline loaded.,
spaCy/examples/pipeline/custom_component_countries_api.py,28,initialise component,
spaCy/examples/pipeline/custom_component_countries_api.py,29,add it to the pipeline,
spaCy/examples/pipeline/custom_component_countries_api.py,31,pipeline contains component name,
spaCy/examples/pipeline/custom_component_countries_api.py,32,Doc contains countries,
spaCy/examples/pipeline/custom_component_countries_api.py,40,country data,
spaCy/examples/pipeline/custom_component_countries_api.py,41,entities,
spaCy/examples/pipeline/custom_component_countries_api.py,50,"component name, will show up in the pipeline",
spaCy/examples/pipeline/custom_component_countries_api.py,57,Make request once on initialisation and store the data,
spaCy/examples/pipeline/custom_component_countries_api.py,59,make sure requests raises an error if it fails,
spaCy/examples/pipeline/custom_component_countries_api.py,62,Convert API response to dict keyed by country name for easy lookup,
spaCy/examples/pipeline/custom_component_countries_api.py,63,This could also be extended using the alternative and foreign language,
spaCy/examples/pipeline/custom_component_countries_api.py,64,names provided by the API,
spaCy/examples/pipeline/custom_component_countries_api.py,66,get entity label ID,
spaCy/examples/pipeline/custom_component_countries_api.py,68,Set up the PhraseMatcher with Doc patterns for each country name,
spaCy/examples/pipeline/custom_component_countries_api.py,73,Register attribute on the Token. We'll be overwriting this based on,
spaCy/examples/pipeline/custom_component_countries_api.py,74,"the matches, so we're only setting a default value, not a getter.",
spaCy/examples/pipeline/custom_component_countries_api.py,75,"If no default value is set, it defaults to None.",
spaCy/examples/pipeline/custom_component_countries_api.py,81,Register attributes on Doc and Span via a getter that checks if one of,
spaCy/examples/pipeline/custom_component_countries_api.py,82,the contained tokens is set to is_country == True.,
spaCy/examples/pipeline/custom_component_countries_api.py,92,keep the spans for later so we can merge them afterwards,
spaCy/examples/pipeline/custom_component_countries_api.py,94,Generate Span representing the entity & set label,
spaCy/examples/pipeline/custom_component_countries_api.py,97,Set custom attribute on each token of the entity,
spaCy/examples/pipeline/custom_component_countries_api.py,98,"Can be extended with other data returned by the API, like",
spaCy/examples/pipeline/custom_component_countries_api.py,99,"currencies, country code, flag, calling code etc.",
spaCy/examples/pipeline/custom_component_countries_api.py,105,Overwrite doc.ents and add entity – be careful not to replace!,
spaCy/examples/pipeline/custom_component_countries_api.py,108,Iterate over all spans and merge them into one token. This is done,
spaCy/examples/pipeline/custom_component_countries_api.py,109,"after setting the entities – otherwise, it would cause mismatched",
spaCy/examples/pipeline/custom_component_countries_api.py,110,indices!,
spaCy/examples/pipeline/custom_component_countries_api.py,112,don't forget to return the Doc!,
spaCy/examples/pipeline/custom_component_countries_api.py,125,Expected output:,
spaCy/examples/pipeline/custom_component_countries_api.py,126,Pipeline ['rest_countries'],
spaCy/examples/pipeline/custom_component_countries_api.py,127,Doc has countries True,
spaCy/examples/pipeline/custom_component_countries_api.py,128,"Colombia Bogotá [4.0, -72.0] https://restcountries.eu/data/col.svg",
spaCy/examples/pipeline/custom_component_countries_api.py,129,"Czech Republic Prague [49.75, 15.5] https://restcountries.eu/data/cze.svg",
spaCy/examples/pipeline/custom_component_countries_api.py,130,"Entities [('Colombia', 'GPE'), ('Czech Republic', 'GPE')]",
spaCy/examples/pipeline/multi_processing.py,1,!/usr/bin/env python,
spaCy/examples/pipeline/multi_processing.py,2,coding: utf8,
spaCy/examples/pipeline/multi_processing.py,32,load spaCy model,
spaCy/examples/pipeline/multi_processing.py,36,load and pre-process the IMBD dataset,
spaCy/examples/pipeline/multi_processing.py,51,return None in case same batch is called again,
spaCy/examples/pipeline/multi_processing.py,63,"True-case, i.e. try to normalize sentence-initial capitals.",
spaCy/examples/pipeline/multi_processing.py,64,Only do this if the lower-cased form is more probable.,
spaCy/examples/pipeline/custom_sentence_segmentation.py,31,We're not checking for is_title here to ignore arbitrary titlecased,
spaCy/examples/pipeline/custom_sentence_segmentation.py,32,tokens within sentences,
spaCy/examples/pipeline/custom_sentence_segmentation.py,33,elif token.is_title:,
spaCy/examples/pipeline/custom_sentence_segmentation.py,34,return True,
spaCy/examples/pipeline/custom_component_entities.py,1,!/usr/bin/env python,
spaCy/examples/pipeline/custom_component_entities.py,2,coding: utf8,
spaCy/examples/pipeline/custom_component_entities.py,27,"For simplicity, we start off with only the blank English Language class",
spaCy/examples/pipeline/custom_component_entities.py,28,and no model or pre-defined pipeline loaded.,
spaCy/examples/pipeline/custom_component_entities.py,30,set default companies if none are set via args,
spaCy/examples/pipeline/custom_component_entities.py,31,etc.,
spaCy/examples/pipeline/custom_component_entities.py,32,initialise component,
spaCy/examples/pipeline/custom_component_entities.py,33,add last to the pipeline,
spaCy/examples/pipeline/custom_component_entities.py,36,pipeline contains component name,
spaCy/examples/pipeline/custom_component_entities.py,37,company names from the list are merged,
spaCy/examples/pipeline/custom_component_entities.py,38,Doc contains tech orgs,
spaCy/examples/pipeline/custom_component_entities.py,39,"""Alphabet Inc."" is a tech org",
spaCy/examples/pipeline/custom_component_entities.py,40,"""is"" is not",
spaCy/examples/pipeline/custom_component_entities.py,41,all orgs are entities,
spaCy/examples/pipeline/custom_component_entities.py,51,"component name, will show up in the pipeline",
spaCy/examples/pipeline/custom_component_entities.py,58,get entity label ID,
spaCy/examples/pipeline/custom_component_entities.py,60,"Set up the PhraseMatcher – it can now take Doc objects as patterns,",
spaCy/examples/pipeline/custom_component_entities.py,61,"so even if the list of companies is long, it's very efficient",
spaCy/examples/pipeline/custom_component_entities.py,66,Register attribute on the Token. We'll be overwriting this based on,
spaCy/examples/pipeline/custom_component_entities.py,67,"the matches, so we're only setting a default value, not a getter.",
spaCy/examples/pipeline/custom_component_entities.py,70,Register attributes on Doc and Span via a getter that checks if one of,
spaCy/examples/pipeline/custom_component_entities.py,71,the contained tokens is set to is_tech_org == True.,
spaCy/examples/pipeline/custom_component_entities.py,81,keep the spans for later so we can merge them afterwards,
spaCy/examples/pipeline/custom_component_entities.py,83,Generate Span representing the entity & set label,
spaCy/examples/pipeline/custom_component_entities.py,86,Set custom attribute on each token of the entity,
spaCy/examples/pipeline/custom_component_entities.py,89,Overwrite doc.ents and add entity – be careful not to replace!,
spaCy/examples/pipeline/custom_component_entities.py,92,Iterate over all spans and merge them into one token. This is done,
spaCy/examples/pipeline/custom_component_entities.py,93,"after setting the entities – otherwise, it would cause mismatched",
spaCy/examples/pipeline/custom_component_entities.py,94,indices!,
spaCy/examples/pipeline/custom_component_entities.py,96,don't forget to return the Doc!,
spaCy/examples/pipeline/custom_component_entities.py,109,Expected output:,
spaCy/examples/pipeline/custom_component_entities.py,110,Pipeline ['tech_companies'],
spaCy/examples/pipeline/custom_component_entities.py,111,"Tokens ['Alphabet Inc.', 'is', 'the', 'company', 'behind', 'Google', '.']",
spaCy/examples/pipeline/custom_component_entities.py,112,Doc has_tech_org True,
spaCy/examples/pipeline/custom_component_entities.py,113,Token 0 is_tech_org True,
spaCy/examples/pipeline/custom_component_entities.py,114,Token 1 is_tech_org False,
spaCy/examples/pipeline/custom_component_entities.py,115,"Entities [('Alphabet Inc.', 'ORG'), ('Google', 'ORG')]",
