file path,line #,comment,satd
scikit-learn/conftest.py,1,Even if empty this file is useful so that when running from the root folder,
scikit-learn/conftest.py,2,./sklearn is added to sys.path by pytest. See,
scikit-learn/conftest.py,3,https://docs.pytest.org/en/latest/pythonpath.html for more details.  For,
scikit-learn/conftest.py,4,"example, this allows to build extensions in place and run pytest",
scikit-learn/conftest.py,5,doc/modules/clustering.rst and use sklearn from the local folder rather than,
scikit-learn/conftest.py,6,the one from site-packages.,
scikit-learn/conftest.py,36,FeatureHasher is not compatible with PyPy,
scikit-learn/conftest.py,45,Skip tests which require internet if the flag is provided,
scikit-learn/conftest.py,53,numpy changed the str/repr formatting of numpy arrays in 1.14. We want to,
scikit-learn/conftest.py,54,run doctests only for numpy >= 1.14.,
scikit-learn/conftest.py,90,declare our custom markers to avoid PytestUnknownMarkWarning,
scikit-learn/conftest.py,112,TODO: Remove when modules are deprecated in 0.24,
scikit-learn/conftest.py,113,Configures pytest to ignore deprecated modules.,
scikit-learn/setup.py,1,! /usr/bin/env python,
scikit-learn/setup.py,2,,
scikit-learn/setup.py,3,Copyright (C) 2007-2009 Cournapeau David <cournape@gmail.com>,
scikit-learn/setup.py,4,2010 Fabian Pedregosa <fabian.pedregosa@inria.fr>,
scikit-learn/setup.py,5,License: 3-clause BSD,
scikit-learn/setup.py,18,Python 2 compat: just to be able to declare that Python >=3.6 is needed.,
scikit-learn/setup.py,21,This is a bit (!) hackish: we are setting a global variable so that the,
scikit-learn/setup.py,22,main sklearn __init__ can detect if it is being loaded by the setup,
scikit-learn/setup.py,23,"routine, to avoid attempting to load components that aren't built yet:",
scikit-learn/setup.py,24,the numpy distutils extensions that are used by scikit-learn to,
scikit-learn/setup.py,25,recursively build the compiled extensions in sub-packages is based on the,
scikit-learn/setup.py,26,Python import machinery.,
scikit-learn/setup.py,45,We can actually import a restricted version of sklearn that,
scikit-learn/setup.py,46,does not need the compiled code,
scikit-learn/setup.py,61,Optional setuptools features,
scikit-learn/setup.py,62,"We need to import setuptools early, if we want setuptools features,",
scikit-learn/setup.py,63,as it monkey-patches the 'setup' function,
scikit-learn/setup.py,64,"For some commands, use setuptools",
scikit-learn/setup.py,75,the package can run out of an .egg file,
scikit-learn/setup.py,88,Custom clean command to remove build artifacts,
scikit-learn/setup.py,95,Remove c files if we are not within a sdist package,
scikit-learn/setup.py,120,custom build_ext command to set OpenMP compile flags depending on os and,
scikit-learn/setup.py,121,compiler,
scikit-learn/setup.py,122,build_ext has to be imported after setuptools,
scikit-learn/setup.py,124,noqa,
scikit-learn/setup.py,142,Numpy should not be a dependency just to be able to introspect,
scikit-learn/setup.py,143,that python 3.6 is required.,
scikit-learn/setup.py,147,Optional wheelhouse-uploader features,
scikit-learn/setup.py,148,To automate release of binary packages for scikit-learn we need a tool,
scikit-learn/setup.py,149,to download the packages generated by travis and appveyor workers (with,
scikit-learn/setup.py,150,version number matching the current release) and upload them all at once,
scikit-learn/setup.py,151,to PyPI at release time.,
scikit-learn/setup.py,152,The URL of the artifact repositories are configured in the setup.cfg file.,
scikit-learn/setup.py,170,Avoid non-useful msg:,
scikit-learn/setup.py,171,"""Ignoring attempt to set 'name' (from ... """,
scikit-learn/setup.py,177,Cython is required by config.add_subpackage for templated extensions,
scikit-learn/setup.py,178,that need the tempita sub-submodule. So check that we have the correct,
scikit-learn/setup.py,179,version of Cython so as to be able to raise a more informative error,
scikit-learn/setup.py,180,message from the start if it's not the case.,
scikit-learn/setup.py,274,"For these actions, NumPy is not required",
scikit-learn/setup.py,275,,
scikit-learn/setup.py,276,They are required to succeed without Numpy for example when,
scikit-learn/setup.py,277,pip is used to install Scikit-learn when Numpy is not yet present in,
scikit-learn/setup.py,278,the system.,
scikit-learn/benchmarks/bench_hist_gradient_boosting.py,7,"To use this experimental feature, we need to explicitly ask for it:",
scikit-learn/benchmarks/bench_hist_gradient_boosting.py,8,noqa,
scikit-learn/benchmarks/bench_hist_gradient_boosting.py,107,loss='auto' does not work with get_equivalent_estimator(),
scikit-learn/benchmarks/bench_hist_gradient_boosting.py,111,regression,
scikit-learn/benchmarks/bench_covertype.py,44,Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>,
scikit-learn/benchmarks/bench_covertype.py,45,Arnaud Joly <arnaud.v.joly@gmail.com>,
scikit-learn/benchmarks/bench_covertype.py,46,License: BSD 3 clause,
scikit-learn/benchmarks/bench_covertype.py,64,Memoize the data extraction and memory map the resulting,
scikit-learn/benchmarks/bench_covertype.py,65,train / test splits in readonly mode,
scikit-learn/benchmarks/bench_covertype.py,73,,
scikit-learn/benchmarks/bench_covertype.py,74,Load dataset,
scikit-learn/benchmarks/bench_covertype.py,81,"Create train-test split (as [Joachims, 2006])",
scikit-learn/benchmarks/bench_covertype.py,89,Standardize first 10 features (the numerical ones),
scikit-learn/benchmarks/bench_random_projections.py,39,number of microseconds in a second,
scikit-learn/benchmarks/bench_random_projections.py,49,start time,
scikit-learn/benchmarks/bench_random_projections.py,53,stop time,
scikit-learn/benchmarks/bench_random_projections.py,56,start time,
scikit-learn/benchmarks/bench_random_projections.py,60,stop time,
scikit-learn/benchmarks/bench_random_projections.py,66,Make some random data with uniformly located non zero entries with,
scikit-learn/benchmarks/bench_random_projections.py,67,Gaussian distributed values,
scikit-learn/benchmarks/bench_random_projections.py,86,,
scikit-learn/benchmarks/bench_random_projections.py,87,Option parser,
scikit-learn/benchmarks/bench_random_projections.py,88,,
scikit-learn/benchmarks/bench_random_projections.py,146,,
scikit-learn/benchmarks/bench_random_projections.py,147,Generate dataset,
scikit-learn/benchmarks/bench_random_projections.py,148,,
scikit-learn/benchmarks/bench_random_projections.py,166,,
scikit-learn/benchmarks/bench_random_projections.py,167,Set transformer input,
scikit-learn/benchmarks/bench_random_projections.py,168,,
scikit-learn/benchmarks/bench_random_projections.py,171,,
scikit-learn/benchmarks/bench_random_projections.py,172,Set GaussianRandomProjection input,
scikit-learn/benchmarks/bench_random_projections.py,180,,
scikit-learn/benchmarks/bench_random_projections.py,181,Set SparseRandomProjection input,
scikit-learn/benchmarks/bench_random_projections.py,192,,
scikit-learn/benchmarks/bench_random_projections.py,193,Perform benchmark,
scikit-learn/benchmarks/bench_random_projections.py,194,,
scikit-learn/benchmarks/bench_random_projections.py,221,,
scikit-learn/benchmarks/bench_random_projections.py,222,Print results,
scikit-learn/benchmarks/bench_random_projections.py,223,,
scikit-learn/benchmarks/bench_plot_fastkmeans.py,39,let's prepare the data in small chunks,
scikit-learn/benchmarks/bench_plot_fastkmeans.py,93,register the 3d projection,
scikit-learn/benchmarks/bench_tree.py,21,to store the results,
scikit-learn/benchmarks/bench_tree.py,25,number of microseconds in a second,
scikit-learn/benchmarks/bench_tree.py,35,start time,
scikit-learn/benchmarks/bench_tree.py,40,stop time,
scikit-learn/benchmarks/bench_tree.py,53,start time,
scikit-learn/benchmarks/bench_tree.py,58,stop time,
scikit-learn/benchmarks/bench_lasso.py,39,Normalize data,
scikit-learn/benchmarks/bench_lasso.py,64,regularization parameter,
scikit-learn/benchmarks/bench_multilabel_metrics.py,1,!/usr/bin/env python,
scikit-learn/benchmarks/bench_saga.py,78,Makes cpu cache even for all fit calls,
scikit-learn/benchmarks/bench_saga.py,90,Lightning predict_proba is not implemented for n_classes > 2,
scikit-learn/benchmarks/bench_tsne_mnist.py,8,License: BSD 3 clause,
scikit-learn/benchmarks/bench_tsne_mnist.py,47,Normalize features,
scikit-learn/benchmarks/bench_tsne_mnist.py,100,Put TSNE in methods,
scikit-learn/benchmarks/bench_tsne_mnist.py,124,PCA preprocessing is done elsewhere in the benchmark script,
scikit-learn/benchmarks/bench_tsne_mnist.py,125,TODO find a way to report the number of iterations,
scikit-learn/benchmarks/bench_isotonic.py,33,Triggers O(n^2) complexity on the original implementation.,
scikit-learn/benchmarks/bench_isotonic.py,89,"If we're not plotting, dump the timing to stdout",
scikit-learn/benchmarks/bench_plot_svd.py,57,register the 3d projection,
scikit-learn/benchmarks/bench_plot_svd.py,71,plot the actual surface,
scikit-learn/benchmarks/bench_plot_svd.py,74,dummy point plot to stick the legend to since surface plot do not,
scikit-learn/benchmarks/bench_plot_svd.py,75,support legends (yet?),
scikit-learn/benchmarks/bench_plot_omp_lars.py,34,dataset_kwargs = {,
scikit-learn/benchmarks/bench_plot_omp_lars.py,35,"'n_train_samples': n_samples,",
scikit-learn/benchmarks/bench_plot_omp_lars.py,36,"'n_test_samples': 2,",
scikit-learn/benchmarks/bench_plot_omp_lars.py,37,"'n_features': n_features,",
scikit-learn/benchmarks/bench_plot_omp_lars.py,38,"'n_informative': n_informative,",
scikit-learn/benchmarks/bench_plot_omp_lars.py,39,"'effective_rank': min(n_samples, n_features) / 10,",
scikit-learn/benchmarks/bench_plot_omp_lars.py,40,"#'effective_rank': None,",
scikit-learn/benchmarks/bench_plot_omp_lars.py,41,"'bias': 0.0,",
scikit-learn/benchmarks/bench_plot_omp_lars.py,42,},
scikit-learn/benchmarks/bench_plot_omp_lars.py,59,precomputed Gram matrix,
scikit-learn/benchmarks/bench_lof.py,30,to control the random selection of anomalies in SA,
scikit-learn/benchmarks/bench_lof.py,32,"datasets available: ['http', 'smtp', 'SA', 'SF', 'shuttle', 'forestcover']",
scikit-learn/benchmarks/bench_lof.py,37,loading and vectorization,
scikit-learn/benchmarks/bench_lof.py,49,we remove data with label 4,
scikit-learn/benchmarks/bench_lof.py,50,normal data are then those of class 1,
scikit-learn/benchmarks/bench_lof.py,60,normal data are those with attribute 2,
scikit-learn/benchmarks/bench_lof.py,61,abnormal those with attribute 4,
scikit-learn/benchmarks/bench_lof.py,93,"the lower, the more normal",
scikit-learn/benchmarks/bench_plot_incremental_pca.py,87,Compare runtimes and error for fixed batch size,
scikit-learn/benchmarks/bench_plot_incremental_pca.py,119,Create flat baselines to compare the variation over batch size,
scikit-learn/benchmarks/bench_plot_incremental_pca.py,140,limit dataset to 5000 people (don't care who they are!),
scikit-learn/benchmarks/bench_feature_expansions.py,23,CSR,
scikit-learn/benchmarks/bench_feature_expansions.py,27,Dense,
scikit-learn/benchmarks/bench_feature_expansions.py,32,densely dashdotdotted,
scikit-learn/benchmarks/bench_feature_expansions.py,33,solid,
scikit-learn/benchmarks/bench_plot_nmf.py,4,Authors: Tom Dupre la Tour (benchmark),
scikit-learn/benchmarks/bench_plot_nmf.py,5,Chih-Jen Linn (original projected gradient NMF implementation),
scikit-learn/benchmarks/bench_plot_nmf.py,6,"Anthony Di Franco (projected gradient, Python and NumPy port)",
scikit-learn/benchmarks/bench_plot_nmf.py,7,License: BSD 3 clause,
scikit-learn/benchmarks/bench_plot_nmf.py,33,,
scikit-learn/benchmarks/bench_plot_nmf.py,34,Start of _PGNMF,
scikit-learn/benchmarks/bench_plot_nmf.py,35,,
scikit-learn/benchmarks/bench_plot_nmf.py,36,This class implements a projected gradient solver for the NMF.,
scikit-learn/benchmarks/bench_plot_nmf.py,37,"The projected gradient solver was removed from scikit-learn in version 0.19,",
scikit-learn/benchmarks/bench_plot_nmf.py,38,and a simplified copy is used here for comparison purpose only.,
scikit-learn/benchmarks/bench_plot_nmf.py,39,"It is not tested, and it may change or disappear without notice.",
scikit-learn/benchmarks/bench_plot_nmf.py,103,values justified in the paper (alpha is renamed gamma),
scikit-learn/benchmarks/bench_plot_nmf.py,112,The following multiplication with a boolean array is more than twice,
scikit-learn/benchmarks/bench_plot_nmf.py,113,as fast as indexing into grad.,
scikit-learn/benchmarks/bench_plot_nmf.py,120,Gradient step.,
scikit-learn/benchmarks/bench_plot_nmf.py,122,Projection step.,
scikit-learn/benchmarks/bench_plot_nmf.py,159,"max(0.001, tol) to force alternating minimizations of W and H",
scikit-learn/benchmarks/bench_plot_nmf.py,164,stopping condition as discussed in paper,
scikit-learn/benchmarks/bench_plot_nmf.py,171,update W,
scikit-learn/benchmarks/bench_plot_nmf.py,179,update H,
scikit-learn/benchmarks/bench_plot_nmf.py,185,fix up negative zeros,
scikit-learn/benchmarks/bench_plot_nmf.py,251,"check W and H, or initialize them",
scikit-learn/benchmarks/bench_plot_nmf.py,262,fit_transform,
scikit-learn/benchmarks/bench_plot_nmf.py,266,transform,
scikit-learn/benchmarks/bench_plot_nmf.py,280,,
scikit-learn/benchmarks/bench_plot_nmf.py,281,End of _PGNMF,
scikit-learn/benchmarks/bench_plot_nmf.py,282,,
scikit-learn/benchmarks/bench_plot_nmf.py,313,use joblib to cache the results.,
scikit-learn/benchmarks/bench_plot_nmf.py,314,X_shape is specified in arguments for avoiding hashing X,
scikit-learn/benchmarks/bench_plot_nmf.py,356,"print(""loss: %.6f, time: %.3f sec"" % (this_loss, duration))",
scikit-learn/benchmarks/bench_plot_nmf.py,361,Use a panda dataframe to organize the results,
scikit-learn/benchmarks/bench_plot_nmf.py,366,plot the results,
scikit-learn/benchmarks/bench_plot_nmf.py,404,"first benchmark on 20 newsgroup dataset: sparse, shape(11314, 39116)",
scikit-learn/benchmarks/bench_plot_nmf.py,413,"second benchmark on Olivetti faces dataset: dense, shape(400, 4096)",
scikit-learn/benchmarks/bench_sgd_regression.py,1,Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>,
scikit-learn/benchmarks/bench_sgd_regression.py,2,License: BSD 3 clause,
scikit-learn/benchmarks/bench_sgd_regression.py,51,Shuffle data,
scikit-learn/benchmarks/bench_sgd_regression.py,112,Plot results,
scikit-learn/benchmarks/bench_plot_neighbors.py,39,------------------------------------------------------------,
scikit-learn/benchmarks/bench_plot_neighbors.py,40,varying N,
scikit-learn/benchmarks/bench_plot_neighbors.py,62,------------------------------------------------------------,
scikit-learn/benchmarks/bench_plot_neighbors.py,63,varying D,
scikit-learn/benchmarks/bench_plot_neighbors.py,85,------------------------------------------------------------,
scikit-learn/benchmarks/bench_plot_neighbors.py,86,varying k,
scikit-learn/benchmarks/bench_isolation_forest.py,47,Set this to true for plotting score histograms for each dataset:,
scikit-learn/benchmarks/bench_isolation_forest.py,50,"datasets available = ['http', 'smtp', 'SA', 'SF', 'shuttle', 'forestcover']",
scikit-learn/benchmarks/bench_isolation_forest.py,53,Loop over all datasets for fitting and scoring the estimator:,
scikit-learn/benchmarks/bench_isolation_forest.py,56,Loading and vectorizing the data:,
scikit-learn/benchmarks/bench_isolation_forest.py,70,we remove data with label 4,
scikit-learn/benchmarks/bench_isolation_forest.py,71,normal data are then those of class 1,
scikit-learn/benchmarks/bench_isolation_forest.py,82,normal data are those with attribute 2,
scikit-learn/benchmarks/bench_isolation_forest.py,83,abnormal those with attribute 4,
scikit-learn/benchmarks/bench_isolation_forest.py,128,"the lower, the more abnormal",
scikit-learn/benchmarks/bench_isolation_forest.py,141,Show ROC Curves,
scikit-learn/benchmarks/bench_isolation_forest.py,147,Print AUC score and train/test time:,
scikit-learn/benchmarks/bench_plot_lasso_path.py,35,"'effective_rank': None,",
scikit-learn/benchmarks/bench_plot_lasso_path.py,46,precomputed Gram matrix,
scikit-learn/benchmarks/bench_plot_lasso_path.py,84,register the 3d projection,
scikit-learn/benchmarks/bench_plot_lasso_path.py,101,plot the actual surface,
scikit-learn/benchmarks/bench_plot_lasso_path.py,104,dummy point plot to stick the legend to since surface plot do not,
scikit-learn/benchmarks/bench_plot_lasso_path.py,105,support legends (yet?),
scikit-learn/benchmarks/bench_plot_lasso_path.py,106,"ax.plot([1], [1], [1], color=c, label=label)",
scikit-learn/benchmarks/bench_plot_lasso_path.py,113,ax.legend(),
scikit-learn/benchmarks/bench_sparsify.py,61,sparsify input,
scikit-learn/benchmarks/bench_sparsify.py,66,sparsify coef,
scikit-learn/benchmarks/bench_sparsify.py,70,add noise,
scikit-learn/benchmarks/bench_sparsify.py,73,Split data in train set and test set,
scikit-learn/benchmarks/bench_sparsify.py,79,,
scikit-learn/benchmarks/bench_sample_without_replacement.py,19,number of microseconds in a second,
scikit-learn/benchmarks/bench_sample_without_replacement.py,26,start time,
scikit-learn/benchmarks/bench_sample_without_replacement.py,30,stop time,
scikit-learn/benchmarks/bench_sample_without_replacement.py,35,,
scikit-learn/benchmarks/bench_sample_without_replacement.py,36,Option parser,
scikit-learn/benchmarks/bench_sample_without_replacement.py,37,,
scikit-learn/benchmarks/bench_sample_without_replacement.py,62,"op.add_option(""--random-seed"",",
scikit-learn/benchmarks/bench_sample_without_replacement.py,63,"dest=""random_seed"", default=13, type=int,",
scikit-learn/benchmarks/bench_sample_without_replacement.py,64,"help=""Seed used by the random number generators."")",
scikit-learn/benchmarks/bench_sample_without_replacement.py,77,,
scikit-learn/benchmarks/bench_sample_without_replacement.py,78,List sampling algorithm,
scikit-learn/benchmarks/bench_sample_without_replacement.py,79,,
scikit-learn/benchmarks/bench_sample_without_replacement.py,80,We assume that sampling algorithm has the following signature:,
scikit-learn/benchmarks/bench_sample_without_replacement.py,81,"sample(n_population, n_sample)",
scikit-learn/benchmarks/bench_sample_without_replacement.py,82,,
scikit-learn/benchmarks/bench_sample_without_replacement.py,85,,
scikit-learn/benchmarks/bench_sample_without_replacement.py,86,Set Python core input,
scikit-learn/benchmarks/bench_sample_without_replacement.py,91,,
scikit-learn/benchmarks/bench_sample_without_replacement.py,92,Set custom automatic method selection,
scikit-learn/benchmarks/bench_sample_without_replacement.py,98,,
scikit-learn/benchmarks/bench_sample_without_replacement.py,99,Set custom tracking based method,
scikit-learn/benchmarks/bench_sample_without_replacement.py,107,,
scikit-learn/benchmarks/bench_sample_without_replacement.py,108,Set custom reservoir based method,
scikit-learn/benchmarks/bench_sample_without_replacement.py,116,,
scikit-learn/benchmarks/bench_sample_without_replacement.py,117,Set custom reservoir based method,
scikit-learn/benchmarks/bench_sample_without_replacement.py,125,,
scikit-learn/benchmarks/bench_sample_without_replacement.py,126,Numpy permutation based,
scikit-learn/benchmarks/bench_sample_without_replacement.py,131,,
scikit-learn/benchmarks/bench_sample_without_replacement.py,132,Remove unspecified algorithm,
scikit-learn/benchmarks/bench_sample_without_replacement.py,137,,
scikit-learn/benchmarks/bench_sample_without_replacement.py,138,Perform benchmark,
scikit-learn/benchmarks/bench_sample_without_replacement.py,139,,
scikit-learn/benchmarks/bench_sample_without_replacement.py,166,Print results,
scikit-learn/benchmarks/bench_sample_without_replacement.py,167,,
scikit-learn/benchmarks/bench_sample_without_replacement.py,195,Sort legend labels,
scikit-learn/benchmarks/bench_mnist.py,29,Author: Issam H. Laradji,
scikit-learn/benchmarks/bench_mnist.py,30,Arnaud Joly <arnaud.v.joly@gmail.com>,
scikit-learn/benchmarks/bench_mnist.py,31,License: BSD 3 clause,
scikit-learn/benchmarks/bench_mnist.py,54,Memoize the data extraction and memory map the resulting,
scikit-learn/benchmarks/bench_mnist.py,55,train / test splits in readonly mode,
scikit-learn/benchmarks/bench_mnist.py,63,,
scikit-learn/benchmarks/bench_mnist.py,64,Load dataset,
scikit-learn/benchmarks/bench_mnist.py,70,Normalize features,
scikit-learn/benchmarks/bench_mnist.py,73,"Create train-test split (as [Joachims, 2006])",
scikit-learn/benchmarks/bench_plot_parallel_pairwise.py,1,Author: Mathieu Blondel <mathieu@mblondel.org>,
scikit-learn/benchmarks/bench_plot_parallel_pairwise.py,2,License: BSD 3 clause,
scikit-learn/benchmarks/bench_glmnet.py,25,alpha = 0.01,
scikit-learn/benchmarks/bench_glmnet.py,35,start time,
scikit-learn/benchmarks/bench_glmnet.py,39,stop time,
scikit-learn/benchmarks/bench_glmnet.py,50,Delayed import of matplotlib.pyplot,
scikit-learn/benchmarks/bench_glmnet.py,89,now do a benchmark where the number of points is fixed,
scikit-learn/benchmarks/bench_glmnet.py,90,and the variable is the number of features,
scikit-learn/benchmarks/bench_hist_gradient_boosting_higgsboson.py,12,"To use this experimental feature, we need to explicitly ask for it:",
scikit-learn/benchmarks/bench_hist_gradient_boosting_higgsboson.py,13,noqa,
scikit-learn/benchmarks/bench_plot_randomized_svd.py,65,Author: Giorgio Patrini,
scikit-learn/benchmarks/bench_plot_randomized_svd.py,93,"If this is enabled, tests are much slower and will crash with the large data",
scikit-learn/benchmarks/bench_plot_randomized_svd.py,96,TODO: compute approximate spectral norms with the power method as in,
scikit-learn/benchmarks/bench_plot_randomized_svd.py,97,Estimating the largest eigenvalues by the power and Lanczos methods with,
scikit-learn/benchmarks/bench_plot_randomized_svd.py,98,"a random start, Jacek Kuczynski and Henryk Wozniakowski, SIAM Journal on",
scikit-learn/benchmarks/bench_plot_randomized_svd.py,99,"Matrix Analysis and Applications, 13 (4): 1094-1122, 1992.",
scikit-learn/benchmarks/bench_plot_randomized_svd.py,100,"This approximation is a very fast estimate of the spectral norm, but depends",
scikit-learn/benchmarks/bench_plot_randomized_svd.py,101,on starting random vectors.,
scikit-learn/benchmarks/bench_plot_randomized_svd.py,103,"Determine when to switch to batch computation for matrix norms,",
scikit-learn/benchmarks/bench_plot_randomized_svd.py,104,in case the reconstructed (dense) matrix is too large,
scikit-learn/benchmarks/bench_plot_randomized_svd.py,107,The following datasets can be downloaded manually from:,
scikit-learn/benchmarks/bench_plot_randomized_svd.py,108,CIFAR 10: https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz,
scikit-learn/benchmarks/bench_plot_randomized_svd.py,109,SVHN: http://ufldl.stanford.edu/housenumbers/train_32x32.mat,
scikit-learn/benchmarks/bench_plot_randomized_svd.py,251,There is a different convention for l here,
scikit-learn/benchmarks/bench_plot_randomized_svd.py,270,"s = sp.linalg.norm(A, ord=2)  # slow",
scikit-learn/benchmarks/bench_plot_randomized_svd.py,281,"if the input is not too big, just call scipy",
scikit-learn/benchmarks/bench_20newsgroups.py,29,,
scikit-learn/benchmarks/bench_20newsgroups.py,30,Data,
scikit-learn/benchmarks/bench_rcv1_logreg_convergence.py,1,Authors: Tom Dupre la Tour <tom.dupre-la-tour@m4x.org>,
scikit-learn/benchmarks/bench_rcv1_logreg_convergence.py,2,Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/benchmarks/bench_rcv1_logreg_convergence.py,3,,
scikit-learn/benchmarks/bench_rcv1_logreg_convergence.py,4,License: BSD 3 clause,
scikit-learn/benchmarks/bench_rcv1_logreg_convergence.py,24,compute logistic loss,
scikit-learn/benchmarks/bench_rcv1_logreg_convergence.py,34,We use joblib to cache individual fits. Note that we do not pass the dataset,
scikit-learn/benchmarks/bench_rcv1_logreg_convergence.py,35,"as argument as the hashing would be too slow, so we assume that the dataset",
scikit-learn/benchmarks/bench_rcv1_logreg_convergence.py,36,never changes.,
scikit-learn/benchmarks/bench_rcv1_logreg_convergence.py,150,consider the binary classification problem 'CCAT' vs the rest,
scikit-learn/benchmarks/bench_rcv1_logreg_convergence.py,155,parameters,
scikit-learn/benchmarks/bench_rcv1_logreg_convergence.py,160,max_iter range,
scikit-learn/benchmarks/bench_rcv1_logreg_convergence.py,199,compute the same step_size than in LR-sag,
scikit-learn/benchmarks/bench_rcv1_logreg_convergence.py,215,"We keep only 200 features, to have a dense dataset,",
scikit-learn/benchmarks/bench_rcv1_logreg_convergence.py,216,"and compare to lightning SAG, which seems incorrect in the sparse case.",
scikit-learn/benchmarks/bench_rcv1_logreg_convergence.py,224,Split training and testing. Switch train and test subset compared to,
scikit-learn/benchmarks/bench_rcv1_logreg_convergence.py,225,"LYRL2004 split, to have a larger training dataset.",
scikit-learn/doc/conf.py,1,-*- coding: utf-8 -*-,
scikit-learn/doc/conf.py,2,,
scikit-learn/doc/conf.py,3,"scikit-learn documentation build configuration file, created by",
scikit-learn/doc/conf.py,4,sphinx-quickstart on Fri Jan  8 09:13:42 2010.,
scikit-learn/doc/conf.py,5,,
scikit-learn/doc/conf.py,6,This file is execfile()d with the current directory set to its containing,
scikit-learn/doc/conf.py,7,dir.,
scikit-learn/doc/conf.py,8,,
scikit-learn/doc/conf.py,9,Note that not all possible configuration values are present in this,
scikit-learn/doc/conf.py,10,autogenerated file.,
scikit-learn/doc/conf.py,11,,
scikit-learn/doc/conf.py,12,All configuration values have a default; values that are commented out,
scikit-learn/doc/conf.py,13,serve to show the default.,
scikit-learn/doc/conf.py,21,If extensions (or modules to document with autodoc) are in another,
scikit-learn/doc/conf.py,22,"directory, add these directories to sys.path here. If the directory",
scikit-learn/doc/conf.py,23,"is relative to the documentation root, use os.path.abspath to make it",
scikit-learn/doc/conf.py,24,"absolute, like shown here.",
scikit-learn/doc/conf.py,30,-- General configuration ---------------------------------------------------,
scikit-learn/doc/conf.py,32,"Add any Sphinx extension module names here, as strings. They can be",
scikit-learn/doc/conf.py,33,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom ones.,
scikit-learn/doc/conf.py,44,this is needed for some reason...,
scikit-learn/doc/conf.py,45,see https://github.com/numpy/numpydoc/issues/69,
scikit-learn/doc/conf.py,49,"For maths, use mathjax by default and svg if NO_MATHJAX env variable is set",
scikit-learn/doc/conf.py,50,(useful for viewing the doc offline),
scikit-learn/doc/conf.py,65,"Add any paths that contain templates here, relative to this directory.",
scikit-learn/doc/conf.py,68,generate autosummary even if no references,
scikit-learn/doc/conf.py,71,The suffix of source filenames.,
scikit-learn/doc/conf.py,74,The encoding of source files.,
scikit-learn/doc/conf.py,75,source_encoding = 'utf-8',
scikit-learn/doc/conf.py,77,The master toctree document.,
scikit-learn/doc/conf.py,80,General information about the project.,
scikit-learn/doc/conf.py,84,"The version info for the project you're documenting, acts as replacement for",
scikit-learn/doc/conf.py,85,"|version| and |release|, also used in various other places throughout the",
scikit-learn/doc/conf.py,86,built documents.,
scikit-learn/doc/conf.py,87,,
scikit-learn/doc/conf.py,88,The short X.Y version.,
scikit-learn/doc/conf.py,92,"The full version, including alpha/beta/rc tags.",
scikit-learn/doc/conf.py,93,Removes post from release name,
scikit-learn/doc/conf.py,99,The language for content autogenerated by Sphinx. Refer to documentation,
scikit-learn/doc/conf.py,100,for a list of supported languages.,
scikit-learn/doc/conf.py,101,language = None,
scikit-learn/doc/conf.py,103,"There are two options for replacing |today|: either, you set today to some",
scikit-learn/doc/conf.py,104,"non-false value, then it is used:",
scikit-learn/doc/conf.py,105,today = '',
scikit-learn/doc/conf.py,106,"Else, today_fmt is used as the format for a strftime call.",
scikit-learn/doc/conf.py,107,"today_fmt = '%B %d, %Y'",
scikit-learn/doc/conf.py,109,"List of patterns, relative to source directory, that match files and",
scikit-learn/doc/conf.py,110,directories to ignore when looking for source files.,
scikit-learn/doc/conf.py,113,The reST default role (used for this markup: `text`) to use for all,
scikit-learn/doc/conf.py,114,documents.,
scikit-learn/doc/conf.py,117,"If true, '()' will be appended to :func: etc. cross-reference text.",
scikit-learn/doc/conf.py,120,"If true, the current module name will be prepended to all description",
scikit-learn/doc/conf.py,121,unit titles (such as .. function::).,
scikit-learn/doc/conf.py,122,add_module_names = True,
scikit-learn/doc/conf.py,124,"If true, sectionauthor and moduleauthor directives will be shown in the",
scikit-learn/doc/conf.py,125,output. They are ignored by default.,
scikit-learn/doc/conf.py,126,show_authors = False,
scikit-learn/doc/conf.py,128,The name of the Pygments (syntax highlighting) style to use.,
scikit-learn/doc/conf.py,131,A list of ignored prefixes for module index sorting.,
scikit-learn/doc/conf.py,132,modindex_common_prefix = [],
scikit-learn/doc/conf.py,135,-- Options for HTML output -------------------------------------------------,
scikit-learn/doc/conf.py,137,The theme to use for HTML and HTML Help pages.  Major themes that come with,
scikit-learn/doc/conf.py,138,Sphinx are currently 'default' and 'sphinxdoc'.,
scikit-learn/doc/conf.py,141,Theme options are theme-specific and customize the look and feel of a theme,
scikit-learn/doc/conf.py,142,"further.  For a list of options available for each theme, see the",
scikit-learn/doc/conf.py,143,documentation.,
scikit-learn/doc/conf.py,147,"Add any paths that contain custom themes here, relative to this directory.",
scikit-learn/doc/conf.py,151,"The name for this set of Sphinx documents.  If None, it defaults to",
scikit-learn/doc/conf.py,152,"""<project> v<release> documentation"".",
scikit-learn/doc/conf.py,153,html_title = None,
scikit-learn/doc/conf.py,155,A shorter title for the navigation bar.  Default is the same as html_title.,
scikit-learn/doc/conf.py,158,The name of an image file (relative to this directory) to place at the top,
scikit-learn/doc/conf.py,159,of the sidebar.,
scikit-learn/doc/conf.py,162,The name of an image file (within the static path) to use as favicon of the,
scikit-learn/doc/conf.py,163,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32,
scikit-learn/doc/conf.py,164,pixels large.,
scikit-learn/doc/conf.py,167,"Add any paths that contain custom static files (such as style sheets) here,",
scikit-learn/doc/conf.py,168,"relative to this directory. They are copied after the builtin static files,",
scikit-learn/doc/conf.py,169,"so a file named ""default.css"" will overwrite the builtin ""default.css"".",
scikit-learn/doc/conf.py,172,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,",
scikit-learn/doc/conf.py,173,using the given strftime format.,
scikit-learn/doc/conf.py,174,"html_last_updated_fmt = '%b %d, %Y'",
scikit-learn/doc/conf.py,176,"Custom sidebar templates, maps document names to template names.",
scikit-learn/doc/conf.py,177,html_sidebars = {},
scikit-learn/doc/conf.py,179,"Additional templates that should be rendered to pages, maps page names to",
scikit-learn/doc/conf.py,180,template names.,
scikit-learn/doc/conf.py,183,redirects to index,
scikit-learn/doc/conf.py,185,"If false, no module index is generated.",
scikit-learn/doc/conf.py,188,"If false, no index is generated.",
scikit-learn/doc/conf.py,191,"If true, the index is split into individual pages for each letter.",
scikit-learn/doc/conf.py,192,html_split_index = False,
scikit-learn/doc/conf.py,194,"If true, links to the reST sources are added to the pages.",
scikit-learn/doc/conf.py,195,html_show_sourcelink = True,
scikit-learn/doc/conf.py,197,"If true, an OpenSearch description file will be output, and all pages will",
scikit-learn/doc/conf.py,198,contain a <link> tag referring to it.  The value of this option must be the,
scikit-learn/doc/conf.py,199,base URL from which the finished HTML is served.,
scikit-learn/doc/conf.py,200,html_use_opensearch = '',
scikit-learn/doc/conf.py,202,"If nonempty, this is the file name suffix for HTML files (e.g. "".xhtml"").",
scikit-learn/doc/conf.py,203,html_file_suffix = '',
scikit-learn/doc/conf.py,205,Output file base name for HTML help builder.,
scikit-learn/doc/conf.py,208,"If true, the reST sources are included in the HTML build as _sources/name.",
scikit-learn/doc/conf.py,211,-- Options for LaTeX output ------------------------------------------------,
scikit-learn/doc/conf.py,213,The paper size ('letterpaper' or 'a4paper').,
scikit-learn/doc/conf.py,214,"'papersize': 'letterpaper',",
scikit-learn/doc/conf.py,216,"The font size ('10pt', '11pt' or '12pt').",
scikit-learn/doc/conf.py,217,"'pointsize': '10pt',",
scikit-learn/doc/conf.py,219,Additional stuff for the LaTeX preamble.,
scikit-learn/doc/conf.py,226,Grouping the document tree into LaTeX files. List of tuples,
scikit-learn/doc/conf.py,227,"(source start file, target name, title, author, documentclass",
scikit-learn/doc/conf.py,228,[howto/manual]).,
scikit-learn/doc/conf.py,232,The name of an image file (relative to this directory) to place at the top of,
scikit-learn/doc/conf.py,233,the title page.,
scikit-learn/doc/conf.py,236,Documents to append as an appendix to all manuals.,
scikit-learn/doc/conf.py,237,latex_appendices = [],
scikit-learn/doc/conf.py,239,"If false, no module index is generated.",
scikit-learn/doc/conf.py,244,intersphinx configuration,
scikit-learn/doc/conf.py,315,avoid generating too many cross links,
scikit-learn/doc/conf.py,320,The following dictionary contains the information used to create the,
scikit-learn/doc/conf.py,321,thumbnails for the front page of the scikit-learn home page.,
scikit-learn/doc/conf.py,322,key: first image in set,
scikit-learn/doc/conf.py,323,"values: (number of plot in set, height of thumbnail)",
scikit-learn/doc/conf.py,327,enable experimental module so that experimental estimators can be,
scikit-learn/doc/conf.py,328,discovered properly by sphinx,
scikit-learn/doc/conf.py,329,noqa,
scikit-learn/doc/conf.py,330,noqa,
scikit-learn/doc/conf.py,351,searchindex only exist when generating html,
scikit-learn/doc/conf.py,368,Config for sphinx_issues,
scikit-learn/doc/conf.py,370,we use the issues path for PRs since the issues URL will forward,
scikit-learn/doc/conf.py,375,to hide/show the prompt in code examples:,
scikit-learn/doc/conf.py,380,The following is used by sphinx.ext.linkcode to provide links to github,
scikit-learn/doc/conf.py,390,Reduces the output of estimators,
scikit-learn/doc/conftest.py,24,skip the test in rcv1.rst if the dataset is not already loaded,
scikit-learn/doc/conftest.py,48,noqa,
scikit-learn/doc/conftest.py,55,noqa,
scikit-learn/doc/conftest.py,62,noqa,
scikit-learn/doc/conftest.py,66,ignore deprecation warnings from scipy.misc.face,
scikit-learn/doc/tutorial/machine_learning_map/svg2imagemap.py,1,!/usr/local/bin/python,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1,module pyparsing.py,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,2,,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3,Copyright (c) 2003-2016  Paul T. McGuire,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4,,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,5,"Permission is hereby granted, free of charge, to any person obtaining",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,6,a copy of this software and associated documentation files (the,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,7,"""Software""), to deal in the Software without restriction, including",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,8,"without limitation the rights to use, copy, modify, merge, publish,",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,9,"distribute, sublicense, and/or sell copies of the Software, and to",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,10,"permit persons to whom the Software is furnished to do so, subject to",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,11,the following conditions:,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,12,,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,13,The above copyright notice and this permission notice shall be,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,14,included in all copies or substantial portions of the Software.,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,15,,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,16,"THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND,",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,17,"EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,18,"MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,19,IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,20,"CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,21,"TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,22,SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,23,,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,24,flake8: noqa,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,94,"~ sys.stderr.write( ""testing pyparsing module, version %s, %s\n"" % (__version__,__versionTime__ ) )",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,125,"build list of single arg builtins, that can be used as parse actions",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,141,"If this works, then _ustr(obj) has the same behaviour as str(obj), so",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,142,it won't break any existing code.,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,146,Else encode it,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,152,"build list of single arg builtins, tolerant of Python version, that can be used as parse actions",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,166,ampersand must be replaced first,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,185,"Performance tuning: we construct a *lot* of these, so keep this",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,186,constructor as small and fast as possible,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,271,~ class ReparseException(ParseBaseException):,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,272,"~ """"""Experimental class - parse actions can raise this exception to cause",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,273,~ pyparsing to reparse the input string:,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,274,"~ - with a modified input string, and/or",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,275,~ - with a modified start location,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,276,"~ Set the values of the ReparseException in the constructor, and raise the",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,277,~ exception in a parse action to cause pyparsing to use the new string/location.,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,278,~ Setting the values as None causes no change to be made.,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,279,"~ """"""",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,280,"~ def __init_( self, newstring, restartLoc ):",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,281,~ self.newParseText = newstring,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,282,~ self.reparseLoc = restartLoc,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,348,"Performance tuning: we construct a *lot* of these, so keep this",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,349,constructor as small and fast as possible,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,372,"will always return a str, but use _ustr for consistency",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,416,convert int to slice,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,421,get removed indices,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,424,fixup indices in token dictionary,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,578,fixup indices in token dictionary,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,659,useful for merging many ParseResults using sum() builtin,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,662,this may raise a TypeError - so be it,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,754,collapse out indents if formatting is not desired,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,788,"individual token, see if there is a name for it",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,917,add support for pickle protocol,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,994,Only works on Python 3.x - nonlocal is toxic to Python 2 installs,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,995,~ 'decorator to trim function calls to match the arity of the target',
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,996,"~ def _trim_arity(func, maxargs=3):",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,997,~ if func in singleArgBuiltins:,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,998,"~ return lambda s,l,t: func(t)",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,999,~ limit = 0,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1000,~ foundArity = False,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1001,~ def wrapper(*args):,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1002,"~ nonlocal limit,foundArity",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1003,~ while 1:,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1004,~ try:,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1005,~ ret = func(*args[limit:]),
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1006,~ foundArity = True,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1007,~ return ret,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1008,~ except TypeError:,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1009,~ if limit == maxargs or foundArity:,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1010,~ raise,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1011,~ limit += 1,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1012,~ continue,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1013,~ return wrapper,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1015,this version is Python 2.x-3.x cross-compatible,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1032,synthesize what would be returned by traceback.extract_stack at the call to,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1033,"user's parse action 'func', so that we don't incur call penalty at parse time",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1036,"IF ANY CODE CHANGES, EVEN JUST COMMENTS OR BLANK LINES, BETWEEN THE NEXT LINE AND",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1037,"THE CALL TO FUNC INSIDE WRAPPER, LINE_DIFF MUST BE MODIFIED!!!!",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1048,re-raise TypeErrors if they did not come from our arity testing,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1064,copy func name to wrapper for sensible debug output,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1119,"~ self.name = ""<unknown>""  # don't define self.name, let subclasses try/except upcall",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1126,used when checking for left-recursion,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1131,used to optimize exception handling for subclasses that don't advance parse index,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1133,used to mark results names as modal (report only last) or cumulative (list all),
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1134,custom debug actions,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1136,used to avoid redundant calls to preParse,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1341,~ @profile,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1343,and doActions ),
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1346,"~ print (""Match"",self,""at loc"",loc,""(%d,%d)"" % ( lineno(loc,instring), col(loc,instring) ))",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1360,"~ print (""Exception raised:"", err)",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1394,"~ print ""Exception raised in user parse action:"", err",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1408,"~ print (""Matched"",self,""->"",retTokens.asList())",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1508,argument cache for optimizing repeated calls when backtracking through recursive expressions,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1509,this is set later by enabledPackrat(); this is here so that resetCache() doesn't fail,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1513,this method gets repeatedly called during backtracking with the same arguments -,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1514,we can cache these arguments and save ourselves the trouble of re-parsing the contained expression,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1526,"cache a copy of the exception, without the traceback",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1611,~ self.saveAsList = True,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1626,"catch and re-raise exception from here, clears out pyparsing internal stack trace",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1699,"catch and re-raise exception from here, clears out pyparsing internal stack trace",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1721,"force preservation of <TAB>s, to minimize unwanted transformation of string, and to",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1722,keep string locs straight between transformString and scanString,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1742,"catch and re-raise exception from here, clears out pyparsing internal stack trace",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1769,"catch and re-raise exception from here, clears out pyparsing internal stack trace",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,2163,"catch and re-raise exception from here, clears out pyparsing internal stack trace",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,2400,Performance tuning: this routine gets called a *lot*,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,2401,"if this is a single character match string  and the first character matches,",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,2402,"short-circuit as quickly as possible, and avoid calling startswith",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,2403,~ @profile,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,2490,Preserve the defining literal.,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,2839,remove white space from quote chars - wont work anyway,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,2911,strip off quotes,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,2915,replace escaped whitespace,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,2926,replace escaped characters,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,2930,replace escaped quotes,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3038,~ self.leaveWhitespace(),
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3163,see if entire string up to here is just whitespace and ignoreables,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3242,"if sequence of strings provided, wrap with Literal",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3298,"collapse nested And's of the form And( And( And( a,b), c), d) to And( a,b,c,d )",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3299,but only if there are no parse actions or resultsNames on the nested And's,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3300,(likewise for Or's and MatchFirst's),
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3371,"pass False as last arg to _parse for first element, since we already",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3372,pre-parsed the string as part of our And pre-parsing,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3398,"And( [ self, other ] )",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3455,"save match among all matches, to retry longest to shortest",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3479,"Or( [ self, other ] )",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3536,"only got here if no expression matched, raise exception for match that made it the furthest",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3547,"MatchFirst( [ self, other ] )",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3662,"add any unmatched Optionals, in case they have default values defined",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3804,~ self.leaveWhitespace(),
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3805,"do NOT use self.leaveWhitespace(), don't want to propagate to exprs",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3839,must be at least one (but first see if we are the stopOn sentinel;,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3840,"if so, fail)",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4078,break if failOn expression matches,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4083,advance past ignore expressions,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4093,"no match, advance loc in string",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4096,"matched skipto expr, done",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4100,"ran off the end of the input string without matching skipto expr, fail",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4103,build up return values,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4175,stubbed out for now - creates awful memory and perf issues,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4204,", savelist )",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4226,"suppress whitespace-stripping in contained parse expressions, but re-enable it on the Combine itself",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4324,ParseResults(i),
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4420,,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4421,global helpers,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4422,,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4501,flatten t tokens,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4539,~  escape these chars: ^-],
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4605,"~ print (strs,""->"", ""|"".join( [ _escapeRegexChars(sym) for sym in symbols] ))",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4616,"last resort, just use MatchFirst",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4722,convenience constants for positional expressions,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,5095,try to avoid LR with this extra test,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,5326,~ FollowedBy(blockStatementExpr) +,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,5344,"it's easy to get these comment structures wrong - they're very common, so may as well make them available",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,5371,some other useful expressions - using lower-case class name since we are really using this as a namespace,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,5541,streamlining this expression makes the docs nicer-looking,
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,5662,"demo runTests method, including embedded comments in test string",
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,5696,"any int or real number, returned as float",
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,1,!/usr/local/bin/python,
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,16,ParserElement.enablePackrat(),
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,40,"~ raise ParseException( instring, loc, self.errmsg )",
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,62,"note that almost all these fields are optional,",
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,63,and this can match almost anything. We rely on Pythons built-in,
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,64,float() function to clear out invalid values - loosely matching like this,
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,65,speeds up parsing quite a lot,
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,77,same as FP constant but don't allow a - sign,
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,87,comma or whitespace can separate values all over the place in SVG,
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,101,commands,
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,112,rx,
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,113,ry,
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,119,"rx, ry",
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,120,rotation,
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,121,"large-arc-flag, sweep-flag",
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,122,"(x,y)",
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,133,"curve = Group(Command(""C"") + Arguments(coordinatePairTripleSequence))",
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,143,~ number.debug = True,
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py,9,Author: Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py,10,License: Simplified BSD,
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py,22,The training data folder must be passed as first argument,
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py,26,Split the dataset in training and test set:,
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py,31,TASK: Build a vectorizer that splits strings into sequence of 1 to 3,
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py,32,characters instead of word tokens,
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py,36,TASK: Build a vectorizer / classifier pipeline using the previous analyzer,
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py,37,the pipeline instance should stored in a variable named clf,
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py,43,TASK: Fit the pipeline on the training set,
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py,46,TASK: Predict the outcome on the testing set in a variable named y_predicted,
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py,49,Print the classification report,
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py,53,Plot the confusion matrix,
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py,57,import matlotlib.pyplot as plt,
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py,58,"plt.matshow(cm, cmap=plt.cm.jet)",
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py,59,plt.show(),
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py,61,Predict the result on some short new sentences:,
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,11,Author: Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,12,License: Simplified BSD,
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,25,"NOTE: we put the following in a 'if __name__ == ""__main__""' protected",
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,26,block to be able to use a multi-core grid search that also works under,
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,27,"Windows, see: http://docs.python.org/library/multiprocessing.html#windows",
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,28,The multiprocessing module is used as the backend of joblib.Parallel,
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,29,that is used when n_jobs != 1 in GridSearchCV,
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,31,the training data folder must be passed as first argument,
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,36,split the dataset in training and test set:,
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,40,TASK: Build a vectorizer / classifier pipeline that filters out tokens,
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,41,that are too rare or too frequent,
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,47,TASK: Build a grid search to find out whether unigrams or bigrams are,
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,48,more useful.,
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,49,Fit the pipeline on the training set using grid search for the parameters,
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,56,TASK: print the mean and std for each candidate along with the parameter,
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,57,settings for all the candidates explored by grid search.,
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,65,TASK: Predict the outcome on the testing set and store it in a variable,
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,66,named y_predicted,
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,69,Print the classification report,
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,73,Print and plot the confusion matrix,
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,77,import matplotlib.pyplot as plt,
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,78,plt.matshow(cm),
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,79,plt.show(),
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py,9,Author: Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py,10,License: Simplified BSD,
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py,22,The training data folder must be passed as first argument,
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py,26,Split the dataset in training and test set:,
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py,31,TASK: Build a vectorizer that splits strings into sequence of 1 to 3,
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py,32,characters instead of word tokens,
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py,34,TASK: Build a vectorizer / classifier pipeline using the previous analyzer,
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py,35,the pipeline instance should stored in a variable named clf,
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py,37,TASK: Fit the pipeline on the training set,
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py,39,TASK: Predict the outcome on the testing set in a variable named y_predicted,
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py,41,Print the classification report,
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py,45,Plot the confusion matrix,
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py,49,import matplotlib.pyplot as plt,
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py,50,"plt.matshow(cm, cmap=plt.cm.jet)",
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py,51,plt.show(),
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py,53,Predict the result on some short new sentences:,
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,11,Author: Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,12,License: Simplified BSD,
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,25,"NOTE: we put the following in a 'if __name__ == ""__main__""' protected",
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,26,block to be able to use a multi-core grid search that also works under,
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,27,"Windows, see: http://docs.python.org/library/multiprocessing.html#windows",
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,28,The multiprocessing module is used as the backend of joblib.Parallel,
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,29,that is used when n_jobs != 1 in GridSearchCV,
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,31,the training data folder must be passed as first argument,
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,36,split the dataset in training and test set:,
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,40,TASK: Build a vectorizer / classifier pipeline that filters out tokens,
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,41,that are too rare or too frequent,
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,43,TASK: Build a grid search to find out whether unigrams or bigrams are,
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,44,more useful.,
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,45,Fit the pipeline on the training set using grid search for the parameters,
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,47,TASK: print the cross-validated scores for the each parameters set,
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,48,explored by the grid search,
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,50,TASK: Predict the outcome on the testing set and store it in a variable,
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,51,named y_predicted,
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,53,Print the classification report,
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,57,Print and plot the confusion matrix,
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,61,import matplotlib.pyplot as plt,
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,62,plt.matshow(cm),
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,63,plt.show(),
scikit-learn/doc/tutorial/text_analytics/data/languages/fetch_data.py,2,simple python script to collect text paragraphs from various languages on the,
scikit-learn/doc/tutorial/text_analytics/data/languages/fetch_data.py,3,same topic namely the Wikipedia encyclopedia itself,
scikit-learn/doc/tutorial/text_analytics/data/languages/fetch_data.py,15,noqa: E501,
scikit-learn/doc/tutorial/text_analytics/data/languages/fetch_data.py,25,noqa: E501,
scikit-learn/doc/tutorial/text_analytics/data/languages/fetch_data.py,26,"u'zh': u'http://zh.wikipedia.org/wiki/Wikipedia',",
scikit-learn/doc/tutorial/text_analytics/data/languages/fetch_data.py,53,change the User Agent to avoid being blocked by Wikipedia,
scikit-learn/doc/tutorial/text_analytics/data/languages/fetch_data.py,54,downloading a couple of articles should not be considered abusive,
scikit-learn/doc/tutorial/text_analytics/data/languages/fetch_data.py,59,decode the payload explicitly as UTF-8 since lxml is confused for some,
scikit-learn/doc/tutorial/text_analytics/data/languages/fetch_data.py,60,reason,
scikit-learn/doc/tutorial/text_analytics/data/languages/fetch_data.py,69,skip paragraphs that are too short - probably too noisy and not,
scikit-learn/doc/tutorial/text_analytics/data/languages/fetch_data.py,70,representative of the actual language,
scikit-learn/doc/tutorial/text_analytics/data/languages/fetch_data.py,79,split the paragraph into fake smaller paragraphs to make the,
scikit-learn/doc/tutorial/text_analytics/data/languages/fetch_data.py,80,problem harder e.g. more similar to tweets,
scikit-learn/doc/tutorial/text_analytics/data/languages/fetch_data.py,82,FIXME: whitespace tokenizing does not work on chinese and japanese,
scikit-learn/doc/sphinxext/github_link.py,44,Python 2 only,
scikit-learn/doc/sphinxext/custom_references_resolver.py,48,process 'py' domain first for python classes,
scikit-learn/doc/sphinxext/custom_references_resolver.py,57,resolve :term:,
scikit-learn/doc/sphinxext/custom_references_resolver.py,61,replace literal nodes with inline nodes,
scikit-learn/doc/sphinxext/custom_references_resolver.py,70,"next, do the standard domain",
scikit-learn/doc/sphinxext/custom_references_resolver.py,83,the domain doesn't yet support the new interface,
scikit-learn/doc/sphinxext/custom_references_resolver.py,84,we have to manually collect possible references (SLOW),
scikit-learn/doc/sphinxext/custom_references_resolver.py,93,no results considered to be <code>,
scikit-learn/doc/sphinxext/custom_references_resolver.py,99,"Override ""any"" class with the actual role type to get the styling",
scikit-learn/doc/sphinxext/custom_references_resolver.py,100,approximately correct.,
scikit-learn/doc/sphinxext/custom_references_resolver.py,114,Support sphinx 1.6.*,
scikit-learn/doc/sphinxext/sphinx_issues.py,1,-*- coding: utf-8 -*-,
scikit-learn/doc/sphinxext/sphinx_issues.py,98,External repo,
scikit-learn/doc/sphinxext/sphinx_issues.py,195,Format template for issues URI,
scikit-learn/doc/sphinxext/sphinx_issues.py,196,e.g. 'https://github.com/sloria/marshmallow/issues/{issue},
scikit-learn/doc/sphinxext/sphinx_issues.py,198,Format template for PR URI,
scikit-learn/doc/sphinxext/sphinx_issues.py,199,e.g. 'https://github.com/sloria/marshmallow/pull/{issue},
scikit-learn/doc/sphinxext/sphinx_issues.py,201,Format template for commit URI,
scikit-learn/doc/sphinxext/sphinx_issues.py,202,e.g. 'https://github.com/sloria/marshmallow/commits/{commit},
scikit-learn/doc/sphinxext/sphinx_issues.py,204,"Shortcut for Github, e.g. 'sloria/marshmallow'",
scikit-learn/doc/sphinxext/sphinx_issues.py,206,Format template for user profile URI,
scikit-learn/doc/sphinxext/sphinx_issues.py,207,e.g. 'https://github.com/{user}',
scikit-learn/build_tools/generate_authors_table.py,40,get members of scikit-learn core-dev on GitHub,
scikit-learn/build_tools/generate_authors_table.py,43,30 per page,
scikit-learn/build_tools/generate_authors_table.py,48,get members of scikit-learn on GitHub,
scikit-learn/build_tools/generate_authors_table.py,50,30 per page,
scikit-learn/build_tools/generate_authors_table.py,56,keep only the logins,
scikit-learn/build_tools/generate_authors_table.py,60,add missing contributors with GitHub accounts,
scikit-learn/build_tools/generate_authors_table.py,62,add missing contributors without GitHub accounts,
scikit-learn/build_tools/generate_authors_table.py,64,remove CI bots,
scikit-learn/build_tools/generate_authors_table.py,69,"remove duplicate, and get the difference of the two sets",
scikit-learn/build_tools/generate_authors_table.py,74,get profiles from GitHub,
scikit-learn/build_tools/generate_authors_table.py,78,sort by last name,
scikit-learn/build_tools/generate_authors_table.py,96,fix missing names,
scikit-learn/build_tools/circle/list_versions.py,1,!/usr/bin/env python3,
scikit-learn/build_tools/circle/list_versions.py,3,List all available versions of the documentation,
scikit-learn/build_tools/circle/list_versions.py,20,https://stackoverflow.com/questions/1094841/reusable-library-to-get-human-readable-version-of-file-size,
scikit-learn/build_tools/circle/list_versions.py,51,noqa,
scikit-learn/build_tools/circle/list_versions.py,52,noqa,
scikit-learn/build_tools/circle/list_versions.py,56,"Gather data for each version directory, including symlinks",
scikit-learn/build_tools/circle/list_versions.py,74,Symlinks should have same data as target,
scikit-learn/build_tools/circle/list_versions.py,79,"Output in order: dev, stable, decreasing other version",
scikit-learn/build_tools/circle/list_versions.py,86,symlink came first,
scikit-learn/examples/plot_kernel_approximation.py,32,,
scikit-learn/examples/plot_kernel_approximation.py,33,"Python package and dataset imports, load dataset",
scikit-learn/examples/plot_kernel_approximation.py,34,---------------------------------------------------,
scikit-learn/examples/plot_kernel_approximation.py,37,Author: Gael Varoquaux <gael dot varoquaux at normalesup dot org>,
scikit-learn/examples/plot_kernel_approximation.py,38,Andreas Mueller <amueller@ais.uni-bonn.de>,
scikit-learn/examples/plot_kernel_approximation.py,39,License: BSD 3 clause,
scikit-learn/examples/plot_kernel_approximation.py,43,Standard scientific Python imports,
scikit-learn/examples/plot_kernel_approximation.py,48,"Import datasets, classifiers and performance metrics",
scikit-learn/examples/plot_kernel_approximation.py,54,The digits dataset,
scikit-learn/examples/plot_kernel_approximation.py,58,,
scikit-learn/examples/plot_kernel_approximation.py,59,Timing and accuracy plots,
scikit-learn/examples/plot_kernel_approximation.py,60,--------------------------------------------------,
scikit-learn/examples/plot_kernel_approximation.py,61,"To apply an classifier on this data, we need to flatten the image, to",
scikit-learn/examples/plot_kernel_approximation.py,62,"turn the data in a (samples, feature) matrix:",
scikit-learn/examples/plot_kernel_approximation.py,67,We learn the digits on the first half of the digits,
scikit-learn/examples/plot_kernel_approximation.py,72,Now predict the value of the digit on the second half:,
scikit-learn/examples/plot_kernel_approximation.py,75,data_test = scaler.transform(data_test),
scikit-learn/examples/plot_kernel_approximation.py,77,Create a classifier: a support vector classifier,
scikit-learn/examples/plot_kernel_approximation.py,81,create pipeline from kernel approximation,
scikit-learn/examples/plot_kernel_approximation.py,82,and linear svm,
scikit-learn/examples/plot_kernel_approximation.py,91,fit and predict using linear and kernel svm:,
scikit-learn/examples/plot_kernel_approximation.py,125,plot the results:,
scikit-learn/examples/plot_kernel_approximation.py,128,second y axis for timings,
scikit-learn/examples/plot_kernel_approximation.py,139,horizontal lines for exact rbf and linear kernels:,
scikit-learn/examples/plot_kernel_approximation.py,150,vertical line for dataset dimensionality = 64,
scikit-learn/examples/plot_kernel_approximation.py,153,legends and labels,
scikit-learn/examples/plot_kernel_approximation.py,168,,
scikit-learn/examples/plot_kernel_approximation.py,169,Decision Surfaces of RBF Kernel SVM and Linear SVM,
scikit-learn/examples/plot_kernel_approximation.py,170,--------------------------------------------------------,
scikit-learn/examples/plot_kernel_approximation.py,171,The second plot visualized the decision surfaces of the RBF kernel SVM and,
scikit-learn/examples/plot_kernel_approximation.py,172,the linear SVM with approximate kernel maps.,
scikit-learn/examples/plot_kernel_approximation.py,173,The plot shows decision surfaces of the classifiers projected onto,
scikit-learn/examples/plot_kernel_approximation.py,174,the first two principal components of the data. This visualization should,
scikit-learn/examples/plot_kernel_approximation.py,175,be taken with a grain of salt since it is just an interesting slice through,
scikit-learn/examples/plot_kernel_approximation.py,176,the decision surface in 64 dimensions. In particular note that,
scikit-learn/examples/plot_kernel_approximation.py,177,a datapoint (represented as a dot) does not necessarily be classified,
scikit-learn/examples/plot_kernel_approximation.py,178,"into the region it is lying in, since it will not lie on the plane",
scikit-learn/examples/plot_kernel_approximation.py,179,that the first two principal components span.,
scikit-learn/examples/plot_kernel_approximation.py,180,The usage of :class:`RBFSampler` and :class:`Nystroem` is described in detail,
scikit-learn/examples/plot_kernel_approximation.py,181,in :ref:`kernel_approximation`.,
scikit-learn/examples/plot_kernel_approximation.py,183,"visualize the decision surface, projected down to the first",
scikit-learn/examples/plot_kernel_approximation.py,184,two principal components of the dataset,
scikit-learn/examples/plot_kernel_approximation.py,189,Generate grid along first two principal components,
scikit-learn/examples/plot_kernel_approximation.py,191,steps along first component,
scikit-learn/examples/plot_kernel_approximation.py,193,steps along second component,
scikit-learn/examples/plot_kernel_approximation.py,195,combine,
scikit-learn/examples/plot_kernel_approximation.py,199,title for the plots,
scikit-learn/examples/plot_kernel_approximation.py,208,predict and plot,
scikit-learn/examples/plot_kernel_approximation.py,211,"Plot the decision boundary. For that, we will assign a color to each",
scikit-learn/examples/plot_kernel_approximation.py,212,"point in the mesh [x_min, x_max]x[y_min, y_max].",
scikit-learn/examples/plot_kernel_approximation.py,216,Put the result into a color plot,
scikit-learn/examples/plot_kernel_approximation.py,221,Plot also the training points,
scikit-learn/examples/plot_changed_only_pprint_parameter.py,21,"LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,",
scikit-learn/examples/plot_changed_only_pprint_parameter.py,22,"intercept_scaling=1, l1_ratio=None, max_iter=100,",
scikit-learn/examples/plot_changed_only_pprint_parameter.py,23,"multi_class='auto', n_jobs=None, penalty='l1',",
scikit-learn/examples/plot_changed_only_pprint_parameter.py,24,"random_state=None, solver='warn', tol=0.0001, verbose=0,",
scikit-learn/examples/plot_changed_only_pprint_parameter.py,25,warm_start=False),
scikit-learn/examples/plot_changed_only_pprint_parameter.py,30,LogisticRegression(penalty='l1'),
scikit-learn/examples/plot_isotonic_regression.py,16,Author: Nelle Varoquaux <nelle.varoquaux@gmail.com>,
scikit-learn/examples/plot_isotonic_regression.py,17,Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/examples/plot_isotonic_regression.py,18,License: BSD,
scikit-learn/examples/plot_isotonic_regression.py,33,,
scikit-learn/examples/plot_isotonic_regression.py,34,Fit IsotonicRegression and LinearRegression models,
scikit-learn/examples/plot_isotonic_regression.py,41,x needs to be 2d for LinearRegression,
scikit-learn/examples/plot_isotonic_regression.py,43,,
scikit-learn/examples/plot_isotonic_regression.py,44,Plot result,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,28,,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,29,Train models on the diabetes dataset,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,30,================================================,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,31,,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,32,"First, we train a decision tree and a multi-layer perceptron on the diabetes",
scikit-learn/examples/plot_partial_dependence_visualization_api.py,33,dataset.,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,46,,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,47,Plotting partial dependence for two features,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,48,============================================,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,49,,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,50,"We plot partial dependence curves for features ""age"" and ""bmi"" (body mass",
scikit-learn/examples/plot_partial_dependence_visualization_api.py,51,"index) for the decision tree. With two features,",
scikit-learn/examples/plot_partial_dependence_visualization_api.py,52,:func:`~sklearn.inspection.plot_partial_dependence` expects to plot two,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,53,curves. Here the plot function place a grid of two plots using the space,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,54,defined by `ax` .,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,59,,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,60,The partial depdendence curves can be plotted for the multi-layer perceptron.,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,61,"In this case, `line_kw` is passed to",
scikit-learn/examples/plot_partial_dependence_visualization_api.py,62,:func:`~sklearn.inspection.plot_partial_dependence` to change the color of,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,63,the curve.,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,69,,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,70,Plotting partial dependence of the two models together,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,71,======================================================,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,72,,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,73,The `tree_disp` and `mlp_disp`,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,74,:class:`~sklearn.inspection.PartialDependenceDisplay` objects contain all the,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,75,computed information needed to recreate the partial dependence curves. This,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,76,means we can easily create additional plots without needing to recompute the,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,77,curves.,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,78,,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,79,"One way to plot the curves is to place them in the same figure, with the",
scikit-learn/examples/plot_partial_dependence_visualization_api.py,80,"curves of each model on each row. First, we create a figure with two axes",
scikit-learn/examples/plot_partial_dependence_visualization_api.py,81,within two rows and one column. The two axes are passed to the,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,82,:func:`~sklearn.inspection.PartialDependenceDisplay.plot` functions of,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,83,`tree_disp` and `mlp_disp`. The given axes will be used by the plotting,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,84,function to draw the partial dependence. The resulting plot places the,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,85,decision tree partial dependence curves in the first row of the,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,86,multi-layer perceptron in the second row.,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,94,,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,95,"Another way to compare the curves is to plot them on top of each other. Here,",
scikit-learn/examples/plot_partial_dependence_visualization_api.py,96,we create a figure with one row and two columns. The axes are passed into the,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,97,":func:`~sklearn.inspection.PartialDependenceDisplay.plot` function as a list,",
scikit-learn/examples/plot_partial_dependence_visualization_api.py,98,which will plot the partial dependence curves of each model on the same axes.,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,99,The length of the axes list must be equal to the number of plots drawn.,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,101,Sets this image as the thumbnail for sphinx gallery,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,102,sphinx_gallery_thumbnail_number = 4,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,110,,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,111,`tree_disp.axes_` is a numpy array container the axes used to draw the,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,112,partial dependence plots. This can be passed to `mlp_disp` to have the same,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,113,"affect of drawing the plots on top of each other. Furthermore, the",
scikit-learn/examples/plot_partial_dependence_visualization_api.py,114,"`mlp_disp.figure_` stores the figure, which allows for resizing the figure",
scikit-learn/examples/plot_partial_dependence_visualization_api.py,115,"after calling `plot`. In this case `tree_disp.axes_` has two dimensions, thus",
scikit-learn/examples/plot_partial_dependence_visualization_api.py,116,`plot` will only show the y label and y ticks on the left most plot.,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,126,,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,127,Plotting partial dependence for one feature,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,128,===========================================,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,129,,
scikit-learn/examples/plot_partial_dependence_visualization_api.py,130,"Here, we plot the partial dependence curves for a single feature, ""age"", on",
scikit-learn/examples/plot_partial_dependence_visualization_api.py,131,"the same axes. In this case, `tree_disp.axes_` is passed into the second",
scikit-learn/examples/plot_partial_dependence_visualization_api.py,132,plot function.,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,28,`normed` is being deprecated in favor of `density` in histograms,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,34,,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,35,Theoretical bounds,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,36,==================,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,37,The distortion introduced by a random projection `p` is asserted by,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,38,the fact that `p` is defining an eps-embedding with good probability,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,39,as defined by:,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,40,,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,41,.. math::,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,42,(1 - eps) \|u - v\|^2 < \|p(u) - p(v)\|^2 < (1 + eps) \|u - v\|^2,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,43,,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,44,"Where u and v are any rows taken from a dataset of shape [n_samples,",
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,45,"n_features] and p is a projection by a random Gaussian N(0, 1) matrix",
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,46,"with shape [n_components, n_features] (or a sparse Achlioptas matrix).",
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,47,,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,48,The minimum number of components to guarantees the eps-embedding is,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,49,given by:,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,50,,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,51,.. math::,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,52,n\_components >= 4 log(n\_samples) / (eps^2 / 2 - eps^3 / 3),
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,53,,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,54,,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,55,"The first plot shows that with an increasing number of samples ``n_samples``,",
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,56,the minimal number of dimensions ``n_components`` increased logarithmically,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,57,in order to guarantee an ``eps``-embedding.,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,59,range of admissible distortions,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,63,range of number of samples (observation) to embed,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,78,,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,79,The second plot shows that an increase of the admissible,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,80,distortion ``eps`` allows to reduce drastically the minimal number of,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,81,dimensions ``n_components`` for a given number of samples ``n_samples``,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,83,range of admissible distortions,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,86,range of number of samples (observation) to embed,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,101,,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,102,Empirical validation,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,103,====================,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,104,,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,105,We validate the above bounds on the 20 newsgroups text document,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,106,(TF-IDF word frequencies) dataset or on the digits dataset:,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,107,,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,108,- for the 20 newsgroups dataset some 500 documents with 100k,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,109,features in total are projected using a sparse random matrix to smaller,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,110,euclidean spaces with various values for the target number of dimensions,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,111,``n_components``.,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,112,,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,113,"- for the digits dataset, some 8x8 gray level pixels data for 500",
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,114,handwritten digits pictures are randomly projected to spaces for various,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,115,larger number of dimensions ``n_components``.,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,116,,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,117,The default dataset is the 20 newsgroups dataset. To run the example on the,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,118,"digits dataset, pass the ``--use-digits-dataset`` command line argument to",
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,119,this script.,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,126,,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,127,"For each value of ``n_components``, we plot:",
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,128,,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,129,- 2D distribution of sample pairs with pairwise distances in original,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,130,and projected spaces as x and y axis respectively.,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,131,,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,132,- 1D histogram of the ratio of those distances (projected / original).,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,141,select only non-identical samples pairs,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,182,TODO: compute the expected value of eps and add them to the previous plot,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,183,as vertical lines / region,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,188,,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,189,We can see that for low values of ``n_components`` the distribution is wide,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,190,with many distorted pairs and a skewed distribution (due to the hard,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,191,limit of zero ratio on the left as distances are always positives),
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,192,while for larger values of n_components the distortion is controlled,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,193,and the distances are well preserved by the random projection.,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,196,,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,197,Remarks,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,198,=======,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,199,,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,200,"According to the JL lemma, projecting 500 samples without too much distortion",
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,201,"will require at least several thousands dimensions, irrespective of the",
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,202,number of features of the original dataset.,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,203,,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,204,Hence using random projections on the digits dataset which only has 64,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,205,features in the input space does not make sense: it does not allow,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,206,for dimensionality reduction in this case.,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,207,,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,208,On the twenty newsgroups on the other hand the dimensionality can be,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,209,decreased from 56436 down to 10000 while reasonably preserving,
scikit-learn/examples/plot_johnson_lindenstrauss_bound.py,210,pairwise distances.,
scikit-learn/examples/plot_multioutput_face_completion.py,27,Load the faces datasets,
scikit-learn/examples/plot_multioutput_face_completion.py,31,Test on independent people,
scikit-learn/examples/plot_multioutput_face_completion.py,33,Test on a subset of people,
scikit-learn/examples/plot_multioutput_face_completion.py,40,Upper half of the faces,
scikit-learn/examples/plot_multioutput_face_completion.py,42,Lower half of the faces,
scikit-learn/examples/plot_multioutput_face_completion.py,47,Fit estimators,
scikit-learn/examples/plot_multioutput_face_completion.py,61,Plot the completed faces,
scikit-learn/examples/plot_kernel_ridge_regression.py,34,Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,
scikit-learn/examples/plot_kernel_ridge_regression.py,35,License: BSD 3 clause,
scikit-learn/examples/plot_kernel_ridge_regression.py,50,,
scikit-learn/examples/plot_kernel_ridge_regression.py,51,Generate sample data,
scikit-learn/examples/plot_kernel_ridge_regression.py,55,Add noise to targets,
scikit-learn/examples/plot_kernel_ridge_regression.py,60,,
scikit-learn/examples/plot_kernel_ridge_regression.py,61,Fit regression model,
scikit-learn/examples/plot_kernel_ridge_regression.py,99,,
scikit-learn/examples/plot_kernel_ridge_regression.py,100,Look at the results,
scikit-learn/examples/plot_kernel_ridge_regression.py,115,Visualize training and prediction time,
scikit-learn/examples/plot_kernel_ridge_regression.py,118,Generate sample data,
scikit-learn/examples/plot_kernel_ridge_regression.py,149,Visualize learning curves,
scikit-learn/examples/plot_anomaly_comparison.py,54,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/examples/plot_anomaly_comparison.py,55,Albert Thomas <albert.thomas@telecom-paristech.fr>,
scikit-learn/examples/plot_anomaly_comparison.py,56,License: BSD 3 clause,
scikit-learn/examples/plot_anomaly_comparison.py,74,Example settings,
scikit-learn/examples/plot_anomaly_comparison.py,80,define outlier/anomaly detection methods to be compared,
scikit-learn/examples/plot_anomaly_comparison.py,90,Define datasets,
scikit-learn/examples/plot_anomaly_comparison.py,103,Compare given classifiers under given settings,
scikit-learn/examples/plot_anomaly_comparison.py,115,Add outliers,
scikit-learn/examples/plot_anomaly_comparison.py,127,fit the data and tag outliers,
scikit-learn/examples/plot_anomaly_comparison.py,133,plot the levels lines and the points,
scikit-learn/examples/plot_anomaly_comparison.py,134,LOF does not implement predict,
scikit-learn/examples/plot_roc_curve_visualization_api.py,12,,
scikit-learn/examples/plot_roc_curve_visualization_api.py,13,Load Data and Train a SVC,
scikit-learn/examples/plot_roc_curve_visualization_api.py,14,-------------------------,
scikit-learn/examples/plot_roc_curve_visualization_api.py,15,"First, we load the wine dataset and convert it to a binary classification",
scikit-learn/examples/plot_roc_curve_visualization_api.py,16,"problem. Then, we train a support vector classifier on a training dataset.",
scikit-learn/examples/plot_roc_curve_visualization_api.py,31,,
scikit-learn/examples/plot_roc_curve_visualization_api.py,32,Plotting the ROC Curve,
scikit-learn/examples/plot_roc_curve_visualization_api.py,33,----------------------,
scikit-learn/examples/plot_roc_curve_visualization_api.py,34,"Next, we plot the ROC curve with a single call to",
scikit-learn/examples/plot_roc_curve_visualization_api.py,35,:func:`sklearn.metrics.plot_roc_curve`. The returned `svc_disp` object allows,
scikit-learn/examples/plot_roc_curve_visualization_api.py,36,us to continue using the already computed ROC curve for the SVC in future,
scikit-learn/examples/plot_roc_curve_visualization_api.py,37,plots.,
scikit-learn/examples/plot_roc_curve_visualization_api.py,41,,
scikit-learn/examples/plot_roc_curve_visualization_api.py,42,Training a Random Forest and Plotting the ROC Curve,
scikit-learn/examples/plot_roc_curve_visualization_api.py,43,--------------------------------------------------------,
scikit-learn/examples/plot_roc_curve_visualization_api.py,44,We train a random forest classifier and create a plot comparing it to the SVC,
scikit-learn/examples/plot_roc_curve_visualization_api.py,45,ROC curve. Notice how `svc_disp` uses,
scikit-learn/examples/plot_roc_curve_visualization_api.py,46,:func:`~sklearn.metrics.RocCurveDisplay.plot` to plot the SVC ROC curve,
scikit-learn/examples/plot_roc_curve_visualization_api.py,47,"without recomputing the values of the roc curve itself. Furthermore, we",
scikit-learn/examples/plot_roc_curve_visualization_api.py,48,pass `alpha=0.8` to the plot functions to adjust the alpha values of the,
scikit-learn/examples/plot_roc_curve_visualization_api.py,49,curves.,
scikit-learn/examples/plot_multilabel.py,1,"Authors: Vlad Niculae, Mathieu Blondel",
scikit-learn/examples/plot_multilabel.py,2,License: BSD 3 clause,
scikit-learn/examples/plot_multilabel.py,45,get the separating hyperplane,
scikit-learn/examples/plot_multilabel.py,48,make sure the line is long enough,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,18,Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,19,Maria Telenczuk <https://github.com/maikia>,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,20,Katrina Ni <https://github.com/nilichen>,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,21,,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,22,License: BSD 3 clause,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,31,,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,32,Load the data,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,33,-------------------------------------,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,34,,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,35,First we need to load the data.,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,40,,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,41,Data preprocessing,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,42,-------------------------------------,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,43,,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,44,"Next, we will split our dataset to use 90% for training and leave the rest",
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,45,for testing. We will also set the regression model parameters. You can play,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,46,with these parameters to see how the results change.,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,47,,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,48,n_estimators : the number of boosting stages that will be performed.,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,49,"Later, we will plot deviance against boosting iterations.",
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,50,,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,51,max_depth : limits the number of nodes in the tree.,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,52,The best value depends on the interaction of the input variables.,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,53,,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,54,min_samples_split : the minimum number of samples required to split an,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,55,internal node.,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,56,,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,57,learning_rate : how much the contribution of each tree will shrink.,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,58,,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,59,loss : loss function to optimize. The least squares function is  used in this,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,60,"case however, there are many other options (see",
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,61,:class:`~sklearn.ensemble.GradientBoostingRegressor` ).,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,72,,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,73,Fit regression model,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,74,-------------------------------------,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,75,,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,76,Now we will initiate the gradient boosting regressors and fit it with our,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,77,training data. Let's also look and the mean squared error on the test data.,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,85,,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,86,Plot training deviance,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,87,-------------------------------------,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,88,,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,89,"Finally, we will visualize the results. To do that we will first compute the",
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,90,test set deviance and then plot it against boosting iterations.,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,109,,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,110,Plot feature importance,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,111,-------------------------------------,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,112,,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,113,"Careful, impurity-based feature importances can be misleading for",
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,114,"high cardinality features (many unique values). As an alternative,",
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,115,the permutation importances of ``reg`` can be computed on a,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,116,held out test set. See :ref:`permutation_importance` for more details.,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,117,,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,118,"For this example, the impurity-based and permutation methods identify the",
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,119,same 2 strongly predictive features but not in the same order. The third most,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,120,"predictive feature, ""bp"", is also the same for the 2 methods. The remaining",
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,121,features are less predictive and the error bars of the permutation plot,
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,122,show that they overlap with 0.,
scikit-learn/examples/ensemble/plot_voting_probas.py,45,predict class probabilities for all classifiers,
scikit-learn/examples/ensemble/plot_voting_probas.py,48,get class probabilities for the first sample in the dataset,
scikit-learn/examples/ensemble/plot_voting_probas.py,53,plotting,
scikit-learn/examples/ensemble/plot_voting_probas.py,55,number of groups,
scikit-learn/examples/ensemble/plot_voting_probas.py,56,group positions,
scikit-learn/examples/ensemble/plot_voting_probas.py,57,bar width,
scikit-learn/examples/ensemble/plot_voting_probas.py,61,bars for classifier 1-3,
scikit-learn/examples/ensemble/plot_voting_probas.py,67,bars for VotingClassifier,
scikit-learn/examples/ensemble/plot_voting_probas.py,73,plot annotations,
scikit-learn/examples/ensemble/plot_forest_importances_faces.py,21,Number of cores to use to perform parallel fitting of the forest model,
scikit-learn/examples/ensemble/plot_forest_importances_faces.py,24,Load the faces dataset,
scikit-learn/examples/ensemble/plot_forest_importances_faces.py,28,Limit to 5 classes,
scikit-learn/examples/ensemble/plot_forest_importances_faces.py,32,Build a forest and compute the pixel importances,
scikit-learn/examples/ensemble/plot_forest_importances_faces.py,45,Plot pixel importances,
scikit-learn/examples/ensemble/plot_random_forest_embedding.py,35,make a synthetic dataset,
scikit-learn/examples/ensemble/plot_random_forest_embedding.py,38,use RandomTreesEmbedding to transform data,
scikit-learn/examples/ensemble/plot_random_forest_embedding.py,42,Visualize result after dimensionality reduction using truncated SVD,
scikit-learn/examples/ensemble/plot_random_forest_embedding.py,46,Learn a Naive Bayes classifier on the transformed data,
scikit-learn/examples/ensemble/plot_random_forest_embedding.py,51,Learn an ExtraTreesClassifier for comparison,
scikit-learn/examples/ensemble/plot_random_forest_embedding.py,56,scatter plot of original and reduced data,
scikit-learn/examples/ensemble/plot_random_forest_embedding.py,72,"Plot the decision in original space. For that, we will assign a color",
scikit-learn/examples/ensemble/plot_random_forest_embedding.py,73,"to each point in the mesh [x_min, x_max]x[y_min, y_max].",
scikit-learn/examples/ensemble/plot_random_forest_embedding.py,79,transform grid using RandomTreesEmbedding,
scikit-learn/examples/ensemble/plot_random_forest_embedding.py,92,transform grid using ExtraTreesClassifier,
scikit-learn/examples/ensemble/plot_monotonic_constraints.py,21,noqa,
scikit-learn/examples/ensemble/plot_monotonic_constraints.py,33,positive correlation with y,
scikit-learn/examples/ensemble/plot_monotonic_constraints.py,34,negative correlation with y,
scikit-learn/examples/ensemble/plot_monotonic_constraints.py,44,Without any constraint,
scikit-learn/examples/ensemble/plot_monotonic_constraints.py,52,With positive and negative constraints,
scikit-learn/examples/ensemble/plot_voting_decision_regions.py,38,Loading some example data,
scikit-learn/examples/ensemble/plot_voting_decision_regions.py,43,Training classifiers,
scikit-learn/examples/ensemble/plot_voting_decision_regions.py,56,Plotting decision regions,
scikit-learn/examples/ensemble/plot_gradient_boosting_oob.py,28,Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>,
scikit-learn/examples/ensemble/plot_gradient_boosting_oob.py,29,,
scikit-learn/examples/ensemble/plot_gradient_boosting_oob.py,30,License: BSD 3 clause,
scikit-learn/examples/ensemble/plot_gradient_boosting_oob.py,41,Generate data (adapted from G. Ridgeway's gbm example),
scikit-learn/examples/ensemble/plot_gradient_boosting_oob.py,57,Fit classifier with out-of-bag estimates,
scikit-learn/examples/ensemble/plot_gradient_boosting_oob.py,89,Estimate best n_estimator using cross-validation,
scikit-learn/examples/ensemble/plot_gradient_boosting_oob.py,92,Compute best n_estimator for test data,
scikit-learn/examples/ensemble/plot_gradient_boosting_oob.py,95,negative cumulative sum of oob improvements,
scikit-learn/examples/ensemble/plot_gradient_boosting_oob.py,98,min loss according to OOB,
scikit-learn/examples/ensemble/plot_gradient_boosting_oob.py,101,min loss according to test (normalize such that first loss is 0),
scikit-learn/examples/ensemble/plot_gradient_boosting_oob.py,105,min loss according to cv (normalize such that first loss is 0),
scikit-learn/examples/ensemble/plot_gradient_boosting_oob.py,109,color brew for the three curves,
scikit-learn/examples/ensemble/plot_gradient_boosting_oob.py,114,plot curves and vertical lines for best iterations,
scikit-learn/examples/ensemble/plot_gradient_boosting_oob.py,122,add three vertical lines to xticks,
scikit-learn/examples/ensemble/plot_adaboost_twoclass.py,21,Author: Noel Dawe <noel.dawe@gmail.com>,
scikit-learn/examples/ensemble/plot_adaboost_twoclass.py,22,,
scikit-learn/examples/ensemble/plot_adaboost_twoclass.py,23,License: BSD 3 clause,
scikit-learn/examples/ensemble/plot_adaboost_twoclass.py,33,Construct dataset,
scikit-learn/examples/ensemble/plot_adaboost_twoclass.py,43,Create and fit an AdaBoosted decision tree,
scikit-learn/examples/ensemble/plot_adaboost_twoclass.py,56,Plot the decision boundaries,
scikit-learn/examples/ensemble/plot_adaboost_twoclass.py,68,Plot the training points,
scikit-learn/examples/ensemble/plot_adaboost_twoclass.py,82,Plot the two-class decision scores,
scikit-learn/examples/ensemble/plot_bias_variance.py,66,Author: Gilles Louppe <g.louppe@gmail.com>,
scikit-learn/examples/ensemble/plot_bias_variance.py,67,License: BSD 3 clause,
scikit-learn/examples/ensemble/plot_bias_variance.py,75,Settings,
scikit-learn/examples/ensemble/plot_bias_variance.py,76,Number of iterations for computing expectations,
scikit-learn/examples/ensemble/plot_bias_variance.py,77,Size of the training set,
scikit-learn/examples/ensemble/plot_bias_variance.py,78,Size of the test set,
scikit-learn/examples/ensemble/plot_bias_variance.py,79,Standard deviation of the noise,
scikit-learn/examples/ensemble/plot_bias_variance.py,82,Change this for exploring the bias-variance decomposition of other,
scikit-learn/examples/ensemble/plot_bias_variance.py,83,"estimators. This should work well for estimators with high variance (e.g.,",
scikit-learn/examples/ensemble/plot_bias_variance.py,84,"decision trees or KNN), but poorly for estimators with low variance (e.g.,",
scikit-learn/examples/ensemble/plot_bias_variance.py,85,linear models).,
scikit-learn/examples/ensemble/plot_bias_variance.py,92,Generate data,
scikit-learn/examples/ensemble/plot_bias_variance.py,128,Loop over estimators to compare,
scikit-learn/examples/ensemble/plot_bias_variance.py,130,Compute predictions,
scikit-learn/examples/ensemble/plot_bias_variance.py,137,Bias^2 + Variance + Noise decomposition of the mean squared error,
scikit-learn/examples/ensemble/plot_bias_variance.py,157,Plot figures,
scikit-learn/examples/ensemble/plot_adaboost_regression.py,17,Author: Noel Dawe <noel.dawe@gmail.com>,
scikit-learn/examples/ensemble/plot_adaboost_regression.py,18,,
scikit-learn/examples/ensemble/plot_adaboost_regression.py,19,License: BSD 3 clause,
scikit-learn/examples/ensemble/plot_adaboost_regression.py,21,importing necessary libraries,
scikit-learn/examples/ensemble/plot_adaboost_regression.py,27,Create the dataset,
scikit-learn/examples/ensemble/plot_adaboost_regression.py,32,Fit regression model,
scikit-learn/examples/ensemble/plot_adaboost_regression.py,41,Predict,
scikit-learn/examples/ensemble/plot_adaboost_regression.py,45,Plot the results,
scikit-learn/examples/ensemble/plot_stack_predictors.py,20,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>,
scikit-learn/examples/ensemble/plot_stack_predictors.py,21,Maria Telenczuk    <https://github.com/maikia>,
scikit-learn/examples/ensemble/plot_stack_predictors.py,22,License: BSD 3 clause,
scikit-learn/examples/ensemble/plot_stack_predictors.py,25,,
scikit-learn/examples/ensemble/plot_stack_predictors.py,26,Download the dataset,
scikit-learn/examples/ensemble/plot_stack_predictors.py,27,,
scikit-learn/examples/ensemble/plot_stack_predictors.py,28,,
scikit-learn/examples/ensemble/plot_stack_predictors.py,29,We will use `Ames Housing`_ dataset which was first compiled by Dean De Cock,
scikit-learn/examples/ensemble/plot_stack_predictors.py,30,and became better known after it was used in Kaggle challenge. It is a set,
scikit-learn/examples/ensemble/plot_stack_predictors.py,31,"of 1460 residential homes in Ames, Iowa, each described by 80 features. We",
scikit-learn/examples/ensemble/plot_stack_predictors.py,32,will use it to predict the final logarithmic price of the houses. In this,
scikit-learn/examples/ensemble/plot_stack_predictors.py,33,example we will use only 20 most interesting features chosen using,
scikit-learn/examples/ensemble/plot_stack_predictors.py,34,GradientBoostingRegressor() and limit number of entries (here we won't go,
scikit-learn/examples/ensemble/plot_stack_predictors.py,35,into the details on how to select the most interesting features).,
scikit-learn/examples/ensemble/plot_stack_predictors.py,36,,
scikit-learn/examples/ensemble/plot_stack_predictors.py,37,The Ames housing dataset is not shipped with scikit-learn and therefore we,
scikit-learn/examples/ensemble/plot_stack_predictors.py,38,will fetch it from `OpenML`_.,
scikit-learn/examples/ensemble/plot_stack_predictors.py,39,,
scikit-learn/examples/ensemble/plot_stack_predictors.py,40,.. _`Ames Housing`: http://jse.amstat.org/v19n3/decock.pdf,
scikit-learn/examples/ensemble/plot_stack_predictors.py,41,.. _`OpenML`: https://www.openml.org/d/42165,
scikit-learn/examples/ensemble/plot_stack_predictors.py,71,,
scikit-learn/examples/ensemble/plot_stack_predictors.py,72,Make pipeline to preprocess the data,
scikit-learn/examples/ensemble/plot_stack_predictors.py,73,,
scikit-learn/examples/ensemble/plot_stack_predictors.py,74,,
scikit-learn/examples/ensemble/plot_stack_predictors.py,75,Before we can use Ames dataset we still need to do some preprocessing.,
scikit-learn/examples/ensemble/plot_stack_predictors.py,76,"First, the dataset has many missing values. To impute them, we will exchange",
scikit-learn/examples/ensemble/plot_stack_predictors.py,77,categorical missing values with the new category 'missing' while the,
scikit-learn/examples/ensemble/plot_stack_predictors.py,78,numerical missing values with the 'mean' of the column. We will also encode,
scikit-learn/examples/ensemble/plot_stack_predictors.py,79,the categories with either :class:`sklearn.preprocessing.OneHotEncoder,
scikit-learn/examples/ensemble/plot_stack_predictors.py,80,<sklearn.preprocessing.OneHotEncoder>` or,
scikit-learn/examples/ensemble/plot_stack_predictors.py,81,:class:`sklearn.preprocessing.OrdinalEncoder,
scikit-learn/examples/ensemble/plot_stack_predictors.py,82,<sklearn.preprocessing.OrdinalEncoder>` depending for which type of model we,
scikit-learn/examples/ensemble/plot_stack_predictors.py,83,will use them (linear or non-linear model). To falicitate this preprocessing,
scikit-learn/examples/ensemble/plot_stack_predictors.py,84,we will make two pipelines.,
scikit-learn/examples/ensemble/plot_stack_predictors.py,85,You can skip this section if your data is ready to use and does,
scikit-learn/examples/ensemble/plot_stack_predictors.py,86,not need preprocessing,
scikit-learn/examples/ensemble/plot_stack_predictors.py,104,noqa,
scikit-learn/examples/ensemble/plot_stack_predictors.py,126,transformation to use for non-linear estimators,
scikit-learn/examples/ensemble/plot_stack_predictors.py,132,transformation to use for linear estimators,
scikit-learn/examples/ensemble/plot_stack_predictors.py,139,,
scikit-learn/examples/ensemble/plot_stack_predictors.py,140,Stack of predictors on a single data set,
scikit-learn/examples/ensemble/plot_stack_predictors.py,141,,
scikit-learn/examples/ensemble/plot_stack_predictors.py,142,,
scikit-learn/examples/ensemble/plot_stack_predictors.py,143,It is sometimes tedious to find the model which will best perform on a given,
scikit-learn/examples/ensemble/plot_stack_predictors.py,144,dataset. Stacking provide an alternative by combining the outputs of several,
scikit-learn/examples/ensemble/plot_stack_predictors.py,145,"learners, without the need to choose a model specifically. The performance of",
scikit-learn/examples/ensemble/plot_stack_predictors.py,146,stacking is usually close to the best model and sometimes it can outperform,
scikit-learn/examples/ensemble/plot_stack_predictors.py,147,the prediction performance of each individual model.,
scikit-learn/examples/ensemble/plot_stack_predictors.py,148,,
scikit-learn/examples/ensemble/plot_stack_predictors.py,149,"Here, we combine 3 learners (linear and non-linear) and use a ridge regressor",
scikit-learn/examples/ensemble/plot_stack_predictors.py,150,to combine their outputs together.,
scikit-learn/examples/ensemble/plot_stack_predictors.py,151,,
scikit-learn/examples/ensemble/plot_stack_predictors.py,152,Note: although we will make new pipelines with the processors which we wrote,
scikit-learn/examples/ensemble/plot_stack_predictors.py,153,"in the previous section for the 3 learners, the final estimator RidgeCV()",
scikit-learn/examples/ensemble/plot_stack_predictors.py,154,does not need preprocessing of the data as it will be fed with the already,
scikit-learn/examples/ensemble/plot_stack_predictors.py,155,preprocessed output from the 3 learners.,
scikit-learn/examples/ensemble/plot_stack_predictors.py,158,noqa,
scikit-learn/examples/ensemble/plot_stack_predictors.py,184,,
scikit-learn/examples/ensemble/plot_stack_predictors.py,185,Measure and plot the results,
scikit-learn/examples/ensemble/plot_stack_predictors.py,186,,
scikit-learn/examples/ensemble/plot_stack_predictors.py,187,,
scikit-learn/examples/ensemble/plot_stack_predictors.py,188,Now we can use Ames Housing dataset to make the predictions. We check the,
scikit-learn/examples/ensemble/plot_stack_predictors.py,189,performance of each individual predictor as well as of the stack of the,
scikit-learn/examples/ensemble/plot_stack_predictors.py,190,regressors.,
scikit-learn/examples/ensemble/plot_stack_predictors.py,191,,
scikit-learn/examples/ensemble/plot_stack_predictors.py,192,The function ``plot_regression_results`` is used to plot the predicted and,
scikit-learn/examples/ensemble/plot_stack_predictors.py,193,true targets.,
scikit-learn/examples/ensemble/plot_stack_predictors.py,253,,
scikit-learn/examples/ensemble/plot_stack_predictors.py,254,The stacked regressor will combine the strengths of the different regressors.,
scikit-learn/examples/ensemble/plot_stack_predictors.py,255,"However, we also see that training the stacked regressor is much more",
scikit-learn/examples/ensemble/plot_stack_predictors.py,256,computationally expensive.,
scikit-learn/examples/ensemble/plot_feature_transformation.py,24,Author: Tim Head <betatim@gmail.com>,
scikit-learn/examples/ensemble/plot_feature_transformation.py,25,,
scikit-learn/examples/ensemble/plot_feature_transformation.py,26,License: BSD 3 clause,
scikit-learn/examples/ensemble/plot_feature_transformation.py,46,It is important to train the ensemble of trees on a different subset,
scikit-learn/examples/ensemble/plot_feature_transformation.py,47,of the training data than the linear regression model to avoid,
scikit-learn/examples/ensemble/plot_feature_transformation.py,48,"overfitting, in particular if the total number of leaves is",
scikit-learn/examples/ensemble/plot_feature_transformation.py,49,similar to the number of training samples,
scikit-learn/examples/ensemble/plot_feature_transformation.py,53,Unsupervised transformation based on totally random trees,
scikit-learn/examples/ensemble/plot_feature_transformation.py,63,Supervised transformation based on random forests,
scikit-learn/examples/ensemble/plot_feature_transformation.py,74,Supervised transformation based on gradient boosted trees,
scikit-learn/examples/ensemble/plot_feature_transformation.py,86,The gradient boosted model by itself,
scikit-learn/examples/ensemble/plot_feature_transformation.py,90,The random forest model by itself,
scikit-learn/examples/ensemble/plot_adaboost_hastie_10_2.py,23,"Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>,",
scikit-learn/examples/ensemble/plot_adaboost_hastie_10_2.py,24,Noel Dawe <noel.dawe@gmail.com>,
scikit-learn/examples/ensemble/plot_adaboost_hastie_10_2.py,25,,
scikit-learn/examples/ensemble/plot_adaboost_hastie_10_2.py,26,License: BSD 3 clause,
scikit-learn/examples/ensemble/plot_adaboost_hastie_10_2.py,38,A learning rate of 1. may not be optimal for both SAMME and SAMME.R,
scikit-learn/examples/ensemble/plot_forest_iris.py,54,Parameters,
scikit-learn/examples/ensemble/plot_forest_iris.py,58,fine step width for decision surface contours,
scikit-learn/examples/ensemble/plot_forest_iris.py,59,step widths for coarse classifier guesses,
scikit-learn/examples/ensemble/plot_forest_iris.py,60,fix the seed on each iteration,
scikit-learn/examples/ensemble/plot_forest_iris.py,62,Load data,
scikit-learn/examples/ensemble/plot_forest_iris.py,75,We only take the two corresponding features,
scikit-learn/examples/ensemble/plot_forest_iris.py,79,Shuffle,
scikit-learn/examples/ensemble/plot_forest_iris.py,86,Standardize,
scikit-learn/examples/ensemble/plot_forest_iris.py,91,Train,
scikit-learn/examples/ensemble/plot_forest_iris.py,95,Create a title for each column and the console by using str() and,
scikit-learn/examples/ensemble/plot_forest_iris.py,96,slicing away useless parts of the string,
scikit-learn/examples/ensemble/plot_forest_iris.py,109,Add a title at the top of each column,
scikit-learn/examples/ensemble/plot_forest_iris.py,112,Now plot the decision boundary using a fine mesh as input to a,
scikit-learn/examples/ensemble/plot_forest_iris.py,113,filled contour plot,
scikit-learn/examples/ensemble/plot_forest_iris.py,119,Plot either a single DecisionTreeClassifier or alpha blend the,
scikit-learn/examples/ensemble/plot_forest_iris.py,120,decision surfaces of the ensemble of classifiers,
scikit-learn/examples/ensemble/plot_forest_iris.py,126,Choose alpha blend level with respect to the number,
scikit-learn/examples/ensemble/plot_forest_iris.py,127,of estimators,
scikit-learn/examples/ensemble/plot_forest_iris.py,128,that are in use (noting that AdaBoost can use fewer estimators,
scikit-learn/examples/ensemble/plot_forest_iris.py,129,than its maximum if it achieves a good enough fit early on),
scikit-learn/examples/ensemble/plot_forest_iris.py,136,Build a coarser grid to plot a set of ensemble classifications,
scikit-learn/examples/ensemble/plot_forest_iris.py,137,to show how these are different to what we see in the decision,
scikit-learn/examples/ensemble/plot_forest_iris.py,138,surfaces. These points are regularly space and do not have a,
scikit-learn/examples/ensemble/plot_forest_iris.py,139,black outline,
scikit-learn/examples/ensemble/plot_forest_iris.py,150,"Plot the training points, these are clustered together and have a",
scikit-learn/examples/ensemble/plot_forest_iris.py,151,black outline,
scikit-learn/examples/ensemble/plot_forest_iris.py,155,move on to the next plot in sequence,
scikit-learn/examples/ensemble/plot_gradient_boosting_quantile.py,22,----------------------------------------------------------------------,
scikit-learn/examples/ensemble/plot_gradient_boosting_quantile.py,23,First the noiseless case,
scikit-learn/examples/ensemble/plot_gradient_boosting_quantile.py,27,Observations,
scikit-learn/examples/ensemble/plot_gradient_boosting_quantile.py,35,"Mesh the input space for evaluations of the real function, the prediction and",
scikit-learn/examples/ensemble/plot_gradient_boosting_quantile.py,36,its MSE,
scikit-learn/examples/ensemble/plot_gradient_boosting_quantile.py,49,Make the prediction on the meshed x-axis,
scikit-learn/examples/ensemble/plot_gradient_boosting_quantile.py,55,Make the prediction on the meshed x-axis,
scikit-learn/examples/ensemble/plot_gradient_boosting_quantile.py,61,Make the prediction on the meshed x-axis,
scikit-learn/examples/ensemble/plot_gradient_boosting_quantile.py,64,"Plot the function, the prediction and the 95% confidence interval based on",
scikit-learn/examples/ensemble/plot_gradient_boosting_quantile.py,65,the MSE,
scikit-learn/examples/ensemble/plot_ensemble_oob.py,28,Author: Kian Ho <hui.kian.ho@gmail.com>,
scikit-learn/examples/ensemble/plot_ensemble_oob.py,29,Gilles Louppe <g.louppe@gmail.com>,
scikit-learn/examples/ensemble/plot_ensemble_oob.py,30,Andreas Mueller <amueller@ais.uni-bonn.de>,
scikit-learn/examples/ensemble/plot_ensemble_oob.py,31,,
scikit-learn/examples/ensemble/plot_ensemble_oob.py,32,License: BSD 3 Clause,
scikit-learn/examples/ensemble/plot_ensemble_oob.py,38,Generate a binary classification dataset.,
scikit-learn/examples/ensemble/plot_ensemble_oob.py,43,NOTE: Setting the `warm_start` construction parameter to `True` disables,
scikit-learn/examples/ensemble/plot_ensemble_oob.py,44,support for parallelized ensembles but is necessary for tracking the OOB,
scikit-learn/examples/ensemble/plot_ensemble_oob.py,45,error trajectory during training.,
scikit-learn/examples/ensemble/plot_ensemble_oob.py,61,"Map a classifier name to a list of (<n_estimators>, <error rate>) pairs.",
scikit-learn/examples/ensemble/plot_ensemble_oob.py,64,Range of `n_estimators` values to explore.,
scikit-learn/examples/ensemble/plot_ensemble_oob.py,73,Record the OOB error for each `n_estimators=i` setting.,
scikit-learn/examples/ensemble/plot_ensemble_oob.py,77,"Generate the ""OOB error rate"" vs. ""n_estimators"" plot.",
scikit-learn/examples/ensemble/plot_random_forest_regression_multioutput.py,25,Author: Tim Head <betatim@gmail.com>,
scikit-learn/examples/ensemble/plot_random_forest_regression_multioutput.py,26,,
scikit-learn/examples/ensemble/plot_random_forest_regression_multioutput.py,27,License: BSD 3 clause,
scikit-learn/examples/ensemble/plot_random_forest_regression_multioutput.py,36,Create a random dataset,
scikit-learn/examples/ensemble/plot_random_forest_regression_multioutput.py,55,Predict on new data,
scikit-learn/examples/ensemble/plot_random_forest_regression_multioutput.py,59,Plot the results,
scikit-learn/examples/ensemble/plot_isolation_forest.py,33,Generate train data,
scikit-learn/examples/ensemble/plot_isolation_forest.py,36,Generate some regular novel observations,
scikit-learn/examples/ensemble/plot_isolation_forest.py,39,Generate some abnormal novel observations,
scikit-learn/examples/ensemble/plot_isolation_forest.py,42,fit the model,
scikit-learn/examples/ensemble/plot_isolation_forest.py,49,"plot the line, the samples, and the nearest vectors to the plane",
scikit-learn/examples/ensemble/plot_voting_regressor.py,35,,
scikit-learn/examples/ensemble/plot_voting_regressor.py,36,Training classifiers,
scikit-learn/examples/ensemble/plot_voting_regressor.py,37,--------------------------------,
scikit-learn/examples/ensemble/plot_voting_regressor.py,38,,
scikit-learn/examples/ensemble/plot_voting_regressor.py,39,"First, we will load the diabetes dataset and initiate a gradient boosting",
scikit-learn/examples/ensemble/plot_voting_regressor.py,40,"regressor, a random forest regressor and a linear regression. Next, we will",
scikit-learn/examples/ensemble/plot_voting_regressor.py,41,use the 3 regressors to build the voting regressor:,
scikit-learn/examples/ensemble/plot_voting_regressor.py,45,Train classifiers,
scikit-learn/examples/ensemble/plot_voting_regressor.py,57,,
scikit-learn/examples/ensemble/plot_voting_regressor.py,58,Making predictions,
scikit-learn/examples/ensemble/plot_voting_regressor.py,59,--------------------------------,
scikit-learn/examples/ensemble/plot_voting_regressor.py,60,,
scikit-learn/examples/ensemble/plot_voting_regressor.py,61,Now we will use each of the regressors to make the 20 first predictions.,
scikit-learn/examples/ensemble/plot_voting_regressor.py,70,,
scikit-learn/examples/ensemble/plot_voting_regressor.py,71,Plot the results,
scikit-learn/examples/ensemble/plot_voting_regressor.py,72,--------------------------------,
scikit-learn/examples/ensemble/plot_voting_regressor.py,73,,
scikit-learn/examples/ensemble/plot_voting_regressor.py,74,"Finally, we will visualize the 20 predictions. The red stars show the average",
scikit-learn/examples/ensemble/plot_voting_regressor.py,75,prediction made by :class:`~ensemble.VotingRegressor`.,
scikit-learn/examples/ensemble/plot_forest_importances.py,27,Build a classification task using 3 informative features,
scikit-learn/examples/ensemble/plot_forest_importances.py,37,Build a forest and compute the impurity-based feature importances,
scikit-learn/examples/ensemble/plot_forest_importances.py,47,Print the feature ranking,
scikit-learn/examples/ensemble/plot_forest_importances.py,53,Plot the impurity-based feature importances of the forest,
scikit-learn/examples/ensemble/plot_adaboost_multiclass.py,28,Author: Noel Dawe <noel.dawe@gmail.com>,
scikit-learn/examples/ensemble/plot_adaboost_multiclass.py,29,,
scikit-learn/examples/ensemble/plot_adaboost_multiclass.py,30,License: BSD 3 clause,
scikit-learn/examples/ensemble/plot_adaboost_multiclass.py,75,"Boosting might terminate early, but the following arrays are always",
scikit-learn/examples/ensemble/plot_adaboost_multiclass.py,76,n_estimators long. We crop them to the actual number of trees here:,
scikit-learn/examples/ensemble/plot_adaboost_multiclass.py,116,prevent overlapping y-axis labels,
scikit-learn/examples/ensemble/plot_gradient_boosting_early_stopping.py,34,Authors: Vighnesh Birodkar <vighneshbirodkar@nyu.edu>,
scikit-learn/examples/ensemble/plot_gradient_boosting_early_stopping.py,35,Raghav RV <rvraghav93@gmail.com>,
scikit-learn/examples/ensemble/plot_gradient_boosting_early_stopping.py,36,License: BSD 3 clause,
scikit-learn/examples/ensemble/plot_gradient_boosting_early_stopping.py,67,We specify that if the scores don't improve by atleast 0.01 for the last,
scikit-learn/examples/ensemble/plot_gradient_boosting_early_stopping.py,68,"10 stages, stop fitting additional stages",
scikit-learn/examples/ensemble/plot_gradient_boosting_early_stopping.py,94,,
scikit-learn/examples/ensemble/plot_gradient_boosting_early_stopping.py,95,Compare scores with and without early stopping,
scikit-learn/examples/ensemble/plot_gradient_boosting_early_stopping.py,96,----------------------------------------------,
scikit-learn/examples/ensemble/plot_gradient_boosting_early_stopping.py,132,,
scikit-learn/examples/ensemble/plot_gradient_boosting_early_stopping.py,133,Compare fit times with and without early stopping,
scikit-learn/examples/ensemble/plot_gradient_boosting_early_stopping.py,134,-------------------------------------------------,
scikit-learn/examples/ensemble/plot_gradient_boosting_regularization.py,24,Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>,
scikit-learn/examples/ensemble/plot_gradient_boosting_regularization.py,25,,
scikit-learn/examples/ensemble/plot_gradient_boosting_regularization.py,26,License: BSD 3 clause,
scikit-learn/examples/ensemble/plot_gradient_boosting_regularization.py,38,"map labels from {-1, 1} to {0, 1}",
scikit-learn/examples/ensemble/plot_gradient_boosting_regularization.py,65,compute test set deviance,
scikit-learn/examples/ensemble/plot_gradient_boosting_regularization.py,69,"clf.loss_ assumes that y_test[i] in {0, 1}",
scikit-learn/examples/exercises/plot_cv_diabetes.py,40,plot error lines showing +/- std. errors of the scores,
scikit-learn/examples/exercises/plot_cv_diabetes.py,46,alpha=0.2 controls the translucency of the fill color,
scikit-learn/examples/exercises/plot_cv_diabetes.py,54,,
scikit-learn/examples/exercises/plot_cv_diabetes.py,55,Bonus: how much can you trust the selection of alpha?,
scikit-learn/examples/exercises/plot_cv_diabetes.py,57,To answer this question we use the LassoCV object that sets its alpha,
scikit-learn/examples/exercises/plot_cv_diabetes.py,58,parameter automatically from the data by internal cross-validation (i.e. it,
scikit-learn/examples/exercises/plot_cv_diabetes.py,59,performs cross-validation on the training data it receives).,
scikit-learn/examples/exercises/plot_cv_diabetes.py,60,We use external cross-validation to see how much the automatically obtained,
scikit-learn/examples/exercises/plot_cv_diabetes.py,61,alphas differ across different cross-validation folds.,
scikit-learn/examples/exercises/plot_cv_digits.py,31,Do the plotting,
scikit-learn/examples/exercises/plot_iris_exercise.py,37,fit the model,
scikit-learn/examples/exercises/plot_iris_exercise.py,47,Circle out the test data,
scikit-learn/examples/exercises/plot_iris_exercise.py,60,Put the result into a color plot,
scikit-learn/examples/datasets/plot_iris_dataset.py,1,!/usr/bin/python,
scikit-learn/examples/datasets/plot_iris_dataset.py,2,-*- coding: utf-8 -*-,
scikit-learn/examples/datasets/plot_iris_dataset.py,22,Code source: Gaël Varoquaux,
scikit-learn/examples/datasets/plot_iris_dataset.py,23,Modified for documentation by Jaques Grobler,
scikit-learn/examples/datasets/plot_iris_dataset.py,24,License: BSD 3 clause,
scikit-learn/examples/datasets/plot_iris_dataset.py,31,import some data to play with,
scikit-learn/examples/datasets/plot_iris_dataset.py,33,we only take the first two features.,
scikit-learn/examples/datasets/plot_iris_dataset.py,42,Plot the training points,
scikit-learn/examples/datasets/plot_iris_dataset.py,53,To getter a better understanding of interaction of the dimensions,
scikit-learn/examples/datasets/plot_iris_dataset.py,54,plot the first three PCA dimensions,
scikit-learn/examples/datasets/plot_random_multilabel_dataset.py,45,red,
scikit-learn/examples/datasets/plot_random_multilabel_dataset.py,46,blue,
scikit-learn/examples/datasets/plot_random_multilabel_dataset.py,47,purple,
scikit-learn/examples/datasets/plot_random_multilabel_dataset.py,48,yellow,
scikit-learn/examples/datasets/plot_random_multilabel_dataset.py,49,orange,
scikit-learn/examples/datasets/plot_random_multilabel_dataset.py,50,green,
scikit-learn/examples/datasets/plot_random_multilabel_dataset.py,51,brown,
scikit-learn/examples/datasets/plot_random_multilabel_dataset.py,54,Use same random seed for multiple calls to make_multilabel_classification to,
scikit-learn/examples/datasets/plot_random_multilabel_dataset.py,55,ensure same distributions,
scikit-learn/examples/datasets/plot_digits_last_image.py,1,!/usr/bin/python,
scikit-learn/examples/datasets/plot_digits_last_image.py,2,-*- coding: utf-8 -*-,
scikit-learn/examples/datasets/plot_digits_last_image.py,21,Code source: Gaël Varoquaux,
scikit-learn/examples/datasets/plot_digits_last_image.py,22,Modified for documentation by Jaques Grobler,
scikit-learn/examples/datasets/plot_digits_last_image.py,23,License: BSD 3 clause,
scikit-learn/examples/datasets/plot_digits_last_image.py,29,Load the digits dataset,
scikit-learn/examples/datasets/plot_digits_last_image.py,32,Display the first digit,
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,32,,
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,33,Random Forest Feature Importance on Breast Cancer Data,
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,34,------------------------------------------------------,
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,35,"First, we train a random forest on the breast cancer dataset and evaluate",
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,36,its accuracy on a test set:,
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,45,,
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,46,"Next, we plot the tree based feature importance and the permutation",
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,47,importance. The permutation importance plot shows that permuting a feature,
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,48,"drops the accuracy by at most `0.012`, which would suggest that none of the",
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,49,features are important. This is in contradiction with the high test accuracy,
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,50,computed above: some feature must be important. The permutation importance,
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,51,is calculated on the training set to show how much the model relies on each,
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,52,feature during training.,
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,71,,
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,72,Handling Multicollinear Features,
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,73,--------------------------------,
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,74,"When features are collinear, permutating one feature will have little",
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,75,effect on the models performance because it can get the same information,
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,76,from a correlated feature. One way to handle multicollinear features is by,
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,77,"performing hierarchical clustering on the Spearman rank-order correlations,",
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,78,"picking a threshold, and keeping a single feature from each cluster. First,",
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,79,we plot a heatmap of the correlated features:,
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,95,,
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,96,"Next, we manually pick a threshold by visual inspection of the dendrogram",
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,97,to group our features into clusters and choose a feature from each cluster to,
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,98,"keep, select those features from our dataset, and train a new random forest.",
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,99,The test accuracy of the new random forest did not change much compared to,
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,100,the random forest trained on the complete dataset.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,39,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,40,The dataset: wages,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,41,------------------,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,42,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,43,We fetch the data from `OpenML <http://openml.org/>`_.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,44,Note that setting the parameter `as_frame` to True will retrieve the data,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,45,as a pandas dataframe.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,51,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,52,"Then, we identify features `X` and targets `y`: the column WAGE is our",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,53,"target variable (i.e., the variable which we want to predict).",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,54,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,58,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,59,Note that the dataset contains categorical and numerical variables.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,60,We will need to take this into account when preprocessing the dataset,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,61,thereafter.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,65,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,66,Our target for prediction: the wage.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,67,Wages are described as floating-point number in dollars per hour.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,71,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,72,We split the sample into a train and a test dataset.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,73,Only the train dataset will be used in the following exploratory analysis.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,74,This is a way to emulate a real situation where predictions are performed on,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,75,"an unknown target, and we don't want our analysis and decisions to be biased",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,76,by our knowledge of the test data.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,84,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,85,"First, let's get some insights by looking at the variable distributions and",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,86,at the pairwise relationships between them. Only numerical,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,87,"variables will be used. In the following plot, each dot represents a sample.",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,88,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,89,.. _marginal_dependencies:,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,95,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,96,Looking closely at the WAGE distribution reveals that it has a,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,97,"long tail. For this reason, we should take its logarithm",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,98,to turn it approximately into a normal distribution (linear models such,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,99,as ridge or lasso work best for a normal distribution of error).,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,100,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,101,The WAGE is increasing when EDUCATION is increasing.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,102,Note that the dependence between WAGE and EDUCATION,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,103,"represented here is a marginal dependence, i.e., it describes the behavior",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,104,of a specific variable without keeping the others fixed.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,105,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,106,"Also, the EXPERIENCE and AGE are strongly linearly correlated.",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,107,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,108,.. _the-pipeline:,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,109,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,110,The machine-learning pipeline,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,111,-----------------------------,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,112,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,113,"To design our machine-learning pipeline, we first manually",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,114,check the type of data that we are dealing with:,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,118,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,119,"As seen previously, the dataset contains columns with different data types",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,120,and we need to apply a specific preprocessing for each data types.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,121,In particular categorical variables cannot be included in linear model if not,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,122,"coded as integers first. In addition, to avoid categorical features to be",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,123,"treated as ordered values, we need to one-hot-encode them.",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,124,Our pre-processor will,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,125,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,126,"- one-hot encode (i.e., generate a column by category) the categorical",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,127,columns;,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,128,- as a first approach (we will see after how the normalisation of numerical,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,129,"values will affect our discussion), keep numerical values as they are.",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,143,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,144,To describe the dataset as a linear model we use a ridge regressor,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,145,with a very small regularization and to model the logarithm of the WAGE.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,161,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,162,Processing the dataset,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,163,----------------------,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,164,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,165,"First, we fit the model.",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,169,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,170,Then we check the performance of the computed model plotting its predictions,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,171,"on the test set and computing,",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,172,"for example, the median absolute error of the model.",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,193,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,194,The model learnt is far from being a good model making accurate predictions:,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,195,"this is obvious when looking at the plot above, where good predictions",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,196,should lie on the red line.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,197,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,198,"In the following section, we will interpret the coefficients of the model.",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,199,"While we do so, we should keep in mind that any conclusion we draw is",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,200,"about the model that we build, rather than about the true (real-world)",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,201,generative process of the data.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,202,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,203,Interpreting coefficients: scale matters,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,204,---------------------------------------------,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,205,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,206,"First of all, we can take a look to the values of the coefficients of the",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,207,regressor we have fitted.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,222,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,223,"The AGE coefficient is expressed in ""dollars/hour per living years"" while the",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,224,"EDUCATION one is expressed in ""dollars/hour per years of education"". This",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,225,representation of the coefficients has the benefit of making clear the,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,226,practical predictions of the model: an increase of :math:`1` year in AGE,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,227,"means a decrease of :math:`0.030867` dollars/hour, while an increase of",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,228,:math:`1` year in EDUCATION means an increase of :math:`0.054699`,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,229,"dollars/hour. On the other hand, categorical variables (as UNION or SEX) are",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,230,adimensional numbers taking either the value 0 or 1. Their coefficients,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,231,"are expressed in dollars/hour. Then, we cannot compare the magnitude of",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,232,"different coefficients since the features have different natural scales, and",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,233,"hence value ranges, because of their different unit of measure. This is more",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,234,visible if we plot the coefficients.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,241,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,242,"Indeed, from the plot above the most important factor in determining WAGE",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,243,appears to be the,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,244,"variable UNION, even if our intuition might tell us that variables",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,245,like EXPERIENCE should have more impact.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,246,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,247,Looking at the coefficient plot to gauge feature importance can be,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,248,"misleading as some of them vary on a small scale, while others, like AGE,",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,249,"varies a lot more, several decades.",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,250,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,251,This is visible if we compare the standard deviations of different,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,252,features.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,263,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,264,Multiplying the coefficients by the standard deviation of the related,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,265,feature would reduce all the coefficients to the same unit of measure.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,266,As we will see :ref:`after<scaling_num>` this is equivalent to normalize,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,267,"numerical variables to their standard deviation,",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,268,as :math:`y = \sum{coef_i \times X_i} =,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,269,\sum{(coef_i \times std_i) \times (X_i / std_i)}`.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,270,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,271,"In that way, we emphasize that the",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,272,"greater the variance of a feature, the larger the weight of the corresponding",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,273,"coefficient on the output, all else being equal.",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,285,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,286,"Now that the coefficients have been scaled, we can safely compare them.",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,287,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,288,.. warning::,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,289,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,290,Why does the plot above suggest that an increase in age leads to a,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,291,decrease in wage? Why the :ref:`initial pairplot,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,292,<marginal_dependencies>` is telling the opposite?,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,293,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,294,The plot above tells us about dependencies between a specific feature and,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,295,"the target when all other features remain constant, i.e., **conditional",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,296,dependencies**. An increase of the AGE will induce a decrease,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,297,"of the WAGE when all other features remain constant. On the contrary, an",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,298,increase of the EXPERIENCE will induce an increase of the WAGE when all,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,299,other features remain constant.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,300,"Also, AGE, EXPERIENCE and EDUCATION are the three variables that most",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,301,influence the model.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,302,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,303,Checking the variability of the coefficients,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,304,--------------------------------------------,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,305,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,306,We can check the coefficient variability through cross-validation:,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,307,it is a form of data perturbation (related to,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,308,`resampling <https://en.wikipedia.org/wiki/Resampling_(statistics)>`_).,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,309,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,310,If coefficients vary significantly when changing the input dataset,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,311,"their robustness is not guaranteed, and they should probably be interpreted",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,312,with caution.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,335,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,336,The problem of correlated variables,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,337,-----------------------------------,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,338,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,339,The AGE and EXPERIENCE coefficients are affected by strong variability which,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,340,might be due to the collinearity between the 2 features: as AGE and,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,341,"EXPERIENCE vary together in the data, their effect is difficult to tease",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,342,apart.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,343,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,344,To verify this interpretation we plot the variability of the AGE and,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,345,EXPERIENCE coefficient.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,346,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,347,.. _covariation:,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,358,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,359,Two regions are populated: when the EXPERIENCE coefficient is,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,360,positive the AGE one is negative and viceversa.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,361,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,362,To go further we remove one of the 2 features and check what is the impact,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,363,on the model stability.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,386,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,387,The estimation of the EXPERIENCE coefficient is now less variable and,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,388,remain important for all models trained during cross-validation.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,389,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,390,.. _scaling_num:,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,391,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,392,Preprocessing numerical variables,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,393,---------------------------------,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,394,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,395,"As said above (see "":ref:`the-pipeline`""), we could also choose to scale",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,396,numerical values before training the model.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,397,This can be useful to apply a similar amount regularization to all of them,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,398,in the Ridge.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,399,The preprocessor is redefined in order to subtract the mean and scale,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,400,variables to unit variance.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,410,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,411,The model will stay unchanged.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,424,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,425,"Again, we check the performance of the computed",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,426,"model using, for example, the median absolute error of the model and the R",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,427,squared coefficient.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,447,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,448,"For the coefficient analysis, scaling is not needed this time.",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,459,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,460,We now inspect the coefficients across several cross-validation folds.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,478,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,479,The result is quite similar to the non-normalized case.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,480,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,481,Linear models with regularization,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,482,---------------------------------,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,483,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,484,"In machine-learning practice, Ridge Regression is more often used with",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,485,non-negligible regularization.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,486,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,487,"Above, we limited this regularization to a very little amount.",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,488,Regularization improves the conditioning of the problem and reduces the,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,489,variance of the estimates. RidgeCV applies cross validation in order to,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,490,determine which value of the regularization parameter (`alpha`) is best,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,491,suited for prediction.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,506,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,507,First we check which value of :math:`\alpha` has been selected.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,511,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,512,Then we check the quality of the predictions.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,533,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,534,The ability to reproduce the data of the regularized model is similar to,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,535,the one of the non-regularized model.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,546,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,547,The coefficients are significantly different.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,548,AGE and EXPERIENCE coefficients are both positive but they now have less,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,549,influence on the prediction.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,550,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,551,The regularization reduces the influence of correlated,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,552,variables on the model because the weight is shared between the two,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,553,"predictive variables, so neither alone would have strong weights.",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,554,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,555,"On the other hand, the weights obtained with regularization are more",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,556,stable  (see the :ref:`ridge_regression` User Guide section). This,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,557,"increased stability is visible from the plot, obtained from data",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,558,"perturbations, in a cross validation. This plot can  be compared with",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,559,the :ref:`previous one<covariation>`.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,581,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,582,Linear models with sparse coefficients,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,583,--------------------------------------,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,584,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,585,"Another possibility to take into account correlated variables in the dataset,",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,586,is to estimate sparse coefficients. In some way we already did it manually,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,587,when we dropped the AGE column in a previous Ridge estimation.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,588,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,589,Lasso models (see the :ref:`lasso` User Guide section) estimates sparse,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,590,coefficients. LassoCV applies cross validation in order to,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,591,determine which value of the regularization parameter (`alpha`) is best,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,592,suited for the model estimation.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,607,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,608,First we verify which value of :math:`\alpha` has been selected.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,612,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,613,Then we check the quality of the predictions.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,634,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,635,"For our dataset, again the model is not very predictive.",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,646,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,647,A Lasso model identifies the correlation between,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,648,AGE and EXPERIENCE and suppresses one of them for the sake of the prediction.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,649,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,650,It is important to keep in mind that the coefficients that have been,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,651,dropped may still be related to the outcome by themselves: the model,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,652,chose to suppress them because they bring little or no additional,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,653,"information on top of the other features. Additionnaly, this selection",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,654,"is unstable for correlated features, and should be interpreted with",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,655,caution.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,656,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,657,Lessons learned,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,658,---------------,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,659,,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,660,* Coefficients must be scaled to the same unit of measure to retrieve,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,661,feature importance. Scaling them with the standard-deviation of the,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,662,feature is a useful proxy.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,663,* Coefficients in multivariate linear models represent the dependency,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,664,"between a given feature and the target, **conditional** on the other",
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,665,features.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,666,* Correlated features induce instabilities in the coefficients of linear,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,667,models and their effects cannot be well teased apart.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,668,* Different linear models respond differently to feature correlation and,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,669,coefficients could significantly vary from one another.,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,670,* Inspecting coefficients across the folds of a cross-validation loop,
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,671,gives an idea of their stability.,
scikit-learn/examples/inspection/plot_permutation_importance.py,40,,
scikit-learn/examples/inspection/plot_permutation_importance.py,41,Data Loading and Feature Engineering,
scikit-learn/examples/inspection/plot_permutation_importance.py,42,------------------------------------,
scikit-learn/examples/inspection/plot_permutation_importance.py,43,Let's use pandas to load a copy of the titanic dataset. The following shows,
scikit-learn/examples/inspection/plot_permutation_importance.py,44,how to apply separate preprocessing on numerical and categorical features.,
scikit-learn/examples/inspection/plot_permutation_importance.py,45,,
scikit-learn/examples/inspection/plot_permutation_importance.py,46,We further include two random variables that are not correlated in any way,
scikit-learn/examples/inspection/plot_permutation_importance.py,47,with the target variable (``survived``):,
scikit-learn/examples/inspection/plot_permutation_importance.py,48,,
scikit-learn/examples/inspection/plot_permutation_importance.py,49,- ``random_num`` is a high cardinality numerical variable (as many unique,
scikit-learn/examples/inspection/plot_permutation_importance.py,50,values as records).,
scikit-learn/examples/inspection/plot_permutation_importance.py,51,- ``random_cat`` is a low cardinality categorical variable (3 possible,
scikit-learn/examples/inspection/plot_permutation_importance.py,52,values).,
scikit-learn/examples/inspection/plot_permutation_importance.py,84,,
scikit-learn/examples/inspection/plot_permutation_importance.py,85,Accuracy of the Model,
scikit-learn/examples/inspection/plot_permutation_importance.py,86,---------------------,
scikit-learn/examples/inspection/plot_permutation_importance.py,87,"Prior to inspecting the feature importances, it is important to check that",
scikit-learn/examples/inspection/plot_permutation_importance.py,88,the model predictive performance is high enough. Indeed there would be little,
scikit-learn/examples/inspection/plot_permutation_importance.py,89,interest of inspecting the important features of a non-predictive model.,
scikit-learn/examples/inspection/plot_permutation_importance.py,90,,
scikit-learn/examples/inspection/plot_permutation_importance.py,91,Here one can observe that the train accuracy is very high (the forest model,
scikit-learn/examples/inspection/plot_permutation_importance.py,92,has enough capacity to completely memorize the training set) but it can still,
scikit-learn/examples/inspection/plot_permutation_importance.py,93,generalize well enough to the test set thanks to the built-in bagging of,
scikit-learn/examples/inspection/plot_permutation_importance.py,94,random forests.,
scikit-learn/examples/inspection/plot_permutation_importance.py,95,,
scikit-learn/examples/inspection/plot_permutation_importance.py,96,It might be possible to trade some accuracy on the training set for a,
scikit-learn/examples/inspection/plot_permutation_importance.py,97,slightly better accuracy on the test set by limiting the capacity of the,
scikit-learn/examples/inspection/plot_permutation_importance.py,98,trees (for instance by setting ``min_samples_leaf=5`` or,
scikit-learn/examples/inspection/plot_permutation_importance.py,99,``min_samples_leaf=10``) so as to limit overfitting while not introducing too,
scikit-learn/examples/inspection/plot_permutation_importance.py,100,much underfitting.,
scikit-learn/examples/inspection/plot_permutation_importance.py,101,,
scikit-learn/examples/inspection/plot_permutation_importance.py,102,However let's keep our high capacity random forest model for now so as to,
scikit-learn/examples/inspection/plot_permutation_importance.py,103,illustrate some pitfalls with feature importance on variables with many,
scikit-learn/examples/inspection/plot_permutation_importance.py,104,unique values.,
scikit-learn/examples/inspection/plot_permutation_importance.py,109,,
scikit-learn/examples/inspection/plot_permutation_importance.py,110,Tree's Feature Importance from Mean Decrease in Impurity (MDI),
scikit-learn/examples/inspection/plot_permutation_importance.py,111,--------------------------------------------------------------,
scikit-learn/examples/inspection/plot_permutation_importance.py,112,The impurity-based feature importance ranks the numerical features to be the,
scikit-learn/examples/inspection/plot_permutation_importance.py,113,"most important features. As a result, the non-predictive ``random_num``",
scikit-learn/examples/inspection/plot_permutation_importance.py,114,variable is ranked the most important!,
scikit-learn/examples/inspection/plot_permutation_importance.py,115,,
scikit-learn/examples/inspection/plot_permutation_importance.py,116,This problem stems from two limitations of impurity-based feature,
scikit-learn/examples/inspection/plot_permutation_importance.py,117,importances:,
scikit-learn/examples/inspection/plot_permutation_importance.py,118,,
scikit-learn/examples/inspection/plot_permutation_importance.py,119,- impurity-based importances are biased towards high cardinality features;,
scikit-learn/examples/inspection/plot_permutation_importance.py,120,- impurity-based importances are computed on training set statistics and,
scikit-learn/examples/inspection/plot_permutation_importance.py,121,therefore do not reflect the ability of feature to be useful to make,
scikit-learn/examples/inspection/plot_permutation_importance.py,122,predictions that generalize to the test set (when the model has enough,
scikit-learn/examples/inspection/plot_permutation_importance.py,123,capacity).,
scikit-learn/examples/inspection/plot_permutation_importance.py,144,,
scikit-learn/examples/inspection/plot_permutation_importance.py,145,"As an alternative, the permutation importances of ``rf`` are computed on a",
scikit-learn/examples/inspection/plot_permutation_importance.py,146,"held out test set. This shows that the low cardinality categorical feature,",
scikit-learn/examples/inspection/plot_permutation_importance.py,147,``sex`` is the most important feature.,
scikit-learn/examples/inspection/plot_permutation_importance.py,148,,
scikit-learn/examples/inspection/plot_permutation_importance.py,149,Also note that both random features have very low importances (close to 0) as,
scikit-learn/examples/inspection/plot_permutation_importance.py,150,expected.,
scikit-learn/examples/inspection/plot_permutation_importance.py,162,,
scikit-learn/examples/inspection/plot_permutation_importance.py,163,It is also possible to compute the permutation importances on the training,
scikit-learn/examples/inspection/plot_permutation_importance.py,164,set. This reveals that ``random_num`` gets a significantly higher importance,
scikit-learn/examples/inspection/plot_permutation_importance.py,165,ranking than when computed on the test set. The difference between those two,
scikit-learn/examples/inspection/plot_permutation_importance.py,166,plots is a confirmation that the RF model has enough capacity to use that,
scikit-learn/examples/inspection/plot_permutation_importance.py,167,random numerical feature to overfit. You can further confirm this by,
scikit-learn/examples/inspection/plot_permutation_importance.py,168,re-running this example with constrained RF with min_samples_leaf=10.,
scikit-learn/examples/inspection/plot_partial_dependence.py,43,noqa,
scikit-learn/examples/inspection/plot_partial_dependence.py,49,,
scikit-learn/examples/inspection/plot_partial_dependence.py,50,California Housing data preprocessing,
scikit-learn/examples/inspection/plot_partial_dependence.py,51,-------------------------------------,
scikit-learn/examples/inspection/plot_partial_dependence.py,52,,
scikit-learn/examples/inspection/plot_partial_dependence.py,53,Center target to avoid gradient boosting init bias: gradient boosting,
scikit-learn/examples/inspection/plot_partial_dependence.py,54,with the 'recursion' method does not account for the initial estimator,
scikit-learn/examples/inspection/plot_partial_dependence.py,55,"(here the average target, by default)",
scikit-learn/examples/inspection/plot_partial_dependence.py,66,,
scikit-learn/examples/inspection/plot_partial_dependence.py,67,Partial Dependence computation for multi-layer perceptron,
scikit-learn/examples/inspection/plot_partial_dependence.py,68,---------------------------------------------------------,
scikit-learn/examples/inspection/plot_partial_dependence.py,69,,
scikit-learn/examples/inspection/plot_partial_dependence.py,70,Let's fit a MLPRegressor and compute single-variable partial dependence,
scikit-learn/examples/inspection/plot_partial_dependence.py,71,plots,
scikit-learn/examples/inspection/plot_partial_dependence.py,83,,
scikit-learn/examples/inspection/plot_partial_dependence.py,84,We configured a pipeline to scale the numerical input features and tuned the,
scikit-learn/examples/inspection/plot_partial_dependence.py,85,neural network size and learning rate to get a reasonable compromise between,
scikit-learn/examples/inspection/plot_partial_dependence.py,86,training time and predictive performance on a test set.,
scikit-learn/examples/inspection/plot_partial_dependence.py,87,,
scikit-learn/examples/inspection/plot_partial_dependence.py,88,"Importantly, this tabular dataset has very different dynamic ranges for its",
scikit-learn/examples/inspection/plot_partial_dependence.py,89,features. Neural networks tend to be very sensitive to features with varying,
scikit-learn/examples/inspection/plot_partial_dependence.py,90,scales and forgetting to preprocess the numeric feature would lead to a very,
scikit-learn/examples/inspection/plot_partial_dependence.py,91,poor model.,
scikit-learn/examples/inspection/plot_partial_dependence.py,92,,
scikit-learn/examples/inspection/plot_partial_dependence.py,93,It would be possible to get even higher predictive performance with a larger,
scikit-learn/examples/inspection/plot_partial_dependence.py,94,neural network but the training would also be significantly more expensive.,
scikit-learn/examples/inspection/plot_partial_dependence.py,95,,
scikit-learn/examples/inspection/plot_partial_dependence.py,96,Note that it is important to check that the model is accurate enough on a,
scikit-learn/examples/inspection/plot_partial_dependence.py,97,test set before plotting the partial dependence since there would be little,
scikit-learn/examples/inspection/plot_partial_dependence.py,98,use in explaining the impact of a given feature on the prediction function of,
scikit-learn/examples/inspection/plot_partial_dependence.py,99,a poor model.,
scikit-learn/examples/inspection/plot_partial_dependence.py,100,,
scikit-learn/examples/inspection/plot_partial_dependence.py,101,Let's now compute the partial dependence plots for this neural network using,
scikit-learn/examples/inspection/plot_partial_dependence.py,102,the model-agnostic (brute-force) method:,
scikit-learn/examples/inspection/plot_partial_dependence.py,106,"We don't compute the 2-way PDP (5, 1) here, because it is a lot slower",
scikit-learn/examples/inspection/plot_partial_dependence.py,107,with the brute method.,
scikit-learn/examples/inspection/plot_partial_dependence.py,117,,
scikit-learn/examples/inspection/plot_partial_dependence.py,118,Partial Dependence computation for Gradient Boosting,
scikit-learn/examples/inspection/plot_partial_dependence.py,119,----------------------------------------------------,
scikit-learn/examples/inspection/plot_partial_dependence.py,120,,
scikit-learn/examples/inspection/plot_partial_dependence.py,121,Let's now fit a GradientBoostingRegressor and compute the partial dependence,
scikit-learn/examples/inspection/plot_partial_dependence.py,122,plots either or one or two variables at a time.,
scikit-learn/examples/inspection/plot_partial_dependence.py,131,,
scikit-learn/examples/inspection/plot_partial_dependence.py,132,"Here, we used the default hyperparameters for the gradient boosting model",
scikit-learn/examples/inspection/plot_partial_dependence.py,133,without any preprocessing as tree-based models are naturally robust to,
scikit-learn/examples/inspection/plot_partial_dependence.py,134,monotonic transformations of numerical features.,
scikit-learn/examples/inspection/plot_partial_dependence.py,135,,
scikit-learn/examples/inspection/plot_partial_dependence.py,136,"Note that on this tabular dataset, Gradient Boosting Machines are both",
scikit-learn/examples/inspection/plot_partial_dependence.py,137,significantly faster to train and more accurate than neural networks. It is,
scikit-learn/examples/inspection/plot_partial_dependence.py,138,also significantly cheaper to tune their hyperparameters (the default tend to,
scikit-learn/examples/inspection/plot_partial_dependence.py,139,work well while this is not often the case for neural networks).,
scikit-learn/examples/inspection/plot_partial_dependence.py,140,,
scikit-learn/examples/inspection/plot_partial_dependence.py,141,"Finally, as we will see next, computing partial dependence plots tree-based",
scikit-learn/examples/inspection/plot_partial_dependence.py,142,models is also orders of magnitude faster making it cheap to compute partial,
scikit-learn/examples/inspection/plot_partial_dependence.py,143,dependence plots for pairs of interacting features:,
scikit-learn/examples/inspection/plot_partial_dependence.py,158,,
scikit-learn/examples/inspection/plot_partial_dependence.py,159,Analysis of the plots,
scikit-learn/examples/inspection/plot_partial_dependence.py,160,---------------------,
scikit-learn/examples/inspection/plot_partial_dependence.py,161,,
scikit-learn/examples/inspection/plot_partial_dependence.py,162,We can clearly see that the median house price shows a linear relationship,
scikit-learn/examples/inspection/plot_partial_dependence.py,163,with the median income (top left) and that the house price drops when the,
scikit-learn/examples/inspection/plot_partial_dependence.py,164,average occupants per household increases (top middle).,
scikit-learn/examples/inspection/plot_partial_dependence.py,165,The top right plot shows that the house age in a district does not have,
scikit-learn/examples/inspection/plot_partial_dependence.py,166,a strong influence on the (median) house price; so does the average rooms,
scikit-learn/examples/inspection/plot_partial_dependence.py,167,per household.,
scikit-learn/examples/inspection/plot_partial_dependence.py,168,The tick marks on the x-axis represent the deciles of the feature values,
scikit-learn/examples/inspection/plot_partial_dependence.py,169,in the training data.,
scikit-learn/examples/inspection/plot_partial_dependence.py,170,,
scikit-learn/examples/inspection/plot_partial_dependence.py,171,We also observe that :class:`~sklearn.neural_network.MLPRegressor` has much,
scikit-learn/examples/inspection/plot_partial_dependence.py,172,smoother predictions than,
scikit-learn/examples/inspection/plot_partial_dependence.py,173,:class:`~sklearn.ensemble.HistGradientBoostingRegressor`. For the plots to be,
scikit-learn/examples/inspection/plot_partial_dependence.py,174,"comparable, it is necessary to subtract the average value of the target",
scikit-learn/examples/inspection/plot_partial_dependence.py,175,"``y``: The 'recursion' method, used by default for",
scikit-learn/examples/inspection/plot_partial_dependence.py,176,":class:`~sklearn.ensemble.HistGradientBoostingRegressor`, does not account",
scikit-learn/examples/inspection/plot_partial_dependence.py,177,for the initial predictor (in our case the average target). Setting the,
scikit-learn/examples/inspection/plot_partial_dependence.py,178,target average to 0 avoids this bias.,
scikit-learn/examples/inspection/plot_partial_dependence.py,179,,
scikit-learn/examples/inspection/plot_partial_dependence.py,180,Partial dependence plots with two target features enable us to visualize,
scikit-learn/examples/inspection/plot_partial_dependence.py,181,interactions among them. The two-way partial dependence plot shows the,
scikit-learn/examples/inspection/plot_partial_dependence.py,182,dependence of median house price on joint values of house age and average,
scikit-learn/examples/inspection/plot_partial_dependence.py,183,occupants per household. We can clearly see an interaction between the,
scikit-learn/examples/inspection/plot_partial_dependence.py,184,"two features: for an average occupancy greater than two, the house price is",
scikit-learn/examples/inspection/plot_partial_dependence.py,185,"nearly independent of the house age, whereas for values less than two there",
scikit-learn/examples/inspection/plot_partial_dependence.py,186,is a strong dependence on age.,
scikit-learn/examples/inspection/plot_partial_dependence.py,188,,
scikit-learn/examples/inspection/plot_partial_dependence.py,189,3D interaction plots,
scikit-learn/examples/inspection/plot_partial_dependence.py,190,--------------------,
scikit-learn/examples/inspection/plot_partial_dependence.py,191,,
scikit-learn/examples/inspection/plot_partial_dependence.py,192,"Let's make the same partial dependence plot for the 2 features interaction,",
scikit-learn/examples/inspection/plot_partial_dependence.py,193,this time in 3 dimensions.,
scikit-learn/examples/inspection/plot_partial_dependence.py,208,pretty init view,
scikit-learn/examples/cluster/plot_mini_batch_kmeans.py,26,,
scikit-learn/examples/cluster/plot_mini_batch_kmeans.py,27,Generate sample data,
scikit-learn/examples/cluster/plot_mini_batch_kmeans.py,35,,
scikit-learn/examples/cluster/plot_mini_batch_kmeans.py,36,Compute clustering with Means,
scikit-learn/examples/cluster/plot_mini_batch_kmeans.py,43,,
scikit-learn/examples/cluster/plot_mini_batch_kmeans.py,44,Compute clustering with MiniBatchKMeans,
scikit-learn/examples/cluster/plot_mini_batch_kmeans.py,52,,
scikit-learn/examples/cluster/plot_mini_batch_kmeans.py,53,Plot result,
scikit-learn/examples/cluster/plot_mini_batch_kmeans.py,59,We want to have the same colors for the same cluster from the,
scikit-learn/examples/cluster/plot_mini_batch_kmeans.py,60,MiniBatchKMeans and the KMeans algorithm. Let's pair the cluster centers per,
scikit-learn/examples/cluster/plot_mini_batch_kmeans.py,61,closest one.,
scikit-learn/examples/cluster/plot_mini_batch_kmeans.py,70,KMeans,
scikit-learn/examples/cluster/plot_mini_batch_kmeans.py,85,MiniBatchKMeans,
scikit-learn/examples/cluster/plot_mini_batch_kmeans.py,100,Initialise the different array to all False,
scikit-learn/examples/cluster/plot_cluster_comparison.py,40,============,
scikit-learn/examples/cluster/plot_cluster_comparison.py,41,Generate datasets. We choose the size big enough to see the scalability,
scikit-learn/examples/cluster/plot_cluster_comparison.py,42,"of the algorithms, but not too big to avoid too long running times",
scikit-learn/examples/cluster/plot_cluster_comparison.py,43,============,
scikit-learn/examples/cluster/plot_cluster_comparison.py,51,Anisotropicly distributed data,
scikit-learn/examples/cluster/plot_cluster_comparison.py,58,blobs with varied variances,
scikit-learn/examples/cluster/plot_cluster_comparison.py,63,============,
scikit-learn/examples/cluster/plot_cluster_comparison.py,64,Set up cluster parameters,
scikit-learn/examples/cluster/plot_cluster_comparison.py,65,============,
scikit-learn/examples/cluster/plot_cluster_comparison.py,95,update parameters with dataset-specific values,
scikit-learn/examples/cluster/plot_cluster_comparison.py,101,normalize dataset for easier parameter selection,
scikit-learn/examples/cluster/plot_cluster_comparison.py,104,estimate bandwidth for mean shift,
scikit-learn/examples/cluster/plot_cluster_comparison.py,107,connectivity matrix for structured Ward,
scikit-learn/examples/cluster/plot_cluster_comparison.py,110,make connectivity symmetric,
scikit-learn/examples/cluster/plot_cluster_comparison.py,113,============,
scikit-learn/examples/cluster/plot_cluster_comparison.py,114,Create cluster objects,
scikit-learn/examples/cluster/plot_cluster_comparison.py,115,============,
scikit-learn/examples/cluster/plot_cluster_comparison.py,153,catch warnings related to kneighbors_graph,
scikit-learn/examples/cluster/plot_cluster_comparison.py,182,add black color for outliers (if any),
scikit-learn/examples/cluster/plot_agglomerative_clustering.py,24,"Authors: Gael Varoquaux, Nelle Varoquaux",
scikit-learn/examples/cluster/plot_agglomerative_clustering.py,25,License: BSD 3 clause,
scikit-learn/examples/cluster/plot_agglomerative_clustering.py,34,Generate sample data,
scikit-learn/examples/cluster/plot_agglomerative_clustering.py,46,Create a graph capturing local connectivity. Larger number of neighbors,
scikit-learn/examples/cluster/plot_agglomerative_clustering.py,47,will give more homogeneous clusters to the cost of computation,
scikit-learn/examples/cluster/plot_agglomerative_clustering.py,48,time. A very large number of neighbors gives more evenly distributed,
scikit-learn/examples/cluster/plot_agglomerative_clustering.py,49,"cluster sizes, but may not impose the local manifold structure of",
scikit-learn/examples/cluster/plot_agglomerative_clustering.py,50,the data,
scikit-learn/examples/cluster/plot_linkage_comparison.py,38,,
scikit-learn/examples/cluster/plot_linkage_comparison.py,39,Generate datasets. We choose the size big enough to see the scalability,
scikit-learn/examples/cluster/plot_linkage_comparison.py,40,"of the algorithms, but not too big to avoid too long running times",
scikit-learn/examples/cluster/plot_linkage_comparison.py,49,Anisotropicly distributed data,
scikit-learn/examples/cluster/plot_linkage_comparison.py,56,blobs with varied variances,
scikit-learn/examples/cluster/plot_linkage_comparison.py,61,,
scikit-learn/examples/cluster/plot_linkage_comparison.py,62,Run the clustering and plot,
scikit-learn/examples/cluster/plot_linkage_comparison.py,64,Set up cluster parameters,
scikit-learn/examples/cluster/plot_linkage_comparison.py,83,update parameters with dataset-specific values,
scikit-learn/examples/cluster/plot_linkage_comparison.py,89,normalize dataset for easier parameter selection,
scikit-learn/examples/cluster/plot_linkage_comparison.py,92,============,
scikit-learn/examples/cluster/plot_linkage_comparison.py,93,Create cluster objects,
scikit-learn/examples/cluster/plot_linkage_comparison.py,94,============,
scikit-learn/examples/cluster/plot_linkage_comparison.py,114,catch warnings related to kneighbors_graph,
scikit-learn/examples/cluster/plot_coin_ward_segmentation.py,11,"Author : Vincent Michel, 2010",
scikit-learn/examples/cluster/plot_coin_ward_segmentation.py,12,"Alexandre Gramfort, 2011",
scikit-learn/examples/cluster/plot_coin_ward_segmentation.py,13,License: BSD 3 clause,
scikit-learn/examples/cluster/plot_coin_ward_segmentation.py,32,these were introduced in skimage-0.14,
scikit-learn/examples/cluster/plot_coin_ward_segmentation.py,38,,
scikit-learn/examples/cluster/plot_coin_ward_segmentation.py,39,Generate data,
scikit-learn/examples/cluster/plot_coin_ward_segmentation.py,42,Resize it to 20% of the original size to speed up the processing,
scikit-learn/examples/cluster/plot_coin_ward_segmentation.py,43,Applying a Gaussian filter for smoothing prior to down-scaling,
scikit-learn/examples/cluster/plot_coin_ward_segmentation.py,44,reduces aliasing artifacts.,
scikit-learn/examples/cluster/plot_coin_ward_segmentation.py,51,,
scikit-learn/examples/cluster/plot_coin_ward_segmentation.py,52,Define the structure A of the data. Pixels connected to their neighbors.,
scikit-learn/examples/cluster/plot_coin_ward_segmentation.py,55,,
scikit-learn/examples/cluster/plot_coin_ward_segmentation.py,56,Compute clustering,
scikit-learn/examples/cluster/plot_coin_ward_segmentation.py,59,number of regions,
scikit-learn/examples/cluster/plot_coin_ward_segmentation.py,68,,
scikit-learn/examples/cluster/plot_coin_ward_segmentation.py,69,Plot the results on an image,
scikit-learn/examples/cluster/plot_agglomerative_clustering_metrics.py,35,Author: Gael Varoquaux,
scikit-learn/examples/cluster/plot_agglomerative_clustering_metrics.py,36,License: BSD 3-Clause or CC-0,
scikit-learn/examples/cluster/plot_agglomerative_clustering_metrics.py,46,Generate waveform data,
scikit-learn/examples/cluster/plot_agglomerative_clustering_metrics.py,61,Make the noise sparse,
scikit-learn/examples/cluster/plot_agglomerative_clustering_metrics.py,76,Plot the ground-truth labelling,
scikit-learn/examples/cluster/plot_agglomerative_clustering_metrics.py,91,Plot the distances,
scikit-learn/examples/cluster/plot_agglomerative_clustering_metrics.py,115,Plot clustering results,
scikit-learn/examples/cluster/plot_agglomerative_dendrogram.py,1,"Authors: Mathew Kallada, Andreas Mueller",
scikit-learn/examples/cluster/plot_agglomerative_dendrogram.py,2,License: BSD 3 clause,
scikit-learn/examples/cluster/plot_agglomerative_dendrogram.py,20,Create linkage matrix and then plot the dendrogram,
scikit-learn/examples/cluster/plot_agglomerative_dendrogram.py,22,create the counts of samples under each node,
scikit-learn/examples/cluster/plot_agglomerative_dendrogram.py,29,leaf node,
scikit-learn/examples/cluster/plot_agglomerative_dendrogram.py,37,Plot the corresponding dendrogram,
scikit-learn/examples/cluster/plot_agglomerative_dendrogram.py,44,setting distance_threshold=0 ensures we compute the full tree.,
scikit-learn/examples/cluster/plot_agglomerative_dendrogram.py,49,plot the top three levels of the dendrogram,
scikit-learn/examples/cluster/plot_birch_vs_minibatchkmeans.py,16,Authors: Manoj Kumar <manojkumarsivaraj334@gmail.com,
scikit-learn/examples/cluster/plot_birch_vs_minibatchkmeans.py,17,Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>,
scikit-learn/examples/cluster/plot_birch_vs_minibatchkmeans.py,18,License: BSD 3 clause,
scikit-learn/examples/cluster/plot_birch_vs_minibatchkmeans.py,32,Generate centers for the blobs so that it forms a 10 X 10 grid.,
scikit-learn/examples/cluster/plot_birch_vs_minibatchkmeans.py,39,Generate blobs to do a comparison between MiniBatchKMeans and Birch.,
scikit-learn/examples/cluster/plot_birch_vs_minibatchkmeans.py,42,Use all colors that matplotlib provides by default.,
scikit-learn/examples/cluster/plot_birch_vs_minibatchkmeans.py,48,Compute clustering with Birch with and without the final clustering step,
scikit-learn/examples/cluster/plot_birch_vs_minibatchkmeans.py,49,and plot.,
scikit-learn/examples/cluster/plot_birch_vs_minibatchkmeans.py,61,Plot result,
scikit-learn/examples/cluster/plot_birch_vs_minibatchkmeans.py,80,Compute clustering with MiniBatchKMeans.,
scikit-learn/examples/cluster/plot_dbscan.py,1,-*- coding: utf-8 -*-,
scikit-learn/examples/cluster/plot_dbscan.py,20,,
scikit-learn/examples/cluster/plot_dbscan.py,21,Generate sample data,
scikit-learn/examples/cluster/plot_dbscan.py,28,,
scikit-learn/examples/cluster/plot_dbscan.py,29,Compute DBSCAN,
scikit-learn/examples/cluster/plot_dbscan.py,35,"Number of clusters in labels, ignoring noise if present.",
scikit-learn/examples/cluster/plot_dbscan.py,51,,
scikit-learn/examples/cluster/plot_dbscan.py,52,Plot result,
scikit-learn/examples/cluster/plot_dbscan.py,55,Black removed and is used for noise instead.,
scikit-learn/examples/cluster/plot_dbscan.py,61,Black used for noise.,
scikit-learn/examples/cluster/plot_affinity_propagation.py,17,,
scikit-learn/examples/cluster/plot_affinity_propagation.py,18,Generate sample data,
scikit-learn/examples/cluster/plot_affinity_propagation.py,23,,
scikit-learn/examples/cluster/plot_affinity_propagation.py,24,Compute Affinity Propagation,
scikit-learn/examples/cluster/plot_affinity_propagation.py,42,,
scikit-learn/examples/cluster/plot_affinity_propagation.py,43,Plot result,
scikit-learn/examples/cluster/plot_segmentation_toy.py,29,Authors:  Emmanuelle Gouillart <emmanuelle.gouillart@normalesup.org>,
scikit-learn/examples/cluster/plot_segmentation_toy.py,30,Gael Varoquaux <gael.varoquaux@normalesup.org>,
scikit-learn/examples/cluster/plot_segmentation_toy.py,31,License: BSD 3 clause,
scikit-learn/examples/cluster/plot_segmentation_toy.py,54,,
scikit-learn/examples/cluster/plot_segmentation_toy.py,55,4 circles,
scikit-learn/examples/cluster/plot_segmentation_toy.py,58,We use a mask that limits to the foreground: the problem that we are,
scikit-learn/examples/cluster/plot_segmentation_toy.py,59,"interested in here is not separating the objects from the background,",
scikit-learn/examples/cluster/plot_segmentation_toy.py,60,but separating them one from the other.,
scikit-learn/examples/cluster/plot_segmentation_toy.py,66,Convert the image into a graph with the value of the gradient on the,
scikit-learn/examples/cluster/plot_segmentation_toy.py,67,edges.,
scikit-learn/examples/cluster/plot_segmentation_toy.py,70,Take a decreasing function of the gradient: we take it weakly,
scikit-learn/examples/cluster/plot_segmentation_toy.py,71,dependent from the gradient the segmentation is close to a voronoi,
scikit-learn/examples/cluster/plot_segmentation_toy.py,74,"Force the solver to be arpack, since amg is numerically",
scikit-learn/examples/cluster/plot_segmentation_toy.py,75,unstable on this example,
scikit-learn/examples/cluster/plot_segmentation_toy.py,83,,
scikit-learn/examples/cluster/plot_segmentation_toy.py,84,2 circles,
scikit-learn/examples/cluster/plot_inductive_clustering.py,21,Authors: Chirag Nagpal,
scikit-learn/examples/cluster/plot_inductive_clustering.py,22,Christos Aridas,
scikit-learn/examples/cluster/plot_inductive_clustering.py,67,Generate some training data from clustering,
scikit-learn/examples/cluster/plot_inductive_clustering.py,74,Train a clustering algorithm on the training data and get the cluster labels,
scikit-learn/examples/cluster/plot_inductive_clustering.py,85,Generate new samples and plot them along with the original dataset,
scikit-learn/examples/cluster/plot_inductive_clustering.py,96,Declare the inductive learning model that it will be used to,
scikit-learn/examples/cluster/plot_inductive_clustering.py,97,predict cluster membership for unknown instances,
scikit-learn/examples/cluster/plot_inductive_clustering.py,108,Plotting decision regions,
scikit-learn/examples/cluster/plot_color_quantization.py,1,-*- coding: utf-8 -*-,
scikit-learn/examples/cluster/plot_color_quantization.py,21,Authors: Robert Layton <robertlayton@gmail.com>,
scikit-learn/examples/cluster/plot_color_quantization.py,22,Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/examples/cluster/plot_color_quantization.py,23,Mathieu Blondel <mathieu@mblondel.org>,
scikit-learn/examples/cluster/plot_color_quantization.py,24,,
scikit-learn/examples/cluster/plot_color_quantization.py,25,License: BSD 3 clause,
scikit-learn/examples/cluster/plot_color_quantization.py,38,Load the Summer Palace photo,
scikit-learn/examples/cluster/plot_color_quantization.py,41,Convert to floats instead of the default 8 bits integer coding. Dividing by,
scikit-learn/examples/cluster/plot_color_quantization.py,42,255 is important so that plt.imshow behaves works well on float data (need to,
scikit-learn/examples/cluster/plot_color_quantization.py,43,be in the range [0-1]),
scikit-learn/examples/cluster/plot_color_quantization.py,46,Load Image and transform to a 2D numpy array.,
scikit-learn/examples/cluster/plot_color_quantization.py,57,Get labels for all points,
scikit-learn/examples/cluster/plot_color_quantization.py,84,"Display all results, alongside original image",
scikit-learn/examples/cluster/plot_kmeans_digits.py,79,"in this case the seeding of the centers is deterministic, hence we run the",
scikit-learn/examples/cluster/plot_kmeans_digits.py,80,kmeans algorithm only once with n_init=1,
scikit-learn/examples/cluster/plot_kmeans_digits.py,87,,
scikit-learn/examples/cluster/plot_kmeans_digits.py,88,Visualize the results on PCA-reduced data,
scikit-learn/examples/cluster/plot_kmeans_digits.py,94,Step size of the mesh. Decrease to increase the quality of the VQ.,
scikit-learn/examples/cluster/plot_kmeans_digits.py,95,"point in the mesh [x_min, x_max]x[y_min, y_max].",
scikit-learn/examples/cluster/plot_kmeans_digits.py,97,"Plot the decision boundary. For that, we will assign a color to each",
scikit-learn/examples/cluster/plot_kmeans_digits.py,102,Obtain labels for each point in mesh. Use last trained model.,
scikit-learn/examples/cluster/plot_kmeans_digits.py,105,Put the result into a color plot,
scikit-learn/examples/cluster/plot_kmeans_digits.py,115,Plot the centroids as a white X,
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,42,Generating the sample data from make_blobs,
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,43,This particular setting has one distinct cluster and 3 clusters placed close,
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,44,together.,
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,51,For reproducibility,
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,56,Create a subplot with 1 row and 2 columns,
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,60,The 1st subplot is the silhouette plot,
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,61,"The silhouette coefficient can range from -1, 1 but in this example all",
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,62,"lie within [-0.1, 1]",
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,64,The (n_clusters+1)*10 is for inserting blank space between silhouette,
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,65,"plots of individual clusters, to demarcate them clearly.",
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,68,Initialize the clusterer with n_clusters value and a random generator,
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,69,seed of 10 for reproducibility.,
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,73,The silhouette_score gives the average value for all the samples.,
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,74,This gives a perspective into the density and separation of the formed,
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,75,clusters,
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,80,Compute the silhouette scores for each sample,
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,85,Aggregate the silhouette scores for samples belonging to,
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,86,"cluster i, and sort them",
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,100,Label the silhouette plots with their cluster numbers at the middle,
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,103,Compute the new y_lower for next plot,
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,104,10 for the 0 samples,
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,110,The vertical line for average silhouette score of all the values,
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,113,Clear the yaxis labels / ticks,
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,116,2nd Plot showing the actual clusters formed,
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,121,Labeling the clusters,
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,123,Draw white circles at cluster centers,
scikit-learn/examples/cluster/plot_digits_agglomeration.py,1,!/usr/bin/python,
scikit-learn/examples/cluster/plot_digits_agglomeration.py,2,-*- coding: utf-8 -*-,
scikit-learn/examples/cluster/plot_digits_agglomeration.py,14,Code source: Gaël Varoquaux,
scikit-learn/examples/cluster/plot_digits_agglomeration.py,15,Modified for documentation by Jaques Grobler,
scikit-learn/examples/cluster/plot_digits_agglomeration.py,16,License: BSD 3 clause,
scikit-learn/examples/cluster/plot_ward_structured_vs_unstructured.py,22,"Authors : Vincent Michel, 2010",
scikit-learn/examples/cluster/plot_ward_structured_vs_unstructured.py,23,"Alexandre Gramfort, 2010",
scikit-learn/examples/cluster/plot_ward_structured_vs_unstructured.py,24,"Gael Varoquaux, 2010",
scikit-learn/examples/cluster/plot_ward_structured_vs_unstructured.py,25,License: BSD 3 clause,
scikit-learn/examples/cluster/plot_ward_structured_vs_unstructured.py,36,,
scikit-learn/examples/cluster/plot_ward_structured_vs_unstructured.py,37,Generate data (swiss roll dataset),
scikit-learn/examples/cluster/plot_ward_structured_vs_unstructured.py,41,Make it thinner,
scikit-learn/examples/cluster/plot_ward_structured_vs_unstructured.py,44,,
scikit-learn/examples/cluster/plot_ward_structured_vs_unstructured.py,45,Compute clustering,
scikit-learn/examples/cluster/plot_ward_structured_vs_unstructured.py,54,,
scikit-learn/examples/cluster/plot_ward_structured_vs_unstructured.py,55,Plot result,
scikit-learn/examples/cluster/plot_ward_structured_vs_unstructured.py,66,,
scikit-learn/examples/cluster/plot_ward_structured_vs_unstructured.py,67,Define the structure A of the data. Here a 10 nearest neighbors,
scikit-learn/examples/cluster/plot_ward_structured_vs_unstructured.py,71,,
scikit-learn/examples/cluster/plot_ward_structured_vs_unstructured.py,72,Compute clustering,
scikit-learn/examples/cluster/plot_ward_structured_vs_unstructured.py,82,,
scikit-learn/examples/cluster/plot_ward_structured_vs_unstructured.py,83,Plot result,
scikit-learn/examples/cluster/plot_kmeans_stability_low_dim_dense.py,26,Author: Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/examples/cluster/plot_kmeans_stability_low_dim_dense.py,27,License: BSD 3 clause,
scikit-learn/examples/cluster/plot_kmeans_stability_low_dim_dense.py,40,Number of run (with randomly generated dataset) for each strategy so as,
scikit-learn/examples/cluster/plot_kmeans_stability_low_dim_dense.py,41,to be able to compute an estimate of the standard deviation,
scikit-learn/examples/cluster/plot_kmeans_stability_low_dim_dense.py,44,k-means models can do several random inits so as to be able to trade,
scikit-learn/examples/cluster/plot_kmeans_stability_low_dim_dense.py,45,CPU time for convergence robustness,
scikit-learn/examples/cluster/plot_kmeans_stability_low_dim_dense.py,48,Datasets generation parameters,
scikit-learn/examples/cluster/plot_kmeans_stability_low_dim_dense.py,70,Part 1: Quantitative evaluation of various init methods,
scikit-learn/examples/cluster/plot_kmeans_stability_low_dim_dense.py,102,Part 2: Qualitative visual inspection of the convergence,
scikit-learn/examples/cluster/plot_adjusted_for_chance_measures.py,25,Author: Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/examples/cluster/plot_adjusted_for_chance_measures.py,26,License: BSD 3 clause,
scikit-learn/examples/cluster/plot_adjusted_for_chance_measures.py,68,2 independent random clusterings with equal cluster number,
scikit-learn/examples/cluster/plot_adjusted_for_chance_measures.py,96,Random labeling with varying n_clusters against ground class labels,
scikit-learn/examples/cluster/plot_adjusted_for_chance_measures.py,97,with fixed number of clusters,
scikit-learn/examples/cluster/plot_digits_linkage.py,21,Authors: Gael Varoquaux,
scikit-learn/examples/cluster/plot_digits_linkage.py,22,License: BSD 3 clause (C) INRIA 2014,
scikit-learn/examples/cluster/plot_digits_linkage.py,39,Having a larger dataset shows more clearly the behavior of the,
scikit-learn/examples/cluster/plot_digits_linkage.py,40,"methods, but we multiply the size of the dataset only by 2, as the",
scikit-learn/examples/cluster/plot_digits_linkage.py,41,cost of the hierarchical clustering methods are strongly,
scikit-learn/examples/cluster/plot_digits_linkage.py,42,super-linear in n_samples,
scikit-learn/examples/cluster/plot_digits_linkage.py,55,----------------------------------------------------------------------,
scikit-learn/examples/cluster/plot_digits_linkage.py,56,Visualize the clustering,
scikit-learn/examples/cluster/plot_digits_linkage.py,74,----------------------------------------------------------------------,
scikit-learn/examples/cluster/plot_digits_linkage.py,75,2D embedding of the digits dataset,
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,16,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,17,License: BSD 3 clause,
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,37,,
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,38,Generate data,
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,40,image size,
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,51,smooth data,
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,59,add noise,
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,61,,
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,62,Compute the coefs of a Bayesian Ridge with GridSearch,
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,63,cross-validation generator for model selection,
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,68,Ward agglomeration followed by BayesianRidge,
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,73,Select the optimal number of parcels with grid search,
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,75,set the best parameters,
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,80,Anova univariate feature selection followed by BayesianRidge,
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,81,caching function,
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,84,Select the optimal percentage of features with grid search,
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,86,set the best parameters,
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,91,,
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,92,Inverse the transformation to plot the results on an image,
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,107,"Attempt to remove the temporary cachedir, but don't worry if it fails",
scikit-learn/examples/cluster/plot_dict_face_patches.py,36,,
scikit-learn/examples/cluster/plot_dict_face_patches.py,37,Learn the dictionary of images,
scikit-learn/examples/cluster/plot_dict_face_patches.py,47,The online learning part: cycle over the whole dataset 6 times,
scikit-learn/examples/cluster/plot_dict_face_patches.py,69,,
scikit-learn/examples/cluster/plot_dict_face_patches.py,70,Plot the results,
scikit-learn/examples/cluster/plot_mean_shift.py,19,,
scikit-learn/examples/cluster/plot_mean_shift.py,20,Generate sample data,
scikit-learn/examples/cluster/plot_mean_shift.py,24,,
scikit-learn/examples/cluster/plot_mean_shift.py,25,Compute clustering with MeanShift,
scikit-learn/examples/cluster/plot_mean_shift.py,27,The following bandwidth can be automatically detected using,
scikit-learn/examples/cluster/plot_mean_shift.py,40,,
scikit-learn/examples/cluster/plot_mean_shift.py,41,Plot result,
scikit-learn/examples/cluster/plot_cluster_iris.py,1,!/usr/bin/python,
scikit-learn/examples/cluster/plot_cluster_iris.py,2,-*- coding: utf-8 -*-,
scikit-learn/examples/cluster/plot_cluster_iris.py,22,Code source: Gaël Varoquaux,
scikit-learn/examples/cluster/plot_cluster_iris.py,23,Modified for documentation by Jaques Grobler,
scikit-learn/examples/cluster/plot_cluster_iris.py,24,License: BSD 3 clause,
scikit-learn/examples/cluster/plot_cluster_iris.py,28,"Though the following import is not directly being used, it is required",
scikit-learn/examples/cluster/plot_cluster_iris.py,29,for 3D projection to work,
scikit-learn/examples/cluster/plot_cluster_iris.py,67,Plot the ground truth,
scikit-learn/examples/cluster/plot_cluster_iris.py,79,Reorder the labels to have colors matching the cluster results,
scikit-learn/examples/cluster/plot_kmeans_assumptions.py,14,Author: Phil Roth <mr.phil.roth@gmail.com>,
scikit-learn/examples/cluster/plot_kmeans_assumptions.py,15,License: BSD 3 clause,
scikit-learn/examples/cluster/plot_kmeans_assumptions.py,29,Incorrect number of clusters,
scikit-learn/examples/cluster/plot_kmeans_assumptions.py,36,Anisotropicly distributed data,
scikit-learn/examples/cluster/plot_kmeans_assumptions.py,45,Different variance,
scikit-learn/examples/cluster/plot_kmeans_assumptions.py,55,Unevenly sized blobs,
scikit-learn/examples/cluster/plot_face_compress.py,1,!/usr/bin/python,
scikit-learn/examples/cluster/plot_face_compress.py,2,-*- coding: utf-8 -*-,
scikit-learn/examples/cluster/plot_face_compress.py,17,Code source: Gaël Varoquaux,
scikit-learn/examples/cluster/plot_face_compress.py,18,Modified for documentation by Jaques Grobler,
scikit-learn/examples/cluster/plot_face_compress.py,19,License: BSD 3 clause,
scikit-learn/examples/cluster/plot_face_compress.py,28,SciPy >= 0.16 have face in misc,
scikit-learn/examples/cluster/plot_face_compress.py,37,"We need an (n_sample, n_feature) array",
scikit-learn/examples/cluster/plot_face_compress.py,43,create an array from labels and values,
scikit-learn/examples/cluster/plot_face_compress.py,50,original face,
scikit-learn/examples/cluster/plot_face_compress.py,54,compressed face,
scikit-learn/examples/cluster/plot_face_compress.py,58,equal bins face,
scikit-learn/examples/cluster/plot_face_compress.py,61,mean,
scikit-learn/examples/cluster/plot_face_compress.py,67,histogram,
scikit-learn/examples/cluster/plot_optics.py,15,Authors: Shane Grigsby <refuge@rocktalus.com>,
scikit-learn/examples/cluster/plot_optics.py,16,Adrin Jalali <adrin.jalali@gmail.com>,
scikit-learn/examples/cluster/plot_optics.py,17,License: BSD 3 clause,
scikit-learn/examples/cluster/plot_optics.py,25,Generate sample data,
scikit-learn/examples/cluster/plot_optics.py,40,Run the fit,
scikit-learn/examples/cluster/plot_optics.py,61,Reachability plot,
scikit-learn/examples/cluster/plot_optics.py,73,OPTICS,
scikit-learn/examples/cluster/plot_optics.py,81,DBSCAN at 0.5,
scikit-learn/examples/cluster/plot_optics.py,89,DBSCAN at 2.,
scikit-learn/examples/cluster/plot_coin_segmentation.py,22,"Author: Gael Varoquaux <gael.varoquaux@normalesup.org>, Brian Cheung",
scikit-learn/examples/cluster/plot_coin_segmentation.py,23,License: BSD 3 clause,
scikit-learn/examples/cluster/plot_coin_segmentation.py,38,these were introduced in skimage-0.14,
scikit-learn/examples/cluster/plot_coin_segmentation.py,44,load the coins as a numpy array,
scikit-learn/examples/cluster/plot_coin_segmentation.py,47,Resize it to 20% of the original size to speed up the processing,
scikit-learn/examples/cluster/plot_coin_segmentation.py,48,Applying a Gaussian filter for smoothing prior to down-scaling,
scikit-learn/examples/cluster/plot_coin_segmentation.py,49,reduces aliasing artifacts.,
scikit-learn/examples/cluster/plot_coin_segmentation.py,54,Convert the image into a graph with the value of the gradient on the,
scikit-learn/examples/cluster/plot_coin_segmentation.py,55,edges.,
scikit-learn/examples/cluster/plot_coin_segmentation.py,58,Take a decreasing function of the gradient: an exponential,
scikit-learn/examples/cluster/plot_coin_segmentation.py,59,"The smaller beta is, the more independent the segmentation is of the",
scikit-learn/examples/cluster/plot_coin_segmentation.py,60,"actual image. For beta=1, the segmentation is close to a voronoi",
scikit-learn/examples/cluster/plot_coin_segmentation.py,65,Apply spectral clustering (this step goes much faster if you have pyamg,
scikit-learn/examples/cluster/plot_coin_segmentation.py,66,installed),
scikit-learn/examples/cluster/plot_coin_segmentation.py,69,,
scikit-learn/examples/cluster/plot_coin_segmentation.py,70,Visualize the resulting regions,
scikit-learn/examples/tree/plot_iris_dtc.py,25,Parameters,
scikit-learn/examples/tree/plot_iris_dtc.py,30,Load data,
scikit-learn/examples/tree/plot_iris_dtc.py,35,We only take the two corresponding features,
scikit-learn/examples/tree/plot_iris_dtc.py,39,Train,
scikit-learn/examples/tree/plot_iris_dtc.py,42,Plot the decision boundary,
scikit-learn/examples/tree/plot_iris_dtc.py,58,Plot the training points,
scikit-learn/examples/tree/plot_tree_regression.py,18,Import the necessary modules and libraries,
scikit-learn/examples/tree/plot_tree_regression.py,23,Create a random dataset,
scikit-learn/examples/tree/plot_tree_regression.py,29,Fit regression model,
scikit-learn/examples/tree/plot_tree_regression.py,35,Predict,
scikit-learn/examples/tree/plot_tree_regression.py,40,Plot the results,
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,26,,
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,27,Total impurity of leaves vs effective alphas of pruned tree,
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,28,---------------------------------------------------------------,
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,29,"Minimal cost complexity pruning recursively finds the node with the ""weakest",
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,30,"link"". The weakest link is characterized by an effective alpha, where the",
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,31,nodes with the smallest effective alpha are pruned first. To get an idea of,
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,32,"what values of ``ccp_alpha`` could be appropriate, scikit-learn provides",
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,33,:func:`DecisionTreeClassifier.cost_complexity_pruning_path` that returns the,
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,34,effective alphas and the corresponding total leaf impurities at each step of,
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,35,"the pruning process. As alpha increases, more of the tree is pruned, which",
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,36,increases the total impurity of its leaves.,
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,44,,
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,45,"In the following plot, the maximum effective alpha value is removed, because",
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,46,it is the trivial tree with only one node.,
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,53,,
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,54,"Next, we train a decision tree using the effective alphas. The last value",
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,55,"in ``ccp_alphas`` is the alpha value that prunes the whole tree,",
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,56,"leaving the tree, ``clfs[-1]``, with one node.",
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,65,,
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,66,"For the remainder of this example, we remove the last element in",
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,67,"``clfs`` and ``ccp_alphas``, because it is the trivial tree with only one",
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,68,node. Here we show that the number of nodes and tree depth decreases as alpha,
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,69,increases.,
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,86,,
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,87,Accuracy vs alpha for training and testing sets,
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,88,----------------------------------------------------,
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,89,When ``ccp_alpha`` is set to zero and keeping the other default parameters,
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,90,"of :class:`DecisionTreeClassifier`, the tree overfits, leading to",
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,91,"a 100% training accuracy and 88% testing accuracy. As alpha increases, more",
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,92,"of the tree is pruned, thus creating a decision tree that generalizes better.",
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,93,"In this example, setting ``ccp_alpha=0.015`` maximizes the testing accuracy.",
scikit-learn/examples/tree/plot_unveil_tree_structure.py,32,The decision estimator has an attribute called tree_  which stores the entire,
scikit-learn/examples/tree/plot_unveil_tree_structure.py,33,tree structure and allows access to low level attributes. The binary tree,
scikit-learn/examples/tree/plot_unveil_tree_structure.py,34,tree_ is represented as a number of parallel arrays. The i-th element of each,
scikit-learn/examples/tree/plot_unveil_tree_structure.py,35,array holds information about the node `i`. Node 0 is the tree's root. NOTE:,
scikit-learn/examples/tree/plot_unveil_tree_structure.py,36,"Some of the arrays only apply to either leaves or split nodes, resp. In this",
scikit-learn/examples/tree/plot_unveil_tree_structure.py,37,case the values of nodes of the other type are arbitrary!,
scikit-learn/examples/tree/plot_unveil_tree_structure.py,38,,
scikit-learn/examples/tree/plot_unveil_tree_structure.py,39,"Among those arrays, we have:",
scikit-learn/examples/tree/plot_unveil_tree_structure.py,40,"- left_child, id of the left child of the node",
scikit-learn/examples/tree/plot_unveil_tree_structure.py,41,"- right_child, id of the right child of the node",
scikit-learn/examples/tree/plot_unveil_tree_structure.py,42,"- feature, feature used for splitting the node",
scikit-learn/examples/tree/plot_unveil_tree_structure.py,43,"- threshold, threshold value at the node",
scikit-learn/examples/tree/plot_unveil_tree_structure.py,44,,
scikit-learn/examples/tree/plot_unveil_tree_structure.py,46,"Using those arrays, we can parse the tree structure:",
scikit-learn/examples/tree/plot_unveil_tree_structure.py,55,The tree structure can be traversed to compute various properties such,
scikit-learn/examples/tree/plot_unveil_tree_structure.py,56,as the depth of each node and whether or not it is a leaf.,
scikit-learn/examples/tree/plot_unveil_tree_structure.py,59,seed is the root node id and its parent depth,
scikit-learn/examples/tree/plot_unveil_tree_structure.py,64,If we have a test node,
scikit-learn/examples/tree/plot_unveil_tree_structure.py,89,First let's retrieve the decision path of each sample. The decision_path,
scikit-learn/examples/tree/plot_unveil_tree_structure.py,90,method allows to retrieve the node indicator functions. A non zero element of,
scikit-learn/examples/tree/plot_unveil_tree_structure.py,91,"indicator matrix at the position (i, j) indicates that the sample i goes",
scikit-learn/examples/tree/plot_unveil_tree_structure.py,92,through the node j.,
scikit-learn/examples/tree/plot_unveil_tree_structure.py,96,"Similarly, we can also have the leaves ids reached by each sample.",
scikit-learn/examples/tree/plot_unveil_tree_structure.py,100,"Now, it's possible to get the tests that were used to predict a sample or",
scikit-learn/examples/tree/plot_unveil_tree_structure.py,101,"a group of samples. First, let's make it for the sample.",
scikit-learn/examples/tree/plot_unveil_tree_structure.py,125,"For a group of samples, we have the following common node.",
scikit-learn/examples/tree/plot_tree_regression_multioutput.py,23,Create a random dataset,
scikit-learn/examples/tree/plot_tree_regression_multioutput.py,29,Fit regression model,
scikit-learn/examples/tree/plot_tree_regression_multioutput.py,37,Predict,
scikit-learn/examples/tree/plot_tree_regression_multioutput.py,43,Plot the results,
scikit-learn/examples/preprocessing/plot_discretization_strategies.py,1,-*- coding: utf-8 -*-,
scikit-learn/examples/preprocessing/plot_discretization_strategies.py,19,Author: Tom Dupré la Tour,
scikit-learn/examples/preprocessing/plot_discretization_strategies.py,20,License: BSD 3 clause,
scikit-learn/examples/preprocessing/plot_discretization_strategies.py,36,construct the datasets,
scikit-learn/examples/preprocessing/plot_discretization_strategies.py,69,transform the dataset with KBinsDiscretizer,
scikit-learn/examples/preprocessing/plot_discretization_strategies.py,77,horizontal stripes,
scikit-learn/examples/preprocessing/plot_discretization_strategies.py,80,vertical stripes,
scikit-learn/examples/preprocessing/plot_map_data_to_normal.py,36,Author: Eric Chang <ericchang2017@u.northwestern.edu>,
scikit-learn/examples/preprocessing/plot_map_data_to_normal.py,37,Nicolas Hug <contact@nicolas-hug.com>,
scikit-learn/examples/preprocessing/plot_map_data_to_normal.py,38,License: BSD 3 clause,
scikit-learn/examples/preprocessing/plot_map_data_to_normal.py,58,n_quantiles is set to the training set size rather than the default value,
scikit-learn/examples/preprocessing/plot_map_data_to_normal.py,59,to avoid a warning being raised by this example,
scikit-learn/examples/preprocessing/plot_map_data_to_normal.py,65,lognormal distribution,
scikit-learn/examples/preprocessing/plot_map_data_to_normal.py,68,chi-squared distribution,
scikit-learn/examples/preprocessing/plot_map_data_to_normal.py,72,weibull distribution,
scikit-learn/examples/preprocessing/plot_map_data_to_normal.py,76,gaussian distribution,
scikit-learn/examples/preprocessing/plot_map_data_to_normal.py,80,uniform distribution,
scikit-learn/examples/preprocessing/plot_map_data_to_normal.py,83,bimodal distribution,
scikit-learn/examples/preprocessing/plot_map_data_to_normal.py,89,create plots,
scikit-learn/examples/preprocessing/plot_map_data_to_normal.py,114,perform power transforms and quantile transform,
scikit-learn/examples/preprocessing/plot_all_scaling.py,1,!/usr/bin/env python,
scikit-learn/examples/preprocessing/plot_all_scaling.py,2,-*- coding: utf-8 -*-,
scikit-learn/examples/preprocessing/plot_all_scaling.py,45,Author:  Raghav RV <rvraghav93@gmail.com>,
scikit-learn/examples/preprocessing/plot_all_scaling.py,46,Guillaume Lemaitre <g.lemaitre58@gmail.com>,
scikit-learn/examples/preprocessing/plot_all_scaling.py,47,Thomas Unterthiner,
scikit-learn/examples/preprocessing/plot_all_scaling.py,48,License: BSD 3 clause,
scikit-learn/examples/preprocessing/plot_all_scaling.py,72,Take only 2 features to make visualization easier,
scikit-learn/examples/preprocessing/plot_all_scaling.py,73,Feature of 0 has a long tail distribution.,
scikit-learn/examples/preprocessing/plot_all_scaling.py,74,Feature 5 has a few but very large outliers.,
scikit-learn/examples/preprocessing/plot_all_scaling.py,102,scale the output between 0 and 1 for the colorbar,
scikit-learn/examples/preprocessing/plot_all_scaling.py,105,plasma does not exist in matplotlib < 1.5,
scikit-learn/examples/preprocessing/plot_all_scaling.py,112,define the axis for the first plot,
scikit-learn/examples/preprocessing/plot_all_scaling.py,126,define the axis for the zoomed-in plot,
scikit-learn/examples/preprocessing/plot_all_scaling.py,138,define the axis for the colorbar,
scikit-learn/examples/preprocessing/plot_all_scaling.py,157,The scatter plot,
scikit-learn/examples/preprocessing/plot_all_scaling.py,161,Removing the top and the right spine for aesthetics,
scikit-learn/examples/preprocessing/plot_all_scaling.py,162,make nice axis layout,
scikit-learn/examples/preprocessing/plot_all_scaling.py,170,Histogram for axis X1 (feature 5),
scikit-learn/examples/preprocessing/plot_all_scaling.py,176,Histogram for axis X0 (feature 0),
scikit-learn/examples/preprocessing/plot_all_scaling.py,182,,
scikit-learn/examples/preprocessing/plot_all_scaling.py,183,Two plots will be shown for each scaler/normalizer/transformer. The left,
scikit-learn/examples/preprocessing/plot_all_scaling.py,184,figure will show a scatter plot of the full data set while the right figure,
scikit-learn/examples/preprocessing/plot_all_scaling.py,185,"will exclude the extreme values considering only 99 % of the data set,",
scikit-learn/examples/preprocessing/plot_all_scaling.py,186,"excluding marginal outliers. In addition, the marginal distributions for each",
scikit-learn/examples/preprocessing/plot_all_scaling.py,187,feature will be shown on the side of the scatter plot.,
scikit-learn/examples/preprocessing/plot_all_scaling.py,199,zoom-in,
scikit-learn/examples/preprocessing/plot_all_scaling.py,219,,
scikit-learn/examples/preprocessing/plot_all_scaling.py,220,.. _results:,
scikit-learn/examples/preprocessing/plot_all_scaling.py,221,,
scikit-learn/examples/preprocessing/plot_all_scaling.py,222,Original data,
scikit-learn/examples/preprocessing/plot_all_scaling.py,223,-------------,
scikit-learn/examples/preprocessing/plot_all_scaling.py,224,,
scikit-learn/examples/preprocessing/plot_all_scaling.py,225,"Each transformation is plotted showing two transformed features, with the",
scikit-learn/examples/preprocessing/plot_all_scaling.py,226,"left plot showing the entire dataset, and the right zoomed-in to show the",
scikit-learn/examples/preprocessing/plot_all_scaling.py,227,dataset without the marginal outliers. A large majority of the samples are,
scikit-learn/examples/preprocessing/plot_all_scaling.py,228,"compacted to a specific range, [0, 10] for the median income and [0, 6] for",
scikit-learn/examples/preprocessing/plot_all_scaling.py,229,the number of households. Note that there are some marginal outliers (some,
scikit-learn/examples/preprocessing/plot_all_scaling.py,230,"blocks have more than 1200 households). Therefore, a specific pre-processing",
scikit-learn/examples/preprocessing/plot_all_scaling.py,231,"can be very beneficial depending of the application. In the following, we",
scikit-learn/examples/preprocessing/plot_all_scaling.py,232,present some insights and behaviors of those pre-processing methods in the,
scikit-learn/examples/preprocessing/plot_all_scaling.py,233,presence of marginal outliers.,
scikit-learn/examples/preprocessing/plot_all_scaling.py,237,,
scikit-learn/examples/preprocessing/plot_all_scaling.py,238,StandardScaler,
scikit-learn/examples/preprocessing/plot_all_scaling.py,239,--------------,
scikit-learn/examples/preprocessing/plot_all_scaling.py,240,,
scikit-learn/examples/preprocessing/plot_all_scaling.py,241,``StandardScaler`` removes the mean and scales the data to unit variance.,
scikit-learn/examples/preprocessing/plot_all_scaling.py,242,"However, the outliers have an influence when computing the empirical mean and",
scikit-learn/examples/preprocessing/plot_all_scaling.py,243,standard deviation which shrink the range of the feature values as shown in,
scikit-learn/examples/preprocessing/plot_all_scaling.py,244,the left figure below. Note in particular that because the outliers on each,
scikit-learn/examples/preprocessing/plot_all_scaling.py,245,"feature have different magnitudes, the spread of the transformed data on",
scikit-learn/examples/preprocessing/plot_all_scaling.py,246,"each feature is very different: most of the data lie in the [-2, 4] range for",
scikit-learn/examples/preprocessing/plot_all_scaling.py,247,the transformed median income feature while the same data is squeezed in the,
scikit-learn/examples/preprocessing/plot_all_scaling.py,248,"smaller [-0.2, 0.2] range for the transformed number of households.",
scikit-learn/examples/preprocessing/plot_all_scaling.py,249,,
scikit-learn/examples/preprocessing/plot_all_scaling.py,250,``StandardScaler`` therefore cannot guarantee balanced feature scales in the,
scikit-learn/examples/preprocessing/plot_all_scaling.py,251,presence of outliers.,
scikit-learn/examples/preprocessing/plot_all_scaling.py,255,,
scikit-learn/examples/preprocessing/plot_all_scaling.py,256,MinMaxScaler,
scikit-learn/examples/preprocessing/plot_all_scaling.py,257,------------,
scikit-learn/examples/preprocessing/plot_all_scaling.py,258,,
scikit-learn/examples/preprocessing/plot_all_scaling.py,259,``MinMaxScaler`` rescales the data set such that all feature values are in,
scikit-learn/examples/preprocessing/plot_all_scaling.py,260,"the range [0, 1] as shown in the right panel below. However, this scaling",
scikit-learn/examples/preprocessing/plot_all_scaling.py,261,"compress all inliers in the narrow range [0, 0.005] for the transformed",
scikit-learn/examples/preprocessing/plot_all_scaling.py,262,number of households.,
scikit-learn/examples/preprocessing/plot_all_scaling.py,263,,
scikit-learn/examples/preprocessing/plot_all_scaling.py,264,"As ``StandardScaler``, ``MinMaxScaler`` is very sensitive to the presence of",
scikit-learn/examples/preprocessing/plot_all_scaling.py,265,outliers.,
scikit-learn/examples/preprocessing/plot_all_scaling.py,269,,
scikit-learn/examples/preprocessing/plot_all_scaling.py,270,MaxAbsScaler,
scikit-learn/examples/preprocessing/plot_all_scaling.py,271,------------,
scikit-learn/examples/preprocessing/plot_all_scaling.py,272,,
scikit-learn/examples/preprocessing/plot_all_scaling.py,273,``MaxAbsScaler`` differs from the previous scaler such that the absolute,
scikit-learn/examples/preprocessing/plot_all_scaling.py,274,"values are mapped in the range [0, 1]. On positive only data, this scaler",
scikit-learn/examples/preprocessing/plot_all_scaling.py,275,behaves similarly to ``MinMaxScaler`` and therefore also suffers from the,
scikit-learn/examples/preprocessing/plot_all_scaling.py,276,presence of large outliers.,
scikit-learn/examples/preprocessing/plot_all_scaling.py,280,,
scikit-learn/examples/preprocessing/plot_all_scaling.py,281,RobustScaler,
scikit-learn/examples/preprocessing/plot_all_scaling.py,282,------------,
scikit-learn/examples/preprocessing/plot_all_scaling.py,283,,
scikit-learn/examples/preprocessing/plot_all_scaling.py,284,"Unlike the previous scalers, the centering and scaling statistics of this",
scikit-learn/examples/preprocessing/plot_all_scaling.py,285,scaler are based on percentiles and are therefore not influenced by a few,
scikit-learn/examples/preprocessing/plot_all_scaling.py,286,"number of very large marginal outliers. Consequently, the resulting range of",
scikit-learn/examples/preprocessing/plot_all_scaling.py,287,"the transformed feature values is larger than for the previous scalers and,",
scikit-learn/examples/preprocessing/plot_all_scaling.py,288,"more importantly, are approximately similar: for both features most of the",
scikit-learn/examples/preprocessing/plot_all_scaling.py,289,"transformed values lie in a [-2, 3] range as seen in the zoomed-in figure.",
scikit-learn/examples/preprocessing/plot_all_scaling.py,290,Note that the outliers themselves are still present in the transformed data.,
scikit-learn/examples/preprocessing/plot_all_scaling.py,291,"If a separate outlier clipping is desirable, a non-linear transformation is",
scikit-learn/examples/preprocessing/plot_all_scaling.py,292,required (see below).,
scikit-learn/examples/preprocessing/plot_all_scaling.py,296,,
scikit-learn/examples/preprocessing/plot_all_scaling.py,297,PowerTransformer,
scikit-learn/examples/preprocessing/plot_all_scaling.py,298,----------------,
scikit-learn/examples/preprocessing/plot_all_scaling.py,299,,
scikit-learn/examples/preprocessing/plot_all_scaling.py,300,``PowerTransformer`` applies a power transformation to each feature to make,
scikit-learn/examples/preprocessing/plot_all_scaling.py,301,"the data more Gaussian-like. Currently, ``PowerTransformer`` implements the",
scikit-learn/examples/preprocessing/plot_all_scaling.py,302,Yeo-Johnson and Box-Cox transforms. The power transform finds the optimal,
scikit-learn/examples/preprocessing/plot_all_scaling.py,303,scaling factor to stabilize variance and mimimize skewness through maximum,
scikit-learn/examples/preprocessing/plot_all_scaling.py,304,"likelihood estimation. By default, ``PowerTransformer`` also applies",
scikit-learn/examples/preprocessing/plot_all_scaling.py,305,"zero-mean, unit variance normalization to the transformed output. Note that",
scikit-learn/examples/preprocessing/plot_all_scaling.py,306,Box-Cox can only be applied to strictly positive data. Income and number of,
scikit-learn/examples/preprocessing/plot_all_scaling.py,307,"households happen to be strictly positive, but if negative values are present",
scikit-learn/examples/preprocessing/plot_all_scaling.py,308,the Yeo-Johnson transformed is to be preferred.,
scikit-learn/examples/preprocessing/plot_all_scaling.py,313,,
scikit-learn/examples/preprocessing/plot_all_scaling.py,314,QuantileTransformer (Gaussian output),
scikit-learn/examples/preprocessing/plot_all_scaling.py,315,-------------------------------------,
scikit-learn/examples/preprocessing/plot_all_scaling.py,316,,
scikit-learn/examples/preprocessing/plot_all_scaling.py,317,``QuantileTransformer`` has an additional ``output_distribution`` parameter,
scikit-learn/examples/preprocessing/plot_all_scaling.py,318,allowing to match a Gaussian distribution instead of a uniform distribution.,
scikit-learn/examples/preprocessing/plot_all_scaling.py,319,Note that this non-parametetric transformer introduces saturation artifacts,
scikit-learn/examples/preprocessing/plot_all_scaling.py,320,for extreme values.,
scikit-learn/examples/preprocessing/plot_all_scaling.py,324,,
scikit-learn/examples/preprocessing/plot_all_scaling.py,325,QuantileTransformer (uniform output),
scikit-learn/examples/preprocessing/plot_all_scaling.py,326,------------------------------------,
scikit-learn/examples/preprocessing/plot_all_scaling.py,327,,
scikit-learn/examples/preprocessing/plot_all_scaling.py,328,``QuantileTransformer`` applies a non-linear transformation such that the,
scikit-learn/examples/preprocessing/plot_all_scaling.py,329,probability density function of each feature will be mapped to a uniform,
scikit-learn/examples/preprocessing/plot_all_scaling.py,330,"distribution. In this case, all the data will be mapped in the range [0, 1],",
scikit-learn/examples/preprocessing/plot_all_scaling.py,331,even the outliers which cannot be distinguished anymore from the inliers.,
scikit-learn/examples/preprocessing/plot_all_scaling.py,332,,
scikit-learn/examples/preprocessing/plot_all_scaling.py,333,"As ``RobustScaler``, ``QuantileTransformer`` is robust to outliers in the",
scikit-learn/examples/preprocessing/plot_all_scaling.py,334,sense that adding or removing outliers in the training set will yield,
scikit-learn/examples/preprocessing/plot_all_scaling.py,335,approximately the same transformation on held out data. But contrary to,
scikit-learn/examples/preprocessing/plot_all_scaling.py,336,"``RobustScaler``, ``QuantileTransformer`` will also automatically collapse",
scikit-learn/examples/preprocessing/plot_all_scaling.py,337,any outlier by setting them to the a priori defined range boundaries (0 and,
scikit-learn/examples/preprocessing/plot_all_scaling.py,338,1).,
scikit-learn/examples/preprocessing/plot_all_scaling.py,342,,
scikit-learn/examples/preprocessing/plot_all_scaling.py,343,Normalizer,
scikit-learn/examples/preprocessing/plot_all_scaling.py,344,----------,
scikit-learn/examples/preprocessing/plot_all_scaling.py,345,,
scikit-learn/examples/preprocessing/plot_all_scaling.py,346,"The ``Normalizer`` rescales the vector for each sample to have unit norm,",
scikit-learn/examples/preprocessing/plot_all_scaling.py,347,independently of the distribution of the samples. It can be seen on both,
scikit-learn/examples/preprocessing/plot_all_scaling.py,348,figures below where all samples are mapped onto the unit circle. In our,
scikit-learn/examples/preprocessing/plot_all_scaling.py,349,example the two selected features have only positive values; therefore the,
scikit-learn/examples/preprocessing/plot_all_scaling.py,350,transformed data only lie in the positive quadrant. This would not be the,
scikit-learn/examples/preprocessing/plot_all_scaling.py,351,case if some original features had a mix of positive and negative values.,
scikit-learn/examples/preprocessing/plot_scaling_importance.py,1,!/usr/bin/python,
scikit-learn/examples/preprocessing/plot_scaling_importance.py,2,-*- coding: utf-8 -*-,
scikit-learn/examples/preprocessing/plot_scaling_importance.py,54,Code source: Tyler Lanigan <tylerlanigan@gmail.com>,
scikit-learn/examples/preprocessing/plot_scaling_importance.py,55,Sebastian Raschka <mail@sebastianraschka.com>,
scikit-learn/examples/preprocessing/plot_scaling_importance.py,57,License: BSD 3 clause,
scikit-learn/examples/preprocessing/plot_scaling_importance.py,65,Make a train/test split using 30% test size,
scikit-learn/examples/preprocessing/plot_scaling_importance.py,70,Fit to data and predict using pipelined GNB and PCA.,
scikit-learn/examples/preprocessing/plot_scaling_importance.py,75,"Fit to data and predict using pipelined scaling, GNB and PCA.",
scikit-learn/examples/preprocessing/plot_scaling_importance.py,80,Show prediction accuracies in scaled and unscaled data.,
scikit-learn/examples/preprocessing/plot_scaling_importance.py,87,Extract PCA from pipeline,
scikit-learn/examples/preprocessing/plot_scaling_importance.py,91,Show first principal components,
scikit-learn/examples/preprocessing/plot_scaling_importance.py,95,Use PCA without and with scale on X_train data for visualization.,
scikit-learn/examples/preprocessing/plot_scaling_importance.py,100,visualize standardized vs. untouched dataset with PCA performed,
scikit-learn/examples/preprocessing/plot_discretization_classification.py,1,!/usr/bin/python,
scikit-learn/examples/preprocessing/plot_discretization_classification.py,2,-*- coding: utf-8 -*-,
scikit-learn/examples/preprocessing/plot_discretization_classification.py,31,Code source: Tom Dupré la Tour,
scikit-learn/examples/preprocessing/plot_discretization_classification.py,32,Adapted from plot_classifier_comparison by Gaël Varoquaux and Andreas Müller,
scikit-learn/examples/preprocessing/plot_discretization_classification.py,33,,
scikit-learn/examples/preprocessing/plot_discretization_classification.py,34,License: BSD 3 clause,
scikit-learn/examples/preprocessing/plot_discretization_classification.py,53,step size in the mesh,
scikit-learn/examples/preprocessing/plot_discretization_classification.py,64,"list of (estimator, param_grid), where param_grid is used in GridSearchCV",
scikit-learn/examples/preprocessing/plot_discretization_classification.py,108,iterate over datasets,
scikit-learn/examples/preprocessing/plot_discretization_classification.py,112,"preprocess dataset, split into training and test part",
scikit-learn/examples/preprocessing/plot_discretization_classification.py,117,create the grid for background colors,
scikit-learn/examples/preprocessing/plot_discretization_classification.py,123,plot the dataset first,
scikit-learn/examples/preprocessing/plot_discretization_classification.py,127,plot the training points,
scikit-learn/examples/preprocessing/plot_discretization_classification.py,130,and testing points,
scikit-learn/examples/preprocessing/plot_discretization_classification.py,138,iterate over classifiers,
scikit-learn/examples/preprocessing/plot_discretization_classification.py,149,"plot the decision boundary. For that, we will assign a color to each",
scikit-learn/examples/preprocessing/plot_discretization_classification.py,150,"point in the mesh [x_min, x_max]*[y_min, y_max].",
scikit-learn/examples/preprocessing/plot_discretization_classification.py,156,put the result into a color plot,
scikit-learn/examples/preprocessing/plot_discretization_classification.py,160,plot the training points,
scikit-learn/examples/preprocessing/plot_discretization_classification.py,163,and testing points,
scikit-learn/examples/preprocessing/plot_discretization_classification.py,180,Add suptitles above the figure,
scikit-learn/examples/preprocessing/plot_discretization.py,1,-*- coding: utf-8 -*-,
scikit-learn/examples/preprocessing/plot_discretization.py,32,Author: Andreas Müller,
scikit-learn/examples/preprocessing/plot_discretization.py,33,Hanmin Qin <qinhanmin2005@sina.com>,
scikit-learn/examples/preprocessing/plot_discretization.py,34,License: BSD 3 clause,
scikit-learn/examples/preprocessing/plot_discretization.py,45,construct the dataset,
scikit-learn/examples/preprocessing/plot_discretization.py,51,transform the dataset with KBinsDiscretizer,
scikit-learn/examples/preprocessing/plot_discretization.py,55,predict with original dataset,
scikit-learn/examples/preprocessing/plot_discretization.py,70,predict with transformed dataset,
scikit-learn/examples/manifold/plot_manifold_sphere.py,1,!/usr/bin/python,
scikit-learn/examples/manifold/plot_manifold_sphere.py,2,-*- coding: utf-8 -*-,
scikit-learn/examples/manifold/plot_manifold_sphere.py,30,Author: Jaques Grobler <jaques.grobler@inria.fr>,
scikit-learn/examples/manifold/plot_manifold_sphere.py,31,License: BSD 3 clause,
scikit-learn/examples/manifold/plot_manifold_sphere.py,45,Next line to silence pyflakes.,
scikit-learn/examples/manifold/plot_manifold_sphere.py,48,Variables for manifold learning.,
scikit-learn/examples/manifold/plot_manifold_sphere.py,52,Create our sphere.,
scikit-learn/examples/manifold/plot_manifold_sphere.py,57,Sever the poles from the sphere.,
scikit-learn/examples/manifold/plot_manifold_sphere.py,64,Plot our dataset.,
scikit-learn/examples/manifold/plot_manifold_sphere.py,75,Perform Locally Linear Embedding Manifold learning,
scikit-learn/examples/manifold/plot_manifold_sphere.py,94,Perform Isomap Manifold learning.,
scikit-learn/examples/manifold/plot_manifold_sphere.py,108,Perform Multi-dimensional scaling.,
scikit-learn/examples/manifold/plot_manifold_sphere.py,122,Perform Spectral Embedding.,
scikit-learn/examples/manifold/plot_manifold_sphere.py,137,Perform t-distributed stochastic neighbor embedding.,
scikit-learn/examples/manifold/plot_mds.py,12,Author: Nelle Varoquaux <nelle.varoquaux@gmail.com>,
scikit-learn/examples/manifold/plot_mds.py,13,License: BSD,
scikit-learn/examples/manifold/plot_mds.py,30,Center the data,
scikit-learn/examples/manifold/plot_mds.py,35,Add noise to the similarities,
scikit-learn/examples/manifold/plot_mds.py,50,Rescale the data,
scikit-learn/examples/manifold/plot_mds.py,54,Rotate the data,
scikit-learn/examples/manifold/plot_mds.py,74,Plot the edges,
scikit-learn/examples/manifold/plot_mds.py,76,"a sequence of (*line0*, *line1*, *line2*), where::",
scikit-learn/examples/manifold/plot_mds.py,77,"linen = (x0, y0), (x1, y1), ... (xm, ym)",
scikit-learn/examples/manifold/plot_lle_digits.py,25,Authors: Fabian Pedregosa <fabian.pedregosa@inria.fr>,
scikit-learn/examples/manifold/plot_lle_digits.py,26,Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/examples/manifold/plot_lle_digits.py,27,Mathieu Blondel <mathieu@mblondel.org>,
scikit-learn/examples/manifold/plot_lle_digits.py,28,Gael Varoquaux,
scikit-learn/examples/manifold/plot_lle_digits.py,29,License: BSD 3 clause (C) INRIA 2011,
scikit-learn/examples/manifold/plot_lle_digits.py,46,----------------------------------------------------------------------,
scikit-learn/examples/manifold/plot_lle_digits.py,47,Scale and visualize the embedding vectors,
scikit-learn/examples/manifold/plot_lle_digits.py,60,only print thumbnails with matplotlib > 1.0,
scikit-learn/examples/manifold/plot_lle_digits.py,61,just something big,
scikit-learn/examples/manifold/plot_lle_digits.py,65,don't show points that are too close,
scikit-learn/examples/manifold/plot_lle_digits.py,77,----------------------------------------------------------------------,
scikit-learn/examples/manifold/plot_lle_digits.py,78,Plot images of the digits,
scikit-learn/examples/manifold/plot_lle_digits.py,93,----------------------------------------------------------------------,
scikit-learn/examples/manifold/plot_lle_digits.py,94,Random 2D projection using a random unitary matrix,
scikit-learn/examples/manifold/plot_lle_digits.py,101,----------------------------------------------------------------------,
scikit-learn/examples/manifold/plot_lle_digits.py,102,Projection on to the first 2 principal components,
scikit-learn/examples/manifold/plot_lle_digits.py,111,----------------------------------------------------------------------,
scikit-learn/examples/manifold/plot_lle_digits.py,112,Projection on to the first 2 linear discriminant components,
scikit-learn/examples/manifold/plot_lle_digits.py,116,Make X invertible,
scikit-learn/examples/manifold/plot_lle_digits.py,125,----------------------------------------------------------------------,
scikit-learn/examples/manifold/plot_lle_digits.py,126,Isomap projection of the digits dataset,
scikit-learn/examples/manifold/plot_lle_digits.py,136,----------------------------------------------------------------------,
scikit-learn/examples/manifold/plot_lle_digits.py,137,Locally linear embedding of the digits dataset,
scikit-learn/examples/manifold/plot_lle_digits.py,149,----------------------------------------------------------------------,
scikit-learn/examples/manifold/plot_lle_digits.py,150,Modified Locally linear embedding of the digits dataset,
scikit-learn/examples/manifold/plot_lle_digits.py,162,----------------------------------------------------------------------,
scikit-learn/examples/manifold/plot_lle_digits.py,163,HLLE embedding of the digits dataset,
scikit-learn/examples/manifold/plot_lle_digits.py,175,----------------------------------------------------------------------,
scikit-learn/examples/manifold/plot_lle_digits.py,176,LTSA embedding of the digits dataset,
scikit-learn/examples/manifold/plot_lle_digits.py,187,----------------------------------------------------------------------,
scikit-learn/examples/manifold/plot_lle_digits.py,188,MDS  embedding of the digits dataset,
scikit-learn/examples/manifold/plot_lle_digits.py,198,----------------------------------------------------------------------,
scikit-learn/examples/manifold/plot_lle_digits.py,199,Random Trees embedding of the digits dataset,
scikit-learn/examples/manifold/plot_lle_digits.py,212,----------------------------------------------------------------------,
scikit-learn/examples/manifold/plot_lle_digits.py,213,Spectral embedding of the digits dataset,
scikit-learn/examples/manifold/plot_lle_digits.py,224,----------------------------------------------------------------------,
scikit-learn/examples/manifold/plot_lle_digits.py,225,t-SNE embedding of the digits dataset,
scikit-learn/examples/manifold/plot_lle_digits.py,235,----------------------------------------------------------------------,
scikit-learn/examples/manifold/plot_lle_digits.py,236,NCA projection of the digits dataset,
scikit-learn/examples/manifold/plot_compare_methods.py,22,Author: Jake Vanderplas -- <vanderplas@astro.washington.edu>,
scikit-learn/examples/manifold/plot_compare_methods.py,36,Next line to silence pyflakes. This import is needed.,
scikit-learn/examples/manifold/plot_compare_methods.py,44,Create figure,
scikit-learn/examples/manifold/plot_compare_methods.py,49,Add 3d scatter plot,
scikit-learn/examples/manifold/plot_compare_methods.py,54,Set-up manifold methods,
scikit-learn/examples/manifold/plot_compare_methods.py,70,Plot results,
scikit-learn/examples/manifold/plot_t_sne_perplexity.py,26,Author: Narine Kokhlikyan <narine@slice.com>,
scikit-learn/examples/manifold/plot_t_sne_perplexity.py,27,License: BSD,
scikit-learn/examples/manifold/plot_t_sne_perplexity.py,71,Another example using s-curve,
scikit-learn/examples/manifold/plot_t_sne_perplexity.py,96,Another example using a 2D uniform grid,
scikit-learn/examples/manifold/plot_swissroll.py,10,Author: Fabian Pedregosa -- <fabian.pedregosa@inria.fr>,
scikit-learn/examples/manifold/plot_swissroll.py,11,License: BSD 3 clause (C) INRIA 2011,
scikit-learn/examples/manifold/plot_swissroll.py,17,This import is needed to modify the way figure behaves,
scikit-learn/examples/manifold/plot_swissroll.py,21,----------------------------------------------------------------------,
scikit-learn/examples/manifold/plot_swissroll.py,22,Locally linear embedding of the swiss roll,
scikit-learn/examples/manifold/plot_swissroll.py,32,----------------------------------------------------------------------,
scikit-learn/examples/manifold/plot_swissroll.py,33,Plot result,
scikit-learn/examples/model_selection/plot_learning_curve.py,108,Plot learning curve,
scikit-learn/examples/model_selection/plot_learning_curve.py,122,Plot n_samples vs fit_times,
scikit-learn/examples/model_selection/plot_learning_curve.py,131,Plot fit_time vs score,
scikit-learn/examples/model_selection/plot_learning_curve.py,148,Cross validation with 100 iterations to get smoother mean test and train,
scikit-learn/examples/model_selection/plot_learning_curve.py,149,"score curves, each time with 20% data randomly selected as a validation set.",
scikit-learn/examples/model_selection/plot_learning_curve.py,157,SVC is more expensive so we do a lower number of CV iterations:,
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,45,Author: Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,46,Peter Prettenhofer <peter.prettenhofer@gmail.com>,
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,47,Mathieu Blondel <mathieu@mblondel.org>,
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,48,License: BSD 3 clause,
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,62,Display progress logs on stdout,
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,67,,
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,68,Load some categories from the training set,
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,73,Uncomment the following to do the analysis on all the categories,
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,74,categories = None,
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,84,,
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,85,Define a pipeline combining a text feature extractor with a simple,
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,86,classifier,
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,93,uncommenting more parameters will give better exploring power but will,
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,94,increase processing time in a combinatorial way,
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,97,"'vect__max_features': (None, 5000, 10000, 50000),",
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,98,unigrams or bigrams,
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,99,"'tfidf__use_idf': (True, False),",
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,100,"'tfidf__norm': ('l1', 'l2'),",
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,104,"'clf__max_iter': (10, 50, 80),",
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,108,multiprocessing requires the fork to happen in a __main__ protected,
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,109,block,
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,111,find the best parameters for both the feature extraction and the,
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,112,classifier,
scikit-learn/examples/model_selection/plot_cv_indices.py,25,,
scikit-learn/examples/model_selection/plot_cv_indices.py,26,Visualize our data,
scikit-learn/examples/model_selection/plot_cv_indices.py,27,------------------,
scikit-learn/examples/model_selection/plot_cv_indices.py,28,,
scikit-learn/examples/model_selection/plot_cv_indices.py,29,"First, we must understand the structure of our data. It has 100 randomly",
scikit-learn/examples/model_selection/plot_cv_indices.py,30,"generated input datapoints, 3 classes split unevenly across datapoints,",
scikit-learn/examples/model_selection/plot_cv_indices.py,31,"and 10 ""groups"" split evenly across datapoints.",
scikit-learn/examples/model_selection/plot_cv_indices.py,32,,
scikit-learn/examples/model_selection/plot_cv_indices.py,33,"As we'll see, some cross-validation objects do specific things with",
scikit-learn/examples/model_selection/plot_cv_indices.py,34,"labeled data, others behave differently with grouped data, and others",
scikit-learn/examples/model_selection/plot_cv_indices.py,35,do not use this information.,
scikit-learn/examples/model_selection/plot_cv_indices.py,36,,
scikit-learn/examples/model_selection/plot_cv_indices.py,37,"To begin, we'll visualize our data.",
scikit-learn/examples/model_selection/plot_cv_indices.py,39,Generate the class/group data,
scikit-learn/examples/model_selection/plot_cv_indices.py,47,Evenly spaced groups repeated once,
scikit-learn/examples/model_selection/plot_cv_indices.py,52,Visualize dataset groups,
scikit-learn/examples/model_selection/plot_cv_indices.py,64,,
scikit-learn/examples/model_selection/plot_cv_indices.py,65,Define a function to visualize cross-validation behavior,
scikit-learn/examples/model_selection/plot_cv_indices.py,66,--------------------------------------------------------,
scikit-learn/examples/model_selection/plot_cv_indices.py,67,,
scikit-learn/examples/model_selection/plot_cv_indices.py,68,We'll define a function that lets us visualize the behavior of each,
scikit-learn/examples/model_selection/plot_cv_indices.py,69,cross-validation object. We'll perform 4 splits of the data. On each,
scikit-learn/examples/model_selection/plot_cv_indices.py,70,"split, we'll visualize the indices chosen for the training set",
scikit-learn/examples/model_selection/plot_cv_indices.py,71,(in blue) and the test set (in red).,
scikit-learn/examples/model_selection/plot_cv_indices.py,77,Generate the training/testing visualizations for each CV split,
scikit-learn/examples/model_selection/plot_cv_indices.py,79,Fill in indices with the training/test groups,
scikit-learn/examples/model_selection/plot_cv_indices.py,84,Visualize the results,
scikit-learn/examples/model_selection/plot_cv_indices.py,89,Plot the data classes and groups at the end,
scikit-learn/examples/model_selection/plot_cv_indices.py,96,Formatting,
scikit-learn/examples/model_selection/plot_cv_indices.py,105,,
scikit-learn/examples/model_selection/plot_cv_indices.py,106,Let's see how it looks for the :class:`~sklearn.model_selection.KFold`,
scikit-learn/examples/model_selection/plot_cv_indices.py,107,cross-validation object:,
scikit-learn/examples/model_selection/plot_cv_indices.py,113,,
scikit-learn/examples/model_selection/plot_cv_indices.py,114,"As you can see, by default the KFold cross-validation iterator does not",
scikit-learn/examples/model_selection/plot_cv_indices.py,115,take either datapoint class or group into consideration. We can change this,
scikit-learn/examples/model_selection/plot_cv_indices.py,116,by using the ``StratifiedKFold`` like so.,
scikit-learn/examples/model_selection/plot_cv_indices.py,122,,
scikit-learn/examples/model_selection/plot_cv_indices.py,123,"In this case, the cross-validation retained the same ratio of classes across",
scikit-learn/examples/model_selection/plot_cv_indices.py,124,each CV split. Next we'll visualize this behavior for a number of CV,
scikit-learn/examples/model_selection/plot_cv_indices.py,125,iterators.,
scikit-learn/examples/model_selection/plot_cv_indices.py,126,,
scikit-learn/examples/model_selection/plot_cv_indices.py,127,Visualize cross-validation indices for many CV objects,
scikit-learn/examples/model_selection/plot_cv_indices.py,128,------------------------------------------------------,
scikit-learn/examples/model_selection/plot_cv_indices.py,129,,
scikit-learn/examples/model_selection/plot_cv_indices.py,130,Let's visually compare the cross validation behavior for many,
scikit-learn/examples/model_selection/plot_cv_indices.py,131,scikit-learn cross-validation objects. Below we will loop through several,
scikit-learn/examples/model_selection/plot_cv_indices.py,132,"common cross-validation objects, visualizing the behavior of each.",
scikit-learn/examples/model_selection/plot_cv_indices.py,133,,
scikit-learn/examples/model_selection/plot_cv_indices.py,134,Note how some use the group/class information while others do not.,
scikit-learn/examples/model_selection/plot_cv_indices.py,147,Make the legend fit,
scikit-learn/examples/model_selection/plot_roc.py,49,Import some data to play with,
scikit-learn/examples/model_selection/plot_roc.py,54,Binarize the output,
scikit-learn/examples/model_selection/plot_roc.py,58,Add noisy features to make the problem harder,
scikit-learn/examples/model_selection/plot_roc.py,63,shuffle and split training and test sets,
scikit-learn/examples/model_selection/plot_roc.py,67,Learn to predict each class against the other,
scikit-learn/examples/model_selection/plot_roc.py,72,Compute ROC curve and ROC area for each class,
scikit-learn/examples/model_selection/plot_roc.py,80,Compute micro-average ROC curve and ROC area,
scikit-learn/examples/model_selection/plot_roc.py,85,,
scikit-learn/examples/model_selection/plot_roc.py,86,Plot of a ROC curve for a specific class,
scikit-learn/examples/model_selection/plot_roc.py,101,,
scikit-learn/examples/model_selection/plot_roc.py,102,Plot ROC curves for the multilabel problem,
scikit-learn/examples/model_selection/plot_roc.py,103,..........................................,
scikit-learn/examples/model_selection/plot_roc.py,104,Compute macro-average ROC curve and ROC area,
scikit-learn/examples/model_selection/plot_roc.py,106,First aggregate all false positive rates,
scikit-learn/examples/model_selection/plot_roc.py,109,Then interpolate all ROC curves at this points,
scikit-learn/examples/model_selection/plot_roc.py,114,Finally average it and compute AUC,
scikit-learn/examples/model_selection/plot_roc.py,121,Plot all ROC curves,
scikit-learn/examples/model_selection/plot_roc.py,149,,
scikit-learn/examples/model_selection/plot_roc.py,150,Area under ROC for the multiclass problem,
scikit-learn/examples/model_selection/plot_roc.py,151,.........................................,
scikit-learn/examples/model_selection/plot_roc.py,152,The :func:`sklearn.metrics.roc_auc_score` function can be used for,
scikit-learn/examples/model_selection/plot_roc.py,153,multi-class classification. The multi-class One-vs-One scheme compares every,
scikit-learn/examples/model_selection/plot_roc.py,154,"unique pairwise combination of classes. In this section, we calculate the AUC",
scikit-learn/examples/model_selection/plot_roc.py,155,"using the OvR and OvO schemes. We report a macro average, and a",
scikit-learn/examples/model_selection/plot_roc.py,156,prevalence-weighted average.,
scikit-learn/examples/model_selection/plot_grid_search_refit_callable.py,19,Author: Wenhao Zhang <wenhaoz@ucla.edu>,
scikit-learn/examples/model_selection/plot_cv_predict.py,19,cross_val_predict returns an array of the same size as `y` where each entry,
scikit-learn/examples/model_selection/plot_cv_predict.py,20,is a prediction obtained by cross validation:,
scikit-learn/examples/model_selection/plot_roc_crossval.py,43,,
scikit-learn/examples/model_selection/plot_roc_crossval.py,44,Data IO and generation,
scikit-learn/examples/model_selection/plot_roc_crossval.py,46,Import some data to play with,
scikit-learn/examples/model_selection/plot_roc_crossval.py,53,Add noisy features,
scikit-learn/examples/model_selection/plot_roc_crossval.py,57,,
scikit-learn/examples/model_selection/plot_roc_crossval.py,58,Classification and ROC analysis,
scikit-learn/examples/model_selection/plot_roc_crossval.py,60,Run classifier with cross-validation and plot ROC curves,
scikit-learn/examples/model_selection/plot_grid_search_digits.py,26,Loading the Digits dataset,
scikit-learn/examples/model_selection/plot_grid_search_digits.py,29,"To apply an classifier on this data, we need to flatten the image, to",
scikit-learn/examples/model_selection/plot_grid_search_digits.py,30,"turn the data in a (samples, feature) matrix:",
scikit-learn/examples/model_selection/plot_grid_search_digits.py,35,Split the dataset in two equal parts,
scikit-learn/examples/model_selection/plot_grid_search_digits.py,39,Set the parameters by cross-validation,
scikit-learn/examples/model_selection/plot_grid_search_digits.py,77,Note the problem is too easy: the hyperparameter plateau is too flat and the,
scikit-learn/examples/model_selection/plot_grid_search_digits.py,78,output model is the same for precision and recall with ties in quality.,
scikit-learn/examples/model_selection/plot_underfitting_overfitting.py,56,Evaluate the models using crossvalidation,
scikit-learn/examples/model_selection/plot_randomized_search.py,33,get some data,
scikit-learn/examples/model_selection/plot_randomized_search.py,36,build a classifier,
scikit-learn/examples/model_selection/plot_randomized_search.py,41,Utility function to report best scores,
scikit-learn/examples/model_selection/plot_randomized_search.py,54,specify parameters and distributions to sample from,
scikit-learn/examples/model_selection/plot_randomized_search.py,59,run randomized search,
scikit-learn/examples/model_selection/plot_randomized_search.py,70,use a full grid over all parameters,
scikit-learn/examples/model_selection/plot_randomized_search.py,75,run grid search,
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,18,Author: Raghav RV <rvraghav93@gmail.com>,
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,19,License: BSD,
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,32,,
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,33,Running ``GridSearchCV`` using multiple evaluation metrics,
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,34,----------------------------------------------------------,
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,35,,
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,39,The scorers can be either be one of the predefined metric strings or a scorer,
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,40,"callable, like the one returned by make_scorer",
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,43,"Setting refit='AUC', refits an estimator on the whole dataset with the",
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,44,parameter setting that has the best cross-validated AUC score.,
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,45,That estimator is made available at ``gs.best_estimator_`` along with,
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,46,"parameters like ``gs.best_score_``, ``gs.best_params_`` and",
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,47,``gs.best_index_``,
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,54,,
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,55,Plotting the result,
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,56,-------------------,
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,69,Get the regular numpy array from the MaskedArray,
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,86,Plot a dotted vertical line at the best score for that scorer marked by x,
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,90,Annotate the best score for that scorer,
scikit-learn/examples/model_selection/plot_precision_recall.py,93,,
scikit-learn/examples/model_selection/plot_precision_recall.py,94,In binary classification settings,
scikit-learn/examples/model_selection/plot_precision_recall.py,95,--------------------------------------------------------,
scikit-learn/examples/model_selection/plot_precision_recall.py,96,,
scikit-learn/examples/model_selection/plot_precision_recall.py,97,Create simple data,
scikit-learn/examples/model_selection/plot_precision_recall.py,98,..................,
scikit-learn/examples/model_selection/plot_precision_recall.py,99,,
scikit-learn/examples/model_selection/plot_precision_recall.py,100,Try to differentiate the two first classes of the iris data,
scikit-learn/examples/model_selection/plot_precision_recall.py,109,Add noisy features,
scikit-learn/examples/model_selection/plot_precision_recall.py,114,"Limit to the two first classes, and split into training and test",
scikit-learn/examples/model_selection/plot_precision_recall.py,119,Create a simple classifier,
scikit-learn/examples/model_selection/plot_precision_recall.py,124,,
scikit-learn/examples/model_selection/plot_precision_recall.py,125,Compute the average precision score,
scikit-learn/examples/model_selection/plot_precision_recall.py,126,...................................,
scikit-learn/examples/model_selection/plot_precision_recall.py,133,,
scikit-learn/examples/model_selection/plot_precision_recall.py,134,Plot the Precision-Recall curve,
scikit-learn/examples/model_selection/plot_precision_recall.py,135,................................,
scikit-learn/examples/model_selection/plot_precision_recall.py,144,,
scikit-learn/examples/model_selection/plot_precision_recall.py,145,In multi-label settings,
scikit-learn/examples/model_selection/plot_precision_recall.py,146,------------------------,
scikit-learn/examples/model_selection/plot_precision_recall.py,147,,
scikit-learn/examples/model_selection/plot_precision_recall.py,148,"Create multi-label data, fit, and predict",
scikit-learn/examples/model_selection/plot_precision_recall.py,149,...........................................,
scikit-learn/examples/model_selection/plot_precision_recall.py,150,,
scikit-learn/examples/model_selection/plot_precision_recall.py,151,"We create a multi-label dataset, to illustrate the precision-recall in",
scikit-learn/examples/model_selection/plot_precision_recall.py,152,multi-label settings,
scikit-learn/examples/model_selection/plot_precision_recall.py,156,Use label_binarize to be multi-label like settings,
scikit-learn/examples/model_selection/plot_precision_recall.py,160,Split into training and test,
scikit-learn/examples/model_selection/plot_precision_recall.py,164,We use OneVsRestClassifier for multi-label prediction,
scikit-learn/examples/model_selection/plot_precision_recall.py,167,Run classifier,
scikit-learn/examples/model_selection/plot_precision_recall.py,173,,
scikit-learn/examples/model_selection/plot_precision_recall.py,174,The average precision score in multi-label settings,
scikit-learn/examples/model_selection/plot_precision_recall.py,175,....................................................,
scikit-learn/examples/model_selection/plot_precision_recall.py,179,For each class,
scikit-learn/examples/model_selection/plot_precision_recall.py,188,"A ""micro-average"": quantifying score on all classes jointly",
scikit-learn/examples/model_selection/plot_precision_recall.py,196,,
scikit-learn/examples/model_selection/plot_precision_recall.py,197,Plot the micro-averaged Precision-Recall curve,
scikit-learn/examples/model_selection/plot_precision_recall.py,198,...............................................,
scikit-learn/examples/model_selection/plot_precision_recall.py,199,,
scikit-learn/examples/model_selection/plot_precision_recall.py,212,,
scikit-learn/examples/model_selection/plot_precision_recall.py,213,Plot Precision-Recall curve for each class and iso-f1 curves,
scikit-learn/examples/model_selection/plot_precision_recall.py,214,.............................................................,
scikit-learn/examples/model_selection/plot_precision_recall.py,215,,
scikit-learn/examples/model_selection/plot_precision_recall.py,217,setup plot details,
scikit-learn/examples/model_selection/plot_confusion_matrix.py,36,import some data to play with,
scikit-learn/examples/model_selection/plot_confusion_matrix.py,42,Split the data into a training set and a test set,
scikit-learn/examples/model_selection/plot_confusion_matrix.py,45,"Run classifier, using a model that is too regularized (C too low) to see",
scikit-learn/examples/model_selection/plot_confusion_matrix.py,46,the impact on the results,
scikit-learn/examples/model_selection/plot_confusion_matrix.py,51,Plot non-normalized confusion matrix,
scikit-learn/examples/model_selection/plot_nested_cross_validation_iris.py,54,Number of random trials,
scikit-learn/examples/model_selection/plot_nested_cross_validation_iris.py,57,Load the dataset,
scikit-learn/examples/model_selection/plot_nested_cross_validation_iris.py,62,Set up possible values of parameters to optimize over,
scikit-learn/examples/model_selection/plot_nested_cross_validation_iris.py,66,"We will use a Support Vector Classifier with ""rbf"" kernel",
scikit-learn/examples/model_selection/plot_nested_cross_validation_iris.py,69,Arrays to store scores,
scikit-learn/examples/model_selection/plot_nested_cross_validation_iris.py,73,Loop for each trial,
scikit-learn/examples/model_selection/plot_nested_cross_validation_iris.py,76,"Choose cross-validation techniques for the inner and outer loops,",
scikit-learn/examples/model_selection/plot_nested_cross_validation_iris.py,77,independently of the dataset.,
scikit-learn/examples/model_selection/plot_nested_cross_validation_iris.py,78,"E.g ""GroupKFold"", ""LeaveOneOut"", ""LeaveOneGroupOut"", etc.",
scikit-learn/examples/model_selection/plot_nested_cross_validation_iris.py,82,Non_nested parameter search and scoring,
scikit-learn/examples/model_selection/plot_nested_cross_validation_iris.py,87,Nested CV with parameter optimization,
scikit-learn/examples/model_selection/plot_nested_cross_validation_iris.py,96,Plot scores on each trial for nested and non-nested CV,
scikit-learn/examples/model_selection/plot_nested_cross_validation_iris.py,108,Plot bar chart of the difference.,
scikit-learn/examples/model_selection/plot_train_error_vs_test_error.py,16,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/examples/model_selection/plot_train_error_vs_test_error.py,17,License: BSD 3 clause,
scikit-learn/examples/model_selection/plot_train_error_vs_test_error.py,22,,
scikit-learn/examples/model_selection/plot_train_error_vs_test_error.py,23,Generate sample data,
scikit-learn/examples/model_selection/plot_train_error_vs_test_error.py,27,only the top 10 features are impacting the model,
scikit-learn/examples/model_selection/plot_train_error_vs_test_error.py,31,Split train and test data,
scikit-learn/examples/model_selection/plot_train_error_vs_test_error.py,35,,
scikit-learn/examples/model_selection/plot_train_error_vs_test_error.py,36,Compute train and test errors,
scikit-learn/examples/model_selection/plot_train_error_vs_test_error.py,51,Estimate the coef_ on full data with optimal regularization parameter,
scikit-learn/examples/model_selection/plot_train_error_vs_test_error.py,55,,
scikit-learn/examples/model_selection/plot_train_error_vs_test_error.py,56,Plot results functions,
scikit-learn/examples/model_selection/plot_train_error_vs_test_error.py,69,Show estimated coef_ vs true coef,
scikit-learn/examples/svm/plot_custom_kernel.py,16,import some data to play with,
scikit-learn/examples/svm/plot_custom_kernel.py,18,we only take the first two features. We could,
scikit-learn/examples/svm/plot_custom_kernel.py,19,avoid this ugly slicing by using a two-dim dataset,
scikit-learn/examples/svm/plot_custom_kernel.py,35,step size in the mesh,
scikit-learn/examples/svm/plot_custom_kernel.py,37,we create an instance of SVM and fit out data.,
scikit-learn/examples/svm/plot_custom_kernel.py,41,"Plot the decision boundary. For that, we will assign a color to each",
scikit-learn/examples/svm/plot_custom_kernel.py,42,"point in the mesh [x_min, x_max]x[y_min, y_max].",
scikit-learn/examples/svm/plot_custom_kernel.py,48,Put the result into a color plot,
scikit-learn/examples/svm/plot_custom_kernel.py,52,Plot also the training points,
scikit-learn/examples/svm/plot_svm_anova.py,23,,
scikit-learn/examples/svm/plot_svm_anova.py,24,Import some data to play with,
scikit-learn/examples/svm/plot_svm_anova.py,26,Add non-informative features,
scikit-learn/examples/svm/plot_svm_anova.py,30,,
scikit-learn/examples/svm/plot_svm_anova.py,31,"Create a feature-selection transform, a scaler and an instance of SVM that we",
scikit-learn/examples/svm/plot_svm_anova.py,32,combine together to have an full-blown estimator,
scikit-learn/examples/svm/plot_svm_anova.py,37,,
scikit-learn/examples/svm/plot_svm_anova.py,38,Plot the cross-validation score as a function of percentile of features,
scikit-learn/examples/svm/plot_separating_hyperplane.py,18,we create 40 separable points,
scikit-learn/examples/svm/plot_separating_hyperplane.py,21,"fit the model, don't regularize for illustration purposes",
scikit-learn/examples/svm/plot_separating_hyperplane.py,27,plot the decision function,
scikit-learn/examples/svm/plot_separating_hyperplane.py,32,create grid to evaluate model,
scikit-learn/examples/svm/plot_separating_hyperplane.py,39,plot decision boundary and margins,
scikit-learn/examples/svm/plot_separating_hyperplane.py,42,plot support vectors,
scikit-learn/examples/svm/plot_rbf_parameters.py,85,Utility function to move the midpoint of a colormap to be around,
scikit-learn/examples/svm/plot_rbf_parameters.py,86,the values of interest.,
scikit-learn/examples/svm/plot_rbf_parameters.py,98,,
scikit-learn/examples/svm/plot_rbf_parameters.py,99,Load and prepare data set,
scikit-learn/examples/svm/plot_rbf_parameters.py,100,,
scikit-learn/examples/svm/plot_rbf_parameters.py,101,dataset for grid search,
scikit-learn/examples/svm/plot_rbf_parameters.py,107,Dataset for decision function visualization: we only keep the first two,
scikit-learn/examples/svm/plot_rbf_parameters.py,108,features in X and sub-sample the dataset to keep only 2 classes and,
scikit-learn/examples/svm/plot_rbf_parameters.py,109,make it a binary classification problem.,
scikit-learn/examples/svm/plot_rbf_parameters.py,116,It is usually a good idea to scale the data for SVM training.,
scikit-learn/examples/svm/plot_rbf_parameters.py,117,"We are cheating a bit in this example in scaling all of the data,",
scikit-learn/examples/svm/plot_rbf_parameters.py,118,instead of fitting the transformation on the training set and,
scikit-learn/examples/svm/plot_rbf_parameters.py,119,just applying it on the test set.,
scikit-learn/examples/svm/plot_rbf_parameters.py,125,,
scikit-learn/examples/svm/plot_rbf_parameters.py,126,Train classifiers,
scikit-learn/examples/svm/plot_rbf_parameters.py,127,,
scikit-learn/examples/svm/plot_rbf_parameters.py,128,"For an initial search, a logarithmic grid with basis",
scikit-learn/examples/svm/plot_rbf_parameters.py,129,"10 is often helpful. Using a basis of 2, a finer",
scikit-learn/examples/svm/plot_rbf_parameters.py,130,tuning can be achieved but at a much higher cost.,
scikit-learn/examples/svm/plot_rbf_parameters.py,142,Now we need to fit a classifier for all parameters in the 2d version,
scikit-learn/examples/svm/plot_rbf_parameters.py,143,(we use a smaller set of parameters here because it takes a while to train),
scikit-learn/examples/svm/plot_rbf_parameters.py,154,,
scikit-learn/examples/svm/plot_rbf_parameters.py,155,Visualization,
scikit-learn/examples/svm/plot_rbf_parameters.py,156,,
scikit-learn/examples/svm/plot_rbf_parameters.py,157,draw visualization of parameter effects,
scikit-learn/examples/svm/plot_rbf_parameters.py,162,evaluate decision function in a grid,
scikit-learn/examples/svm/plot_rbf_parameters.py,166,visualize decision function for these parameters,
scikit-learn/examples/svm/plot_rbf_parameters.py,171,visualize parameter's effect on decision function,
scikit-learn/examples/svm/plot_rbf_parameters.py,182,Draw heatmap of the validation accuracy as a function of gamma and C,
scikit-learn/examples/svm/plot_rbf_parameters.py,183,,
scikit-learn/examples/svm/plot_rbf_parameters.py,184,The score are encoded as colors with the hot colormap which varies from dark,
scikit-learn/examples/svm/plot_rbf_parameters.py,185,red to bright yellow. As the most interesting scores are all located in the,
scikit-learn/examples/svm/plot_rbf_parameters.py,186,0.92 to 0.97 range we use a custom normalizer to set the mid-point to 0.92 so,
scikit-learn/examples/svm/plot_rbf_parameters.py,187,as to make it easier to visualize the small variations of score values in the,
scikit-learn/examples/svm/plot_rbf_parameters.py,188,interesting range while not brutally collapsing all the low score values to,
scikit-learn/examples/svm/plot_rbf_parameters.py,189,the same color.,
scikit-learn/examples/svm/plot_svm_scale_c.py,82,Author: Andreas Mueller <amueller@ais.uni-bonn.de>,
scikit-learn/examples/svm/plot_svm_scale_c.py,83,Jaques Grobler <jaques.grobler@inria.fr>,
scikit-learn/examples/svm/plot_svm_scale_c.py,84,License: BSD 3 clause,
scikit-learn/examples/svm/plot_svm_scale_c.py,98,set up dataset,
scikit-learn/examples/svm/plot_svm_scale_c.py,102,l1 data (only 5 informative features),
scikit-learn/examples/svm/plot_svm_scale_c.py,107,"l2 data: non sparse, but less features",
scikit-learn/examples/svm/plot_svm_scale_c.py,122,set up the plot for each regressor,
scikit-learn/examples/svm/plot_svm_scale_c.py,127,"To get nice curve, we need a large number of iterations to",
scikit-learn/examples/svm/plot_svm_scale_c.py,128,reduce the variance,
scikit-learn/examples/svm/plot_svm_scale_c.py,143,scale the C's,
scikit-learn/examples/svm/plot_svm_nonlinear.py,24,fit the model,
scikit-learn/examples/svm/plot_svm_nonlinear.py,28,plot the decision function for each datapoint on the grid,
scikit-learn/examples/svm/plot_iris_svc.py,80,import some data to play with,
scikit-learn/examples/svm/plot_iris_svc.py,82,Take the first two features. We could avoid this by using a two-dim dataset,
scikit-learn/examples/svm/plot_iris_svc.py,86,we create an instance of SVM and fit out data. We do not scale our,
scikit-learn/examples/svm/plot_iris_svc.py,87,data since we want to plot the support vectors,
scikit-learn/examples/svm/plot_iris_svc.py,88,SVM regularization parameter,
scikit-learn/examples/svm/plot_iris_svc.py,95,title for the plots,
scikit-learn/examples/svm/plot_iris_svc.py,101,Set-up 2x2 grid for plotting.,
scikit-learn/examples/svm/plot_svm_kernels.py,1,!/usr/bin/python,
scikit-learn/examples/svm/plot_svm_kernels.py,2,-*- coding: utf-8 -*-,
scikit-learn/examples/svm/plot_svm_kernels.py,18,Code source: Gaël Varoquaux,
scikit-learn/examples/svm/plot_svm_kernels.py,19,License: BSD 3 clause,
scikit-learn/examples/svm/plot_svm_kernels.py,26,Our dataset and targets,
scikit-learn/examples/svm/plot_svm_kernels.py,36,--,
scikit-learn/examples/svm/plot_svm_kernels.py,46,figure number,
scikit-learn/examples/svm/plot_svm_kernels.py,49,fit the model,
scikit-learn/examples/svm/plot_svm_kernels.py,54,"plot the line, the points, and the nearest vectors to the plane",
scikit-learn/examples/svm/plot_svm_kernels.py,72,Put the result into a color plot,
scikit-learn/examples/svm/plot_linearsvc_support_vectors.py,21,"""hinge"" is the standard SVM loss",
scikit-learn/examples/svm/plot_linearsvc_support_vectors.py,23,obtain the support vectors through the decision function,
scikit-learn/examples/svm/plot_linearsvc_support_vectors.py,25,we can also calculate the decision function manually,
scikit-learn/examples/svm/plot_linearsvc_support_vectors.py,26,"decision_function = np.dot(X, clf.coef_[0]) + clf.intercept_[0]",
scikit-learn/examples/svm/plot_separating_hyperplane_unbalanced.py,34,we create two clusters of random points,
scikit-learn/examples/svm/plot_separating_hyperplane_unbalanced.py,44,fit the model and get the separating hyperplane,
scikit-learn/examples/svm/plot_separating_hyperplane_unbalanced.py,48,fit the model and get the separating hyperplane using weighted classes,
scikit-learn/examples/svm/plot_separating_hyperplane_unbalanced.py,52,plot the samples,
scikit-learn/examples/svm/plot_separating_hyperplane_unbalanced.py,55,plot the decision functions for both classifiers,
scikit-learn/examples/svm/plot_separating_hyperplane_unbalanced.py,60,create grid to evaluate model,
scikit-learn/examples/svm/plot_separating_hyperplane_unbalanced.py,66,get the separating hyperplane,
scikit-learn/examples/svm/plot_separating_hyperplane_unbalanced.py,69,plot decision boundary and margins,
scikit-learn/examples/svm/plot_separating_hyperplane_unbalanced.py,72,get the separating hyperplane for weighted classes,
scikit-learn/examples/svm/plot_separating_hyperplane_unbalanced.py,75,plot decision boundary and margins for weighted classes,
scikit-learn/examples/svm/plot_svm_regression.py,15,,
scikit-learn/examples/svm/plot_svm_regression.py,16,Generate sample data,
scikit-learn/examples/svm/plot_svm_regression.py,20,,
scikit-learn/examples/svm/plot_svm_regression.py,21,Add noise to targets,
scikit-learn/examples/svm/plot_svm_regression.py,24,,
scikit-learn/examples/svm/plot_svm_regression.py,25,Fit regression model,
scikit-learn/examples/svm/plot_svm_regression.py,31,,
scikit-learn/examples/svm/plot_svm_regression.py,32,Look at the results,
scikit-learn/examples/svm/plot_svm_tie_breaking.py,18,"Code source: Andreas Mueller, Adrin Jalali",
scikit-learn/examples/svm/plot_svm_tie_breaking.py,19,License: BSD 3 clause,
scikit-learn/examples/svm/plot_svm_margin.py,1,!/usr/bin/python,
scikit-learn/examples/svm/plot_svm_margin.py,2,-*- coding: utf-8 -*-,
scikit-learn/examples/svm/plot_svm_margin.py,21,Code source: Gaël Varoquaux,
scikit-learn/examples/svm/plot_svm_margin.py,22,Modified for documentation by Jaques Grobler,
scikit-learn/examples/svm/plot_svm_margin.py,23,License: BSD 3 clause,
scikit-learn/examples/svm/plot_svm_margin.py,29,we create 40 separable points,
scikit-learn/examples/svm/plot_svm_margin.py,34,figure number,
scikit-learn/examples/svm/plot_svm_margin.py,37,fit the model,
scikit-learn/examples/svm/plot_svm_margin.py,43,get the separating hyperplane,
scikit-learn/examples/svm/plot_svm_margin.py,49,plot the parallels to the separating hyperplane that pass through the,
scikit-learn/examples/svm/plot_svm_margin.py,50,support vectors (margin away from hyperplane in direction,
scikit-learn/examples/svm/plot_svm_margin.py,51,perpendicular to hyperplane). This is sqrt(1+a^2) away vertically in,
scikit-learn/examples/svm/plot_svm_margin.py,52,2-d.,
scikit-learn/examples/svm/plot_svm_margin.py,57,"plot the line, the points, and the nearest vectors to the plane",
scikit-learn/examples/svm/plot_svm_margin.py,78,Put the result into a color plot,
scikit-learn/examples/svm/plot_oneclass.py,20,Generate train data,
scikit-learn/examples/svm/plot_oneclass.py,23,Generate some regular novel observations,
scikit-learn/examples/svm/plot_oneclass.py,26,Generate some abnormal novel observations,
scikit-learn/examples/svm/plot_oneclass.py,29,fit the model,
scikit-learn/examples/svm/plot_oneclass.py,39,"plot the line, the points, and the nearest vectors to the plane",
scikit-learn/examples/svm/plot_weighted_samples.py,23,plot the decision function,
scikit-learn/examples/svm/plot_weighted_samples.py,29,"plot the line, the points, and the nearest vectors to the plane",
scikit-learn/examples/svm/plot_weighted_samples.py,38,we create 20 points,
scikit-learn/examples/svm/plot_weighted_samples.py,44,and bigger weights to some outliers,
scikit-learn/examples/svm/plot_weighted_samples.py,48,"for reference, first fit without sample weights",
scikit-learn/examples/svm/plot_weighted_samples.py,50,fit the model,
scikit-learn/examples/mixture/plot_gmm_selection.py,29,Number of samples per component,
scikit-learn/examples/mixture/plot_gmm_selection.py,32,"Generate random sample, two components",
scikit-learn/examples/mixture/plot_gmm_selection.py,44,Fit a Gaussian mixture with EM,
scikit-learn/examples/mixture/plot_gmm_selection.py,59,Plot the BIC scores,
scikit-learn/examples/mixture/plot_gmm_selection.py,76,Plot the winner,
scikit-learn/examples/mixture/plot_gmm_selection.py,86,Plot an ellipse to show the Gaussian component,
scikit-learn/examples/mixture/plot_gmm_selection.py,88,convert to degrees,
scikit-learn/examples/mixture/plot_gmm_pdf.py,18,"generate random sample, two components",
scikit-learn/examples/mixture/plot_gmm_pdf.py,21,"generate spherical data centered on (20, 20)",
scikit-learn/examples/mixture/plot_gmm_pdf.py,24,generate zero centered stretched Gaussian data,
scikit-learn/examples/mixture/plot_gmm_pdf.py,28,concatenate the two datasets into the final training set,
scikit-learn/examples/mixture/plot_gmm_pdf.py,31,fit a Gaussian Mixture Model with two components,
scikit-learn/examples/mixture/plot_gmm_pdf.py,35,display predicted scores by the model as a contour plot,
scikit-learn/examples/mixture/plot_concentration_prior.py,30,Author: Thierry Guillemot <thierry.guillemot.work@gmail.com>,
scikit-learn/examples/mixture/plot_concentration_prior.py,31,License: BSD 3 clause,
scikit-learn/examples/mixture/plot_concentration_prior.py,48,Ellipse needs degrees,
scikit-learn/examples/mixture/plot_concentration_prior.py,50,eigenvector normalization,
scikit-learn/examples/mixture/plot_concentration_prior.py,87,Parameters of the dataset,
scikit-learn/examples/mixture/plot_concentration_prior.py,99,mean_precision_prior= 0.8 to minimize the influence of the prior,
scikit-learn/examples/mixture/plot_concentration_prior.py,114,Generate data,
scikit-learn/examples/mixture/plot_concentration_prior.py,122,Plot results in two different figures,
scikit-learn/examples/mixture/plot_gmm_covariances.py,29,"Author: Ron Weiss <ronweiss@gmail.com>, Gael Varoquaux",
scikit-learn/examples/mixture/plot_gmm_covariances.py,30,Modified by Thierry Guillemot <thierry.guillemot.work@gmail.com>,
scikit-learn/examples/mixture/plot_gmm_covariances.py,31,License: BSD 3 clause,
scikit-learn/examples/mixture/plot_gmm_covariances.py,60,convert to degrees,
scikit-learn/examples/mixture/plot_gmm_covariances.py,71,Break up the dataset into non-overlapping training (75%) and testing,
scikit-learn/examples/mixture/plot_gmm_covariances.py,72,(25%) sets.,
scikit-learn/examples/mixture/plot_gmm_covariances.py,74,Only take the first fold.,
scikit-learn/examples/mixture/plot_gmm_covariances.py,85,Try GMMs using different types of covariances.,
scikit-learn/examples/mixture/plot_gmm_covariances.py,98,"Since we have class labels for the training data, we can",
scikit-learn/examples/mixture/plot_gmm_covariances.py,99,initialize the GMM parameters in a supervised manner.,
scikit-learn/examples/mixture/plot_gmm_covariances.py,103,Train the other parameters using the EM algorithm.,
scikit-learn/examples/mixture/plot_gmm_covariances.py,113,Plot the test data with crosses,
scikit-learn/examples/mixture/plot_gmm_sin.py,64,as the DP will not use every component it has access to,
scikit-learn/examples/mixture/plot_gmm_sin.py,65,"unless it needs it, we shouldn't plot the redundant",
scikit-learn/examples/mixture/plot_gmm_sin.py,66,components.,
scikit-learn/examples/mixture/plot_gmm_sin.py,71,Plot an ellipse to show the Gaussian component,
scikit-learn/examples/mixture/plot_gmm_sin.py,73,convert to degrees,
scikit-learn/examples/mixture/plot_gmm_sin.py,89,as the DP will not use every component it has access to,
scikit-learn/examples/mixture/plot_gmm_sin.py,90,"unless it needs it, we shouldn't plot the redundant",
scikit-learn/examples/mixture/plot_gmm_sin.py,91,components.,
scikit-learn/examples/mixture/plot_gmm_sin.py,103,Parameters,
scikit-learn/examples/mixture/plot_gmm_sin.py,106,Generate random sample following a sine curve,
scikit-learn/examples/mixture/plot_gmm_sin.py,120,Fit a Gaussian mixture with EM using ten components,
scikit-learn/examples/mixture/plot_gmm.py,46,as the DP will not use every component it has access to,
scikit-learn/examples/mixture/plot_gmm.py,47,"unless it needs it, we shouldn't plot the redundant",
scikit-learn/examples/mixture/plot_gmm.py,48,components.,
scikit-learn/examples/mixture/plot_gmm.py,53,Plot an ellipse to show the Gaussian component,
scikit-learn/examples/mixture/plot_gmm.py,55,convert to degrees,
scikit-learn/examples/mixture/plot_gmm.py,68,Number of samples per component,
scikit-learn/examples/mixture/plot_gmm.py,71,"Generate random sample, two components",
scikit-learn/examples/mixture/plot_gmm.py,77,Fit a Gaussian mixture with EM using five components,
scikit-learn/examples/mixture/plot_gmm.py,82,Fit a Dirichlet process Gaussian mixture using five components,
scikit-learn/examples/semi_supervised/plot_label_propagation_digits_active_learning.py,23,Authors: Clay Woolam <clay@woolam.org>,
scikit-learn/examples/semi_supervised/plot_label_propagation_digits_active_learning.py,24,License: BSD,
scikit-learn/examples/semi_supervised/plot_label_propagation_digits_active_learning.py,76,compute the entropies of transduced label distributions,
scikit-learn/examples/semi_supervised/plot_label_propagation_digits_active_learning.py,80,select up to 5 digit examples that the classifier is most uncertain about,
scikit-learn/examples/semi_supervised/plot_label_propagation_digits_active_learning.py,85,keep track of indices that we get labels for,
scikit-learn/examples/semi_supervised/plot_label_propagation_digits_active_learning.py,88,"for more than 5 iterations, visualize the gain only on the first 5",
scikit-learn/examples/semi_supervised/plot_label_propagation_digits_active_learning.py,96,"for more than 5 iterations, visualize the gain only on the first 5",
scikit-learn/examples/semi_supervised/plot_label_propagation_digits_active_learning.py,104,"labeling 5 points, remote from labeled set",
scikit-learn/examples/semi_supervised/plot_label_propagation_versus_svm_iris.py,15,Authors: Clay Woolam <clay@woolam.org>,
scikit-learn/examples/semi_supervised/plot_label_propagation_versus_svm_iris.py,16,License: BSD,
scikit-learn/examples/semi_supervised/plot_label_propagation_versus_svm_iris.py,31,step size in the mesh,
scikit-learn/examples/semi_supervised/plot_label_propagation_versus_svm_iris.py,38,we create an instance of SVM and fit out data. We do not scale our,
scikit-learn/examples/semi_supervised/plot_label_propagation_versus_svm_iris.py,39,data since we want to plot the support vectors,
scikit-learn/examples/semi_supervised/plot_label_propagation_versus_svm_iris.py,45,create a mesh to plot in,
scikit-learn/examples/semi_supervised/plot_label_propagation_versus_svm_iris.py,51,title for the plots,
scikit-learn/examples/semi_supervised/plot_label_propagation_versus_svm_iris.py,60,"Plot the decision boundary. For that, we will assign a color to each",
scikit-learn/examples/semi_supervised/plot_label_propagation_versus_svm_iris.py,61,"point in the mesh [x_min, x_max]x[y_min, y_max].",
scikit-learn/examples/semi_supervised/plot_label_propagation_versus_svm_iris.py,65,Put the result into a color plot,
scikit-learn/examples/semi_supervised/plot_label_propagation_versus_svm_iris.py,70,Plot also the training points,
scikit-learn/examples/semi_supervised/plot_label_propagation_digits.py,19,Authors: Clay Woolam <clay@woolam.org>,
scikit-learn/examples/semi_supervised/plot_label_propagation_digits.py,20,License: BSD,
scikit-learn/examples/semi_supervised/plot_label_propagation_digits.py,48,,
scikit-learn/examples/semi_supervised/plot_label_propagation_digits.py,49,Shuffle everything around,
scikit-learn/examples/semi_supervised/plot_label_propagation_digits.py,53,,
scikit-learn/examples/semi_supervised/plot_label_propagation_digits.py,54,Learn with LabelSpreading,
scikit-learn/examples/semi_supervised/plot_label_propagation_digits.py,70,,
scikit-learn/examples/semi_supervised/plot_label_propagation_digits.py,71,Calculate uncertainty values for each transduced distribution,
scikit-learn/examples/semi_supervised/plot_label_propagation_digits.py,74,,
scikit-learn/examples/semi_supervised/plot_label_propagation_digits.py,75,Pick the top 10 most uncertain labels,
scikit-learn/examples/semi_supervised/plot_label_propagation_digits.py,78,,
scikit-learn/examples/semi_supervised/plot_label_propagation_digits.py,79,Plot,
scikit-learn/examples/semi_supervised/plot_label_propagation_structure.py,14,Authors: Clay Woolam <clay@woolam.org>,
scikit-learn/examples/semi_supervised/plot_label_propagation_structure.py,15,Andreas Mueller <amueller@ais.uni-bonn.de>,
scikit-learn/examples/semi_supervised/plot_label_propagation_structure.py,16,License: BSD,
scikit-learn/examples/semi_supervised/plot_label_propagation_structure.py,23,generate ring with inner box,
scikit-learn/examples/semi_supervised/plot_label_propagation_structure.py,31,,
scikit-learn/examples/semi_supervised/plot_label_propagation_structure.py,32,Learn with LabelSpreading,
scikit-learn/examples/semi_supervised/plot_label_propagation_structure.py,36,,
scikit-learn/examples/semi_supervised/plot_label_propagation_structure.py,37,Plot output labels,
scikit-learn/examples/multioutput/plot_classifier_chain_yeast.py,35,Author: Adam Kleczewski,
scikit-learn/examples/multioutput/plot_classifier_chain_yeast.py,36,License: BSD 3 clause,
scikit-learn/examples/multioutput/plot_classifier_chain_yeast.py,49,Load a multi-label dataset from https://www.openml.org/d/40597,
scikit-learn/examples/multioutput/plot_classifier_chain_yeast.py,55,Fit an independent logistic regression model for each class using the,
scikit-learn/examples/multioutput/plot_classifier_chain_yeast.py,56,OneVsRestClassifier wrapper.,
scikit-learn/examples/multioutput/plot_classifier_chain_yeast.py,63,Fit an ensemble of logistic regression classifier chains and take the,
scikit-learn/examples/multioutput/plot_classifier_chain_yeast.py,64,take the average prediction of all the chains.,
scikit-learn/examples/multioutput/plot_classifier_chain_yeast.py,99,"Plot the Jaccard similarity scores for the independent model, each of the",
scikit-learn/examples/multioutput/plot_classifier_chain_yeast.py,100,"chains, and the ensemble (note that the vertical axis on this plot does",
scikit-learn/examples/multioutput/plot_classifier_chain_yeast.py,101,not begin at 0).,
scikit-learn/examples/linear_model/plot_omp.py,20,generate the data,
scikit-learn/examples/linear_model/plot_omp.py,22,y = Xw,
scikit-learn/examples/linear_model/plot_omp.py,23,|x|_0 = n_nonzero_coefs,
scikit-learn/examples/linear_model/plot_omp.py,33,distort the clean signal,
scikit-learn/examples/linear_model/plot_omp.py,36,plot the sparse signal,
scikit-learn/examples/linear_model/plot_omp.py,43,plot the noise-free reconstruction,
scikit-learn/examples/linear_model/plot_omp.py,53,plot the noisy reconstruction,
scikit-learn/examples/linear_model/plot_omp.py,62,plot the noisy reconstruction with number of non-zeros set by CV,
scikit-learn/examples/linear_model/plot_bayesian_ridge_curvefit.py,29,Author: Yoshihiro Uchida <nimbus1after2a1sun7shower@gmail.com>,
scikit-learn/examples/linear_model/plot_bayesian_ridge_curvefit.py,40,,
scikit-learn/examples/linear_model/plot_bayesian_ridge_curvefit.py,41,Generate sinusoidal data with noise,
scikit-learn/examples/linear_model/plot_bayesian_ridge_curvefit.py,49,,
scikit-learn/examples/linear_model/plot_bayesian_ridge_curvefit.py,50,Fit by cubic polynomial,
scikit-learn/examples/linear_model/plot_bayesian_ridge_curvefit.py,55,,
scikit-learn/examples/linear_model/plot_bayesian_ridge_curvefit.py,56,Plot the true and predicted curves with log marginal likelihood (L),
scikit-learn/examples/linear_model/plot_bayesian_ridge_curvefit.py,60,Bayesian ridge regression with different initial value pairs,
scikit-learn/examples/linear_model/plot_bayesian_ridge_curvefit.py,62,Default values,
scikit-learn/examples/linear_model/plot_robust_fit.py,46,Make sure that it X is 2D,
scikit-learn/examples/linear_model/plot_theilsen.py,37,Author: Florian Wilhelm -- <florian.wilhelm@gmail.com>,
scikit-learn/examples/linear_model/plot_theilsen.py,38,License: BSD 3 clause,
scikit-learn/examples/linear_model/plot_theilsen.py,54,,
scikit-learn/examples/linear_model/plot_theilsen.py,55,Outliers only in the y direction,
scikit-learn/examples/linear_model/plot_theilsen.py,59,"Linear model y = 3*x + N(2, 0.1**2)",
scikit-learn/examples/linear_model/plot_theilsen.py,65,10% outliers,
scikit-learn/examples/linear_model/plot_theilsen.py,83,,
scikit-learn/examples/linear_model/plot_theilsen.py,84,Outliers in the X direction,
scikit-learn/examples/linear_model/plot_theilsen.py,87,"Linear model y = 3*x + N(2, 0.1**2)",
scikit-learn/examples/linear_model/plot_theilsen.py,91,10% outliers,
scikit-learn/examples/linear_model/plot_huber_vs_ridge.py,15,Authors: Manoj Kumar mks542@nyu.edu,
scikit-learn/examples/linear_model/plot_huber_vs_ridge.py,16,License: BSD 3 clause,
scikit-learn/examples/linear_model/plot_huber_vs_ridge.py,26,Generate toy data.,
scikit-learn/examples/linear_model/plot_huber_vs_ridge.py,31,Add four strong outliers to the dataset.,
scikit-learn/examples/linear_model/plot_huber_vs_ridge.py,42,Fit the huber regressor over a series of epsilon values.,
scikit-learn/examples/linear_model/plot_huber_vs_ridge.py,53,Fit a ridge regressor to compare it to huber regressor.,
scikit-learn/examples/linear_model/plot_ridge_path.py,29,Author: Fabian Pedregosa -- <fabian.pedregosa@inria.fr>,
scikit-learn/examples/linear_model/plot_ridge_path.py,30,License: BSD 3 clause,
scikit-learn/examples/linear_model/plot_ridge_path.py,38,X is the 10x10 Hilbert matrix,
scikit-learn/examples/linear_model/plot_ridge_path.py,42,,
scikit-learn/examples/linear_model/plot_ridge_path.py,43,Compute paths,
scikit-learn/examples/linear_model/plot_ridge_path.py,54,,
scikit-learn/examples/linear_model/plot_ridge_path.py,55,Display results,
scikit-learn/examples/linear_model/plot_ridge_path.py,61,reverse axis,
scikit-learn/examples/linear_model/plot_lasso_model_selection.py,46,"Author: Olivier Grisel, Gael Varoquaux, Alexandre Gramfort",
scikit-learn/examples/linear_model/plot_lasso_model_selection.py,47,License: BSD 3 clause,
scikit-learn/examples/linear_model/plot_lasso_model_selection.py,57,This is to avoid division by zero while doing np.log10,
scikit-learn/examples/linear_model/plot_lasso_model_selection.py,63,add some bad features,
scikit-learn/examples/linear_model/plot_lasso_model_selection.py,65,normalize data as done by Lars to allow for comparison,
scikit-learn/examples/linear_model/plot_lasso_model_selection.py,68,,
scikit-learn/examples/linear_model/plot_lasso_model_selection.py,69,LassoLarsIC: least angle regression with BIC/AIC criterion,
scikit-learn/examples/linear_model/plot_lasso_model_selection.py,101,,
scikit-learn/examples/linear_model/plot_lasso_model_selection.py,102,LassoCV: coordinate descent,
scikit-learn/examples/linear_model/plot_lasso_model_selection.py,104,Compute paths,
scikit-learn/examples/linear_model/plot_lasso_model_selection.py,110,Display results,
scikit-learn/examples/linear_model/plot_lasso_model_selection.py,130,,
scikit-learn/examples/linear_model/plot_lasso_model_selection.py,131,LassoLarsCV: least angle regression,
scikit-learn/examples/linear_model/plot_lasso_model_selection.py,133,Compute paths,
scikit-learn/examples/linear_model/plot_lasso_model_selection.py,139,Display results,
scikit-learn/examples/linear_model/plot_lasso_coordinate_descent_path.py,13,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/examples/linear_model/plot_lasso_coordinate_descent_path.py,14,License: BSD 3 clause,
scikit-learn/examples/linear_model/plot_lasso_coordinate_descent_path.py,27,Standardize data (easier to set the l1_ratio parameter),
scikit-learn/examples/linear_model/plot_lasso_coordinate_descent_path.py,29,Compute paths,
scikit-learn/examples/linear_model/plot_lasso_coordinate_descent_path.py,31,the smaller it is the longer is the path,
scikit-learn/examples/linear_model/plot_lasso_coordinate_descent_path.py,47,Display results,
scikit-learn/examples/linear_model/plot_polynomial_interpolation.py,1,!/usr/bin/env python,
scikit-learn/examples/linear_model/plot_polynomial_interpolation.py,26,Author: Mathieu Blondel,
scikit-learn/examples/linear_model/plot_polynomial_interpolation.py,27,Jake Vanderplas,
scikit-learn/examples/linear_model/plot_polynomial_interpolation.py,28,License: BSD 3 clause,
scikit-learn/examples/linear_model/plot_polynomial_interpolation.py,43,generate points used to plot,
scikit-learn/examples/linear_model/plot_polynomial_interpolation.py,46,generate points and keep a subset of them,
scikit-learn/examples/linear_model/plot_polynomial_interpolation.py,53,create matrix versions of these arrays,
scikit-learn/examples/linear_model/plot_sgd_early_stopping.py,38,Authors: Tom Dupre la Tour,
scikit-learn/examples/linear_model/plot_sgd_early_stopping.py,39,,
scikit-learn/examples/linear_model/plot_sgd_early_stopping.py,40,License: BSD 3 clause,
scikit-learn/examples/linear_model/plot_sgd_early_stopping.py,60,Load data from http://openml.org/d/554,
scikit-learn/examples/linear_model/plot_sgd_early_stopping.py,63,take only two classes for binary classification,
scikit-learn/examples/linear_model/plot_sgd_early_stopping.py,89,Define the estimators to compare,
scikit-learn/examples/linear_model/plot_sgd_early_stopping.py,101,Load the dataset,
scikit-learn/examples/linear_model/plot_sgd_early_stopping.py,120,Transform the results in a pandas dataframe for easy plotting,
scikit-learn/examples/linear_model/plot_sgd_early_stopping.py,127,"Define what to plot (x_axis, y_axis)",
scikit-learn/examples/linear_model/plot_sparse_logistic_regression_mnist.py,31,Author: Arthur Mensch <arthur.mensch@m4x.org>,
scikit-learn/examples/linear_model/plot_sparse_logistic_regression_mnist.py,32,License: BSD 3 clause,
scikit-learn/examples/linear_model/plot_sparse_logistic_regression_mnist.py,34,Turn down for faster convergence,
scikit-learn/examples/linear_model/plot_sparse_logistic_regression_mnist.py,38,Load data from https://www.openml.org/d/554,
scikit-learn/examples/linear_model/plot_sparse_logistic_regression_mnist.py,54,Turn up tolerance for faster convergence,
scikit-learn/examples/linear_model/plot_sparse_logistic_regression_mnist.py,61,print('Best C % .4f' % clf.C_),
scikit-learn/examples/linear_model/plot_logistic_multinomial.py,11,Authors: Tom Dupre la Tour <tom.dupre-la-tour@m4x.org>,
scikit-learn/examples/linear_model/plot_logistic_multinomial.py,12,License: BSD 3 clause,
scikit-learn/examples/linear_model/plot_logistic_multinomial.py,19,make 3-class dataset for classification,
scikit-learn/examples/linear_model/plot_logistic_multinomial.py,29,print the training scores,
scikit-learn/examples/linear_model/plot_logistic_multinomial.py,32,create a mesh to plot in,
scikit-learn/examples/linear_model/plot_logistic_multinomial.py,33,step size in the mesh,
scikit-learn/examples/linear_model/plot_logistic_multinomial.py,39,"Plot the decision boundary. For that, we will assign a color to each",
scikit-learn/examples/linear_model/plot_logistic_multinomial.py,40,"point in the mesh [x_min, x_max]x[y_min, y_max].",
scikit-learn/examples/linear_model/plot_logistic_multinomial.py,42,Put the result into a color plot,
scikit-learn/examples/linear_model/plot_logistic_multinomial.py,49,Plot also the training points,
scikit-learn/examples/linear_model/plot_logistic_multinomial.py,56,Plot the three one-against-all classifiers,
scikit-learn/examples/linear_model/plot_lasso_and_elasticnet.py,18,,
scikit-learn/examples/linear_model/plot_lasso_and_elasticnet.py,19,Generate some sparse data to play with,
scikit-learn/examples/linear_model/plot_lasso_and_elasticnet.py,25,Decreasing coef w. alternated signs for visualization,
scikit-learn/examples/linear_model/plot_lasso_and_elasticnet.py,28,sparsify coef,
scikit-learn/examples/linear_model/plot_lasso_and_elasticnet.py,31,Add noise,
scikit-learn/examples/linear_model/plot_lasso_and_elasticnet.py,34,Split data in train set and test set,
scikit-learn/examples/linear_model/plot_lasso_and_elasticnet.py,39,,
scikit-learn/examples/linear_model/plot_lasso_and_elasticnet.py,40,Lasso,
scikit-learn/examples/linear_model/plot_lasso_and_elasticnet.py,51,,
scikit-learn/examples/linear_model/plot_lasso_and_elasticnet.py,52,ElasticNet,
scikit-learn/examples/linear_model/plot_sgd_iris.py,18,import some data to play with,
scikit-learn/examples/linear_model/plot_sgd_iris.py,21,we only take the first two features. We could,
scikit-learn/examples/linear_model/plot_sgd_iris.py,22,avoid this ugly slicing by using a two-dim dataset,
scikit-learn/examples/linear_model/plot_sgd_iris.py,27,shuffle,
scikit-learn/examples/linear_model/plot_sgd_iris.py,34,standardize,
scikit-learn/examples/linear_model/plot_sgd_iris.py,39,step size in the mesh,
scikit-learn/examples/linear_model/plot_sgd_iris.py,43,create a mesh to plot in,
scikit-learn/examples/linear_model/plot_sgd_iris.py,49,"Plot the decision boundary. For that, we will assign a color to each",
scikit-learn/examples/linear_model/plot_sgd_iris.py,50,"point in the mesh [x_min, x_max]x[y_min, y_max].",
scikit-learn/examples/linear_model/plot_sgd_iris.py,52,Put the result into a color plot,
scikit-learn/examples/linear_model/plot_sgd_iris.py,57,Plot also the training points,
scikit-learn/examples/linear_model/plot_sgd_iris.py,65,Plot the three one-against-all classifiers,
scikit-learn/examples/linear_model/plot_ard.py,33,,
scikit-learn/examples/linear_model/plot_ard.py,34,Generating simulated data with Gaussian weights,
scikit-learn/examples/linear_model/plot_ard.py,36,Parameters of the example,
scikit-learn/examples/linear_model/plot_ard.py,39,Create Gaussian data,
scikit-learn/examples/linear_model/plot_ard.py,41,Create weights with a precision lambda_ of 4.,
scikit-learn/examples/linear_model/plot_ard.py,44,Only keep 10 weights of interest,
scikit-learn/examples/linear_model/plot_ard.py,48,Create noise with a precision alpha of 50.,
scikit-learn/examples/linear_model/plot_ard.py,51,Create the target,
scikit-learn/examples/linear_model/plot_ard.py,54,,
scikit-learn/examples/linear_model/plot_ard.py,55,Fit the ARD Regression,
scikit-learn/examples/linear_model/plot_ard.py,62,,
scikit-learn/examples/linear_model/plot_ard.py,63,"Plot the true weights, the estimated weights, the histogram of the",
scikit-learn/examples/linear_model/plot_ard.py,64,"weights, and predictions with standard deviations",
scikit-learn/examples/linear_model/plot_ard.py,92,Plotting some predictions for polynomial regression,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,29,Authors: Christian Lorentzen <lorentzen.ch@gmail.com>,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,30,Roman Yurchak <rth.yurchak@gmail.com>,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,31,License: BSD 3 clause,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,65,freMTPL2freq dataset from https://www.openml.org/d/41214,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,68,unquote string fields,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,76,,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,77,Let's load the motor claim dataset. We ignore the severity data for this,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,78,study for the sake of simplicitly.,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,79,,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,80,We also subsample the data for the sake of computational cost and running,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,81,time. Using the full dataset would lead to similar conclusions.,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,85,Correct for unreasonable observations (that might be data error),
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,88,,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,89,The remaining columns can be used to predict the frequency of claim events.,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,90,Those columns are very heterogeneous with a mix of categorical and numeric,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,91,"variables with different scales, possibly very unevenly distributed.",
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,92,,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,93,In order to fit linear models with those predictors it is therefore,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,94,necessary to perform standard feature transformations as follows:,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,115,,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,116,The number of claims (``ClaimNb``) is a positive integer that can be modeled,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,117,as a Poisson distribution. It is then assumed to be the number of discrete,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,118,events occurring with a constant rate in a given time interval,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,119,"(``Exposure``, in units of years). Here we model the frequency",
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,120,"``y = ClaimNb / Exposure``, which is still a (scaled) Poisson distribution,",
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,121,and use ``Exposure`` as ``sample_weight``.,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,136,,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,137,"It is worth noting that 92 % of policyholders have zero claims, and if we",
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,138,"were to convert this problem into a binary classification task, it would be",
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,139,significantly imbalanced.,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,140,,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,141,"To evaluate the pertinence of the used metrics, we will consider as a",
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,142,"baseline a ""dummy"" estimator that constantly predicts the mean frequency of",
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,143,the training sample.,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,167,"ignore non-positive predictions, as they are invalid for",
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,168,the Poisson deviance,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,185,,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,186,We start by modeling the target variable with the least squares linear,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,187,"regression model,",
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,193,,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,194,The Poisson deviance cannot be computed on non-positive values predicted by,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,195,the model. For models that do return a few non-positive predictions,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,196,"(e.g. :class:`linear_model.Ridge`) we ignore the corresponding samples,",
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,197,meaning that the obtained Poisson deviance is approximate. An alternative,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,198,approach could be to use :class:`compose.TransformedTargetRegressor`,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,199,meta-estimator to map ``y_pred`` to a strictly positive domain.,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,204,,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,205,Next we fit the Poisson regressor on the target variable. We set the,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,206,regularization strength ``alpha`` to 1 over number of samples in oder to,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,207,mimic the Ridge regressor whose L2 penalty term scales differently with the,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,208,number of samples.,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,220,,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,221,"Finally, we will consider a non-linear model, namely a random forest. Random",
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,222,"forests do not require the categorical data to be one-hot encoded: instead,",
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,223,we can encode each category label with an arbitrary integer using,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,224,":class:`preprocessing.OrdinalEncoder`. With this encoding, the forest will",
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,225,"treat the categorical features as ordered features, which might not be always",
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,226,a desired behavior. However this effect is limited for deep enough trees,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,227,which are able to recover the categorical nature of the features. The main,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,228,advantage of the :class:`preprocessing.OrdinalEncoder` over the,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,229,:class:`preprocessing.OneHotEncoder` is that it will make training faster.,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,252,,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,253,"Like the Ridge regression above, the random forest model minimizes the",
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,254,"conditional squared error, too. However, because of a higher predictive",
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,255,"power, it also results in a smaller Poisson deviance than the Poisson",
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,256,regression model.,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,257,,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,258,Evaluating models with a single train / test split is prone to random,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,259,"fluctuations. If computing resources allow, it should be verified that",
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,260,cross-validated performance metrics would lead to similar conclusions.,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,261,,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,262,The qualitative difference between these models can also be visualized by,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,263,comparing the histogram of observed target values with that of predicted,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,264,values:,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,293,,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,294,The experimental data presents a long tail distribution for ``y``. In all,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,295,"models we predict a mean expected value, so we will have necessarily fewer",
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,296,"extreme values. Additionally, the normal distribution used in ``Ridge`` and",
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,297,"``RandomForestRegressor`` has a constant variance, while for the Poisson",
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,298,"distribution used in ``PoissonRegressor``, the variance is proportional to",
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,299,the mean predicted value.,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,300,,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,301,"Thus, among the considered estimators, ``PoissonRegressor`` is better suited",
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,302,for modeling the long tail distribution of the data as compared to the,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,303,``Ridge`` and ``RandomForestRegressor`` estimators.,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,304,,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,305,To ensure that estimators yield reasonable predictions for different,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,306,"policyholder types, we can bin test samples according to ``y_pred`` returned",
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,307,"by each model. Then for each bin, we compare the mean predicted ``y_pred``,",
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,308,with the mean observed target:,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,379,,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,380,The ``Ridge`` regression model can predict very low expected frequencies,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,381,that do not match the data. It can therefore severly under-estimate the risk,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,382,for some policyholders.,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,383,,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,384,``PoissonRegressor`` and ``RandomForestRegressor`` show better consistency,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,385,"between predicted and observed targets, especially for low predicted target",
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,386,values.,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,387,,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,388,"However, for some business applications, we are not necessarily interested",
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,389,"in the ability of the model to predict the expected frequency value, but",
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,390,instead to predict which policyholder groups are the riskiest and which are,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,391,"the safest. In this case, the model evaluation would cast the problem as a",
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,392,ranking problem rather than a regression problem.,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,393,,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,394,"To compare the 3 models within this perspective, one can plot the fraction of",
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,395,the number of claims vs the fraction of exposure for test samples ordered by,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,396,"the model predictions, from safest to riskiest  according to each model:",
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,400,from safest to riskiest,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,423,Oracle model: y_pred == y_test,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,432,Random Baseline,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,442,,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,443,This plot reveals that the random forest model is slightly better at ranking,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,444,policyholders by risk profiles even if the absolute value of the predicted,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,445,expected frequencies are less well calibrated than for the linear Poisson,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,446,model.,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,447,,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,448,All three models are significantly better than chance but also very far from,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,449,making perfect predictions.,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,450,,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,451,This last point is expected due to the nature of the problem: the occurrence,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,452,of accidents is mostly dominated by circumstantial causes that are not,
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,453,captured in the columns of the dataset or that are indeed random.,
scikit-learn/examples/linear_model/plot_ridge_coeffs.py,40,Author: Kornel Kielczewski -- <kornel.k@plusnet.pl>,
scikit-learn/examples/linear_model/plot_ridge_coeffs.py,61,Train the model with different regularisation strengths,
scikit-learn/examples/linear_model/plot_ridge_coeffs.py,68,Display results,
scikit-learn/examples/linear_model/plot_bayesian_ridge.py,33,,
scikit-learn/examples/linear_model/plot_bayesian_ridge.py,34,Generating simulated data with Gaussian weights,
scikit-learn/examples/linear_model/plot_bayesian_ridge.py,37,Create Gaussian data,
scikit-learn/examples/linear_model/plot_bayesian_ridge.py,38,Create weights with a precision lambda_ of 4.,
scikit-learn/examples/linear_model/plot_bayesian_ridge.py,41,Only keep 10 weights of interest,
scikit-learn/examples/linear_model/plot_bayesian_ridge.py,45,Create noise with a precision alpha of 50.,
scikit-learn/examples/linear_model/plot_bayesian_ridge.py,48,Create the target,
scikit-learn/examples/linear_model/plot_bayesian_ridge.py,51,,
scikit-learn/examples/linear_model/plot_bayesian_ridge.py,52,Fit the Bayesian Ridge Regression and an OLS for comparison,
scikit-learn/examples/linear_model/plot_bayesian_ridge.py,59,,
scikit-learn/examples/linear_model/plot_bayesian_ridge.py,60,"Plot true weights, estimated weights, histogram of the weights, and",
scikit-learn/examples/linear_model/plot_bayesian_ridge.py,61,predictions with standard deviations,
scikit-learn/examples/linear_model/plot_bayesian_ridge.py,90,Plotting some predictions for polynomial regression,
scikit-learn/examples/linear_model/plot_sgd_weighted_samples.py,15,we create 20 points,
scikit-learn/examples/linear_model/plot_sgd_weighted_samples.py,20,and assign a bigger weight to the last 10 samples,
scikit-learn/examples/linear_model/plot_sgd_weighted_samples.py,23,plot the weighted data points,
scikit-learn/examples/linear_model/plot_sgd_weighted_samples.py,29,fit the unweighted model,
scikit-learn/examples/linear_model/plot_sgd_weighted_samples.py,36,fit the weighted model,
scikit-learn/examples/linear_model/plot_sparse_logistic_regression_20newsgroups.py,33,Author: Arthur Mensch,
scikit-learn/examples/linear_model/plot_sparse_logistic_regression_20newsgroups.py,39,We use SAGA solver,
scikit-learn/examples/linear_model/plot_sparse_logistic_regression_20newsgroups.py,42,Turn down for faster run time,
scikit-learn/examples/linear_model/plot_sparse_logistic_regression_20newsgroups.py,63,Add initial chance-level values for plotting purpose,
scikit-learn/examples/linear_model/plot_sparse_logistic_regression_20newsgroups.py,70,Small number of epochs for fast runtime,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,42,Authors: Christian Lorentzen <lorentzen.ch@gmail.com>,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,43,Roman Yurchak <rth.yurchak@gmail.com>,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,44,Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,45,License: BSD 3 clause,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,74,freMTPL2freq dataset from https://www.openml.org/d/41214,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,79,freMTPL2sev dataset from https://www.openml.org/d/41215,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,82,sum ClaimAmount over identical IDs,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,88,unquote string fields,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,113,aggregate observed and predicted variables by feature level,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,148,Use default scorer if it exists,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,166,Score the model consisting of the product of frequency and,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,167,severity models.,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,194,,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,195,"Loading datasets, basic feature extraction and target definitions",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,196,-----------------------------------------------------------------,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,197,,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,198,"We construct the freMTPL2 dataset by joining the freMTPL2freq table,",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,199,"containing the number of claims (``ClaimNb``), with the freMTPL2sev table,",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,200,containing the claim amount (``ClaimAmount``) for the same policy ids,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,201,(``IDpol``).,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,205,"Note: filter out claims with zero amount, as the severity model",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,206,requires strictly positive target values.,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,209,Correct for unreasonable observations (that might be data error),
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,210,and a few exceptionally large claim amounts,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,235,"Insurances companies are interested in modeling the Pure Premium, that is",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,236,the expected total claim amount per unit of exposure for each policyholder,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,237,in their portfolio:,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,240,This can be indirectly approximated by a 2-step modeling: the product of the,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,241,Frequency times the average claim amount per claim:,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,248,,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,249,,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,250,Frequency model -- Poisson distribution,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,251,---------------------------------------,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,252,,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,253,The number of claims (``ClaimNb``) is a positive integer (0 included).,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,254,"Thus, this target can be modelled by a Poisson distribution.",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,255,It is then assumed to be the number of discrete events occuring with a,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,256,"constant rate in a given time interval (``Exposure``, in units of years).",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,257,"Here we model the frequency ``y = ClaimNb / Exposure``, which is still a",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,258,"(scaled) Poisson distribution, and use ``Exposure`` as `sample_weight`.",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,262,The parameters of the model are estimated by minimizing the Poisson deviance,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,263,on the training set via a quasi-Newton solver: l-BFGS. Some of the features,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,264,"are collinear, we use a weak penalization to avoid numerical issues.",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,281,,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,282,"We can visually compare observed and predicted values, aggregated by the",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,283,"drivers age (``DrivAge``), vehicle age (``VehAge``) and the insurance",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,284,bonus/malus (``BonusMalus``).,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,337,,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,338,"According to the observed data, the frequency of accidents is higher for",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,339,"drivers younger than 30 years old, and is positively correlated with the",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,340,`BonusMalus` variable. Our model is able to mostly correctly model this,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,341,behaviour.,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,342,,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,343,Severity Model -  Gamma distribution,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,344,------------------------------------,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,345,The mean claim amount or severity (`AvgClaimAmount`) can be empirically,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,346,shown to follow approximately a Gamma distribution. We fit a GLM model for,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,347,the severity with the same features as the frequency model.,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,348,,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,349,Note:,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,350,,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,351,- We filter out ``ClaimAmount == 0`` as the Gamma distribution has support,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,352,"on :math:`(0, \infty)`, not :math:`[0, \infty)`.",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,353,- We use ``ClaimNb`` as `sample_weight` to account for policies that contain,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,354,more than one claim.,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,379,,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,380,"Here, the scores for the test data call for caution as they are",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,381,significantly worse than for the training data indicating an overfit despite,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,382,the strong regularization.,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,383,,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,384,Note that the resulting model is the average claim amount per claim. As,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,385,"such, it is conditional on having at least one claim, and cannot be used to",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,386,predict the average claim amount per policy in general.,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,396,,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,397,"We can visually compare observed and predicted values, aggregated for",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,398,the drivers age (``DrivAge``).,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,426,,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,427,"Overall, the drivers age (``DrivAge``) has a weak impact on the claim",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,428,"severity, both in observed and predicted data.",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,429,,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,430,Pure Premium Modeling via a Product Model vs single TweedieRegressor,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,431,--------------------------------------------------------------------,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,432,"As mentioned in the introduction, the total claim amount per unit of",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,433,exposure can be modeled as the product of the prediction of the,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,434,frequency model by the prediction of the severity model.,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,435,,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,436,"Alternatively, one can directly model the total loss with a unique",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,437,Compound Poisson Gamma generalized linear model (with a log link function).,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,438,"This model is a special case of the Tweedie GLM with a ""power"" parameter",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,439,":math:`p \in (1, 2)`. Here, we fix apriori the `power` parameter of the",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,440,Tweedie model to some arbitrary value (1.9) in the valid range. Ideally one,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,441,would select this value via grid-search by minimizing the negative,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,442,"log-likelihood of the Tweedie model, but unfortunately the current",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,443,implementation does not allow for this (yet).,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,444,,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,445,We will compare the performance of both approaches.,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,446,"To quantify the performance of both models, one can compute",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,447,the mean deviance of the train and test data assuming a Compound,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,448,Poisson-Gamma distribution of the total claim amount. This is equivalent to,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,449,a Tweedie distribution with a `power` parameter between 1 and 2.,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,450,,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,451,The :func:`sklearn.metrics.mean_tweedie_deviance` depends on a `power`,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,452,"parameter. As we do not know the true value of the `power` parameter, we here",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,453,"compute the mean deviances for a grid of possible values, and compare the",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,454,"models side by side, i.e. we compare them at identical values of `power`.",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,455,"Ideally, we hope that one model will be consistently better than the other,",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,456,regardless of `power`.,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,494,,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,495,"In this example, both modeling approaches yield comparable performance",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,496,"metrics. For implementation reasons, the percentage of explained variance",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,497,:math:`D^2` is not available for the product model.,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,498,,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,499,We can additionally validate these models by comparing observed and,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,500,"predicted total claim amount over the test and train subsets. We see that,",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,501,"on average, both model tend to underestimate the total claim (but this",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,502,behavior depends on the amount of regularization).,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,525,,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,526,"Finally, we can compare the two models using a plot of cumulated claims: for",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,527,"each model, the policyholders are ranked from safest to riskiest and the",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,528,fraction of observed total cumulated claims is plotted on the y axis. This,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,529,plot is often called the ordered Lorenz curve of the model.,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,530,,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,531,The Gini coefficient (based on the area under the curve) can be used as a,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,532,model selection metric to quantify the ability of the model to rank,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,533,policyholders. Note that this metric does not reflect the ability of the,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,534,models to make accurate predictions in terms of absolute value of total,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,535,claim amounts but only in terms of relative amounts as a ranking metric.,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,536,,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,537,Both models are able to rank policyholders by risky-ness significantly,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,538,better than chance although they are also both far from perfect due to the,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,539,natural difficulty of the prediction problem from few features.,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,540,,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,541,Note that the Gini index only characterize the ranking performance of the,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,542,model but not its calibration: any monotonic transformation of the,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,543,predictions leaves the Gini index of the model unchanged.,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,544,,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,545,Finally one should highlight that the Compound Poisson Gamma model that,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,546,is directly fit on the pure premium is operationally simpler to develop and,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,547,maintain as it consists in a single scikit-learn estimator instead of a,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,548,"pair of models, each with its own set of hyperparameters.",
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,555,order samples by increasing predicted risk:,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,578,Oracle model: y_pred == y_test,
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,586,Random baseline,
scikit-learn/examples/linear_model/plot_lasso_lars.py,1,!/usr/bin/env python,
scikit-learn/examples/linear_model/plot_lasso_lars.py,15,Author: Fabian Pedregosa <fabian.pedregosa@inria.fr>,
scikit-learn/examples/linear_model/plot_lasso_lars.py,16,Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/examples/linear_model/plot_lasso_lars.py,17,License: BSD 3 clause,
scikit-learn/examples/linear_model/plot_ols_3d.py,1,!/usr/bin/python,
scikit-learn/examples/linear_model/plot_ols_3d.py,2,-*- coding: utf-8 -*-,
scikit-learn/examples/linear_model/plot_ols_3d.py,18,Code source: Gaël Varoquaux,
scikit-learn/examples/linear_model/plot_ols_3d.py,19,Modified for documentation by Jaques Grobler,
scikit-learn/examples/linear_model/plot_ols_3d.py,20,License: BSD 3 clause,
scikit-learn/examples/linear_model/plot_ols_3d.py,40,,
scikit-learn/examples/linear_model/plot_ols_3d.py,41,Plot the figure,
scikit-learn/examples/linear_model/plot_ols_3d.py,62,Generate the three different figures from different views,
scikit-learn/examples/linear_model/plot_logistic.py,1,!/usr/bin/python,
scikit-learn/examples/linear_model/plot_logistic.py,2,-*- coding: utf-8 -*-,
scikit-learn/examples/linear_model/plot_logistic.py,18,Code source: Gael Varoquaux,
scikit-learn/examples/linear_model/plot_logistic.py,19,License: BSD 3 clause,
scikit-learn/examples/linear_model/plot_logistic.py,27,General a toy dataset:s it's just a straight line with some Gaussian noise:,
scikit-learn/examples/linear_model/plot_logistic.py,38,Fit the classifier,
scikit-learn/examples/linear_model/plot_logistic.py,42,and plot the result,
scikit-learn/examples/linear_model/plot_sgd_comparison.py,10,Author: Rob Zinkov <rob at zinkov dot com>,
scikit-learn/examples/linear_model/plot_sgd_comparison.py,11,License: BSD 3 clause,
scikit-learn/examples/linear_model/plot_ols_ridge_variance.py,1,!/usr/bin/python,
scikit-learn/examples/linear_model/plot_ols_ridge_variance.py,2,-*- coding: utf-8 -*-,
scikit-learn/examples/linear_model/plot_ols_ridge_variance.py,26,Code source: Gaël Varoquaux,
scikit-learn/examples/linear_model/plot_ols_ridge_variance.py,27,Modified for documentation by Jaques Grobler,
scikit-learn/examples/linear_model/plot_ols_ridge_variance.py,28,License: BSD 3 clause,
scikit-learn/examples/linear_model/plot_iris_logistic.py,1,!/usr/bin/python,
scikit-learn/examples/linear_model/plot_iris_logistic.py,2,-*- coding: utf-8 -*-,
scikit-learn/examples/linear_model/plot_iris_logistic.py,17,Code source: Gaël Varoquaux,
scikit-learn/examples/linear_model/plot_iris_logistic.py,18,Modified for documentation by Jaques Grobler,
scikit-learn/examples/linear_model/plot_iris_logistic.py,19,License: BSD 3 clause,
scikit-learn/examples/linear_model/plot_iris_logistic.py,26,import some data to play with,
scikit-learn/examples/linear_model/plot_iris_logistic.py,28,we only take the first two features.,
scikit-learn/examples/linear_model/plot_iris_logistic.py,33,Create an instance of Logistic Regression Classifier and fit the data.,
scikit-learn/examples/linear_model/plot_iris_logistic.py,36,"Plot the decision boundary. For that, we will assign a color to each",
scikit-learn/examples/linear_model/plot_iris_logistic.py,37,"point in the mesh [x_min, x_max]x[y_min, y_max].",
scikit-learn/examples/linear_model/plot_iris_logistic.py,40,step size in the mesh,
scikit-learn/examples/linear_model/plot_iris_logistic.py,44,Put the result into a color plot,
scikit-learn/examples/linear_model/plot_iris_logistic.py,49,Plot also the training points,
scikit-learn/examples/linear_model/plot_logistic_l1_l2_sparsity.py,19,Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/examples/linear_model/plot_logistic_l1_l2_sparsity.py,20,Mathieu Blondel <mathieu@mblondel.org>,
scikit-learn/examples/linear_model/plot_logistic_l1_l2_sparsity.py,21,Andreas Mueller <amueller@ais.uni-bonn.de>,
scikit-learn/examples/linear_model/plot_logistic_l1_l2_sparsity.py,22,License: BSD 3 clause,
scikit-learn/examples/linear_model/plot_logistic_l1_l2_sparsity.py,35,classify small against large digits,
scikit-learn/examples/linear_model/plot_logistic_l1_l2_sparsity.py,38,L1 weight in the Elastic-Net regularization,
scikit-learn/examples/linear_model/plot_logistic_l1_l2_sparsity.py,42,Set regularization parameter,
scikit-learn/examples/linear_model/plot_logistic_l1_l2_sparsity.py,44,turn down tolerance for short training time,
scikit-learn/examples/linear_model/plot_logistic_l1_l2_sparsity.py,57,coef_l1_LR contains zeros due to the,
scikit-learn/examples/linear_model/plot_logistic_l1_l2_sparsity.py,58,L1 sparsity inducing norm,
scikit-learn/examples/linear_model/plot_logistic_path.py,1,!/usr/bin/env python,
scikit-learn/examples/linear_model/plot_logistic_path.py,30,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/examples/linear_model/plot_logistic_path.py,31,License: BSD 3 clause,
scikit-learn/examples/linear_model/plot_logistic_path.py,48,Normalize X to speed-up convergence,
scikit-learn/examples/linear_model/plot_logistic_path.py,50,,
scikit-learn/examples/linear_model/plot_logistic_path.py,51,Demo path functions,
scikit-learn/examples/linear_model/plot_lasso_dense_vs_sparse_data.py,20,,
scikit-learn/examples/linear_model/plot_lasso_dense_vs_sparse_data.py,21,The two Lasso implementations on Dense data,
scikit-learn/examples/linear_model/plot_lasso_dense_vs_sparse_data.py,42,,
scikit-learn/examples/linear_model/plot_lasso_dense_vs_sparse_data.py,43,The two Lasso implementations on Sparse data,
scikit-learn/examples/linear_model/plot_ols.py,1,!/usr/bin/python,
scikit-learn/examples/linear_model/plot_ols.py,2,-*- coding: utf-8 -*-,
scikit-learn/examples/linear_model/plot_ols.py,22,Code source: Jaques Grobler,
scikit-learn/examples/linear_model/plot_ols.py,23,License: BSD 3 clause,
scikit-learn/examples/linear_model/plot_ols.py,31,Load the diabetes dataset,
scikit-learn/examples/linear_model/plot_ols.py,34,Use only one feature,
scikit-learn/examples/linear_model/plot_ols.py,37,Split the data into training/testing sets,
scikit-learn/examples/linear_model/plot_ols.py,41,Split the targets into training/testing sets,
scikit-learn/examples/linear_model/plot_ols.py,45,Create linear regression object,
scikit-learn/examples/linear_model/plot_ols.py,48,Train the model using the training sets,
scikit-learn/examples/linear_model/plot_ols.py,51,Make predictions using the testing set,
scikit-learn/examples/linear_model/plot_ols.py,54,The coefficients,
scikit-learn/examples/linear_model/plot_ols.py,56,The mean squared error,
scikit-learn/examples/linear_model/plot_ols.py,59,The coefficient of determination: 1 is perfect prediction,
scikit-learn/examples/linear_model/plot_ols.py,63,Plot outputs,
scikit-learn/examples/linear_model/plot_sgd_separating_hyperplane.py,17,we create 50 separable points,
scikit-learn/examples/linear_model/plot_sgd_separating_hyperplane.py,20,fit the model,
scikit-learn/examples/linear_model/plot_sgd_separating_hyperplane.py,25,"plot the line, the points, and the nearest vectors to the plane",
scikit-learn/examples/linear_model/plot_ransac.py,24,Add outlier data,
scikit-learn/examples/linear_model/plot_ransac.py,29,Fit line using all data,
scikit-learn/examples/linear_model/plot_ransac.py,33,Robustly fit linear model with RANSAC algorithm,
scikit-learn/examples/linear_model/plot_ransac.py,39,Predict data of estimated models,
scikit-learn/examples/linear_model/plot_ransac.py,44,Compare estimated coefficients,
scikit-learn/examples/linear_model/plot_multi_task_lasso_support.py,1,!/usr/bin/env python,
scikit-learn/examples/linear_model/plot_multi_task_lasso_support.py,18,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/examples/linear_model/plot_multi_task_lasso_support.py,19,License: BSD 3 clause,
scikit-learn/examples/linear_model/plot_multi_task_lasso_support.py,28,Generate some 2D coefficients with sine waves with random frequency and phase,
scikit-learn/examples/linear_model/plot_multi_task_lasso_support.py,42,,
scikit-learn/examples/linear_model/plot_multi_task_lasso_support.py,43,Plot support and time series,
scikit-learn/examples/classification/plot_digits_classification.py,15,Author: Gael Varoquaux <gael dot varoquaux at normalesup dot org>,
scikit-learn/examples/classification/plot_digits_classification.py,16,License: BSD 3 clause,
scikit-learn/examples/classification/plot_digits_classification.py,18,Standard scientific Python imports,
scikit-learn/examples/classification/plot_digits_classification.py,21,"Import datasets, classifiers and performance metrics",
scikit-learn/examples/classification/plot_digits_classification.py,25,The digits dataset,
scikit-learn/examples/classification/plot_digits_classification.py,28,"The data that we are interested in is made of 8x8 images of digits, let's",
scikit-learn/examples/classification/plot_digits_classification.py,29,"have a look at the first 4 images, stored in the `images` attribute of the",
scikit-learn/examples/classification/plot_digits_classification.py,30,"dataset.  If we were working from image files, we could load them using",
scikit-learn/examples/classification/plot_digits_classification.py,31,matplotlib.pyplot.imread.  Note that each image must have the same size. For these,
scikit-learn/examples/classification/plot_digits_classification.py,32,"images, we know which digit they represent: it is given in the 'target' of",
scikit-learn/examples/classification/plot_digits_classification.py,33,the dataset.,
scikit-learn/examples/classification/plot_digits_classification.py,41,"To apply a classifier on this data, we need to flatten the image, to",
scikit-learn/examples/classification/plot_digits_classification.py,42,"turn the data in a (samples, feature) matrix:",
scikit-learn/examples/classification/plot_digits_classification.py,46,Create a classifier: a support vector classifier,
scikit-learn/examples/classification/plot_digits_classification.py,49,Split data into train and test subsets,
scikit-learn/examples/classification/plot_digits_classification.py,53,We learn the digits on the first half of the digits,
scikit-learn/examples/classification/plot_digits_classification.py,56,Now predict the value of the digit on the second half:,
scikit-learn/examples/classification/plot_classification_probability.py,20,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/examples/classification/plot_classification_probability.py,21,License: BSD 3 clause,
scikit-learn/examples/classification/plot_classification_probability.py,34,we only take the first two features for visualization,
scikit-learn/examples/classification/plot_classification_probability.py,40,for GPC,
scikit-learn/examples/classification/plot_classification_probability.py,42,Create different classifiers.,
scikit-learn/examples/classification/plot_classification_probability.py,78,View probabilities:,
scikit-learn/examples/classification/plot_classifier_comparison.py,1,!/usr/bin/python,
scikit-learn/examples/classification/plot_classifier_comparison.py,2,-*- coding: utf-8 -*-,
scikit-learn/examples/classification/plot_classifier_comparison.py,26,Code source: Gaël Varoquaux,
scikit-learn/examples/classification/plot_classifier_comparison.py,27,Andreas Müller,
scikit-learn/examples/classification/plot_classifier_comparison.py,28,Modified for documentation by Jaques Grobler,
scikit-learn/examples/classification/plot_classifier_comparison.py,29,License: BSD 3 clause,
scikit-learn/examples/classification/plot_classifier_comparison.py,47,step size in the mesh,
scikit-learn/examples/classification/plot_classifier_comparison.py,78,iterate over datasets,
scikit-learn/examples/classification/plot_classifier_comparison.py,80,"preprocess dataset, split into training and test part",
scikit-learn/examples/classification/plot_classifier_comparison.py,91,just plot the dataset first,
scikit-learn/examples/classification/plot_classifier_comparison.py,97,Plot the training points,
scikit-learn/examples/classification/plot_classifier_comparison.py,100,Plot the testing points,
scikit-learn/examples/classification/plot_classifier_comparison.py,109,iterate over classifiers,
scikit-learn/examples/classification/plot_classifier_comparison.py,115,"Plot the decision boundary. For that, we will assign a color to each",
scikit-learn/examples/classification/plot_classifier_comparison.py,116,"point in the mesh [x_min, x_max]x[y_min, y_max].",
scikit-learn/examples/classification/plot_classifier_comparison.py,122,Put the result into a color plot,
scikit-learn/examples/classification/plot_classifier_comparison.py,126,Plot the training points,
scikit-learn/examples/classification/plot_classifier_comparison.py,129,Plot the testing points,
scikit-learn/examples/classification/plot_lda_qda.py,23,,
scikit-learn/examples/classification/plot_lda_qda.py,24,Colormap,
scikit-learn/examples/classification/plot_lda_qda.py,33,,
scikit-learn/examples/classification/plot_lda_qda.py,34,Generate datasets,
scikit-learn/examples/classification/plot_lda_qda.py,57,,
scikit-learn/examples/classification/plot_lda_qda.py,58,Plot functions,
scikit-learn/examples/classification/plot_lda_qda.py,69,True Positive,
scikit-learn/examples/classification/plot_lda_qda.py,75,class 0: dots,
scikit-learn/examples/classification/plot_lda_qda.py,78,dark red,
scikit-learn/examples/classification/plot_lda_qda.py,80,class 1: dots,
scikit-learn/examples/classification/plot_lda_qda.py,83,dark blue,
scikit-learn/examples/classification/plot_lda_qda.py,85,class 0 and 1 : areas,
scikit-learn/examples/classification/plot_lda_qda.py,97,means,
scikit-learn/examples/classification/plot_lda_qda.py,110,convert to degrees,
scikit-learn/examples/classification/plot_lda_qda.py,111,filled Gaussian at 2 standard deviation,
scikit-learn/examples/classification/plot_lda_qda.py,136,Linear Discriminant Analysis,
scikit-learn/examples/classification/plot_lda_qda.py,143,Quadratic Discriminant Analysis,
scikit-learn/examples/classification/plot_lda.py,15,samples for training,
scikit-learn/examples/classification/plot_lda.py,16,samples for testing,
scikit-learn/examples/classification/plot_lda.py,17,how often to repeat classification,
scikit-learn/examples/classification/plot_lda.py,18,maximum number of features,
scikit-learn/examples/classification/plot_lda.py,19,step size for the calculation,
scikit-learn/examples/classification/plot_lda.py,33,add non-discriminative features,
scikit-learn/examples/neighbors/plot_nca_illustration.py,12,License: BSD 3 clause,
scikit-learn/examples/neighbors/plot_nca_illustration.py,23,,
scikit-learn/examples/neighbors/plot_nca_illustration.py,24,Original points,
scikit-learn/examples/neighbors/plot_nca_illustration.py,25,---------------,
scikit-learn/examples/neighbors/plot_nca_illustration.py,26,"First we create a data set of 9 samples from 3 classes, and plot the points",
scikit-learn/examples/neighbors/plot_nca_illustration.py,27,"in the original space. For this example, we focus on the classification of",
scikit-learn/examples/neighbors/plot_nca_illustration.py,28,point no. 3. The thickness of a link between point no. 3 and another point,
scikit-learn/examples/neighbors/plot_nca_illustration.py,29,is proportional to their distance.,
scikit-learn/examples/neighbors/plot_nca_illustration.py,44,so that boundaries are displayed correctly as circles,
scikit-learn/examples/neighbors/plot_nca_illustration.py,53,compute exponentiated distances (use the log-sum-exp trick to,
scikit-learn/examples/neighbors/plot_nca_illustration.py,54,avoid numerical instabilities,
scikit-learn/examples/neighbors/plot_nca_illustration.py,74,,
scikit-learn/examples/neighbors/plot_nca_illustration.py,75,Learning an embedding,
scikit-learn/examples/neighbors/plot_nca_illustration.py,76,---------------------,
scikit-learn/examples/neighbors/plot_nca_illustration.py,77,We use :class:`~sklearn.neighbors.NeighborhoodComponentsAnalysis` to learn an,
scikit-learn/examples/neighbors/plot_nca_illustration.py,78,embedding and plot the points after the transformation. We then take the,
scikit-learn/examples/neighbors/plot_nca_illustration.py,79,embedding and find the nearest neighbors.,
scikit-learn/examples/neighbors/plot_digits_kde_sampling.py,21,load the data,
scikit-learn/examples/neighbors/plot_digits_kde_sampling.py,24,project the 64-dimensional data to a lower dimension,
scikit-learn/examples/neighbors/plot_digits_kde_sampling.py,28,use grid search cross-validation to optimize the bandwidth,
scikit-learn/examples/neighbors/plot_digits_kde_sampling.py,35,use the best estimator to compute the kernel density estimate,
scikit-learn/examples/neighbors/plot_digits_kde_sampling.py,38,sample 44 new points from the data,
scikit-learn/examples/neighbors/plot_digits_kde_sampling.py,42,turn data into a 4x11 grid,
scikit-learn/examples/neighbors/plot_digits_kde_sampling.py,46,plot real digits and resampled digits,
scikit-learn/examples/neighbors/plot_lof_novelty_detection.py,37,Generate normal (not abnormal) training observations,
scikit-learn/examples/neighbors/plot_lof_novelty_detection.py,40,Generate new normal (not abnormal) observations,
scikit-learn/examples/neighbors/plot_lof_novelty_detection.py,43,Generate some abnormal novel observations,
scikit-learn/examples/neighbors/plot_lof_novelty_detection.py,46,fit the model for novelty detection (novelty=True),
scikit-learn/examples/neighbors/plot_lof_novelty_detection.py,49,"DO NOT use predict, decision_function and score_samples on X_train as this",
scikit-learn/examples/neighbors/plot_lof_novelty_detection.py,50,"would give wrong results but only on new unseen data (not used in X_train),",
scikit-learn/examples/neighbors/plot_lof_novelty_detection.py,51,"e.g. X_test, X_outliers or the meshgrid",
scikit-learn/examples/neighbors/plot_lof_novelty_detection.py,57,"plot the learned frontier, the points, and the nearest vectors to the plane",
scikit-learn/examples/neighbors/plot_nca_dim_reduction.py,30,License: BSD 3 clause,
scikit-learn/examples/neighbors/plot_nca_dim_reduction.py,48,Load Digits dataset,
scikit-learn/examples/neighbors/plot_nca_dim_reduction.py,51,Split into train/test,
scikit-learn/examples/neighbors/plot_nca_dim_reduction.py,59,Reduce dimension to 2 with PCA,
scikit-learn/examples/neighbors/plot_nca_dim_reduction.py,63,Reduce dimension to 2 with LinearDiscriminantAnalysis,
scikit-learn/examples/neighbors/plot_nca_dim_reduction.py,67,Reduce dimension to 2 with NeighborhoodComponentAnalysis,
scikit-learn/examples/neighbors/plot_nca_dim_reduction.py,72,Use a nearest neighbor classifier to evaluate the methods,
scikit-learn/examples/neighbors/plot_nca_dim_reduction.py,75,Make a list of the methods to be compared,
scikit-learn/examples/neighbors/plot_nca_dim_reduction.py,78,plt.figure(),
scikit-learn/examples/neighbors/plot_nca_dim_reduction.py,81,"plt.subplot(1, 3, i + 1, aspect=1)",
scikit-learn/examples/neighbors/plot_nca_dim_reduction.py,83,Fit the method's model,
scikit-learn/examples/neighbors/plot_nca_dim_reduction.py,86,Fit a nearest neighbor classifier on the embedded training set,
scikit-learn/examples/neighbors/plot_nca_dim_reduction.py,89,Compute the nearest neighbor accuracy on the embedded test set,
scikit-learn/examples/neighbors/plot_nca_dim_reduction.py,92,Embed the data set in 2 dimensions using the fitted model,
scikit-learn/examples/neighbors/plot_nca_dim_reduction.py,95,Plot the projected points and show the evaluation score,
scikit-learn/examples/neighbors/plot_nca_classification.py,17,License: BSD 3 clause,
scikit-learn/examples/neighbors/plot_nca_classification.py,37,we only take two features. We could avoid this ugly,
scikit-learn/examples/neighbors/plot_nca_classification.py,38,slicing by using a two-dim dataset,
scikit-learn/examples/neighbors/plot_nca_classification.py,44,step size in the mesh,
scikit-learn/examples/neighbors/plot_nca_classification.py,46,Create color maps,
scikit-learn/examples/neighbors/plot_nca_classification.py,71,"Plot the decision boundary. For that, we will assign a color to each",
scikit-learn/examples/neighbors/plot_nca_classification.py,72,"point in the mesh [x_min, x_max]x[y_min, y_max].",
scikit-learn/examples/neighbors/plot_nca_classification.py,75,Put the result into a color plot,
scikit-learn/examples/neighbors/plot_nca_classification.py,80,Plot also the training and testing points,
scikit-learn/examples/neighbors/plot_regression.py,13,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/examples/neighbors/plot_regression.py,14,Fabian Pedregosa <fabian.pedregosa@inria.fr>,
scikit-learn/examples/neighbors/plot_regression.py,15,,
scikit-learn/examples/neighbors/plot_regression.py,16,License: BSD 3 clause (C) INRIA,
scikit-learn/examples/neighbors/plot_regression.py,19,,
scikit-learn/examples/neighbors/plot_regression.py,20,Generate sample data,
scikit-learn/examples/neighbors/plot_regression.py,30,Add noise to targets,
scikit-learn/examples/neighbors/plot_regression.py,33,,
scikit-learn/examples/neighbors/plot_regression.py,34,Fit regression model,
scikit-learn/examples/neighbors/plot_kde_1d.py,29,Author: Jake Vanderplas <jakevdp@cs.washington.edu>,
scikit-learn/examples/neighbors/plot_kde_1d.py,30,,
scikit-learn/examples/neighbors/plot_kde_1d.py,38,`normed` is being deprecated in favor of `density` in histograms,
scikit-learn/examples/neighbors/plot_kde_1d.py,44,----------------------------------------------------------------------,
scikit-learn/examples/neighbors/plot_kde_1d.py,45,Plot the progression of histograms to kernels,
scikit-learn/examples/neighbors/plot_kde_1d.py,56,histogram 1,
scikit-learn/examples/neighbors/plot_kde_1d.py,60,histogram 2,
scikit-learn/examples/neighbors/plot_kde_1d.py,64,tophat KDE,
scikit-learn/examples/neighbors/plot_kde_1d.py,70,Gaussian KDE,
scikit-learn/examples/neighbors/plot_kde_1d.py,87,----------------------------------------------------------------------,
scikit-learn/examples/neighbors/plot_kde_1d.py,88,Plot all available kernels,
scikit-learn/examples/neighbors/plot_kde_1d.py,122,----------------------------------------------------------------------,
scikit-learn/examples/neighbors/plot_kde_1d.py,123,Plot a 1D density example,
scikit-learn/examples/neighbors/plot_classification.py,18,import some data to play with,
scikit-learn/examples/neighbors/plot_classification.py,21,we only take the first two features. We could avoid this ugly,
scikit-learn/examples/neighbors/plot_classification.py,22,slicing by using a two-dim dataset,
scikit-learn/examples/neighbors/plot_classification.py,26,step size in the mesh,
scikit-learn/examples/neighbors/plot_classification.py,28,Create color maps,
scikit-learn/examples/neighbors/plot_classification.py,33,we create an instance of Neighbours Classifier and fit the data.,
scikit-learn/examples/neighbors/plot_classification.py,37,"Plot the decision boundary. For that, we will assign a color to each",
scikit-learn/examples/neighbors/plot_classification.py,38,"point in the mesh [x_min, x_max]x[y_min, y_max].",
scikit-learn/examples/neighbors/plot_classification.py,45,Put the result into a color plot,
scikit-learn/examples/neighbors/plot_classification.py,50,Plot also the training points,
scikit-learn/examples/neighbors/approximate_nearest_neighbors.py,44,Author: Tom Dupre la Tour,
scikit-learn/examples/neighbors/approximate_nearest_neighbors.py,45,,
scikit-learn/examples/neighbors/approximate_nearest_neighbors.py,46,License: BSD 3 clause,
scikit-learn/examples/neighbors/approximate_nearest_neighbors.py,91,see more metric in the manual,
scikit-learn/examples/neighbors/approximate_nearest_neighbors.py,92,https://github.com/nmslib/nmslib/tree/master/manual,
scikit-learn/examples/neighbors/approximate_nearest_neighbors.py,109,"For compatibility reasons, as each sample is considered as its own",
scikit-learn/examples/neighbors/approximate_nearest_neighbors.py,110,"neighbor, one extra neighbor will be computed.",
scikit-learn/examples/neighbors/approximate_nearest_neighbors.py,160,"For compatibility reasons, as each sample is considered as its own",
scikit-learn/examples/neighbors/approximate_nearest_neighbors.py,161,"neighbor, one extra neighbor will be computed.",
scikit-learn/examples/neighbors/approximate_nearest_neighbors.py,225,TSNE requires a certain number of neighbors which depends on the,
scikit-learn/examples/neighbors/approximate_nearest_neighbors.py,226,perplexity parameter.,
scikit-learn/examples/neighbors/approximate_nearest_neighbors.py,227,Add one since we include each sample as its own neighbor.,
scikit-learn/examples/neighbors/approximate_nearest_neighbors.py,255,init the plot,
scikit-learn/examples/neighbors/approximate_nearest_neighbors.py,273,print the duration report,
scikit-learn/examples/neighbors/approximate_nearest_neighbors.py,278,plot TSNE embedding which should be very similar across methods,
scikit-learn/examples/neighbors/plot_species_kde.py,38,Author: Jake Vanderplas <jakevdp@cs.washington.edu>,
scikit-learn/examples/neighbors/plot_species_kde.py,39,,
scikit-learn/examples/neighbors/plot_species_kde.py,40,License: BSD 3 clause,
scikit-learn/examples/neighbors/plot_species_kde.py,47,"if basemap is available, we'll use it.",
scikit-learn/examples/neighbors/plot_species_kde.py,48,"otherwise, we'll improvise later...",
scikit-learn/examples/neighbors/plot_species_kde.py,69,"x,y coordinates for corner cells",
scikit-learn/examples/neighbors/plot_species_kde.py,75,x coordinates of the grid cells,
scikit-learn/examples/neighbors/plot_species_kde.py,77,y coordinates of the grid cells,
scikit-learn/examples/neighbors/plot_species_kde.py,83,Get matrices/arrays of species IDs and locations,
scikit-learn/examples/neighbors/plot_species_kde.py,91,Convert lat/long to radians,
scikit-learn/examples/neighbors/plot_species_kde.py,93,Set up the data grid for the contour plot,
scikit-learn/examples/neighbors/plot_species_kde.py,103,Plot map of South America with distributions of each species,
scikit-learn/examples/neighbors/plot_species_kde.py,110,construct a kernel density estimate of the distribution,
scikit-learn/examples/neighbors/plot_species_kde.py,116,evaluate only on the land: -9999 indicates ocean,
scikit-learn/examples/neighbors/plot_species_kde.py,121,plot contours of the density,
scikit-learn/examples/neighbors/plot_nearest_centroid.py,19,import some data to play with,
scikit-learn/examples/neighbors/plot_nearest_centroid.py,21,we only take the first two features. We could avoid this ugly,
scikit-learn/examples/neighbors/plot_nearest_centroid.py,22,slicing by using a two-dim dataset,
scikit-learn/examples/neighbors/plot_nearest_centroid.py,26,step size in the mesh,
scikit-learn/examples/neighbors/plot_nearest_centroid.py,28,Create color maps,
scikit-learn/examples/neighbors/plot_nearest_centroid.py,33,we create an instance of Neighbours Classifier and fit the data.,
scikit-learn/examples/neighbors/plot_nearest_centroid.py,38,"Plot the decision boundary. For that, we will assign a color to each",
scikit-learn/examples/neighbors/plot_nearest_centroid.py,39,"point in the mesh [x_min, x_max]x[y_min, y_max].",
scikit-learn/examples/neighbors/plot_nearest_centroid.py,46,Put the result into a color plot,
scikit-learn/examples/neighbors/plot_nearest_centroid.py,51,Plot also the training points,
scikit-learn/examples/neighbors/plot_caching_nearest_neighbors.py,19,Author: Tom Dupre la Tour,
scikit-learn/examples/neighbors/plot_caching_nearest_neighbors.py,20,,
scikit-learn/examples/neighbors/plot_caching_nearest_neighbors.py,21,License: BSD 3 clause,
scikit-learn/examples/neighbors/plot_caching_nearest_neighbors.py,35,The transformer computes the nearest neighbors graph using the maximum number,
scikit-learn/examples/neighbors/plot_caching_nearest_neighbors.py,36,of neighbors necessary in the grid search. The classifier model filters the,
scikit-learn/examples/neighbors/plot_caching_nearest_neighbors.py,37,nearest neighbors graph as required by its own n_neighbors parameter.,
scikit-learn/examples/neighbors/plot_caching_nearest_neighbors.py,42,Note that we give `memory` a directory to cache the graph computation,
scikit-learn/examples/neighbors/plot_caching_nearest_neighbors.py,43,that will be used several times when tuning the hyperparameters of the,
scikit-learn/examples/neighbors/plot_caching_nearest_neighbors.py,44,classifier.,
scikit-learn/examples/neighbors/plot_caching_nearest_neighbors.py,54,Plot the results of the grid search.,
scikit-learn/examples/neighbors/plot_lof_outlier_detection.py,34,Generate train data,
scikit-learn/examples/neighbors/plot_lof_outlier_detection.py,38,Generate some outliers,
scikit-learn/examples/neighbors/plot_lof_outlier_detection.py,46,fit the model for outlier detection (default),
scikit-learn/examples/neighbors/plot_lof_outlier_detection.py,48,use fit_predict to compute the predicted labels of the training samples,
scikit-learn/examples/neighbors/plot_lof_outlier_detection.py,49,"(when LOF is used for outlier detection, the estimator has no predict,",
scikit-learn/examples/neighbors/plot_lof_outlier_detection.py,50,decision_function and score_samples methods).,
scikit-learn/examples/neighbors/plot_lof_outlier_detection.py,57,plot circles with radius proportional to the outlier scores,
scikit-learn/examples/decomposition/plot_ica_vs_pca.py,32,"Authors: Alexandre Gramfort, Gael Varoquaux",
scikit-learn/examples/decomposition/plot_ica_vs_pca.py,33,License: BSD 3 clause,
scikit-learn/examples/decomposition/plot_ica_vs_pca.py,40,,
scikit-learn/examples/decomposition/plot_ica_vs_pca.py,41,Generate sample data,
scikit-learn/examples/decomposition/plot_ica_vs_pca.py,46,Mix data,
scikit-learn/examples/decomposition/plot_ica_vs_pca.py,47,Mixing matrix,
scikit-learn/examples/decomposition/plot_ica_vs_pca.py,49,Generate observations,
scikit-learn/examples/decomposition/plot_ica_vs_pca.py,55,Estimate the sources,
scikit-learn/examples/decomposition/plot_ica_vs_pca.py,60,,
scikit-learn/examples/decomposition/plot_ica_vs_pca.py,61,Plot results,
scikit-learn/examples/decomposition/plot_ica_vs_pca.py,71,Trick to get legend to work,
scikit-learn/examples/decomposition/plot_kernel_pca.py,11,Authors: Mathieu Blondel,
scikit-learn/examples/decomposition/plot_kernel_pca.py,12,Andreas Mueller,
scikit-learn/examples/decomposition/plot_kernel_pca.py,13,License: BSD 3 clause,
scikit-learn/examples/decomposition/plot_kernel_pca.py,31,Plot results,
scikit-learn/examples/decomposition/plot_kernel_pca.py,48,projection on the first principal component (in the phi space),
scikit-learn/examples/decomposition/plot_pca_vs_lda.py,39,Percentage of variance explained for each components,
scikit-learn/examples/decomposition/plot_image_denoising.py,47,SciPy >= 0.16 have face in misc,
scikit-learn/examples/decomposition/plot_image_denoising.py,53,Convert from uint8 representation with values between 0 and 255 to,
scikit-learn/examples/decomposition/plot_image_denoising.py,54,a floating point representation with values between 0 and 1.,
scikit-learn/examples/decomposition/plot_image_denoising.py,57,downsample for higher speed,
scikit-learn/examples/decomposition/plot_image_denoising.py,62,Distort the right half of the image,
scikit-learn/examples/decomposition/plot_image_denoising.py,67,Extract all reference patches from the left half of the image,
scikit-learn/examples/decomposition/plot_image_denoising.py,77,,
scikit-learn/examples/decomposition/plot_image_denoising.py,78,Learn the dictionary from reference patches,
scikit-learn/examples/decomposition/plot_image_denoising.py,100,,
scikit-learn/examples/decomposition/plot_image_denoising.py,101,Display the distorted image,
scikit-learn/examples/decomposition/plot_image_denoising.py,125,,
scikit-learn/examples/decomposition/plot_image_denoising.py,126,Extract noisy patches and reconstruct them using the dictionary,
scikit-learn/examples/decomposition/plot_incremental_pca.py,23,Authors: Kyle Kastner,
scikit-learn/examples/decomposition/plot_incremental_pca.py,24,License: BSD 3 clause,
scikit-learn/examples/decomposition/plot_pca_vs_fa_model_selection.py,27,Authors: Alexandre Gramfort,
scikit-learn/examples/decomposition/plot_pca_vs_fa_model_selection.py,28,Denis A. Engemann,
scikit-learn/examples/decomposition/plot_pca_vs_fa_model_selection.py,29,License: BSD 3 clause,
scikit-learn/examples/decomposition/plot_pca_vs_fa_model_selection.py,42,,
scikit-learn/examples/decomposition/plot_pca_vs_fa_model_selection.py,43,Create the data,
scikit-learn/examples/decomposition/plot_pca_vs_fa_model_selection.py,51,Adding homoscedastic noise,
scikit-learn/examples/decomposition/plot_pca_vs_fa_model_selection.py,54,Adding heteroscedastic noise,
scikit-learn/examples/decomposition/plot_pca_vs_fa_model_selection.py,58,,
scikit-learn/examples/decomposition/plot_pca_vs_fa_model_selection.py,59,Fit the models,
scikit-learn/examples/decomposition/plot_pca_vs_fa_model_selection.py,61,options for n_components,
scikit-learn/examples/decomposition/plot_pca_vs_fa_model_selection.py,114,compare with other covariance estimators,
scikit-learn/examples/decomposition/plot_faces_decomposition.py,14,"Authors: Vlad Niculae, Alexandre Gramfort",
scikit-learn/examples/decomposition/plot_faces_decomposition.py,15,License: BSD 3 clause,
scikit-learn/examples/decomposition/plot_faces_decomposition.py,27,Display progress logs on stdout,
scikit-learn/examples/decomposition/plot_faces_decomposition.py,35,,
scikit-learn/examples/decomposition/plot_faces_decomposition.py,36,Load faces data,
scikit-learn/examples/decomposition/plot_faces_decomposition.py,41,global centering,
scikit-learn/examples/decomposition/plot_faces_decomposition.py,44,local centering,
scikit-learn/examples/decomposition/plot_faces_decomposition.py,63,,
scikit-learn/examples/decomposition/plot_faces_decomposition.py,64,"List of the different estimators, whether to center and transpose the",
scikit-learn/examples/decomposition/plot_faces_decomposition.py,65,"problem, and whether the transformer uses the clustering API.",
scikit-learn/examples/decomposition/plot_faces_decomposition.py,103,,
scikit-learn/examples/decomposition/plot_faces_decomposition.py,104,Plot a sample of the input data,
scikit-learn/examples/decomposition/plot_faces_decomposition.py,108,,
scikit-learn/examples/decomposition/plot_faces_decomposition.py,109,Do the estimation and plot it,
scikit-learn/examples/decomposition/plot_faces_decomposition.py,125,Plot an image representing the pixelwise variance provided by the,
scikit-learn/examples/decomposition/plot_faces_decomposition.py,126,"estimator e.g its noise_variance_ attribute. The Eigenfaces estimator,",
scikit-learn/examples/decomposition/plot_faces_decomposition.py,127,"via the PCA decomposition, also provides a scalar noise_variance_",
scikit-learn/examples/decomposition/plot_faces_decomposition.py,128,(the mean of pixelwise variance) that cannot be displayed as an image,
scikit-learn/examples/decomposition/plot_faces_decomposition.py,129,so we skip it.,
scikit-learn/examples/decomposition/plot_faces_decomposition.py,131,Skip the Eigenfaces case,
scikit-learn/examples/decomposition/plot_faces_decomposition.py,140,,
scikit-learn/examples/decomposition/plot_faces_decomposition.py,141,Various positivity constraints applied to dictionary learning.,
scikit-learn/examples/decomposition/plot_faces_decomposition.py,172,,
scikit-learn/examples/decomposition/plot_faces_decomposition.py,173,Plot a sample of the input data,
scikit-learn/examples/decomposition/plot_faces_decomposition.py,178,,
scikit-learn/examples/decomposition/plot_faces_decomposition.py,179,Do the estimation and plot it,
scikit-learn/examples/decomposition/plot_sparse_coding.py,47,subsampling factor,
scikit-learn/examples/decomposition/plot_sparse_coding.py,51,Compute a wavelet dictionary,
scikit-learn/examples/decomposition/plot_sparse_coding.py,58,Generate a signal,
scikit-learn/examples/decomposition/plot_sparse_coding.py,64,List the different sparse coding methods in the following format:,
scikit-learn/examples/decomposition/plot_sparse_coding.py,65,"(title, transform_algorithm, transform_alpha,",
scikit-learn/examples/decomposition/plot_sparse_coding.py,66,"transform_n_nozero_coefs, color)",
scikit-learn/examples/decomposition/plot_sparse_coding.py,70,Avoid FutureWarning about default value change when numpy >= 1.14,
scikit-learn/examples/decomposition/plot_sparse_coding.py,79,Do a wavelet approximation,
scikit-learn/examples/decomposition/plot_sparse_coding.py,91,Soft thresholding debiasing,
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,24,,
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,25,Generate sample data,
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,30,Signal 1 : sinusoidal signal,
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,31,Signal 2 : square signal,
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,32,Signal 3: saw tooth signal,
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,35,Add noise,
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,37,Standardize data,
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,38,Mix data,
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,39,Mixing matrix,
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,40,Generate observations,
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,42,Compute ICA,
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,44,Reconstruct signals,
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,45,Get estimated mixing matrix,
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,47,We can `prove` that the ICA model applies by reverting the unmixing.,
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,50,"For comparison, compute PCA",
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,52,Reconstruct signals based on orthogonal components,
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,54,,
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,55,Plot results,
scikit-learn/examples/decomposition/plot_pca_iris.py,1,!/usr/bin/python,
scikit-learn/examples/decomposition/plot_pca_iris.py,2,-*- coding: utf-8 -*-,
scikit-learn/examples/decomposition/plot_pca_iris.py,18,Code source: Gaël Varoquaux,
scikit-learn/examples/decomposition/plot_pca_iris.py,19,License: BSD 3 clause,
scikit-learn/examples/decomposition/plot_pca_iris.py,51,Reorder the labels to have colors matching the cluster results,
scikit-learn/examples/decomposition/plot_pca_3d.py,1,!/usr/bin/python,
scikit-learn/examples/decomposition/plot_pca_3d.py,2,-*- coding: utf-8 -*-,
scikit-learn/examples/decomposition/plot_pca_3d.py,16,Authors: Gael Varoquaux,
scikit-learn/examples/decomposition/plot_pca_3d.py,17,Jaques Grobler,
scikit-learn/examples/decomposition/plot_pca_3d.py,18,Kevin Hughes,
scikit-learn/examples/decomposition/plot_pca_3d.py,19,License: BSD 3 clause,
scikit-learn/examples/decomposition/plot_pca_3d.py,29,,
scikit-learn/examples/decomposition/plot_pca_3d.py,30,Create the data,
scikit-learn/examples/decomposition/plot_pca_3d.py,58,,
scikit-learn/examples/decomposition/plot_pca_3d.py,59,Plot the figures,
scikit-learn/examples/decomposition/plot_pca_3d.py,68,"Using SciPy's SVD, this would be:",
scikit-learn/examples/decomposition/plot_pca_3d.py,69,"_, pca_score, V = scipy.linalg.svd(Y, full_matrices=False)",
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,22,,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,23,New plotting API,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,24,----------------,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,25,,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,26,A new plotting API is available for creating visualizations. This new API,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,27,allows for quickly adjusting the visuals of a plot without involving any,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,28,recomputation. It is also possible to add different plots to the same,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,29,"figure. The following example illustrates :class:`~metrics.plot_roc_curve`,",
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,30,but other plots utilities are supported like,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,31,":class:`~inspection.plot_partial_dependence`,",
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,32,":class:`~metrics.plot_precision_recall_curve`, and",
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,33,:class:`~metrics.plot_confusion_matrix`. Read more about this new API in the,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,34,:ref:`User Guide <visualizations>`.,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,57,,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,58,Stacking Classifier and Regressor,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,59,---------------------------------,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,60,:class:`~ensemble.StackingClassifier` and,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,61,:class:`~ensemble.StackingRegressor`,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,62,allow you to have a stack of estimators with a final classifier or,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,63,a regressor.,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,64,Stacked generalization consists in stacking the output of individual,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,65,estimators and use a classifier to compute the final prediction. Stacking,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,66,allows to use the strength of each individual estimator by using their output,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,67,as input of a final estimator.,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,68,Base estimators are fitted on the full ``X`` while,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,69,the final estimator is trained using cross-validated predictions of the,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,70,base estimators using ``cross_val_predict``.,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,71,,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,72,Read more in the :ref:`User Guide <stacking>`.,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,96,,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,97,Permutation-based feature importance,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,98,------------------------------------,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,99,,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,100,The :func:`inspection.permutation_importance` can be used to get an,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,101,"estimate of the importance of each feature, for any fitted estimator:",
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,120,,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,121,Native support for missing values for gradient boosting,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,122,-------------------------------------------------------,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,123,,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,124,The :class:`ensemble.HistGradientBoostingClassifier`,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,125,and :class:`ensemble.HistGradientBoostingRegressor` now have native,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,126,support for missing values (NaNs). This means that there is no need for,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,127,imputing data when training or predicting.,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,129,noqa,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,139,,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,140,Precomputed sparse nearest neighbors graph,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,141,------------------------------------------,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,142,Most estimators based on nearest neighbors graphs now accept precomputed,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,143,"sparse graphs as input, to reuse the same graph for multiple estimator fits.",
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,144,"To use this feature in a pipeline, one can use the `memory` parameter, along",
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,145,"with one of the two new transformers,",
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,146,:class:`neighbors.KNeighborsTransformer` and,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,147,:class:`neighbors.RadiusNeighborsTransformer`. The precomputation,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,148,can also be performed by custom estimators to use alternative,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,149,"implementations, such as approximate nearest neighbors methods.",
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,150,See more details in the :ref:`User Guide <neighbors_transformer>`.,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,166,We can decrease the number of neighbors and the graph will not be,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,167,recomputed.,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,171,,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,172,KNN Based Imputation,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,173,------------------------------------,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,174,We now support imputation for completing missing values using k-Nearest,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,175,Neighbors.,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,176,,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,177,Each sample's missing values are imputed using the mean value from,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,178,``n_neighbors`` nearest neighbors found in the training set. Two samples are,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,179,close if the features that neither is missing are close.,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,180,"By default, a euclidean distance metric",
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,181,"that supports missing values,",
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,182,":func:`~metrics.nan_euclidean_distances`, is used to find the nearest",
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,183,neighbors.,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,184,,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,185,Read more in the :ref:`User Guide <knnimpute>`.,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,194,,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,195,Tree pruning,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,196,------------,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,197,,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,198,It is now possible to prune most tree-based estimators once the trees are,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,199,built. The pruning is based on minimal cost-complexity. Read more in the,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,200,:ref:`User Guide <minimal_cost_complexity_pruning>` for details.,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,212,,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,213,Retrieve dataframes from OpenML,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,214,-------------------------------,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,215,:func:`datasets.fetch_openml` can now return pandas dataframe and thus,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,216,properly handle datasets with heterogeneous data:,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,223,,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,224,Checking scikit-learn compatibility of an estimator,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,225,---------------------------------------------------,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,226,Developers can check the compatibility of their scikit-learn compatible,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,227,estimators using :func:`~utils.estimator_checks.check_estimator`. For,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,228,"instance, the ``check_estimator(LinearSVC)`` passes.",
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,229,,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,230,We now provide a ``pytest`` specific decorator which allows ``pytest``,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,231,to run all checks independently and report the checks that are failing.,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,242,,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,243,ROC AUC now supports multiclass classification,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,244,----------------------------------------------,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,245,The :func:`roc_auc_score` function can also be used in multi-class,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,246,classification. Two averaging strategies are currently supported: the,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,247,"one-vs-one algorithm computes the average of the pairwise ROC AUC scores, and",
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,248,the one-vs-rest algorithm computes the average of the ROC AUC scores for each,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,249,"class against all other classes. In both cases, the multiclass ROC AUC scores",
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,250,are computed from the probability estimates that a sample belongs to a,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,251,particular class according to the model. The OvO and OvR algorithms support,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,252,weighting uniformly (``average='macro'``) and weighting by the prevalence,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,253,(``average='weighted'``).,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,254,,
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,255,Read more in the :ref:`User Guide <roc_metrics>`.,
scikit-learn/examples/text/plot_document_clustering.py,52,Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>,
scikit-learn/examples/text/plot_document_clustering.py,53,Lars Buitinck,
scikit-learn/examples/text/plot_document_clustering.py,54,License: BSD 3 clause,
scikit-learn/examples/text/plot_document_clustering.py,74,Display progress logs on stdout,
scikit-learn/examples/text/plot_document_clustering.py,78,parse commandline arguments,
scikit-learn/examples/text/plot_document_clustering.py,107,work-around for Jupyter notebook and IPython console,
scikit-learn/examples/text/plot_document_clustering.py,115,,
scikit-learn/examples/text/plot_document_clustering.py,116,Load some categories from the training set,
scikit-learn/examples/text/plot_document_clustering.py,123,Uncomment the following to do the analysis on all the categories,
scikit-learn/examples/text/plot_document_clustering.py,124,categories = None,
scikit-learn/examples/text/plot_document_clustering.py,144,Perform an IDF normalization on the output of HashingVectorizer,
scikit-learn/examples/text/plot_document_clustering.py,166,"Vectorizer results are normalized, which makes KMeans behave as",
scikit-learn/examples/text/plot_document_clustering.py,167,spherical k-means for better results. Since LSA/SVD results are,
scikit-learn/examples/text/plot_document_clustering.py,168,"not normalized, we have to redo the normalization.",
scikit-learn/examples/text/plot_document_clustering.py,184,,
scikit-learn/examples/text/plot_document_clustering.py,185,Do the actual clustering,
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,16,Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>,
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,17,Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,18,Mathieu Blondel <mathieu@mblondel.org>,
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,19,Lars Buitinck,
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,20,License: BSD 3 clause,
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,47,Display progress logs on stdout,
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,84,work-around for Jupyter notebook and IPython console,
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,96,,
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,97,Load data from the training set,
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,98,------------------------------------,
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,99,Let's load data from the newsgroups dataset which comprises around 18000,
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,100,newsgroups posts on 20 topics split in two subsets: one for training (or,
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,101,development) and the other one for testing (or for performance evaluation).,
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,129,order of labels in `target_names` can be different from `categories`,
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,147,split a training set and a test set,
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,173,mapping from integer feature name to original token string,
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,187,keep selected feature names,
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,202,,
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,203,Benchmark classifiers,
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,204,------------------------------------,
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,205,We train and test the datasets with 15 different classification models,
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,206,and get performance results for each model.,
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,264,Train Liblinear model,
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,268,Train SGD model,
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,272,Train SGD with Elastic Net penalty,
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,278,Train NearestCentroid without threshold,
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,283,Train sparse Naive Bayes classifiers,
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,292,"The smaller C, the stronger the regularization.",
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,293,"The more regularization, the more sparsity.",
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,300,,
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,301,Add plots,
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,302,------------------------------------,
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,303,"The bar plot indicates the accuracy, training time (normalized) and test time",
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,304,(normalized) of each classifier.,
scikit-learn/examples/text/plot_hashing_vs_dict_vectorizer.py,18,Author: Lars Buitinck,
scikit-learn/examples/text/plot_hashing_vs_dict_vectorizer.py,19,License: BSD 3 clause,
scikit-learn/examples/text/plot_hashing_vs_dict_vectorizer.py,62,Uncomment the following line to use a larger set (11k+ documents),
scikit-learn/examples/text/plot_hashing_vs_dict_vectorizer.py,63,categories = None,
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,27,,
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,28,Dataset based latent variables model,
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,31,2 latents vars:,
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,49,,
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,50,Canonical (symmetric) PLS,
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,52,Transform data,
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,53,~~~~~~~~~~~~~~,
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,59,Scatter plot of scores,
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,60,~~~~~~~~~~~~~~~~~~~~~~,
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,61,1) On diagonal plot X vs Y scores on each components,
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,89,2) Off diagonal plot components 1 vs 2 for X and Y,
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,117,,
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,118,"PLS regression, with multivariate response, a.k.a. PLS2",
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,125,each Yj = 1*X1 + 2*X2 + noize,
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,132,compare pls2.coef_ with B,
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,137,"PLS regression, with univariate response, a.k.a. PLS1",
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,145,note that the number of components exceeds 1 (the dimension of y),
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,149,,
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,150,CCA (PLS mode B with symmetric deflation),
scikit-learn/examples/impute/plot_iterative_imputer_variants_comparison.py,45,"To use this experimental feature, we need to explicitly ask for it:",
scikit-learn/examples/impute/plot_iterative_imputer_variants_comparison.py,46,noqa,
scikit-learn/examples/impute/plot_iterative_imputer_variants_comparison.py,62,~2k samples is enough for the purpose of the example.,
scikit-learn/examples/impute/plot_iterative_imputer_variants_comparison.py,63,Remove the following two lines for a slower run with different error bars.,
scikit-learn/examples/impute/plot_iterative_imputer_variants_comparison.py,68,"Estimate the score on the entire dataset, with no missing values",
scikit-learn/examples/impute/plot_iterative_imputer_variants_comparison.py,78,Add a single missing value to each row,
scikit-learn/examples/impute/plot_iterative_imputer_variants_comparison.py,85,Estimate the score after imputation (mean and median strategies),
scikit-learn/examples/impute/plot_iterative_imputer_variants_comparison.py,97,Estimate the score after iterative imputation of the missing values,
scikit-learn/examples/impute/plot_iterative_imputer_variants_comparison.py,98,with different estimators,
scikit-learn/examples/impute/plot_iterative_imputer_variants_comparison.py,122,plot california housing results,
scikit-learn/examples/impute/plot_missing_values.py,29,"To use the experimental IterativeImputer, we need to explicitly ask for it:",
scikit-learn/examples/impute/plot_missing_values.py,30,noqa,
scikit-learn/examples/impute/plot_missing_values.py,60,"Estimate the score on the entire dataset, with no missing values",
scikit-learn/examples/impute/plot_missing_values.py,65,Add missing values in 75% of the lines,
scikit-learn/examples/impute/plot_missing_values.py,78,Estimate the score after replacing missing values by 0,
scikit-learn/examples/impute/plot_missing_values.py,84,Estimate the score after imputation (mean strategy) of the missing values,
scikit-learn/examples/impute/plot_missing_values.py,88,Estimate the score after kNN-imputation of the missing values,
scikit-learn/examples/impute/plot_missing_values.py,92,Estimate the score after iterative imputation of the missing values,
scikit-learn/examples/impute/plot_missing_values.py,126,plot diabetes results,
scikit-learn/examples/impute/plot_missing_values.py,141,plot boston results,
scikit-learn/examples/compose/plot_column_transformer.py,26,Author: Matt Terry <matt.terry@gmail.com>,
scikit-learn/examples/compose/plot_column_transformer.py,27,,
scikit-learn/examples/compose/plot_column_transformer.py,28,License: BSD 3 clause,
scikit-learn/examples/compose/plot_column_transformer.py,65,construct object dtype array with two columns,
scikit-learn/examples/compose/plot_column_transformer.py,66,first column = 'subject' and second column = 'body',
scikit-learn/examples/compose/plot_column_transformer.py,84,Extract the subject & body,
scikit-learn/examples/compose/plot_column_transformer.py,87,Use ColumnTransformer to combine the features from subject and body,
scikit-learn/examples/compose/plot_column_transformer.py,90,Pulling features from the post's subject line (first column),
scikit-learn/examples/compose/plot_column_transformer.py,93,Pipeline for standard bag-of-words model for body (second column),
scikit-learn/examples/compose/plot_column_transformer.py,99,Pipeline for pulling ad hoc features from post's body,
scikit-learn/examples/compose/plot_column_transformer.py,101,returns a list of dicts,
scikit-learn/examples/compose/plot_column_transformer.py,102,list of dicts -> feature matrix,
scikit-learn/examples/compose/plot_column_transformer.py,106,weight components in ColumnTransformer,
scikit-learn/examples/compose/plot_column_transformer.py,114,Use a SVC classifier on the combined features,
scikit-learn/examples/compose/plot_column_transformer.py,118,limit the list of categories to make running this example faster.,
scikit-learn/examples/compose/plot_feature_union.py,18,Author: Andreas Mueller <amueller@ais.uni-bonn.de>,
scikit-learn/examples/compose/plot_feature_union.py,19,,
scikit-learn/examples/compose/plot_feature_union.py,20,License: BSD 3 clause,
scikit-learn/examples/compose/plot_feature_union.py,33,This dataset is way too high-dimensional. Better do PCA:,
scikit-learn/examples/compose/plot_feature_union.py,36,"Maybe some original features where good, too?",
scikit-learn/examples/compose/plot_feature_union.py,39,Build estimator from PCA and Univariate selection:,
scikit-learn/examples/compose/plot_feature_union.py,43,Use combined features to transform dataset:,
scikit-learn/examples/compose/plot_feature_union.py,49,"Do grid search over k, n_components and C:",
scikit-learn/examples/compose/plot_compare_reduction.py,1,!/usr/bin/env python,
scikit-learn/examples/compose/plot_compare_reduction.py,2,-*- coding: utf-8 -*-,
scikit-learn/examples/compose/plot_compare_reduction.py,30,"Authors: Robert McGibbon, Joel Nothman, Guillaume Lemaitre",
scikit-learn/examples/compose/plot_compare_reduction.py,45,the reduce_dim stage is populated by the param_grid,
scikit-learn/examples/compose/plot_compare_reduction.py,71,"scores are in the order of param_grid iteration, which is alphabetical",
scikit-learn/examples/compose/plot_compare_reduction.py,73,select score for best C,
scikit-learn/examples/compose/plot_compare_reduction.py,92,,
scikit-learn/examples/compose/plot_compare_reduction.py,93,Caching transformers within a ``Pipeline``,
scikit-learn/examples/compose/plot_compare_reduction.py,94,,
scikit-learn/examples/compose/plot_compare_reduction.py,95,It is sometimes worthwhile storing the state of a specific transformer,
scikit-learn/examples/compose/plot_compare_reduction.py,96,since it could be used again. Using a pipeline in ``GridSearchCV`` triggers,
scikit-learn/examples/compose/plot_compare_reduction.py,97,"such situations. Therefore, we use the argument ``memory`` to enable caching.",
scikit-learn/examples/compose/plot_compare_reduction.py,98,,
scikit-learn/examples/compose/plot_compare_reduction.py,99,.. warning::,
scikit-learn/examples/compose/plot_compare_reduction.py,100,"Note that this example is, however, only an illustration since for this",
scikit-learn/examples/compose/plot_compare_reduction.py,101,specific case fitting PCA is not necessarily slower than loading the,
scikit-learn/examples/compose/plot_compare_reduction.py,102,"cache. Hence, use the ``memory`` constructor parameter when the fitting",
scikit-learn/examples/compose/plot_compare_reduction.py,103,of a transformer is costly.,
scikit-learn/examples/compose/plot_compare_reduction.py,108,Create a temporary folder to store the transformers of the pipeline,
scikit-learn/examples/compose/plot_compare_reduction.py,115,"This time, a cached pipeline will be used within the grid search",
scikit-learn/examples/compose/plot_compare_reduction.py,118,Delete the temporary cache before exiting,
scikit-learn/examples/compose/plot_compare_reduction.py,122,,
scikit-learn/examples/compose/plot_compare_reduction.py,123,The ``PCA`` fitting is only computed at the evaluation of the first,
scikit-learn/examples/compose/plot_compare_reduction.py,124,configuration of the ``C`` parameter of the ``LinearSVC`` classifier. The,
scikit-learn/examples/compose/plot_compare_reduction.py,125,other configurations of ``C`` will trigger the loading of the cached ``PCA``,
scikit-learn/examples/compose/plot_compare_reduction.py,126,"estimator data, leading to save processing time. Therefore, the use of",
scikit-learn/examples/compose/plot_compare_reduction.py,127,caching the pipeline using ``memory`` is highly beneficial when fitting,
scikit-learn/examples/compose/plot_compare_reduction.py,128,a transformer is costly.,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,24,Author: Pedro Morales <part.morales@gmail.com>,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,25,,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,26,License: BSD 3 clause,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,40,Load data from https://www.openml.org/d/40945,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,43,Alternatively X and y can be obtained directly from the frame attribute:,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,44,"X = titanic.frame.drop('survived', axis=1)",
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,45,y = titanic.frame['survived'],
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,47,,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,48,Use ``ColumnTransformer`` by selecting column by names,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,49,,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,50,We will train our classifier with the following features:,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,51,,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,52,Numeric Features:,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,53,,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,54,* ``age``: float;,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,55,* ``fare``: float.,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,56,,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,57,Categorical Features:,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,58,,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,59,"* ``embarked``: categories encoded as strings ``{'C', 'S', 'Q'}``;",
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,60,"* ``sex``: categories encoded as strings ``{'female', 'male'}``;",
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,61,"* ``pclass``: ordinal integers ``{1, 2, 3}``.",
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,62,,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,63,We create the preprocessing pipelines for both numeric and categorical data.,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,80,Append classifier to preprocessing pipeline.,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,81,Now we have a full prediction pipeline.,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,90,,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,91,Use ``ColumnTransformer`` by selecting column by data types,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,92,,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,93,"When dealing with a cleaned dataset, the preprocessing can be automatic by",
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,94,using the data types of the column to decide whether to treat a column as a,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,95,numerical or categorical feature.,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,96,:func:`sklearn.compose.make_column_selector` gives this possibility.,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,97,"First, let's only select a subset of columns to simplify our",
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,98,example.,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,103,,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,104,"Then, we introspect the information regarding each column data type.",
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,108,,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,109,We can observe that the `embarked` and `sex` columns were tagged as,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,110,"`category` columns when loading the data with ``fetch_openml``. Therefore, we",
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,111,can use this information to dispatch the categorical columns to the,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,112,``categorical_transformer`` and the remaining columns to the,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,113,``numerical_transformer``.,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,115,,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,116,".. note:: In practice, you will have to handle yourself the column data type.",
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,117,"If you want some columns to be considered as `category`, you will have to",
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,118,"convert them into categorical columns. If you are using pandas, you can",
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,119,refer to their documentation regarding `Categorical data,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,120,<https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html>`_.,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,129,Reproduce the identical fit/score process,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,135,,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,136,Using the prediction pipeline in a grid search,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,137,,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,138,Grid search can also be performed on the different preprocessing steps,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,139,"defined in the ``ColumnTransformer`` object, together with the classifier's",
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,140,hyperparameters as part of the ``Pipeline``.,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,141,We will search for both the imputer strategy of the numeric preprocessing,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,142,and the regularization parameter of the logistic regression using,
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,143,:class:`sklearn.model_selection.GridSearchCV`.,
scikit-learn/examples/compose/plot_transformed_target.py,1,!/usr/bin/env python,
scikit-learn/examples/compose/plot_transformed_target.py,2,-*- coding: utf-8 -*-,
scikit-learn/examples/compose/plot_transformed_target.py,17,Author: Guillaume Lemaitre <guillaume.lemaitre@inria.fr>,
scikit-learn/examples/compose/plot_transformed_target.py,18,License: BSD 3 clause,
scikit-learn/examples/compose/plot_transformed_target.py,28,,
scikit-learn/examples/compose/plot_transformed_target.py,29,Synthetic example,
scikit-learn/examples/compose/plot_transformed_target.py,30,,
scikit-learn/examples/compose/plot_transformed_target.py,39,`normed` is being deprecated in favor of `density` in histograms,
scikit-learn/examples/compose/plot_transformed_target.py,45,,
scikit-learn/examples/compose/plot_transformed_target.py,46,A synthetic random regression problem is generated. The targets ``y`` are,
scikit-learn/examples/compose/plot_transformed_target.py,47,modified by: (i) translating all targets such that all entries are,
scikit-learn/examples/compose/plot_transformed_target.py,48,non-negative and (ii) applying an exponential function to obtain non-linear,
scikit-learn/examples/compose/plot_transformed_target.py,49,targets which cannot be fitted using a simple linear model.,
scikit-learn/examples/compose/plot_transformed_target.py,50,,
scikit-learn/examples/compose/plot_transformed_target.py,51,"Therefore, a logarithmic (`np.log1p`) and an exponential function",
scikit-learn/examples/compose/plot_transformed_target.py,52,(`np.expm1`) will be used to transform the targets before training a linear,
scikit-learn/examples/compose/plot_transformed_target.py,53,regression model and using it for prediction.,
scikit-learn/examples/compose/plot_transformed_target.py,59,,
scikit-learn/examples/compose/plot_transformed_target.py,60,The following illustrate the probability density functions of the target,
scikit-learn/examples/compose/plot_transformed_target.py,61,before and after applying the logarithmic functions.,
scikit-learn/examples/compose/plot_transformed_target.py,81,,
scikit-learn/examples/compose/plot_transformed_target.py,82,"At first, a linear model will be applied on the original targets. Due to the",
scikit-learn/examples/compose/plot_transformed_target.py,83,"non-linearity, the model trained will not be precise during the",
scikit-learn/examples/compose/plot_transformed_target.py,84,"prediction. Subsequently, a logarithmic function is used to linearize the",
scikit-learn/examples/compose/plot_transformed_target.py,85,"targets, allowing better prediction even with a similar linear model as",
scikit-learn/examples/compose/plot_transformed_target.py,86,reported by the median absolute error (MAE).,
scikit-learn/examples/compose/plot_transformed_target.py,123,,
scikit-learn/examples/compose/plot_transformed_target.py,124,Real-world data set,
scikit-learn/examples/compose/plot_transformed_target.py,125,,
scikit-learn/examples/compose/plot_transformed_target.py,127,,
scikit-learn/examples/compose/plot_transformed_target.py,128,"In a similar manner, the boston housing data set is used to show the impact",
scikit-learn/examples/compose/plot_transformed_target.py,129,"of transforming the targets before learning a model. In this example, the",
scikit-learn/examples/compose/plot_transformed_target.py,130,targets to be predicted corresponds to the weighted distances to the five,
scikit-learn/examples/compose/plot_transformed_target.py,131,Boston employment centers.,
scikit-learn/examples/compose/plot_transformed_target.py,145,,
scikit-learn/examples/compose/plot_transformed_target.py,146,A :class:`sklearn.preprocessing.QuantileTransformer` is used such that the,
scikit-learn/examples/compose/plot_transformed_target.py,147,targets follows a normal distribution before applying a,
scikit-learn/examples/compose/plot_transformed_target.py,148,:class:`sklearn.linear_model.RidgeCV` model.,
scikit-learn/examples/compose/plot_transformed_target.py,167,,
scikit-learn/examples/compose/plot_transformed_target.py,168,"The effect of the transformer is weaker than on the synthetic data. However,",
scikit-learn/examples/compose/plot_transformed_target.py,169,the transform induces a decrease of the MAE.,
scikit-learn/examples/compose/plot_digits_pipe.py,1,!/usr/bin/python,
scikit-learn/examples/compose/plot_digits_pipe.py,2,-*- coding: utf-8 -*-,
scikit-learn/examples/compose/plot_digits_pipe.py,18,Code source: Gaël Varoquaux,
scikit-learn/examples/compose/plot_digits_pipe.py,19,Modified for documentation by Jaques Grobler,
scikit-learn/examples/compose/plot_digits_pipe.py,20,License: BSD 3 clause,
scikit-learn/examples/compose/plot_digits_pipe.py,34,Define a pipeline to search for the best combination of PCA truncation,
scikit-learn/examples/compose/plot_digits_pipe.py,35,and classifier regularization.,
scikit-learn/examples/compose/plot_digits_pipe.py,37,set the tolerance to a large value to make the example faster,
scikit-learn/examples/compose/plot_digits_pipe.py,43,Parameters of pipelines can be set using ‘__’ separated parameter names:,
scikit-learn/examples/compose/plot_digits_pipe.py,53,Plot the PCA spectrum,
scikit-learn/examples/compose/plot_digits_pipe.py,65,"For each number of components, find the best classifier results",
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,29,"Authors: Yann N. Dauphin, Vlad Niculae, Gabriel Synnaeve",
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,30,License: BSD,
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,43,,
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,44,Setting up,
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,78,Load Data,
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,82,0-1 scaling,
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,87,Models we will use,
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,94,,
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,95,Training,
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,97,"Hyper-parameters. These were set by cross-validation,",
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,98,using a GridSearchCV. Here we are not performing cross-validation to,
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,99,save time.,
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,102,"More components tend to give better prediction performance, but larger",
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,103,fitting time,
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,107,Training RBM-Logistic Pipeline,
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,110,Training the Logistic regression classifier directly on the pixel,
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,115,,
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,116,Evaluation,
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,126,,
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,127,Plotting,
scikit-learn/examples/neural_networks/plot_mlp_alpha.py,21,Author: Issam H. Laradji,
scikit-learn/examples/neural_networks/plot_mlp_alpha.py,22,License: BSD 3 clause,
scikit-learn/examples/neural_networks/plot_mlp_alpha.py,33,step size in the mesh,
scikit-learn/examples/neural_networks/plot_mlp_alpha.py,60,iterate over datasets,
scikit-learn/examples/neural_networks/plot_mlp_alpha.py,62,"preprocess dataset, split into training and test part",
scikit-learn/examples/neural_networks/plot_mlp_alpha.py,71,just plot the dataset first,
scikit-learn/examples/neural_networks/plot_mlp_alpha.py,75,Plot the training points,
scikit-learn/examples/neural_networks/plot_mlp_alpha.py,77,and testing points,
scikit-learn/examples/neural_networks/plot_mlp_alpha.py,85,iterate over classifiers,
scikit-learn/examples/neural_networks/plot_mlp_alpha.py,91,"Plot the decision boundary. For that, we will assign a color to each",
scikit-learn/examples/neural_networks/plot_mlp_alpha.py,92,"point in the mesh [x_min, x_max]x[y_min, y_max].",
scikit-learn/examples/neural_networks/plot_mlp_alpha.py,98,Put the result into a color plot,
scikit-learn/examples/neural_networks/plot_mlp_alpha.py,102,Plot also the training points,
scikit-learn/examples/neural_networks/plot_mlp_alpha.py,105,and testing points,
scikit-learn/examples/neural_networks/plot_mnist_filters.py,29,Load data from https://www.openml.org/d/554,
scikit-learn/examples/neural_networks/plot_mnist_filters.py,33,"rescale the data, use the traditional train/test split",
scikit-learn/examples/neural_networks/plot_mnist_filters.py,46,use global min / max to ensure all weights are shown on the same scale,
scikit-learn/examples/neural_networks/plot_mlp_training_curves.py,27,different learning rate schedules and momentum parameters,
scikit-learn/examples/neural_networks/plot_mlp_training_curves.py,57,"for each dataset, plot learning for each learning strategy",
scikit-learn/examples/neural_networks/plot_mlp_training_curves.py,64,digits is larger but converges fairly quickly,
scikit-learn/examples/neural_networks/plot_mlp_training_curves.py,74,some parameter combinations will not converge as can be seen on the,
scikit-learn/examples/neural_networks/plot_mlp_training_curves.py,75,plots so they are ignored here,
scikit-learn/examples/neural_networks/plot_mlp_training_curves.py,89,load / generate some toy datasets,
scikit-learn/examples/gaussian_process/plot_compare_gpr_krr.py,49,Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,
scikit-learn/examples/gaussian_process/plot_compare_gpr_krr.py,50,License: BSD 3 clause,
scikit-learn/examples/gaussian_process/plot_compare_gpr_krr.py,66,Generate sample data,
scikit-learn/examples/gaussian_process/plot_compare_gpr_krr.py,69,add noise,
scikit-learn/examples/gaussian_process/plot_compare_gpr_krr.py,71,Fit KernelRidge with parameter selection based on 5-fold cross validation,
scikit-learn/examples/gaussian_process/plot_compare_gpr_krr.py,88,Predict using kernel ridge,
scikit-learn/examples/gaussian_process/plot_compare_gpr_krr.py,94,Predict using gaussian process regressor,
scikit-learn/examples/gaussian_process/plot_compare_gpr_krr.py,104,Plot results,
scikit-learn/examples/gaussian_process/plot_gpr_noisy.py,20,Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,
scikit-learn/examples/gaussian_process/plot_gpr_noisy.py,21,,
scikit-learn/examples/gaussian_process/plot_gpr_noisy.py,22,License: BSD 3 clause,
scikit-learn/examples/gaussian_process/plot_gpr_noisy.py,37,First run,
scikit-learn/examples/gaussian_process/plot_gpr_noisy.py,56,Second run,
scikit-learn/examples/gaussian_process/plot_gpr_noisy.py,75,Plot LML landscape,
scikit-learn/examples/gaussian_process/plot_gpr_on_structured_data.py,148,whether there are 'A's in the sequence,
scikit-learn/examples/gaussian_process/plot_gpr_co2.py,60,Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,
scikit-learn/examples/gaussian_process/plot_gpr_co2.py,61,,
scikit-learn/examples/gaussian_process/plot_gpr_co2.py,62,License: BSD 3 clause,
scikit-learn/examples/gaussian_process/plot_gpr_co2.py,93,aggregate monthly sum to produce average,
scikit-learn/examples/gaussian_process/plot_gpr_co2.py,104,Kernel with parameters given in GPML book,
scikit-learn/examples/gaussian_process/plot_gpr_co2.py,105,long term smooth rising trend,
scikit-learn/examples/gaussian_process/plot_gpr_co2.py,107,seasonal component,
scikit-learn/examples/gaussian_process/plot_gpr_co2.py,108,medium term irregularity,
scikit-learn/examples/gaussian_process/plot_gpr_co2.py,112,noise terms,
scikit-learn/examples/gaussian_process/plot_gpr_co2.py,123,Kernel with optimized parameters,
scikit-learn/examples/gaussian_process/plot_gpr_co2.py,124,long term smooth rising trend,
scikit-learn/examples/gaussian_process/plot_gpr_co2.py,127,seasonal component,
scikit-learn/examples/gaussian_process/plot_gpr_co2.py,128,medium term irregularities,
scikit-learn/examples/gaussian_process/plot_gpr_co2.py,132,noise terms,
scikit-learn/examples/gaussian_process/plot_gpr_co2.py,146,Illustration,
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,23,Author: Vincent Dubourg <vincent.dubourg@gmail.com>,
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,24,Jake Vanderplas <vanderplas@astro.washington.edu>,
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,25,Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>s,
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,26,License: BSD 3 clause,
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,41,----------------------------------------------------------------------,
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,42,First the noiseless case,
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,45,Observations,
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,48,"Mesh the input space for evaluations of the real function, the prediction and",
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,49,its MSE,
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,52,Instantiate a Gaussian Process model,
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,56,Fit to data using Maximum Likelihood Estimation of the parameters,
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,59,Make the prediction on the meshed x-axis (ask for MSE as well),
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,62,"Plot the function, the prediction and the 95% confidence interval based on",
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,63,the MSE,
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,77,----------------------------------------------------------------------,
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,78,now the noisy case,
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,82,Observations and noise,
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,88,Instantiate a Gaussian Process model,
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,92,Fit to data using Maximum Likelihood Estimation of the parameters,
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,95,Make the prediction on the meshed x-axis (ask for MSE as well),
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,98,"Plot the function, the prediction and the 95% confidence interval based on",
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,99,the MSE,
scikit-learn/examples/gaussian_process/plot_gpc_isoprobability.py,1,!/usr/bin/python,
scikit-learn/examples/gaussian_process/plot_gpc_isoprobability.py,2,-*- coding: utf-8 -*-,
scikit-learn/examples/gaussian_process/plot_gpc_isoprobability.py,14,Author: Vincent Dubourg <vincent.dubourg@gmail.com>,
scikit-learn/examples/gaussian_process/plot_gpc_isoprobability.py,15,Adapted to GaussianProcessClassifier:,
scikit-learn/examples/gaussian_process/plot_gpc_isoprobability.py,16,Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,
scikit-learn/examples/gaussian_process/plot_gpc_isoprobability.py,17,License: BSD 3 clause,
scikit-learn/examples/gaussian_process/plot_gpc_isoprobability.py,27,A few constants,
scikit-learn/examples/gaussian_process/plot_gpc_isoprobability.py,36,Design of experiments,
scikit-learn/examples/gaussian_process/plot_gpc_isoprobability.py,46,Observations,
scikit-learn/examples/gaussian_process/plot_gpc_isoprobability.py,49,Instantiate and fit Gaussian Process Model,
scikit-learn/examples/gaussian_process/plot_gpc_isoprobability.py,55,Evaluate real function and the predicted probability,
scikit-learn/examples/gaussian_process/plot_gpc_isoprobability.py,66,Plot the probabilistic classification iso-values,
scikit-learn/examples/gaussian_process/plot_gpc_iris.py,19,import some data to play with,
scikit-learn/examples/gaussian_process/plot_gpc_iris.py,21,we only take the first two features.,
scikit-learn/examples/gaussian_process/plot_gpc_iris.py,24,step size in the mesh,
scikit-learn/examples/gaussian_process/plot_gpc_iris.py,31,create a mesh to plot in,
scikit-learn/examples/gaussian_process/plot_gpc_iris.py,40,"Plot the predicted probabilities. For that, we will assign a color to",
scikit-learn/examples/gaussian_process/plot_gpc_iris.py,41,"each point in the mesh [x_min, m_max]x[y_min, y_max].",
scikit-learn/examples/gaussian_process/plot_gpc_iris.py,46,Put the result into a color plot,
scikit-learn/examples/gaussian_process/plot_gpc_iris.py,50,Plot also the training points,
scikit-learn/examples/gaussian_process/plot_gpc.py,25,Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,
scikit-learn/examples/gaussian_process/plot_gpc.py,26,,
scikit-learn/examples/gaussian_process/plot_gpc.py,27,License: BSD 3 clause,
scikit-learn/examples/gaussian_process/plot_gpc.py,38,Generate data,
scikit-learn/examples/gaussian_process/plot_gpc.py,44,Specify Gaussian Processes with fixed and optimized hyperparameters,
scikit-learn/examples/gaussian_process/plot_gpc.py,65,Plot posteriors,
scikit-learn/examples/gaussian_process/plot_gpc.py,82,Plot LML landscape,
scikit-learn/examples/gaussian_process/plot_gpc_xor.py,14,Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,
scikit-learn/examples/gaussian_process/plot_gpc_xor.py,15,,
scikit-learn/examples/gaussian_process/plot_gpc_xor.py,16,License: BSD 3 clause,
scikit-learn/examples/gaussian_process/plot_gpc_xor.py,31,fit the model,
scikit-learn/examples/gaussian_process/plot_gpc_xor.py,37,plot the decision function for each datapoint on the grid,
scikit-learn/examples/gaussian_process/plot_gpr_prior_posterior.py,12,Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,
scikit-learn/examples/gaussian_process/plot_gpr_prior_posterior.py,13,,
scikit-learn/examples/gaussian_process/plot_gpr_prior_posterior.py,14,License: BSD 3 clause,
scikit-learn/examples/gaussian_process/plot_gpr_prior_posterior.py,37,Specify Gaussian Process,
scikit-learn/examples/gaussian_process/plot_gpr_prior_posterior.py,40,Plot prior,
scikit-learn/examples/gaussian_process/plot_gpr_prior_posterior.py,54,Generate data and fit GP,
scikit-learn/examples/gaussian_process/plot_gpr_prior_posterior.py,60,Plot posterior,
scikit-learn/examples/covariance/plot_mahalanobis_distances.py,66,generate data,
scikit-learn/examples/covariance/plot_mahalanobis_distances.py,70,add some outliers,
scikit-learn/examples/covariance/plot_mahalanobis_distances.py,75,fit a Minimum Covariance Determinant (MCD) robust estimator to data,
scikit-learn/examples/covariance/plot_mahalanobis_distances.py,78,compare estimators learnt from the full data set with true parameters,
scikit-learn/examples/covariance/plot_mahalanobis_distances.py,81,,
scikit-learn/examples/covariance/plot_mahalanobis_distances.py,82,Display results,
scikit-learn/examples/covariance/plot_mahalanobis_distances.py,86,Show data set,
scikit-learn/examples/covariance/plot_mahalanobis_distances.py,95,Show contours of the distance functions,
scikit-learn/examples/covariance/plot_mahalanobis_distances.py,118,Plot the scores for each point,
scikit-learn/examples/covariance/plot_sparse_cov.py,52,author: Gael Varoquaux <gael.varoquaux@inria.fr>,
scikit-learn/examples/covariance/plot_sparse_cov.py,53,License: BSD 3 clause,
scikit-learn/examples/covariance/plot_sparse_cov.py,54,Copyright: INRIA,
scikit-learn/examples/covariance/plot_sparse_cov.py,62,,
scikit-learn/examples/covariance/plot_sparse_cov.py,63,Generate the data,
scikit-learn/examples/covariance/plot_sparse_cov.py,82,,
scikit-learn/examples/covariance/plot_sparse_cov.py,83,Estimate the covariance,
scikit-learn/examples/covariance/plot_sparse_cov.py,94,,
scikit-learn/examples/covariance/plot_sparse_cov.py,95,Plot the results,
scikit-learn/examples/covariance/plot_sparse_cov.py,99,plot the covariances,
scikit-learn/examples/covariance/plot_sparse_cov.py,112,plot the precisions,
scikit-learn/examples/covariance/plot_sparse_cov.py,129,plot the model selection metric,
scikit-learn/examples/covariance/plot_robust_vs_empirical_covariance.py,63,example settings,
scikit-learn/examples/covariance/plot_robust_vs_empirical_covariance.py,72,definition of arrays to store results,
scikit-learn/examples/covariance/plot_robust_vs_empirical_covariance.py,80,computation,
scikit-learn/examples/covariance/plot_robust_vs_empirical_covariance.py,86,generate data,
scikit-learn/examples/covariance/plot_robust_vs_empirical_covariance.py,88,add some outliers,
scikit-learn/examples/covariance/plot_robust_vs_empirical_covariance.py,96,fit a Minimum Covariance Determinant (MCD) robust estimator to data,
scikit-learn/examples/covariance/plot_robust_vs_empirical_covariance.py,98,compare raw robust estimates with the true location and covariance,
scikit-learn/examples/covariance/plot_robust_vs_empirical_covariance.py,102,compare estimators learned from the full data set with true,
scikit-learn/examples/covariance/plot_robust_vs_empirical_covariance.py,103,parameters,
scikit-learn/examples/covariance/plot_robust_vs_empirical_covariance.py,108,compare with an empirical covariance learned from a pure data set,
scikit-learn/examples/covariance/plot_robust_vs_empirical_covariance.py,109,"(i.e. ""perfect"" mcd)",
scikit-learn/examples/covariance/plot_robust_vs_empirical_covariance.py,116,Display results,
scikit-learn/examples/covariance/plot_covariance_estimation.py,55,,
scikit-learn/examples/covariance/plot_covariance_estimation.py,56,Generate sample data,
scikit-learn/examples/covariance/plot_covariance_estimation.py,62,Color samples,
scikit-learn/examples/covariance/plot_covariance_estimation.py,67,,
scikit-learn/examples/covariance/plot_covariance_estimation.py,68,Compute the likelihood on test data,
scikit-learn/examples/covariance/plot_covariance_estimation.py,70,spanning a range of possible shrinkage coefficient values,
scikit-learn/examples/covariance/plot_covariance_estimation.py,75,"under the ground-truth model, which we would not have access to in real",
scikit-learn/examples/covariance/plot_covariance_estimation.py,76,settings,
scikit-learn/examples/covariance/plot_covariance_estimation.py,81,,
scikit-learn/examples/covariance/plot_covariance_estimation.py,82,Compare different approaches to setting the parameter,
scikit-learn/examples/covariance/plot_covariance_estimation.py,84,GridSearch for an optimal shrinkage coefficient,
scikit-learn/examples/covariance/plot_covariance_estimation.py,89,Ledoit-Wolf optimal shrinkage coefficient estimate,
scikit-learn/examples/covariance/plot_covariance_estimation.py,93,OAS coefficient estimate,
scikit-learn/examples/covariance/plot_covariance_estimation.py,97,,
scikit-learn/examples/covariance/plot_covariance_estimation.py,98,Plot results,
scikit-learn/examples/covariance/plot_covariance_estimation.py,103,range shrinkage curve,
scikit-learn/examples/covariance/plot_covariance_estimation.py,109,adjust view,
scikit-learn/examples/covariance/plot_covariance_estimation.py,116,LW likelihood,
scikit-learn/examples/covariance/plot_covariance_estimation.py,119,OAS likelihood,
scikit-learn/examples/covariance/plot_covariance_estimation.py,122,best CV estimator likelihood,
scikit-learn/examples/covariance/plot_lw_vs_oas.py,32,,
scikit-learn/examples/covariance/plot_lw_vs_oas.py,34,simulation covariance matrix (AR(1) process),
scikit-learn/examples/covariance/plot_lw_vs_oas.py,60,plot MSE,
scikit-learn/examples/covariance/plot_lw_vs_oas.py,71,plot shrinkage coefficient,
scikit-learn/examples/calibration/plot_calibration_multiclass.py,26,Author: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,
scikit-learn/examples/calibration/plot_calibration_multiclass.py,27,License: BSD Style.,
scikit-learn/examples/calibration/plot_calibration_multiclass.py,41,Generate data,
scikit-learn/examples/calibration/plot_calibration_multiclass.py,48,Train uncalibrated random forest classifier on whole train and validation,
scikit-learn/examples/calibration/plot_calibration_multiclass.py,49,data and evaluate on test data,
scikit-learn/examples/calibration/plot_calibration_multiclass.py,55,"Train random forest classifier, calibrate on validation data and evaluate",
scikit-learn/examples/calibration/plot_calibration_multiclass.py,56,on test data,
scikit-learn/examples/calibration/plot_calibration_multiclass.py,65,Plot changes in predicted probabilities via arrows,
scikit-learn/examples/calibration/plot_calibration_multiclass.py,74,Plot perfect predictions,
scikit-learn/examples/calibration/plot_calibration_multiclass.py,79,Plot boundaries of unit simplex,
scikit-learn/examples/calibration/plot_calibration_multiclass.py,82,Annotate points on the simplex,
scikit-learn/examples/calibration/plot_calibration_multiclass.py,112,Add grid,
scikit-learn/examples/calibration/plot_calibration_multiclass.py,132,Illustrate calibrator,
scikit-learn/examples/calibration/plot_calibration_multiclass.py,134,generate grid over 2-simplex,
scikit-learn/examples/calibration/plot_calibration_multiclass.py,147,Plot modifications of calibrator,
scikit-learn/examples/calibration/plot_calibration_multiclass.py,152,Plot boundaries of unit simplex,
scikit-learn/examples/calibration/plot_calibration.py,26,Author: Mathieu Blondel <mathieu@mblondel.org>,
scikit-learn/examples/calibration/plot_calibration.py,27,Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>,
scikit-learn/examples/calibration/plot_calibration.py,28,Balazs Kegl <balazs.kegl@gmail.com>,
scikit-learn/examples/calibration/plot_calibration.py,29,Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,
scikit-learn/examples/calibration/plot_calibration.py,30,License: BSD Style.,
scikit-learn/examples/calibration/plot_calibration.py,44,use 3 bins for calibration_curve as we have 3 clusters here,
scikit-learn/examples/calibration/plot_calibration.py,46,Generate 3 blobs with 2 classes where the second blob contains,
scikit-learn/examples/calibration/plot_calibration.py,47,half positive samples and half negative samples. Probability in this,
scikit-learn/examples/calibration/plot_calibration.py,48,blob is therefore 0.5.,
scikit-learn/examples/calibration/plot_calibration.py,57,"split train, test for calibration",
scikit-learn/examples/calibration/plot_calibration.py,61,Gaussian Naive-Bayes with no calibration,
scikit-learn/examples/calibration/plot_calibration.py,63,GaussianNB itself does not support sample-weights,
scikit-learn/examples/calibration/plot_calibration.py,66,Gaussian Naive-Bayes with isotonic calibration,
scikit-learn/examples/calibration/plot_calibration.py,71,Gaussian Naive-Bayes with sigmoid calibration,
scikit-learn/examples/calibration/plot_calibration.py,87,,
scikit-learn/examples/calibration/plot_calibration.py,88,Plot the data and the predicted probabilities,
scikit-learn/examples/calibration/plot_calibration_curve.py,46,Author: Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>,
scikit-learn/examples/calibration/plot_calibration_curve.py,47,Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,
scikit-learn/examples/calibration/plot_calibration_curve.py,48,License: BSD Style.,
scikit-learn/examples/calibration/plot_calibration_curve.py,62,Create dataset of classification task with many redundant and few,
scikit-learn/examples/calibration/plot_calibration_curve.py,63,informative features,
scikit-learn/examples/calibration/plot_calibration_curve.py,74,Calibrated with isotonic calibration,
scikit-learn/examples/calibration/plot_calibration_curve.py,77,Calibrated with sigmoid calibration,
scikit-learn/examples/calibration/plot_calibration_curve.py,80,Logistic regression with no calibration as baseline,
scikit-learn/examples/calibration/plot_calibration_curve.py,96,use decision function,
scikit-learn/examples/calibration/plot_calibration_curve.py,128,Plot calibration curve for Gaussian Naive Bayes,
scikit-learn/examples/calibration/plot_calibration_curve.py,131,Plot calibration curve for Linear SVC,
scikit-learn/examples/calibration/plot_compare_calibration.py,52,Author: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,
scikit-learn/examples/calibration/plot_compare_calibration.py,53,License: BSD Style.,
scikit-learn/examples/calibration/plot_compare_calibration.py,70,Samples used for training the models,
scikit-learn/examples/calibration/plot_compare_calibration.py,77,Create classifiers,
scikit-learn/examples/calibration/plot_compare_calibration.py,84,,
scikit-learn/examples/calibration/plot_compare_calibration.py,85,Plot calibration plots,
scikit-learn/examples/calibration/plot_compare_calibration.py,99,use decision function,
scikit-learn/examples/bicluster/plot_spectral_biclustering.py,20,Author: Kemal Eren <kemal@kemaleren.com>,
scikit-learn/examples/bicluster/plot_spectral_biclustering.py,21,License: BSD 3 clause,
scikit-learn/examples/bicluster/plot_spectral_biclustering.py,39,shuffle clusters,
scikit-learn/examples/bicluster/plot_bicluster_newsgroups.py,56,exclude 'comp.os.ms-windows.misc',
scikit-learn/examples/bicluster/plot_bicluster_newsgroups.py,103,"Note: the following is identical to X[rows[:, np.newaxis],",
scikit-learn/examples/bicluster/plot_bicluster_newsgroups.py,104,cols].sum() but much faster in scipy <= 0.16,
scikit-learn/examples/bicluster/plot_bicluster_newsgroups.py,132,categories,
scikit-learn/examples/bicluster/plot_bicluster_newsgroups.py,139,words,
scikit-learn/examples/bicluster/plot_spectral_coclustering.py,19,Author: Kemal Eren <kemal@kemaleren.com>,
scikit-learn/examples/bicluster/plot_spectral_coclustering.py,20,License: BSD 3 clause,
scikit-learn/examples/bicluster/plot_spectral_coclustering.py,36,shuffle clusters,
scikit-learn/examples/applications/plot_topics_extraction_with_nmf_lda.py,24,Author: Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/examples/applications/plot_topics_extraction_with_nmf_lda.py,25,Lars Buitinck,
scikit-learn/examples/applications/plot_topics_extraction_with_nmf_lda.py,26,Chyi-Kwei Yau <chyikwei.yau@gmail.com>,
scikit-learn/examples/applications/plot_topics_extraction_with_nmf_lda.py,27,License: BSD 3 clause,
scikit-learn/examples/applications/plot_topics_extraction_with_nmf_lda.py,50,Load the 20 newsgroups dataset and vectorize it. We use a few heuristics,
scikit-learn/examples/applications/plot_topics_extraction_with_nmf_lda.py,51,"to filter out useless terms early on: the posts are stripped of headers,",
scikit-learn/examples/applications/plot_topics_extraction_with_nmf_lda.py,52,"footers and quoted replies, and common English words, words occurring in",
scikit-learn/examples/applications/plot_topics_extraction_with_nmf_lda.py,53,only one document or in at least 95% of the documents are removed.,
scikit-learn/examples/applications/plot_topics_extraction_with_nmf_lda.py,63,Use tf-idf features for NMF.,
scikit-learn/examples/applications/plot_topics_extraction_with_nmf_lda.py,72,Use tf (raw term count) features for LDA.,
scikit-learn/examples/applications/plot_topics_extraction_with_nmf_lda.py,82,Fit the NMF model,
scikit-learn/examples/applications/plot_topics_extraction_with_nmf_lda.py,95,Fit the NMF model,
scikit-learn/examples/applications/plot_stock_market.py,63,Author: Gael Varoquaux gael.varoquaux@normalesup.org,
scikit-learn/examples/applications/plot_stock_market.py,64,License: BSD 3 clause,
scikit-learn/examples/applications/plot_stock_market.py,79,,
scikit-learn/examples/applications/plot_stock_market.py,80,Retrieve the data from Internet,
scikit-learn/examples/applications/plot_stock_market.py,82,The data is from 2003 - 2008. This is reasonably calm: (not too long ago so,
scikit-learn/examples/applications/plot_stock_market.py,83,"that we get high-tech firms, and before the 2008 crash). This kind of",
scikit-learn/examples/applications/plot_stock_market.py,84,historical data can be obtained for from APIs like the quandl.com and,
scikit-learn/examples/applications/plot_stock_market.py,85,alphavantage.co ones.,
scikit-learn/examples/applications/plot_stock_market.py,159,The daily variations of the quotes are what carry most information,
scikit-learn/examples/applications/plot_stock_market.py,163,,
scikit-learn/examples/applications/plot_stock_market.py,164,Learn a graphical structure from the correlations,
scikit-learn/examples/applications/plot_stock_market.py,167,standardize the time series: using correlations rather than covariance,
scikit-learn/examples/applications/plot_stock_market.py,168,is more efficient for structure recovery,
scikit-learn/examples/applications/plot_stock_market.py,173,,
scikit-learn/examples/applications/plot_stock_market.py,174,Cluster using affinity propagation,
scikit-learn/examples/applications/plot_stock_market.py,182,,
scikit-learn/examples/applications/plot_stock_market.py,183,Find a low-dimension embedding for visualization: find the best position of,
scikit-learn/examples/applications/plot_stock_market.py,184,the nodes (the stocks) on a 2D plane,
scikit-learn/examples/applications/plot_stock_market.py,186,We use a dense eigen_solver to achieve reproducibility (arpack is,
scikit-learn/examples/applications/plot_stock_market.py,187,"initiated with random vectors that we don't control). In addition, we",
scikit-learn/examples/applications/plot_stock_market.py,188,use a large number of neighbors to capture the large-scale structure.,
scikit-learn/examples/applications/plot_stock_market.py,194,,
scikit-learn/examples/applications/plot_stock_market.py,195,Visualization,
scikit-learn/examples/applications/plot_stock_market.py,201,Display a graph of the partial correlations,
scikit-learn/examples/applications/plot_stock_market.py,208,Plot the nodes using the coordinates of our embedding,
scikit-learn/examples/applications/plot_stock_market.py,212,Plot the edges,
scikit-learn/examples/applications/plot_stock_market.py,214,"a sequence of (*line0*, *line1*, *line2*), where::",
scikit-learn/examples/applications/plot_stock_market.py,215,"linen = (x0, y0), (x1, y1), ... (xm, ym)",
scikit-learn/examples/applications/plot_stock_market.py,226,Add a label to each node. The challenge here is that we want to,
scikit-learn/examples/applications/plot_stock_market.py,227,position the labels to avoid overlap with other labels,
scikit-learn/examples/applications/plot_species_distribution_modeling.py,38,Authors: Peter Prettenhofer <peter.prettenhofer@gmail.com>,
scikit-learn/examples/applications/plot_species_distribution_modeling.py,39,Jake Vanderplas <vanderplas@astro.washington.edu>,
scikit-learn/examples/applications/plot_species_distribution_modeling.py,40,,
scikit-learn/examples/applications/plot_species_distribution_modeling.py,41,License: BSD 3 clause,
scikit-learn/examples/applications/plot_species_distribution_modeling.py,52,"if basemap is available, we'll use it.",
scikit-learn/examples/applications/plot_species_distribution_modeling.py,53,"otherwise, we'll improvise later...",
scikit-learn/examples/applications/plot_species_distribution_modeling.py,76,"x,y coordinates for corner cells",
scikit-learn/examples/applications/plot_species_distribution_modeling.py,82,x coordinates of the grid cells,
scikit-learn/examples/applications/plot_species_distribution_modeling.py,84,y coordinates of the grid cells,
scikit-learn/examples/applications/plot_species_distribution_modeling.py,101,choose points associated with the desired species,
scikit-learn/examples/applications/plot_species_distribution_modeling.py,105,determine coverage values for each of the training & testing points,
scikit-learn/examples/applications/plot_species_distribution_modeling.py,124,Load the compressed data,
scikit-learn/examples/applications/plot_species_distribution_modeling.py,127,Set up the data grid,
scikit-learn/examples/applications/plot_species_distribution_modeling.py,130,"The grid in x,y coordinates",
scikit-learn/examples/applications/plot_species_distribution_modeling.py,133,create a bunch for each species,
scikit-learn/examples/applications/plot_species_distribution_modeling.py,141,background points (grid coordinates) for evaluation,
scikit-learn/examples/applications/plot_species_distribution_modeling.py,148,We'll make use of the fact that coverages[6] has measurements at all,
scikit-learn/examples/applications/plot_species_distribution_modeling.py,149,land points.  This will help us decide between land and water.,
scikit-learn/examples/applications/plot_species_distribution_modeling.py,152,"Fit, predict, and plot for each species.",
scikit-learn/examples/applications/plot_species_distribution_modeling.py,157,Standardize features,
scikit-learn/examples/applications/plot_species_distribution_modeling.py,162,Fit OneClassSVM,
scikit-learn/examples/applications/plot_species_distribution_modeling.py,168,Plot map of South America,
scikit-learn/examples/applications/plot_species_distribution_modeling.py,187,Predict species distribution using the training data,
scikit-learn/examples/applications/plot_species_distribution_modeling.py,190,We'll predict only for the land points.,
scikit-learn/examples/applications/plot_species_distribution_modeling.py,201,plot contours of the prediction,
scikit-learn/examples/applications/plot_species_distribution_modeling.py,205,scatter training/testing points,
scikit-learn/examples/applications/plot_species_distribution_modeling.py,216,Compute AUC with regards to background points,
scikit-learn/examples/applications/wikipedia_principal_eigenvector.py,32,Author: Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/examples/applications/wikipedia_principal_eigenvector.py,33,License: BSD 3 clause,
scikit-learn/examples/applications/wikipedia_principal_eigenvector.py,51,,
scikit-learn/examples/applications/wikipedia_principal_eigenvector.py,52,"Where to download the data, if not already on disk",
scikit-learn/examples/applications/wikipedia_principal_eigenvector.py,72,,
scikit-learn/examples/applications/wikipedia_principal_eigenvector.py,73,Loading the redirect files,
scikit-learn/examples/applications/wikipedia_principal_eigenvector.py,104,compute the transitive closure,
scikit-learn/examples/applications/wikipedia_principal_eigenvector.py,164,stop after 5M links to make it possible to work in RAM,
scikit-learn/examples/applications/wikipedia_principal_eigenvector.py,174,print the names of the wikipedia related strongest components of the,
scikit-learn/examples/applications/wikipedia_principal_eigenvector.py,175,principal singular vector which should be similar to the highest eigenvector,
scikit-learn/examples/applications/wikipedia_principal_eigenvector.py,202,initial guess,
scikit-learn/examples/applications/wikipedia_principal_eigenvector.py,208,check convergence: normalized l_inf norm,
scikit-learn/examples/applications/plot_outlier_detection_wine.py,38,Author: Virgile Fritsch <virgile.fritsch@inria.fr>,
scikit-learn/examples/applications/plot_outlier_detection_wine.py,39,License: BSD 3 clause,
scikit-learn/examples/applications/plot_outlier_detection_wine.py,48,"Define ""classifiers"" to be used",
scikit-learn/examples/applications/plot_outlier_detection_wine.py,59,Get data,
scikit-learn/examples/applications/plot_outlier_detection_wine.py,60,two clusters,
scikit-learn/examples/applications/plot_outlier_detection_wine.py,62,Learn a frontier for outlier detection with several classifiers,
scikit-learn/examples/applications/plot_outlier_detection_wine.py,75,Plot the results (= shape of the data points cloud),
scikit-learn/examples/applications/plot_outlier_detection_wine.py,76,two clusters,
scikit-learn/examples/applications/plot_outlier_detection_wine.py,97,,
scikit-learn/examples/applications/plot_outlier_detection_wine.py,98,Second example,
scikit-learn/examples/applications/plot_outlier_detection_wine.py,99,--------------,
scikit-learn/examples/applications/plot_outlier_detection_wine.py,100,The second example shows the ability of the Minimum Covariance Determinant,
scikit-learn/examples/applications/plot_outlier_detection_wine.py,101,robust estimator of covariance to concentrate on the main mode of the data,
scikit-learn/examples/applications/plot_outlier_detection_wine.py,102,"distribution: the location seems to be well estimated, although the",
scikit-learn/examples/applications/plot_outlier_detection_wine.py,103,"covariance is hard to estimate due to the banana-shaped distribution. Anyway,",
scikit-learn/examples/applications/plot_outlier_detection_wine.py,104,we can get rid of some outlying observations. The One-Class SVM is able to,
scikit-learn/examples/applications/plot_outlier_detection_wine.py,105,"capture the real data structure, but the difficulty is to adjust its kernel",
scikit-learn/examples/applications/plot_outlier_detection_wine.py,106,bandwidth parameter so as to obtain a good compromise between the shape of,
scikit-learn/examples/applications/plot_outlier_detection_wine.py,107,the data scatter matrix and the risk of over-fitting the data.,
scikit-learn/examples/applications/plot_outlier_detection_wine.py,109,Get data,
scikit-learn/examples/applications/plot_outlier_detection_wine.py,110,"""banana""-shaped",
scikit-learn/examples/applications/plot_outlier_detection_wine.py,112,Learn a frontier for outlier detection with several classifiers,
scikit-learn/examples/applications/plot_outlier_detection_wine.py,125,Plot the results (= shape of the data points cloud),
scikit-learn/examples/applications/plot_outlier_detection_wine.py,126,"""banana"" shape",
scikit-learn/examples/applications/plot_prediction_latency.py,16,Authors: Eustache Diemert <eustache@diemert.fr>,
scikit-learn/examples/applications/plot_prediction_latency.py,17,License: BSD 3 clause,
scikit-learn/examples/applications/plot_prediction_latency.py,37,Hack to detect whether we are running by the sphinx builder,
scikit-learn/examples/applications/plot_prediction_latency.py,267,,
scikit-learn/examples/applications/plot_prediction_latency.py,268,Main code,
scikit-learn/examples/applications/plot_prediction_latency.py,272,,
scikit-learn/examples/applications/plot_prediction_latency.py,273,Benchmark bulk/atomic prediction speed for various regressors,
scikit-learn/examples/applications/plot_prediction_latency.py,296,benchmark n_features influence on prediction speed,
scikit-learn/examples/applications/plot_prediction_latency.py,304,benchmark throughput,
scikit-learn/examples/applications/plot_tomography_l1_reconstruction.py,40,Author: Emmanuelle Gouillart <emmanuelle.gouillart@nsup.org>,
scikit-learn/examples/applications/plot_tomography_l1_reconstruction.py,41,License: BSD 3 clause,
scikit-learn/examples/applications/plot_tomography_l1_reconstruction.py,113,"Generate synthetic images, and projections",
scikit-learn/examples/applications/plot_tomography_l1_reconstruction.py,120,Reconstruction with L2 (Ridge) penalization,
scikit-learn/examples/applications/plot_tomography_l1_reconstruction.py,125,Reconstruction with L1 (Lasso) penalization,
scikit-learn/examples/applications/plot_tomography_l1_reconstruction.py,126,the best value of alpha was determined using cross validation,
scikit-learn/examples/applications/plot_tomography_l1_reconstruction.py,127,with LassoCV,
scikit-learn/examples/applications/plot_model_complexity_influence.py,19,Author: Eustache Diemert <eustache@diemert.fr>,
scikit-learn/examples/applications/plot_model_complexity_influence.py,20,License: BSD 3 clause,
scikit-learn/examples/applications/plot_model_complexity_influence.py,37,,
scikit-learn/examples/applications/plot_model_complexity_influence.py,38,Routines,
scikit-learn/examples/applications/plot_model_complexity_influence.py,41,Initialize random generator,
scikit-learn/examples/applications/plot_model_complexity_influence.py,125,,
scikit-learn/examples/applications/plot_model_complexity_influence.py,126,Main code,
scikit-learn/examples/applications/plot_out_of_core_classification.py,16,Authors: Eustache Diemert <eustache@diemert.fr>,
scikit-learn/examples/applications/plot_out_of_core_classification.py,17,@FedericoV <https://github.com/FedericoV/>,
scikit-learn/examples/applications/plot_out_of_core_classification.py,18,License: BSD 3 clause,
scikit-learn/examples/applications/plot_out_of_core_classification.py,43,Hack to detect whether we are running by the sphinx builder,
scikit-learn/examples/applications/plot_out_of_core_classification.py,46,,
scikit-learn/examples/applications/plot_out_of_core_classification.py,47,Reuters Dataset related routines,
scikit-learn/examples/applications/plot_out_of_core_classification.py,48,--------------------------------,
scikit-learn/examples/applications/plot_out_of_core_classification.py,49,,
scikit-learn/examples/applications/plot_out_of_core_classification.py,50,The dataset used in this example is Reuters-21578 as provided by the UCI ML,
scikit-learn/examples/applications/plot_out_of_core_classification.py,51,repository. It will be automatically downloaded and uncompressed on first,
scikit-learn/examples/applications/plot_out_of_core_classification.py,52,run.,
scikit-learn/examples/applications/plot_out_of_core_classification.py,181,,
scikit-learn/examples/applications/plot_out_of_core_classification.py,182,Main,
scikit-learn/examples/applications/plot_out_of_core_classification.py,183,----,
scikit-learn/examples/applications/plot_out_of_core_classification.py,184,,
scikit-learn/examples/applications/plot_out_of_core_classification.py,185,Create the vectorizer and limit the number of features to a reasonable,
scikit-learn/examples/applications/plot_out_of_core_classification.py,186,maximum,
scikit-learn/examples/applications/plot_out_of_core_classification.py,192,Iterator over parsed Reuters SGML files.,
scikit-learn/examples/applications/plot_out_of_core_classification.py,195,"We learn a binary classification between the ""acq"" class and all the others.",
scikit-learn/examples/applications/plot_out_of_core_classification.py,196,"""acq"" was chosen as it is more or less evenly distributed in the Reuters",
scikit-learn/examples/applications/plot_out_of_core_classification.py,197,"files. For other datasets, one should take care of creating a test set with",
scikit-learn/examples/applications/plot_out_of_core_classification.py,198,a realistic portion of positive instances.,
scikit-learn/examples/applications/plot_out_of_core_classification.py,202,Here are some classifiers that support the `partial_fit` method,
scikit-learn/examples/applications/plot_out_of_core_classification.py,234,test data statistics,
scikit-learn/examples/applications/plot_out_of_core_classification.py,237,First we hold out a number of examples to estimate accuracy,
scikit-learn/examples/applications/plot_out_of_core_classification.py,270,Discard test set,
scikit-learn/examples/applications/plot_out_of_core_classification.py,272,We will feed the classifier with mini-batches of 1000 documents; this means,
scikit-learn/examples/applications/plot_out_of_core_classification.py,273,we have at most 1000 docs in memory at any time.  The smaller the document,
scikit-learn/examples/applications/plot_out_of_core_classification.py,274,"batch, the bigger the relative overhead of the partial fit methods.",
scikit-learn/examples/applications/plot_out_of_core_classification.py,277,Create the data_stream that parses Reuters SGML files and iterates on,
scikit-learn/examples/applications/plot_out_of_core_classification.py,278,documents as a stream.,
scikit-learn/examples/applications/plot_out_of_core_classification.py,282,Main loop : iterate on mini-batches of examples,
scikit-learn/examples/applications/plot_out_of_core_classification.py,291,update estimator with examples in the current mini-batch,
scikit-learn/examples/applications/plot_out_of_core_classification.py,294,accumulate test accuracy stats,
scikit-learn/examples/applications/plot_out_of_core_classification.py,314,,
scikit-learn/examples/applications/plot_out_of_core_classification.py,315,Plot results,
scikit-learn/examples/applications/plot_out_of_core_classification.py,316,------------,
scikit-learn/examples/applications/plot_out_of_core_classification.py,317,,
scikit-learn/examples/applications/plot_out_of_core_classification.py,318,The plot represents the learning curve of the classifier: the evolution,
scikit-learn/examples/applications/plot_out_of_core_classification.py,319,of classification accuracy over the course of the mini-batches. Accuracy is,
scikit-learn/examples/applications/plot_out_of_core_classification.py,320,"measured on the first 1000 samples, held out as a validation set.",
scikit-learn/examples/applications/plot_out_of_core_classification.py,321,,
scikit-learn/examples/applications/plot_out_of_core_classification.py,322,"To limit the memory consumption, we queue examples up to a fixed amount",
scikit-learn/examples/applications/plot_out_of_core_classification.py,323,before feeding them to the learner.,
scikit-learn/examples/applications/plot_out_of_core_classification.py,340,Plot accuracy evolution,
scikit-learn/examples/applications/plot_out_of_core_classification.py,343,Plot accuracy evolution with #examples,
scikit-learn/examples/applications/plot_out_of_core_classification.py,352,Plot accuracy evolution with runtime,
scikit-learn/examples/applications/plot_out_of_core_classification.py,359,Plot fitting times,
scikit-learn/examples/applications/plot_out_of_core_classification.py,395,Plot prediction times,
scikit-learn/examples/applications/svm_gui.py,19,Author: Peter Prettenhoer <peter.prettenhofer@gmail.com>,
scikit-learn/examples/applications/svm_gui.py,20,,
scikit-learn/examples/applications/svm_gui.py,21,License: BSD 3 clause,
scikit-learn/examples/applications/svm_gui.py,29,NavigationToolbar2TkAgg was deprecated in matplotlib 2.2,
scikit-learn/examples/applications/svm_gui.py,84,Whether or not a model has been fitted,
scikit-learn/examples/applications/svm_gui.py,133,update decision surface if already fitted.,
scikit-learn/examples/applications/svm_gui.py,155,support for matplotlib (1.*),
scikit-learn/examples/applications/plot_face_recognition.py,45,Display progress logs on stdout,
scikit-learn/examples/applications/plot_face_recognition.py,49,,
scikit-learn/examples/applications/plot_face_recognition.py,50,"Download the data, if not already on disk and load it as numpy arrays",
scikit-learn/examples/applications/plot_face_recognition.py,54,introspect the images arrays to find the shapes (for plotting),
scikit-learn/examples/applications/plot_face_recognition.py,57,for machine learning we use the 2 data directly (as relative pixel,
scikit-learn/examples/applications/plot_face_recognition.py,58,positions info is ignored by this model),
scikit-learn/examples/applications/plot_face_recognition.py,62,the label to predict is the id of the person,
scikit-learn/examples/applications/plot_face_recognition.py,73,,
scikit-learn/examples/applications/plot_face_recognition.py,74,Split into a training set and a test set using a stratified k fold,
scikit-learn/examples/applications/plot_face_recognition.py,76,split into a training and testing set,
scikit-learn/examples/applications/plot_face_recognition.py,81,,
scikit-learn/examples/applications/plot_face_recognition.py,82,Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled,
scikit-learn/examples/applications/plot_face_recognition.py,83,dataset): unsupervised feature extraction / dimensionality reduction,
scikit-learn/examples/applications/plot_face_recognition.py,102,,
scikit-learn/examples/applications/plot_face_recognition.py,103,Train a SVM classification model,
scikit-learn/examples/applications/plot_face_recognition.py,118,,
scikit-learn/examples/applications/plot_face_recognition.py,119,Quantitative evaluation of the model quality on the test set,
scikit-learn/examples/applications/plot_face_recognition.py,130,,
scikit-learn/examples/applications/plot_face_recognition.py,131,Qualitative evaluation of the predictions using matplotlib,
scikit-learn/examples/applications/plot_face_recognition.py,145,plot the result of the prediction on a portion of the test set,
scikit-learn/examples/applications/plot_face_recognition.py,157,plot the gallery of the most significative eigenfaces,
scikit-learn/examples/feature_selection/plot_rfe_with_cross_validation.py,17,Build a classification task using 3 informative features,
scikit-learn/examples/feature_selection/plot_rfe_with_cross_validation.py,22,Create the RFE object and compute a cross-validated score.,
scikit-learn/examples/feature_selection/plot_rfe_with_cross_validation.py,24,"The ""accuracy"" scoring is proportional to the number of correct",
scikit-learn/examples/feature_selection/plot_rfe_with_cross_validation.py,25,classifications,
scikit-learn/examples/feature_selection/plot_rfe_with_cross_validation.py,32,Plot number of features VS. cross-validation scores,
scikit-learn/examples/feature_selection/plot_feature_selection_pipeline.py,21,import some data to play with,
scikit-learn/examples/feature_selection/plot_feature_selection_pipeline.py,28,ANOVA SVM-C,
scikit-learn/examples/feature_selection/plot_feature_selection_pipeline.py,29,"1) anova filter, take 3 best ranked features",
scikit-learn/examples/feature_selection/plot_feature_selection_pipeline.py,31,2) svm,
scikit-learn/examples/feature_selection/plot_feature_selection.py,34,,
scikit-learn/examples/feature_selection/plot_feature_selection.py,35,Import some data to play with,
scikit-learn/examples/feature_selection/plot_feature_selection.py,37,The iris dataset,
scikit-learn/examples/feature_selection/plot_feature_selection.py,40,Some noisy data not correlated,
scikit-learn/examples/feature_selection/plot_feature_selection.py,43,Add the noisy data to the informative features,
scikit-learn/examples/feature_selection/plot_feature_selection.py,46,Split dataset to select feature and evaluate the classifier,
scikit-learn/examples/feature_selection/plot_feature_selection.py,56,,
scikit-learn/examples/feature_selection/plot_feature_selection.py,57,Univariate feature selection with F-test for feature scoring,
scikit-learn/examples/feature_selection/plot_feature_selection.py,58,We use the default selection function to select the four,
scikit-learn/examples/feature_selection/plot_feature_selection.py,59,most significant features,
scikit-learn/examples/feature_selection/plot_feature_selection.py,67,,
scikit-learn/examples/feature_selection/plot_feature_selection.py,68,Compare to the weights of an SVM,
scikit-learn/examples/feature_selection/plot_permutation_test_for_classification.py,14,Author:  Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/examples/feature_selection/plot_permutation_test_for_classification.py,15,License: BSD 3 clause,
scikit-learn/examples/feature_selection/plot_permutation_test_for_classification.py,28,,
scikit-learn/examples/feature_selection/plot_permutation_test_for_classification.py,29,Loading a dataset,
scikit-learn/examples/feature_selection/plot_permutation_test_for_classification.py,35,Some noisy data not correlated,
scikit-learn/examples/feature_selection/plot_permutation_test_for_classification.py,39,Add noisy data to the informative features for make the task harder,
scikit-learn/examples/feature_selection/plot_permutation_test_for_classification.py,50,,
scikit-learn/examples/feature_selection/plot_permutation_test_for_classification.py,51,View histogram of permutation scores,
scikit-learn/examples/feature_selection/plot_permutation_test_for_classification.py,55,"BUG: vlines(..., linestyle='--') fails on older versions of matplotlib",
scikit-learn/examples/feature_selection/plot_permutation_test_for_classification.py,56,"plt.vlines(score, ylim[0], ylim[1], linestyle='--',",
scikit-learn/examples/feature_selection/plot_permutation_test_for_classification.py,57,"color='g', linewidth=3, label='Classification Score'",
scikit-learn/examples/feature_selection/plot_permutation_test_for_classification.py,58,' (pvalue %s)' % pvalue),
scikit-learn/examples/feature_selection/plot_permutation_test_for_classification.py,59,"plt.vlines(1.0 / n_classes, ylim[0], ylim[1], linestyle='--',",
scikit-learn/examples/feature_selection/plot_permutation_test_for_classification.py,60,"color='k', linewidth=3, label='Luck')",
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,34,,
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,35,Load the data,
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,36,---------------------------------------------------------,
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,37,,
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,38,"First, let's load the diabetes dataset which is available from within",
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,39,"sklearn. Then, we will look what features are collected for the diabates",
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,40,patients:,
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,50,,
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,51,Find importance of the features,
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,52,---------------------------------------------------------,
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,53,,
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,54,To decide on the importance of the features we are going to use LassoCV,
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,55,estimator. The features with the highest absolute `coef_` value are,
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,56,considered the most important,
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,62,,
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,63,Select from the model features with the higest score,
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,64,---------------------------------------------------------,
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,65,,
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,66,Now we want to select the two features which are the most important.,
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,67,SelectFromModel() allows for setting the threshold. Only the features with,
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,68,"the `coef_` higher than the threshold will remain. Here, we want to set the",
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,69,threshold slightly above the third highest `coef_` calculated by LassoCV(),
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,70,from our data.,
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,85,,
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,86,Plot the two most important features,
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,87,---------------------------------------------------------,
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,88,,
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,89,Finally we will plot the selected two features from the data.,
scikit-learn/examples/feature_selection/plot_rfe_digits.py,21,Load the digits dataset,
scikit-learn/examples/feature_selection/plot_rfe_digits.py,26,Create the RFE object and rank each pixel,
scikit-learn/examples/feature_selection/plot_rfe_digits.py,32,Plot pixel ranking,
scikit-learn/sklearn/naive_bayes.py,1,-*- coding: utf-8 -*-,
scikit-learn/sklearn/naive_bayes.py,9,Author: Vincent Michel <vincent.michel@inria.fr>,
scikit-learn/sklearn/naive_bayes.py,10,Minor fixes by Fabian Pedregosa,
scikit-learn/sklearn/naive_bayes.py,11,Amit Aides <amitibo@tx.technion.ac.il>,
scikit-learn/sklearn/naive_bayes.py,12,Yehuda Finkelstein <yehudaf@tx.technion.ac.il>,
scikit-learn/sklearn/naive_bayes.py,13,Lars Buitinck,
scikit-learn/sklearn/naive_bayes.py,14,Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,
scikit-learn/sklearn/naive_bayes.py,15,(parts based on earlier work by Mathieu Blondel),
scikit-learn/sklearn/naive_bayes.py,16,,
scikit-learn/sklearn/naive_bayes.py,17,License: BSD 3 clause,
scikit-learn/sklearn/naive_bayes.py,56,Note that this is not marked @abstractmethod as long as the,
scikit-learn/sklearn/naive_bayes.py,57,deprecated public alias sklearn.naive_bayes.BayesNB exists,
scikit-learn/sklearn/naive_bayes.py,58,(until 0.24) to preserve backward compat for 3rd party projects,
scikit-learn/sklearn/naive_bayes.py,59,with existing derived classes.,
scikit-learn/sklearn/naive_bayes.py,98,"normalize by P(x) = P(f_1, ..., f_n)",
scikit-learn/sklearn/naive_bayes.py,257,Compute (potentially weighted) mean and variance of new datapoints,
scikit-learn/sklearn/naive_bayes.py,273,"Combine mean of old and new data, taking into consideration",
scikit-learn/sklearn/naive_bayes.py,274,(weighted) number of observations,
scikit-learn/sklearn/naive_bayes.py,277,"Combine variance of old and new data, taking into consideration",
scikit-learn/sklearn/naive_bayes.py,278,(weighted) number of observations. This is achieved by combining,
scikit-learn/sklearn/naive_bayes.py,279,the sum-of-squared-differences (ssd),
scikit-learn/sklearn/naive_bayes.py,364,"If the ratio of data variance between dimensions is too small, it",
scikit-learn/sklearn/naive_bayes.py,365,"will cause numerical errors. To address this, we artificially",
scikit-learn/sklearn/naive_bayes.py,366,"boost the variance by epsilon, a small fraction of the standard",
scikit-learn/sklearn/naive_bayes.py,367,deviation of the largest dimension.,
scikit-learn/sklearn/naive_bayes.py,374,This is the first call to partial_fit:,
scikit-learn/sklearn/naive_bayes.py,375,initialize various cumulative counters,
scikit-learn/sklearn/naive_bayes.py,383,Initialise the class prior,
scikit-learn/sklearn/naive_bayes.py,384,Take into account the priors,
scikit-learn/sklearn/naive_bayes.py,387,Check that the provide prior match the number of classes,
scikit-learn/sklearn/naive_bayes.py,391,Check that the sum is 1,
scikit-learn/sklearn/naive_bayes.py,394,Check that the prior are non-negative,
scikit-learn/sklearn/naive_bayes.py,399,Initialize the priors to zeros for each class,
scikit-learn/sklearn/naive_bayes.py,406,Put epsilon back in each time,
scikit-learn/sklearn/naive_bayes.py,440,Update if only no priors is provided,
scikit-learn/sklearn/naive_bayes.py,442,"Empirical prior, with sample_weight taken into account",
scikit-learn/sklearn/naive_bayes.py,487,silence the warning when count is 0 because class was not yet,
scikit-learn/sklearn/naive_bayes.py,488,observed,
scikit-learn/sklearn/naive_bayes.py,492,"empirical prior, with sample_weight taken into account",
scikit-learn/sklearn/naive_bayes.py,552,This is the first call to partial_fit:,
scikit-learn/sklearn/naive_bayes.py,553,initialize various cumulative counters,
scikit-learn/sklearn/naive_bayes.py,569,label_binarize() returns arrays with dtype=np.int64.,
scikit-learn/sklearn/naive_bayes.py,570,We convert it to np.float64 to support sample_weight consistently,
scikit-learn/sklearn/naive_bayes.py,579,Count raw events from data before updating the class log prior,
scikit-learn/sklearn/naive_bayes.py,580,and feature log probas,
scikit-learn/sklearn/naive_bayes.py,583,XXX: OPTIM: we could introduce a public finalization method to,
scikit-learn/sklearn/naive_bayes.py,584,be called by the user explicitly just once after several consecutive,
scikit-learn/sklearn/naive_bayes.py,585,calls to partial_fit and prior any call to predict[_[log_]proba],
scikit-learn/sklearn/naive_bayes.py,586,to avoid computing the smooth log probas at each call to partial fit,
scikit-learn/sklearn/naive_bayes.py,621,LabelBinarizer().fit_transform() returns arrays with dtype=np.int64.,
scikit-learn/sklearn/naive_bayes.py,622,We convert it to np.float64 to support sample_weight consistently;,
scikit-learn/sklearn/naive_bayes.py,623,this means we also don't have to cast X to floating point,
scikit-learn/sklearn/naive_bayes.py,632,Count raw events from data before updating the class log prior,
scikit-learn/sklearn/naive_bayes.py,633,and feature log probas,
scikit-learn/sklearn/naive_bayes.py,648,XXX The following is a stopgap measure; we need to set the dimensions,
scikit-learn/sklearn/naive_bayes.py,649,of class_log_prior_ and feature_log_prob_ correctly.,
scikit-learn/sklearn/naive_bayes.py,871,"_BaseNB.predict uses argmax, but ComplementNB operates with argmin.",
scikit-learn/sklearn/naive_bayes.py,1006,Compute  neg_prob · (1 - X).T  as  ∑neg_prob - X · neg_prob,
scikit-learn/sklearn/naive_bayes.py,1174,we append a column full of zeros for each new category,
scikit-learn/sklearn/naive_bayes.py,1220,TODO: remove in 0.24,
scikit-learn/sklearn/naive_bayes.py,1227,TODO: remove in 0.24,
scikit-learn/sklearn/base.py,7,Author: Gael Varoquaux <gael.varoquaux@normalesup.org>,
scikit-learn/sklearn/base.py,8,License: BSD 3 clause,
scikit-learn/sklearn/base.py,60,XXX: not handling dictionaries,
scikit-learn/sklearn/base.py,85,quick sanity check of the parameters of the clone,
scikit-learn/sklearn/base.py,112,Do a multi-line justified repr:,
scikit-learn/sklearn/base.py,120,use str for representing floating point numbers,
scikit-learn/sklearn/base.py,121,this way we get consistent representation across,
scikit-learn/sklearn/base.py,122,architectures and versions.,
scikit-learn/sklearn/base.py,125,use repr of the rest,
scikit-learn/sklearn/base.py,141,Strip trailing space to avoid nightmare in doctests,
scikit-learn/sklearn/base.py,159,fetch the constructor or the original constructor before,
scikit-learn/sklearn/base.py,160,deprecation wrapping if any,
scikit-learn/sklearn/base.py,163,No explicit constructor to introspect,
scikit-learn/sklearn/base.py,166,introspect the constructor arguments to find the model parameters,
scikit-learn/sklearn/base.py,167,to represent,
scikit-learn/sklearn/base.py,169,Consider the constructor parameters excluding 'self',
scikit-learn/sklearn/base.py,180,Extract and sort argument names excluding 'self',
scikit-learn/sklearn/base.py,235,Simple optimization to gain speed (inspect is slow),
scikit-learn/sklearn/base.py,239,grouped by prefix,
scikit-learn/sklearn/base.py,260,N_CHAR_MAX is the (approximate) maximum number of non-blank,
scikit-learn/sklearn/base.py,261,characters to render. We pass it as an optional parameter to ease,
scikit-learn/sklearn/base.py,262,the tests.,
scikit-learn/sklearn/base.py,266,number of elements to show in sequences,
scikit-learn/sklearn/base.py,268,use ellipsis for sequences with a lot of elements,
scikit-learn/sklearn/base.py,275,Use bruteforce ellipsis when there are a lot of non-blank characters,
scikit-learn/sklearn/base.py,278,apprx number of chars to keep on both ends,
scikit-learn/sklearn/base.py,280,The regex '^(\s*\S){%d}' % n,
scikit-learn/sklearn/base.py,281,matches from the start of the string until the nth non-blank,
scikit-learn/sklearn/base.py,282,character:,
scikit-learn/sklearn/base.py,283,- ^ matches the start of string,
scikit-learn/sklearn/base.py,284,- (pattern){n} matches n repetitions of pattern,
scikit-learn/sklearn/base.py,285,- \s*\S matches a non-blank char following zero or more blanks,
scikit-learn/sklearn/base.py,290,The left side and right side aren't on the same line.,
scikit-learn/sklearn/base.py,291,"To avoid weird cuts, e.g.:",
scikit-learn/sklearn/base.py,292,"categoric...ore',",
scikit-learn/sklearn/base.py,293,we need to start the right side with an appropriate newline,
scikit-learn/sklearn/base.py,294,character so that it renders properly as:,
scikit-learn/sklearn/base.py,295,categoric...,
scikit-learn/sklearn/base.py,296,"handle_unknown='ignore',",
scikit-learn/sklearn/base.py,297,so we add [^\n]*\n which matches until the next \n,
scikit-learn/sklearn/base.py,303,Only add ellipsis if it results in a shorter repr,
scikit-learn/sklearn/base.py,341,need the if because mixins might not have _more_tags,
scikit-learn/sklearn/base.py,342,but might do redundant work in estimators,
scikit-learn/sklearn/base.py,343,(i.e. calling more tags on BaseEstimator multiple times),
scikit-learn/sklearn/base.py,519,non-optimized default implementation; override when a better,
scikit-learn/sklearn/base.py,520,method is possible for a given clustering algorithm,
scikit-learn/sklearn/base.py,626,non-optimized default implementation; override when a better,
scikit-learn/sklearn/base.py,627,method is possible for a given clustering algorithm,
scikit-learn/sklearn/base.py,629,fit method of arity 1 (unsupervised transformation),
scikit-learn/sklearn/base.py,632,fit method of arity 2 (supervised transformation),
scikit-learn/sklearn/base.py,679,override for transductive outlier detectors like LocalOulierFactor,
scikit-learn/sklearn/dummy.py,1,Author: Mathieu Blondel <mathieu@mblondel.org>,
scikit-learn/sklearn/dummy.py,2,Arnaud Joly <a.joly@ulg.ac.be>,
scikit-learn/sklearn/dummy.py,3,Maheshakya Wijewardena <maheshakya.10@cse.mrt.ac.lk>,
scikit-learn/sklearn/dummy.py,4,License: BSD 3 clause,
scikit-learn/sklearn/dummy.py,129,TODO: Remove in 0.24,
scikit-learn/sklearn/dummy.py,159,No input validation is done for X,
scikit-learn/sklearn/dummy.py,183,Checking in case of constant strategy if the constant,
scikit-learn/sklearn/dummy.py,184,provided by the user is in y.,
scikit-learn/sklearn/dummy.py,213,numpy random_state expects Python int and not long as size argument,
scikit-learn/sklearn/dummy.py,214,under Windows,
scikit-learn/sklearn/dummy.py,223,Get same type even for self.n_outputs_ == 1,
scikit-learn/sklearn/dummy.py,228,Compute probability only once,
scikit-learn/sklearn/dummy.py,291,numpy random_state expects Python int and not long as size argument,
scikit-learn/sklearn/dummy.py,292,under Windows,
scikit-learn/sklearn/dummy.py,301,Get same type even for self.n_outputs_ == 1,
scikit-learn/sklearn/dummy.py,398,mypy error: Decorated property not supported,
scikit-learn/sklearn/dummy.py,399,type: ignore,
scikit-learn/sklearn/dummy.py,495,No input validation is done for X,
scikit-learn/sklearn/dummy.py,626,mypy error: Decorated property not supported,
scikit-learn/sklearn/dummy.py,627,type: ignore,
scikit-learn/sklearn/calibration.py,3,Author: Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>,
scikit-learn/sklearn/calibration.py,4,Balazs Kegl <balazs.kegl@gmail.com>,
scikit-learn/sklearn/calibration.py,5,Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,
scikit-learn/sklearn/calibration.py,6,Mathieu Blondel <mathieu@mblondel.org>,
scikit-learn/sklearn/calibration.py,7,,
scikit-learn/sklearn/calibration.py,8,License: BSD 3 clause,
scikit-learn/sklearn/calibration.py,133,Check that each cross-validation fold can have at least one,
scikit-learn/sklearn/calibration.py,134,example per class,
scikit-learn/sklearn/calibration.py,146,we want all classifiers that don't expose a random_state,
scikit-learn/sklearn/calibration.py,147,to be deterministic (and we don't want to expose this one).,
scikit-learn/sklearn/calibration.py,207,Compute the arithmetic mean of the predictions of the calibrated,
scikit-learn/sklearn/calibration.py,208,classifiers,
scikit-learn/sklearn/calibration.py,377,Normalize the probabilities,
scikit-learn/sklearn/calibration.py,383,XXX : for some reason all probas can be 0,
scikit-learn/sklearn/calibration.py,386,Deal with cases where the predicted probability minimally exceeds 1.0,
scikit-learn/sklearn/calibration.py,421,F follows Platt's notations,
scikit-learn/sklearn/calibration.py,423,Bayesian priors (see Platt end of section 2.2),
scikit-learn/sklearn/calibration.py,432,From Platt (beginning of Section 2.2),
scikit-learn/sklearn/calibration.py,441,gradient of the objective function,
scikit-learn/sklearn/calibration.py,567,"Normalize predicted values into interval [0, 1]",
scikit-learn/sklearn/calibration.py,579,Determine bin edges by distribution of data,
scikit-learn/sklearn/kernel_approximation.py,6,Author: Andreas Mueller <amueller@ais.uni-bonn.de>,
scikit-learn/sklearn/kernel_approximation.py,7,,
scikit-learn/sklearn/kernel_approximation.py,8,License: BSD 3 clause,
scikit-learn/sklearn/kernel_approximation.py,217,transform by inverse CDF of sech,
scikit-learn/sklearn/kernel_approximation.py,344,"See reference, figure 2 c)",
scikit-learn/sklearn/kernel_approximation.py,380,zeroth component,
scikit-learn/sklearn/kernel_approximation.py,381,1/cosh = sech,
scikit-learn/sklearn/kernel_approximation.py,382,cosh(0) = 1.0,
scikit-learn/sklearn/kernel_approximation.py,564,get basis vectors,
scikit-learn/sklearn/kernel_approximation.py,566,XXX should we just bail?,
scikit-learn/sklearn/kernel_approximation.py,583,sqrt of kernel matrix on basis vectors,
scikit-learn/sklearn/kernel_ridge.py,3,Authors: Mathieu Blondel <mathieu@mblondel.org>,
scikit-learn/sklearn/kernel_ridge.py,4,Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,
scikit-learn/sklearn/kernel_ridge.py,5,License: BSD 3 clause,
scikit-learn/sklearn/kernel_ridge.py,158,Convert data,
scikit-learn/sklearn/multiclass.py,31,Author: Mathieu Blondel <mathieu@mblondel.org>,
scikit-learn/sklearn/multiclass.py,32,Author: Hamzeh Alsalhi <93hamsal@gmail.com>,
scikit-learn/sklearn/multiclass.py,33,,
scikit-learn/sklearn/multiclass.py,34,License: BSD 3 clause,
scikit-learn/sklearn/multiclass.py,97,probabilities of the positive class,
scikit-learn/sklearn/multiclass.py,224,"A sparse LabelBinarizer, with sparse_output=True, has been shown to",
scikit-learn/sklearn/multiclass.py,225,outperform or match a dense label binarizer in all cases and has also,
scikit-learn/sklearn/multiclass.py,226,resulted in less or equal memory consumption in the fit_ovr function,
scikit-learn/sklearn/multiclass.py,227,overall.,
scikit-learn/sklearn/multiclass.py,233,In cases where individual estimators are very fast to train setting,
scikit-learn/sklearn/multiclass.py,234,n_jobs > 1 in can results in slower performance due to the overhead,
scikit-learn/sklearn/multiclass.py,235,of spawning threads.  See joblib issue #112.,
scikit-learn/sklearn/multiclass.py,278,"A sparse LabelBinarizer, with sparse_output=True, has been",
scikit-learn/sklearn/multiclass.py,279,shown to outperform or match a dense label binarizer in all,
scikit-learn/sklearn/multiclass.py,280,cases and has also resulted in less or equal memory consumption,
scikit-learn/sklearn/multiclass.py,281,in the fit_ovr function overall.,
scikit-learn/sklearn/multiclass.py,366,"Y[i, j] gives the probability that sample i has the label j.",
scikit-learn/sklearn/multiclass.py,367,"In the multi-label case, these are not disjoint.",
scikit-learn/sklearn/multiclass.py,371,"Only one estimator, but we still want to return probabilities",
scikit-learn/sklearn/multiclass.py,372,for two classes.,
scikit-learn/sklearn/multiclass.py,376,"Then, probabilities should be normalized to 1.",
scikit-learn/sklearn/multiclass.py,439,For consistency with other estimators we raise a AttributeError so,
scikit-learn/sklearn/multiclass.py,440,that hasattr() fails if the OVR estimator isn't fitted.,
scikit-learn/sklearn/multiclass.py,792,FIXME: there are more elaborate methods than generating the codebook,
scikit-learn/sklearn/multiclass.py,793,randomly.,
scikit-learn/sklearn/discriminant_analysis.py,5,Authors: Clemens Brunner,
scikit-learn/sklearn/discriminant_analysis.py,6,Martin Billinger,
scikit-learn/sklearn/discriminant_analysis.py,7,Matthieu Perrot,
scikit-learn/sklearn/discriminant_analysis.py,8,Mathieu Blondel,
scikit-learn/sklearn/discriminant_analysis.py,10,License: BSD 3-Clause,
scikit-learn/sklearn/discriminant_analysis.py,54,standardize features,
scikit-learn/sklearn/discriminant_analysis.py,57,rescale,
scikit-learn/sklearn/discriminant_analysis.py,257,used only in svd solver,
scikit-learn/sklearn/discriminant_analysis.py,258,used only in svd solver,
scikit-learn/sklearn/discriminant_analysis.py,334,within scatter,
scikit-learn/sklearn/discriminant_analysis.py,335,total scatter,
scikit-learn/sklearn/discriminant_analysis.py,336,between scatter,
scikit-learn/sklearn/discriminant_analysis.py,341,sort eigenvectors,
scikit-learn/sklearn/discriminant_analysis.py,375,1) within (univariate) scaling by with classes std-dev,
scikit-learn/sklearn/discriminant_analysis.py,377,avoid division by zero in normalization,
scikit-learn/sklearn/discriminant_analysis.py,381,2) Within variance scaling,
scikit-learn/sklearn/discriminant_analysis.py,383,SVD of centered (within)scaled data,
scikit-learn/sklearn/discriminant_analysis.py,387,Scaling of within covariance is: V' 1/S,
scikit-learn/sklearn/discriminant_analysis.py,390,3) Between variance scaling,
scikit-learn/sklearn/discriminant_analysis.py,391,Scale weighted centers,
scikit-learn/sklearn/discriminant_analysis.py,394,Centers are living in a space with n_classes-1 dim (maximum),
scikit-learn/sklearn/discriminant_analysis.py,395,Use SVD to find projection in the space spanned by the,
scikit-learn/sklearn/discriminant_analysis.py,396,(n_classes) centers,
scikit-learn/sklearn/discriminant_analysis.py,437,estimate priors from sample,
scikit-learn/sklearn/discriminant_analysis.py,438,non-negative ints,
scikit-learn/sklearn/discriminant_analysis.py,450,Maximum number of components no matter what n_components is,
scikit-learn/sklearn/discriminant_analysis.py,451,specified:,
scikit-learn/sklearn/discriminant_analysis.py,475,treat binary case as a special case,
scikit-learn/sklearn/discriminant_analysis.py,678,Xgc = U * S * V.T,
scikit-learn/sklearn/discriminant_analysis.py,686,cov = V * (S^2 / (n-1)) * V.T,
scikit-learn/sklearn/discriminant_analysis.py,708,"shape = [len(X), n_classes]",
scikit-learn/sklearn/discriminant_analysis.py,728,handle special case of two classes,
scikit-learn/sklearn/discriminant_analysis.py,764,compute the likelihood of the underlying gaussian models,
scikit-learn/sklearn/discriminant_analysis.py,765,up to a multiplicative constant.,
scikit-learn/sklearn/discriminant_analysis.py,767,compute posterior probabilities,
scikit-learn/sklearn/discriminant_analysis.py,783,XXX : can do better to avoid precision overflows,
scikit-learn/sklearn/isotonic.py,1,Authors: Fabian Pedregosa <fabian@fseoane.net>,
scikit-learn/sklearn/isotonic.py,2,Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/sklearn/isotonic.py,3,Nelle Varoquaux <nelle.varoquaux@gmail.com>,
scikit-learn/sklearn/isotonic.py,4,License: BSD 3 clause,
scikit-learn/sklearn/isotonic.py,55,Calculate Spearman rho estimate and set return accordingly.,
scikit-learn/sklearn/isotonic.py,59,"Run Fisher transform to get the rho CI, but handle rho=+/-1",
scikit-learn/sklearn/isotonic.py,64,"Use a 95% CI, i.e., +/-1.96 S.E.",
scikit-learn/sklearn/isotonic.py,65,https://en.wikipedia.org/wiki/Fisher_transformation,
scikit-learn/sklearn/isotonic.py,69,Warn if the CI spans zero.,
scikit-learn/sklearn/isotonic.py,124,"Older versions of np.clip don't accept None as a bound, so use np.inf",
scikit-learn/sklearn/isotonic.py,220,Handle the out_of_bounds argument by setting bounds_error,
scikit-learn/sklearn/isotonic.py,228,"single y, constant prediction",
scikit-learn/sklearn/isotonic.py,238,Determine increasing if auto-determination requested,
scikit-learn/sklearn/isotonic.py,244,"If sample_weights is passed, removed zero-weight values and clean",
scikit-learn/sklearn/isotonic.py,245,order,
scikit-learn/sklearn/isotonic.py,260,Handle the left and right bounds on X,
scikit-learn/sklearn/isotonic.py,264,Remove unnecessary points for faster prediction,
scikit-learn/sklearn/isotonic.py,266,"Aside from the 1st and last point, remove points whose y values",
scikit-learn/sklearn/isotonic.py,267,are equal to both the point before and the point after it.,
scikit-learn/sklearn/isotonic.py,274,The ability to turn off trim_duplicates is only used to it make,
scikit-learn/sklearn/isotonic.py,275,easier to unit test that removing duplicates in y does not have,
scikit-learn/sklearn/isotonic.py,276,any impact the resulting interpolation function (besides,
scikit-learn/sklearn/isotonic.py,277,prediction speed).,
scikit-learn/sklearn/isotonic.py,310,Transform y by running the isotonic regression algorithm and,
scikit-learn/sklearn/isotonic.py,311,transform X accordingly.,
scikit-learn/sklearn/isotonic.py,314,It is necessary to store the non-redundant part of the training set,
scikit-learn/sklearn/isotonic.py,315,on the model to make it possible to support model persistence via,
scikit-learn/sklearn/isotonic.py,316,the pickle module as the object built by scipy.interp1d is not,
scikit-learn/sklearn/isotonic.py,317,picklable directly.,
scikit-learn/sklearn/isotonic.py,320,Build the interpolation function,
scikit-learn/sklearn/isotonic.py,348,Handle the out_of_bounds argument by clipping if needed,
scikit-learn/sklearn/isotonic.py,359,"on scipy 0.17, interp1d up-casts to float64, so we cast back",
scikit-learn/sklearn/isotonic.py,382,remove interpolation method,
scikit-learn/sklearn/__init__.py,26,"PEP0440 compatible formatted version, see:",
scikit-learn/sklearn/__init__.py,27,https://www.python.org/dev/peps/pep-0440/,
scikit-learn/sklearn/__init__.py,28,,
scikit-learn/sklearn/__init__.py,29,Generic release markers:,
scikit-learn/sklearn/__init__.py,30,X.Y,
scikit-learn/sklearn/__init__.py,31,X.Y.Z   # For bugfix releases,
scikit-learn/sklearn/__init__.py,32,,
scikit-learn/sklearn/__init__.py,33,Admissible pre-release markers:,
scikit-learn/sklearn/__init__.py,34,X.YaN   # Alpha release,
scikit-learn/sklearn/__init__.py,35,X.YbN   # Beta release,
scikit-learn/sklearn/__init__.py,36,X.YrcN  # Release Candidate,
scikit-learn/sklearn/__init__.py,37,X.Y     # Final release,
scikit-learn/sklearn/__init__.py,38,,
scikit-learn/sklearn/__init__.py,39,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.,
scikit-learn/sklearn/__init__.py,40,'X.Y.dev0' is the canonical version of 'X.Y.dev',
scikit-learn/sklearn/__init__.py,41,,
scikit-learn/sklearn/__init__.py,45,"On OSX, we can get a runtime error due to multiple OpenMP libraries loaded",
scikit-learn/sklearn/__init__.py,46,simultaneously. This can happen for instance when calling BLAS inside a,
scikit-learn/sklearn/__init__.py,47,prange. Setting the following environment variable allows multiple OpenMP,
scikit-learn/sklearn/__init__.py,48,libraries to be loaded. It should not degrade performances since we manually,
scikit-learn/sklearn/__init__.py,49,"take care of potential over-subcription performance issues, in sections of",
scikit-learn/sklearn/__init__.py,50,"the code where nested OpenMP loops can happen, by dynamically reconfiguring",
scikit-learn/sklearn/__init__.py,51,the inner OpenMP runtime to temporarily disable it while under the scope of,
scikit-learn/sklearn/__init__.py,52,the outer OpenMP parallel section.,
scikit-learn/sklearn/__init__.py,55,Workaround issue discovered in intel-openmp 2019.5:,
scikit-learn/sklearn/__init__.py,56,https://github.com/ContinuumIO/anaconda-issues/issues/11294,
scikit-learn/sklearn/__init__.py,60,This variable is injected in the __builtins__ by the build,
scikit-learn/sklearn/__init__.py,61,process. It is used to enable importing subpackages of sklearn when,
scikit-learn/sklearn/__init__.py,62,the binaries are not built,
scikit-learn/sklearn/__init__.py,63,mypy error: Cannot determine type of '__SKLEARN_SETUP__',
scikit-learn/sklearn/__init__.py,64,type: ignore,
scikit-learn/sklearn/__init__.py,70,We are not importing the rest of scikit-learn during the build,
scikit-learn/sklearn/__init__.py,71,"process, as it may not be compiled yet",
scikit-learn/sklearn/__init__.py,73,`_distributor_init` allows distributors to run custom init code.,
scikit-learn/sklearn/__init__.py,74,"For instance, for the Windows wheel, this is used to pre-load the",
scikit-learn/sklearn/__init__.py,75,vcomp shared library runtime for OpenMP embedded in the sklearn/.libs,
scikit-learn/sklearn/__init__.py,76,sub-folder.,
scikit-learn/sklearn/__init__.py,77,It is necessary to do this prior to importing show_versions as the,
scikit-learn/sklearn/__init__.py,78,later is linked to the OpenMP runtime to make it possible to introspect,
scikit-learn/sklearn/__init__.py,79,it and importing it first would fail if the OpenMP dll cannot be found.,
scikit-learn/sklearn/__init__.py,80,noqa: F401,
scikit-learn/sklearn/__init__.py,81,noqa: F401,
scikit-learn/sklearn/__init__.py,95,Non-modules:,
scikit-learn/sklearn/__init__.py,106,"Check if a random seed exists in the environment, if not create one.",
scikit-learn/sklearn/pipeline.py,5,Author: Edouard Duchesnay,
scikit-learn/sklearn/pipeline.py,6,Gael Varoquaux,
scikit-learn/sklearn/pipeline.py,7,Virgile Fritsch,
scikit-learn/sklearn/pipeline.py,8,Alexandre Gramfort,
scikit-learn/sklearn/pipeline.py,9,Lars Buitinck,
scikit-learn/sklearn/pipeline.py,10,License: BSD,
scikit-learn/sklearn/pipeline.py,104,BaseEstimator interface,
scikit-learn/sklearn/pipeline.py,144,validate names,
scikit-learn/sklearn/pipeline.py,147,validate estimators,
scikit-learn/sklearn/pipeline.py,161,We allow last estimator to be None as an identity transformation,
scikit-learn/sklearn/pipeline.py,208,"Not an int, try get step by name",
scikit-learn/sklearn/pipeline.py,218,Use Bunch object to improve autocomplete,
scikit-learn/sklearn/pipeline.py,250,Estimator interface,
scikit-learn/sklearn/pipeline.py,253,shallow copy of steps - this should really be steps_,
scikit-learn/sklearn/pipeline.py,256,Setup the memory,
scikit-learn/sklearn/pipeline.py,271,joblib >= 0.12,
scikit-learn/sklearn/pipeline.py,273,we do not clone when caching is disabled to,
scikit-learn/sklearn/pipeline.py,274,preserve backward compatibility,
scikit-learn/sklearn/pipeline.py,279,joblib < 0.11,
scikit-learn/sklearn/pipeline.py,281,we do not clone when caching is disabled to,
scikit-learn/sklearn/pipeline.py,282,preserve backward compatibility,
scikit-learn/sklearn/pipeline.py,288,Fit or load from cache the current transformer,
scikit-learn/sklearn/pipeline.py,294,Replace the transformer of the step with the fitted,
scikit-learn/sklearn/pipeline.py,295,transformer. This is necessary when loading the transformer,
scikit-learn/sklearn/pipeline.py,296,from the cache.,
scikit-learn/sklearn/pipeline.py,535,"_final_estimator is None or has transform, otherwise attribute error",
scikit-learn/sklearn/pipeline.py,536,XXX: Handling the None case means we can't use if_delegate_has_method,
scikit-learn/sklearn/pipeline.py,565,raise AttributeError if necessary for hasattr behaviour,
scikit-learn/sklearn/pipeline.py,566,XXX: Handling the None case means we can't use if_delegate_has_method,
scikit-learn/sklearn/pipeline.py,614,check if first estimator expects pairwise input,
scikit-learn/sklearn/pipeline.py,619,delegate to first step (which will call _check_is_fitted),
scikit-learn/sklearn/pipeline.py,700,"if we have a weight for this transformer, multiply output",
scikit-learn/sklearn/pipeline.py,839,validate names,
scikit-learn/sklearn/pipeline.py,842,validate estimators,
scikit-learn/sklearn/pipeline.py,844,TODO: Remove in 0.24 when None is removed,
scikit-learn/sklearn/pipeline.py,905,All transformers are None,
scikit-learn/sklearn/pipeline.py,931,All transformers are None,
scikit-learn/sklearn/pipeline.py,980,All transformers are None,
scikit-learn/sklearn/pipeline.py,996,X is passed to all transformers so we just delegate to the first one,
scikit-learn/sklearn/pipeline.py,1042,We do not currently support `transformer_weights` as we may want to,
scikit-learn/sklearn/pipeline.py,1043,change its type spec in make_union,
scikit-learn/sklearn/multioutput.py,9,Author: Tim Head <betatim@gmail.com>,
scikit-learn/sklearn/multioutput.py,10,Author: Hugo Bowne-Anderson <hugobowne@gmail.com>,
scikit-learn/sklearn/multioutput.py,11,Author: Chris Rivera <chris.richard.rivera@gmail.com>,
scikit-learn/sklearn/multioutput.py,12,Author: Michael Williamson,
scikit-learn/sklearn/multioutput.py,13,Author: James Ashton Nichols <james.ashton.nichols@gmail.com>,
scikit-learn/sklearn/multioutput.py,14,,
scikit-learn/sklearn/multioutput.py,15,License: BSD 3 clause,
scikit-learn/sklearn/multioutput.py,407,FIXME,
scikit-learn/sklearn/multioutput.py,775,TODO: remove in 0.24,
scikit-learn/sklearn/setup.py,22,submodules with build utilities,
scikit-learn/sklearn/setup.py,26,submodules which do not have their own setup.py,
scikit-learn/sklearn/setup.py,27,we must manually add sub-submodules & tests,
scikit-learn/sklearn/setup.py,60,submodules which have their own setup.py,
scikit-learn/sklearn/setup.py,74,add cython extension module for isotonic regression,
scikit-learn/sklearn/setup.py,81,add the test directory,
scikit-learn/sklearn/setup.py,84,Skip cythonization as we do not want to include the generated,
scikit-learn/sklearn/setup.py,85,C/C++ files in the release tarballs as they are not necessarily,
scikit-learn/sklearn/setup.py,86,forward compatible with future versions of Python for instance.,
scikit-learn/sklearn/random_projection.py,1,-*- coding: utf8,
scikit-learn/sklearn/random_projection.py,26,"Authors: Olivier Grisel <olivier.grisel@ensta.org>,",
scikit-learn/sklearn/random_projection.py,27,Arnaud Joly <a.joly@ulg.ac.be>,
scikit-learn/sklearn/random_projection.py,28,License: BSD 3 clause,
scikit-learn/sklearn/random_projection.py,154,TODO: remove in 0.24,
scikit-learn/sklearn/random_projection.py,201,TODO: remove in 0.24,
scikit-learn/sklearn/random_projection.py,275,skip index generation if totally dense,
scikit-learn/sklearn/random_projection.py,280,Generate location of non zero elements,
scikit-learn/sklearn/random_projection.py,285,find the indices of the non-zero components for row i,
scikit-learn/sklearn/random_projection.py,295,Among non zero components the probability of the sign is 50%/50%,
scikit-learn/sklearn/random_projection.py,298,build the CSR structure by concatenating the rows,
scikit-learn/sklearn/random_projection.py,392,"Generate a projection matrix of size [n_components, n_features]",
scikit-learn/sklearn/random_projection.py,396,Check contract,
scikit-learn/sklearn/ensemble/_bagging.py,3,Author: Gilles Louppe <g.louppe@gmail.com>,
scikit-learn/sklearn/ensemble/_bagging.py,4,License: BSD 3 clause,
scikit-learn/sklearn/ensemble/_bagging.py,36,Draw sample indices,
scikit-learn/sklearn/ensemble/_bagging.py,50,Get valid random state,
scikit-learn/sklearn/ensemble/_bagging.py,53,Draw indices,
scikit-learn/sklearn/ensemble/_bagging.py,65,Retrieve settings,
scikit-learn/sklearn/ensemble/_bagging.py,76,Build estimators,
scikit-learn/sklearn/ensemble/_bagging.py,89,"Draw random feature, sample indices",
scikit-learn/sklearn/ensemble/_bagging.py,96,"Draw samples, using sample weights, and then fit",
scikit-learn/sklearn/ensemble/_bagging.py,138,Resort to voting,
scikit-learn/sklearn/ensemble/_bagging.py,280,Convert data (X is required to be 2d and indexable),
scikit-learn/sklearn/ensemble/_bagging.py,288,Remap output,
scikit-learn/sklearn/ensemble/_bagging.py,293,Check parameters,
scikit-learn/sklearn/ensemble/_bagging.py,299,Validate max_samples,
scikit-learn/sklearn/ensemble/_bagging.py,308,Store validated integer row sampling value,
scikit-learn/sklearn/ensemble/_bagging.py,311,Validate max_features,
scikit-learn/sklearn/ensemble/_bagging.py,324,Store validated integer feature sampling value,
scikit-learn/sklearn/ensemble/_bagging.py,327,Other checks,
scikit-learn/sklearn/ensemble/_bagging.py,340,"Free allocated memory, if any",
scikit-learn/sklearn/ensemble/_bagging.py,356,Parallel loop,
scikit-learn/sklearn/ensemble/_bagging.py,361,Advance random state to state after training,
scikit-learn/sklearn/ensemble/_bagging.py,362,the first n_estimators,
scikit-learn/sklearn/ensemble/_bagging.py,382,Reduce,
scikit-learn/sklearn/ensemble/_bagging.py,404,Get drawn indices along both sample and feature axes,
scikit-learn/sklearn/ensemble/_bagging.py,406,Operations accessing random_state must be performed identically,
scikit-learn/sklearn/ensemble/_bagging.py,407,to those in `_parallel_build_estimators()`,
scikit-learn/sklearn/ensemble/_bagging.py,621,Create mask for OOB samples,
scikit-learn/sklearn/ensemble/_bagging.py,702,Check data,
scikit-learn/sklearn/ensemble/_bagging.py,714,Parallel loop,
scikit-learn/sklearn/ensemble/_bagging.py,727,Reduce,
scikit-learn/sklearn/ensemble/_bagging.py,753,Check data,
scikit-learn/sklearn/ensemble/_bagging.py,765,Parallel loop,
scikit-learn/sklearn/ensemble/_bagging.py,777,Reduce,
scikit-learn/sklearn/ensemble/_bagging.py,811,Check data,
scikit-learn/sklearn/ensemble/_bagging.py,823,Parallel loop,
scikit-learn/sklearn/ensemble/_bagging.py,834,Reduce,
scikit-learn/sklearn/ensemble/_bagging.py,1023,Check data,
scikit-learn/sklearn/ensemble/_bagging.py,1029,Parallel loop,
scikit-learn/sklearn/ensemble/_bagging.py,1040,Reduce,
scikit-learn/sklearn/ensemble/_bagging.py,1059,Create mask for OOB samples,
scikit-learn/sklearn/ensemble/_gb_losses.py,103,compute leaf for each sample in ``X``.,
scikit-learn/sklearn/ensemble/_gb_losses.py,106,mask all which are not in sample mask.,
scikit-learn/sklearn/ensemble/_gb_losses.py,110,update each leaf (= perform line search),
scikit-learn/sklearn/ensemble/_gb_losses.py,116,update predictions (both in-bag and out-of-bag),
scikit-learn/sklearn/ensemble/_gb_losses.py,257,update predictions,
scikit-learn/sklearn/ensemble/_gb_losses.py,578,we only need to fit one tree for binary clf.,
scikit-learn/sklearn/ensemble/_gb_losses.py,582,"return the most common class, taking into account the samples",
scikit-learn/sklearn/ensemble/_gb_losses.py,583,weights,
scikit-learn/sklearn/ensemble/_gb_losses.py,601,"logaddexp(0, v) == log(1.0 + exp(v))",
scikit-learn/sklearn/ensemble/_gb_losses.py,644,prevents overflow and division by zero,
scikit-learn/sklearn/ensemble/_gb_losses.py,665,log(x / (1 - x)) is the inverse of the sigmoid (expit) function,
scikit-learn/sklearn/ensemble/_gb_losses.py,708,create one-hot label encoding,
scikit-learn/sklearn/ensemble/_gb_losses.py,753,prevents overflow and division by zero,
scikit-learn/sklearn/ensemble/_gb_losses.py,794,we only need to fit one tree for binary clf.,
scikit-learn/sklearn/ensemble/_gb_losses.py,849,prevents overflow and division by zero,
scikit-learn/sklearn/ensemble/_gb_losses.py,869,"according to The Elements of Statistical Learning sec. 10.5, the",
scikit-learn/sklearn/ensemble/_gb_losses.py,870,minimizer of the exponential loss is .5 * log odds ratio. So this is,
scikit-learn/sklearn/ensemble/_gb_losses.py,871,the equivalent to .5 * binomial_deviance.get_init_raw_predictions(),
scikit-learn/sklearn/ensemble/_gb_losses.py,881,"for both, multinomial and binomial",
scikit-learn/sklearn/ensemble/_stacking.py,3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>,
scikit-learn/sklearn/ensemble/_stacking.py,4,License: BSD 3 clause,
scikit-learn/sklearn/ensemble/_stacking.py,73,case where the the estimator returned a 1D array,
scikit-learn/sklearn/ensemble/_stacking.py,79,Remove the first column when using probabilities in,
scikit-learn/sklearn/ensemble/_stacking.py,80,binary classification because both features are perfectly,
scikit-learn/sklearn/ensemble/_stacking.py,81,collinear.,
scikit-learn/sklearn/ensemble/_stacking.py,134,"all_estimators contains all estimators, the one to be fitted and the",
scikit-learn/sklearn/ensemble/_stacking.py,135,'drop' string.,
scikit-learn/sklearn/ensemble/_stacking.py,141,Fit the base estimators on the whole training data. Those,
scikit-learn/sklearn/ensemble/_stacking.py,142,"base estimators will be used in transform, predict, and",
scikit-learn/sklearn/ensemble/_stacking.py,143,predict_proba. They are exposed publicly.,
scikit-learn/sklearn/ensemble/_stacking.py,160,"To train the meta-classifier using the most data as possible, we use",
scikit-learn/sklearn/ensemble/_stacking.py,161,a cross-validation to obtain the output of the stacked estimators.,
scikit-learn/sklearn/ensemble/_stacking.py,163,"To ensure that the data provided to each estimator are the same, we",
scikit-learn/sklearn/ensemble/_stacking.py,164,need to set the random state of the cv if there is one and we need to,
scikit-learn/sklearn/ensemble/_stacking.py,165,take a copy.,
scikit-learn/sklearn/ensemble/_stacking.py,186,Only not None or not 'drop' estimators will be used in transform.,
scikit-learn/sklearn/ensemble/_stacking.py,187,Remove the None from the method as well.,
scikit-learn/sklearn/ensemble/_forest.py,35,Authors: Gilles Louppe <g.louppe@gmail.com>,
scikit-learn/sklearn/ensemble/_forest.py,36,Brian Holt <bdholt1@gmail.com>,
scikit-learn/sklearn/ensemble/_forest.py,37,Joly Arnaud <arnaud.v.joly@gmail.com>,
scikit-learn/sklearn/ensemble/_forest.py,38,Fares Hedayati <fares.hedayati@gmail.com>,
scikit-learn/sklearn/ensemble/_forest.py,39,,
scikit-learn/sklearn/ensemble/_forest.py,40,License: BSD 3 clause,
scikit-learn/sklearn/ensemble/_forest.py,296,Validate or convert input data,
scikit-learn/sklearn/ensemble/_forest.py,307,Pre-sort indices to avoid that each individual tree of the,
scikit-learn/sklearn/ensemble/_forest.py,308,ensemble sorts the indices.,
scikit-learn/sklearn/ensemble/_forest.py,311,Remap output,
scikit-learn/sklearn/ensemble/_forest.py,322,reshape is necessary to preserve the data contiguity against vs,
scikit-learn/sklearn/ensemble/_forest.py,323,"[:, np.newaxis] that does not.",
scikit-learn/sklearn/ensemble/_forest.py,339,Get bootstrap sample size,
scikit-learn/sklearn/ensemble/_forest.py,345,Check parameters,
scikit-learn/sklearn/ensemble/_forest.py,355,"Free allocated memory, if any",
scikit-learn/sklearn/ensemble/_forest.py,370,We draw from the random state to get the random state we,
scikit-learn/sklearn/ensemble/_forest.py,371,would have got if we hadn't used a warm_start.,
scikit-learn/sklearn/ensemble/_forest.py,378,Parallel loop: we prefer the threading backend as the Cython code,
scikit-learn/sklearn/ensemble/_forest.py,379,for fitting the trees is internally releasing the Python GIL,
scikit-learn/sklearn/ensemble/_forest.py,380,making threading more efficient than multiprocessing in,
scikit-learn/sklearn/ensemble/_forest.py,381,"that case. However, for joblib 0.12+ we respect any",
scikit-learn/sklearn/ensemble/_forest.py,382,"parallel_backend contexts set at a higher level,",
scikit-learn/sklearn/ensemble/_forest.py,383,since correctness does not rely on using threads.,
scikit-learn/sklearn/ensemble/_forest.py,392,Collect newly grown trees,
scikit-learn/sklearn/ensemble/_forest.py,398,Decapsulate classes_ attributes,
scikit-learn/sklearn/ensemble/_forest.py,411,Default implementation,
scikit-learn/sklearn/ensemble/_forest.py,634,"all dtypes should be the same, so just take the first",
scikit-learn/sklearn/ensemble/_forest.py,670,Check data,
scikit-learn/sklearn/ensemble/_forest.py,673,Assign chunk of trees to jobs,
scikit-learn/sklearn/ensemble/_forest.py,676,avoid storing the output of every estimator by summing them here,
scikit-learn/sklearn/ensemble/_forest.py,780,Check data,
scikit-learn/sklearn/ensemble/_forest.py,783,Assign chunk of trees to jobs,
scikit-learn/sklearn/ensemble/_forest.py,786,avoid storing the output of every estimator by summing them here,
scikit-learn/sklearn/ensemble/_forest.py,792,Parallel loop,
scikit-learn/sklearn/ensemble/_forest.py,872,Note: we don't sum in parallel because the GIL isn't released in,
scikit-learn/sklearn/ensemble/_forest.py,873,the fast method.,
scikit-learn/sklearn/ensemble/_forest.py,876,Average over the forest,
scikit-learn/sklearn/ensemble/_forest.py,2329,Pre-sort indices to avoid that each individual tree of the,
scikit-learn/sklearn/ensemble/_forest.py,2330,ensemble sorts the indices.,
scikit-learn/sklearn/ensemble/__init__.py,26,Avoid errors in type checkers (e.g. mypy) for experimental estimators.,
scikit-learn/sklearn/ensemble/__init__.py,27,TODO: remove this check once the estimator is no longer experimental.,
scikit-learn/sklearn/ensemble/__init__.py,28,noqa,
scikit-learn/sklearn/ensemble/_weight_boosting.py,19,Authors: Noel Dawe <noel@dawe.me>,
scikit-learn/sklearn/ensemble/_weight_boosting.py,20,Gilles Louppe <g.louppe@gmail.com>,
scikit-learn/sklearn/ensemble/_weight_boosting.py,21,Hamzeh Alsalhi <ha258@cornell.edu>,
scikit-learn/sklearn/ensemble/_weight_boosting.py,22,Arnaud Joly <arnaud.v.joly@gmail.com>,
scikit-learn/sklearn/ensemble/_weight_boosting.py,23,,
scikit-learn/sklearn/ensemble/_weight_boosting.py,24,License: BSD 3 clause,
scikit-learn/sklearn/ensemble/_weight_boosting.py,100,Check parameters,
scikit-learn/sklearn/ensemble/_weight_boosting.py,116,Check parameters,
scikit-learn/sklearn/ensemble/_weight_boosting.py,119,Clear any previous fit results,
scikit-learn/sklearn/ensemble/_weight_boosting.py,124,Initializion of the random number instance that will be used to,
scikit-learn/sklearn/ensemble/_weight_boosting.py,125,generate a seed at each iteration,
scikit-learn/sklearn/ensemble/_weight_boosting.py,129,Boosting step,
scikit-learn/sklearn/ensemble/_weight_boosting.py,136,Early termination,
scikit-learn/sklearn/ensemble/_weight_boosting.py,143,Stop if error is zero,
scikit-learn/sklearn/ensemble/_weight_boosting.py,149,Stop if the sum of sample weights has become non-positive,
scikit-learn/sklearn/ensemble/_weight_boosting.py,154,Normalize,
scikit-learn/sklearn/ensemble/_weight_boosting.py,275,Displace zero probabilities so the log is defined.,
scikit-learn/sklearn/ensemble/_weight_boosting.py,276,Also fix negative elements which may occur with,
scikit-learn/sklearn/ensemble/_weight_boosting.py,277,negative sample weights.,
scikit-learn/sklearn/ensemble/_weight_boosting.py,438,Check that algorithm is supported,
scikit-learn/sklearn/ensemble/_weight_boosting.py,442,Fit,
scikit-learn/sklearn/ensemble/_weight_boosting.py,450,SAMME-R requires predict_proba-enabled base estimators,
scikit-learn/sklearn/ensemble/_weight_boosting.py,505,"elif self.algorithm == ""SAMME"":",
scikit-learn/sklearn/ensemble/_weight_boosting.py,524,Instances incorrectly classified,
scikit-learn/sklearn/ensemble/_weight_boosting.py,527,Error fraction,
scikit-learn/sklearn/ensemble/_weight_boosting.py,531,Stop if classification is perfect,
scikit-learn/sklearn/ensemble/_weight_boosting.py,535,Construct y coding as described in Zhu et al [2]:,
scikit-learn/sklearn/ensemble/_weight_boosting.py,536,,
scikit-learn/sklearn/ensemble/_weight_boosting.py,537,y_k = 1 if c == k else -1 / (K - 1),
scikit-learn/sklearn/ensemble/_weight_boosting.py,538,,
scikit-learn/sklearn/ensemble/_weight_boosting.py,539,"where K == n_classes_ and c, k in [0, K) are indices along the second",
scikit-learn/sklearn/ensemble/_weight_boosting.py,540,axis of the y coding with c being the index corresponding to the true,
scikit-learn/sklearn/ensemble/_weight_boosting.py,541,class label.,
scikit-learn/sklearn/ensemble/_weight_boosting.py,547,Displace zero probabilities so the log is defined.,
scikit-learn/sklearn/ensemble/_weight_boosting.py,548,Also fix negative elements which may occur with,
scikit-learn/sklearn/ensemble/_weight_boosting.py,549,negative sample weights.,
scikit-learn/sklearn/ensemble/_weight_boosting.py,550,alias for readability,
scikit-learn/sklearn/ensemble/_weight_boosting.py,553,Boost weight using multi-class AdaBoost SAMME.R alg,
scikit-learn/sklearn/ensemble/_weight_boosting.py,558,Only boost the weights if it will fit again,
scikit-learn/sklearn/ensemble/_weight_boosting.py,560,Only boost positive weights,
scikit-learn/sklearn/ensemble/_weight_boosting.py,579,Instances incorrectly classified,
scikit-learn/sklearn/ensemble/_weight_boosting.py,582,Error fraction,
scikit-learn/sklearn/ensemble/_weight_boosting.py,586,Stop if classification is perfect,
scikit-learn/sklearn/ensemble/_weight_boosting.py,592,Stop if the error is at least as bad as random guessing,
scikit-learn/sklearn/ensemble/_weight_boosting.py,601,Boost weight using multi-class AdaBoost SAMME alg,
scikit-learn/sklearn/ensemble/_weight_boosting.py,606,Only boost the weights if I will fit again,
scikit-learn/sklearn/ensemble/_weight_boosting.py,608,Only boost positive weights,
scikit-learn/sklearn/ensemble/_weight_boosting.py,701,The weights are all 1. for SAMME.R,
scikit-learn/sklearn/ensemble/_weight_boosting.py,704,"self.algorithm == ""SAMME""",
scikit-learn/sklearn/ensemble/_weight_boosting.py,750,The weights are all 1. for SAMME.R,
scikit-learn/sklearn/ensemble/_weight_boosting.py,752,"elif self.algorithm == ""SAMME"":",
scikit-learn/sklearn/ensemble/_weight_boosting.py,1001,Check loss,
scikit-learn/sklearn/ensemble/_weight_boosting.py,1006,Fit,
scikit-learn/sklearn/ensemble/_weight_boosting.py,1058,Weighted sampling of the training set with replacement,
scikit-learn/sklearn/ensemble/_weight_boosting.py,1064,Fit on the bootstrapped sample and obtain a prediction,
scikit-learn/sklearn/ensemble/_weight_boosting.py,1065,for all samples in the training set,
scikit-learn/sklearn/ensemble/_weight_boosting.py,1085,Calculate the average loss,
scikit-learn/sklearn/ensemble/_weight_boosting.py,1089,Stop if fit is perfect,
scikit-learn/sklearn/ensemble/_weight_boosting.py,1093,Discard current estimator only if it isn't the only one,
scikit-learn/sklearn/ensemble/_weight_boosting.py,1100,Boost weight using AdaBoost.R2 alg,
scikit-learn/sklearn/ensemble/_weight_boosting.py,1111,Evaluate predictions of all estimators,
scikit-learn/sklearn/ensemble/_weight_boosting.py,1115,Sort the predictions,
scikit-learn/sklearn/ensemble/_weight_boosting.py,1118,Find index of median prediction for each sample,
scikit-learn/sklearn/ensemble/_weight_boosting.py,1125,Return median predictions,
scikit-learn/sklearn/ensemble/setup.py,14,Histogram-based gradient boosting files,
scikit-learn/sklearn/ensemble/_base.py,3,Authors: Gilles Louppe,
scikit-learn/sklearn/ensemble/_base.py,4,License: BSD 3 clause,
scikit-learn/sklearn/ensemble/_base.py,109,overwrite _required_parameters from MetaEstimatorMixin,
scikit-learn/sklearn/ensemble/_base.py,115,Set parameters,
scikit-learn/sklearn/ensemble/_base.py,120,Don't instantiate estimators now! Parameters of base_estimator might,
scikit-learn/sklearn/ensemble/_base.py,121,"still change. Eg., when grid-searching with the nested object syntax.",
scikit-learn/sklearn/ensemble/_base.py,122,self.estimators_ needs to be filled by the derived classes in fit.,
scikit-learn/sklearn/ensemble/_base.py,178,Compute the number of jobs,
scikit-learn/sklearn/ensemble/_base.py,181,Partition estimators between jobs,
scikit-learn/sklearn/ensemble/_base.py,227,defined by MetaEstimatorMixin,
scikit-learn/sklearn/ensemble/_base.py,230,FIXME: deprecate the usage of None to drop an estimator from the,
scikit-learn/sklearn/ensemble/_base.py,231,ensemble. Remove in 0.24,
scikit-learn/sklearn/ensemble/_gb.py,19,"Authors: Peter Prettenhofer, Scott White, Gilles Louppe, Emanuele Olivetti,",
scikit-learn/sklearn/ensemble/_gb.py,20,"Arnaud Joly, Jacob Schreiber",
scikit-learn/sklearn/ensemble/_gb.py,21,License: BSD 3 clause,
scikit-learn/sklearn/ensemble/_gb.py,83,header fields and line format str,
scikit-learn/sklearn/ensemble/_gb.py,86,do oob?,
scikit-learn/sklearn/ensemble/_gb.py,93,print the header line,
scikit-learn/sklearn/ensemble/_gb.py,98,plot verbose info each time i % verbose_mod == 0,
scikit-learn/sklearn/ensemble/_gb.py,114,we need to take into account if we fit additional estimators.,
scikit-learn/sklearn/ensemble/_gb.py,115,iteration relative to the start iter,
scikit-learn/sklearn/ensemble/_gb.py,129,adjust verbose frequency (powers of 10),
scikit-learn/sklearn/ensemble/_gb.py,178,Need to pass a copy of raw_predictions to negative_gradient(),
scikit-learn/sklearn/ensemble/_gb.py,179,because raw_predictions is partially updated at the end of the loop,
scikit-learn/sklearn/ensemble/_gb.py,180,"in update_terminal_regions(), and gradients need to be evaluated at",
scikit-learn/sklearn/ensemble/_gb.py,181,iteration i - 1.,
scikit-learn/sklearn/ensemble/_gb.py,191,induce regression tree on residuals,
scikit-learn/sklearn/ensemble/_gb.py,207,no inplace multiplication!,
scikit-learn/sklearn/ensemble/_gb.py,214,update tree leaves,
scikit-learn/sklearn/ensemble/_gb.py,219,add tree to ensemble,
scikit-learn/sklearn/ensemble/_gb.py,255,init must be an estimator or 'zero',
scikit-learn/sklearn/ensemble/_gb.py,270,if is_classification,
scikit-learn/sklearn/ensemble/_gb.py,274,is regression,
scikit-learn/sklearn/ensemble/_gb.py,288,float,
scikit-learn/sklearn/ensemble/_gb.py,321,do oob?,
scikit-learn/sklearn/ensemble/_gb.py,341,self.n_estimators is the number of additional est to fit,
scikit-learn/sklearn/ensemble/_gb.py,351,if do oob resize arrays or create new if not available,
scikit-learn/sklearn/ensemble/_gb.py,401,if not warmstart - clear the estimator state,
scikit-learn/sklearn/ensemble/_gb.py,405,Check input,
scikit-learn/sklearn/ensemble/_gb.py,406,"Since check_array converts both X and y to the same dtype, but the",
scikit-learn/sklearn/ensemble/_gb.py,407,"trees use different types for X and y, checking them separately.",
scikit-learn/sklearn/ensemble/_gb.py,429,We choose to error here. The problem is that the init,
scikit-learn/sklearn/ensemble/_gb.py,430,"estimator would be trained on y, which has some missing",
scikit-learn/sklearn/ensemble/_gb.py,431,"classes now, so its predictions would not have the",
scikit-learn/sklearn/ensemble/_gb.py,432,correct shape.,
scikit-learn/sklearn/ensemble/_gb.py,444,init state,
scikit-learn/sklearn/ensemble/_gb.py,447,fit initial model and initialize raw predictions,
scikit-learn/sklearn/ensemble/_gb.py,452,XXX clean this once we have a support_sample_weight tag,
scikit-learn/sklearn/ensemble/_gb.py,460,regular estimator without SW support,
scikit-learn/sklearn/ensemble/_gb.py,465,pipeline,
scikit-learn/sklearn/ensemble/_gb.py,467,regular estimator whose input checking failed,
scikit-learn/sklearn/ensemble/_gb.py,475,The rng state must be preserved if warm_start is True,
scikit-learn/sklearn/ensemble/_gb.py,479,add more estimators to fitted model,
scikit-learn/sklearn/ensemble/_gb.py,480,invariant: warm_start = True,
scikit-learn/sklearn/ensemble/_gb.py,488,The requirements of _decision_function (called in two lines,
scikit-learn/sklearn/ensemble/_gb.py,489,below) are more constrained than fit. It accepts only CSR,
scikit-learn/sklearn/ensemble/_gb.py,490,matrices.,
scikit-learn/sklearn/ensemble/_gb.py,497,fit the boosting stages,
scikit-learn/sklearn/ensemble/_gb.py,502,change shape of arrays after fit (early-stopping or additional ests),
scikit-learn/sklearn/ensemble/_gb.py,537,We create a generator to get the predictions for X_val after,
scikit-learn/sklearn/ensemble/_gb.py,538,the addition of each successive stage,
scikit-learn/sklearn/ensemble/_gb.py,541,perform boosting iterations,
scikit-learn/sklearn/ensemble/_gb.py,545,subsampling,
scikit-learn/sklearn/ensemble/_gb.py,549,OOB score before adding this stage,
scikit-learn/sklearn/ensemble/_gb.py,554,fit next stage of trees,
scikit-learn/sklearn/ensemble/_gb.py,559,track deviance (= loss),
scikit-learn/sklearn/ensemble/_gb.py,569,no need to fancy index w/ no subsampling,
scikit-learn/sklearn/ensemble/_gb.py,580,We also provide an early stopping based on the score from,
scikit-learn/sklearn/ensemble/_gb.py,581,"validation set (X_val, y_val), if n_iter_no_change is set",
scikit-learn/sklearn/ensemble/_gb.py,583,"By calling next(y_val_pred_iter), we get the predictions",
scikit-learn/sklearn/ensemble/_gb.py,584,for X_val after the addition of the current stage,
scikit-learn/sklearn/ensemble/_gb.py,588,Require validation_score to be better (less) than at least,
scikit-learn/sklearn/ensemble/_gb.py,589,one of the last n_iter_no_change evaluations,
scikit-learn/sklearn/ensemble/_gb.py,598,we don't need _make_estimator,
scikit-learn/sklearn/ensemble/_gb.py,677,degenerate case where all trees have only one node,
scikit-learn/sklearn/ensemble/_gb.py,727,'sample_weight' is not utilised but is used for,
scikit-learn/sklearn/ensemble/_gb.py,728,consistency with similar method _validate_y of GBC,
scikit-learn/sklearn/ensemble/_gb.py,732,Default implementation,
scikit-learn/sklearn/ensemble/_gb.py,758,n_classes will be equal to 1 in the binary classification or the,
scikit-learn/sklearn/ensemble/_gb.py,759,regression case.,
scikit-learn/sklearn/ensemble/_gb.py,1610,In regression we can directly return the raw value from the trees.,
scikit-learn/sklearn/ensemble/_iforest.py,1,Authors: Nicolas Goix <nicolas.goix@telecom-paristech.fr>,
scikit-learn/sklearn/ensemble/_iforest.py,2,Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>,
scikit-learn/sklearn/ensemble/_iforest.py,3,License: BSD 3 clause,
scikit-learn/sklearn/ensemble/_iforest.py,202,here above max_features has no links with self.max_features,
scikit-learn/sklearn/ensemble/_iforest.py,220,"ExtraTreeRegressor releases the GIL, so it's more efficient to use",
scikit-learn/sklearn/ensemble/_iforest.py,221,a thread-based backend rather than a process-based backend so as,
scikit-learn/sklearn/ensemble/_iforest.py,222,to avoid suffering from communication overhead and extra memory,
scikit-learn/sklearn/ensemble/_iforest.py,223,copies.,
scikit-learn/sklearn/ensemble/_iforest.py,263,Pre-sort indices to avoid that each individual tree of the,
scikit-learn/sklearn/ensemble/_iforest.py,264,ensemble sorts the indices.,
scikit-learn/sklearn/ensemble/_iforest.py,270,"ensure that max_sample is in [1, n_samples]:",
scikit-learn/sklearn/ensemble/_iforest.py,290,float,
scikit-learn/sklearn/ensemble/_iforest.py,303,0.5 plays a special role as described in the original paper.,
scikit-learn/sklearn/ensemble/_iforest.py,304,we take the opposite as we consider the opposite of their score.,
scikit-learn/sklearn/ensemble/_iforest.py,308,"else, define offset_ wrt contamination parameter",
scikit-learn/sklearn/ensemble/_iforest.py,364,We subtract self.offset_ to make 0 be the threshold value for being,
scikit-learn/sklearn/ensemble/_iforest.py,365,an outlier:,
scikit-learn/sklearn/ensemble/_iforest.py,393,code structure from ForestClassifier/predict_proba,
scikit-learn/sklearn/ensemble/_iforest.py,396,Check data,
scikit-learn/sklearn/ensemble/_iforest.py,404,Take the opposite of the scores as bigger is better (here less,
scikit-learn/sklearn/ensemble/_iforest.py,405,abnormal),
scikit-learn/sklearn/ensemble/_iforest.py,417,We get as many rows as possible within our working_memory budget,
scikit-learn/sklearn/ensemble/_iforest.py,418,(defined by sklearn.get_config()['working_memory']) to store,
scikit-learn/sklearn/ensemble/_iforest.py,419,self._max_features in each row during computation.,
scikit-learn/sklearn/ensemble/_iforest.py,420,,
scikit-learn/sklearn/ensemble/_iforest.py,421,Note:,
scikit-learn/sklearn/ensemble/_iforest.py,422,"- this will get at least 1 row, even if 1 row of score will",
scikit-learn/sklearn/ensemble/_iforest.py,423,exceed working_memory.,
scikit-learn/sklearn/ensemble/_iforest.py,424,- this does only account for temporary memory usage while loading,
scikit-learn/sklearn/ensemble/_iforest.py,425,the data needed to compute the scores -- the returned scores,
scikit-learn/sklearn/ensemble/_iforest.py,426,themselves are 1D.,
scikit-learn/sklearn/ensemble/_iforest.py,435,compute score on the slices of test samples:,
scikit-learn/sklearn/ensemble/_voting.py,9,"Authors: Sebastian Raschka <se.raschka@gmail.com>,",
scikit-learn/sklearn/ensemble/_voting.py,10,"Gilles Louppe <g.louppe@gmail.com>,",
scikit-learn/sklearn/ensemble/_voting.py,11,Ramil Nugmanov <stsouko@live.ru>,
scikit-learn/sklearn/ensemble/_voting.py,12,Mohamed Ali Jamaoui <m.ali.jamaoui@gmail.com>,
scikit-learn/sklearn/ensemble/_voting.py,13,,
scikit-learn/sklearn/ensemble/_voting.py,14,License: BSD 3 clause,
scikit-learn/sklearn/ensemble/_voting.py,85,Uses None or 'drop' as placeholder for dropped estimators,
scikit-learn/sklearn/ensemble/_voting.py,95,For consistency with other estimators we raise a AttributeError so,
scikit-learn/sklearn/ensemble/_voting.py,96,that hasattr() fails if the estimator isn't fitted.,
scikit-learn/sklearn/ensemble/_voting.py,273,'hard' voting,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/predictor.py,4,Author: Nicolas Hug,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/binning.py,8,Author: Nicolas Hug,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/binning.py,58,ignore missing values when computing bin thresholds,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/binning.py,68,We sort again the data in this case. We could compute,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/binning.py,69,approximate midpoint percentiles using the output of,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/binning.py,70,"np.unique(col_data, return_counts) instead but this is more",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/binning.py,71,work and the performance benefit will be limited because we,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/binning.py,72,work on a fixed-size subsample of the full data.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/binning.py,79,We avoid having +inf thresholds: +inf thresholds are only allowed in,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/binning.py,80,"a ""split on nan"" situation.",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/binning.py,160,min is 3: at least 2 distinct bins and a missing values bin,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,7,Author: Nicolas Hug,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,39,This variable indicates whether the loss requires the leaves values to,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,40,be updated once the tree has been trained. The trees are trained to,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,41,predict a Newton-Raphson step (see grower._finalize_leaf()). But for,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,42,some losses (e.g. least absolute deviation) we need to adjust the tree,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,43,"values to account for the ""line search"" of the gradient descent",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,44,procedure. See the original paper Greedy Function Approximation: A,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,45,Gradient Boosting Machine by Friedman,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,46,(https://statweb.stanford.edu/~jhf/ftp/trebst.pdf) for the theory.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,83,"If the hessians are constant, we consider they are equal to 1.",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,84,- This is correct for the half LS loss,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,85,"- For LAD loss, hessians are actually 0, but they are always",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,86,ignored anyway.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,157,"If sample weights are provided, the hessians and gradients",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,158,"are multiplied by sample_weight, which means the hessians are",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,159,equal to sample weights.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,163,"shape (1, n_samples) --> (n_samples,). reshape(-1) is more likely to",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,164,return a view.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,178,"shape (1, n_samples) --> (n_samples,). reshape(-1) is more likely to",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,179,return a view.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,199,"If sample weights are provided, the hessians and gradients",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,200,"are multiplied by sample_weight, which means the hessians are",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,201,equal to sample weights.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,204,This variable indicates whether the loss requires the leaves values to,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,205,be updated once the tree has been trained. The trees are trained to,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,206,predict a Newton-Raphson step (see grower._finalize_leaf()). But for,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,207,some losses (e.g. least absolute deviation) we need to adjust the tree,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,208,"values to account for the ""line search"" of the gradient descent",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,209,procedure. See the original paper Greedy Function Approximation: A,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,210,Gradient Boosting Machine by Friedman,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,211,(https://statweb.stanford.edu/~jhf/ftp/trebst.pdf) for the theory.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,215,"shape (1, n_samples) --> (n_samples,). reshape(-1) is more likely to",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,216,return a view.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,233,"shape (1, n_samples) --> (n_samples,). reshape(-1) is more likely to",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,234,return a view.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,247,Update the values predicted by the tree with,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,248,median(y_true - raw_predictions).,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,249,See note about need_update_leaves_values in BaseLoss.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,251,TODO: ideally this should be computed in parallel over the leaves,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,252,"using something similar to _update_raw_predictions(), but this",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,253,requires a cython version of median(),
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,265,Note that the regularization is ignored here,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,286,"shape (1, n_samples) --> (n_samples,). reshape(-1) is more likely to",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,287,return a view.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,289,"logaddexp(0, x) = log(1 + exp(x))",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,302,"log(x / 1 - x) is the anti function of sigmoid, or the link function",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,303,of the Binomial model.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,308,"shape (1, n_samples) --> (n_samples,). reshape(-1) is more likely to",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,309,return a view.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,317,"shape (1, n_samples) --> (n_samples,). reshape(-1) is more likely to",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,318,return a view.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,364,TODO: This could be done in parallel,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,365,compute softmax (using exp(log(softmax))),
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,2,Author: Nicolas Hug,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,114,time spent finding the best splits,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,115,time spent splitting nodes,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,116,time spent computing histograms,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,117,time spent predicting X for gradient and hessians update,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,123,Do not create unit sample weights by default to later skip some,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,124,computation,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,128,TODO: remove when PDP suports sample weights,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,133,"When warm starting, we want to re-use the same seed that was used",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,134,the first time fit was called (e.g. for subsampling or for the,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,135,train/val split).,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,141,used for validation in predict,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,143,we need this stateful variable to tell raw_predict() that it was,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,144,"called from fit() (this current method), and that the data it has",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,145,received is pre-binned.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,146,"predicting is faster on pre-binned data, so we want early stopping",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,147,predictions to be made on pre-binned data. Unfortunately the scorer_,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,148,"can only call predict() or predict_proba(), not raw_predict(), and",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,149,there's no way to tell the scorer that it needs to predict binned,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,150,data.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,159,create validation data if needed,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,162,stratify for classification,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,165,Save the state of the RNG for the training and validation split.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,166,This is needed in order to have the same split when using,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,167,warm starting.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,176,"TODO: incorporate sample_weight in sampling here, as well as",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,177,stratify,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,189,Bin the data,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,190,"For ease of use of the API, the user-facing GBDT classes accept the",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,191,"parameter max_bins, which doesn't take into account the bin for",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,192,"missing values (which is always allocated). However, since max_bins",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,193,"isn't the true maximal number of bins, all other private classes",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,194,"(binmapper, histbuilder...) accept n_bins instead, which is the",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,195,"actual total number of bins. Everywhere in the code, the",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,196,convention is that n_bins == max_bins + 1,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,197,+ 1 for missing values,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,211,"First time calling fit, or no warm start",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,213,Clear random state and score attributes,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,216,initialize raw_predictions: those are the accumulated values,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,217,predicted by the trees for the training data. raw_predictions has,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,218,"shape (n_trees_per_iteration, n_samples) where",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,219,"n_trees_per_iterations is n_classes in multiclass classification,",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,220,else 1.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,230,predictors is a matrix (list of lists) of TreePredictor objects,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,231,"with shape (n_iter_, n_trees_per_iteration)",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,234,Initialize structures and attributes related to early stopping,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,235,set if scoring != loss,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,236,set if scoring == loss and use val,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,241,populate train_score and validation_score with the,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,242,predictions of the initial model (before the first tree),
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,245,we're going to compute scoring w.r.t the loss. As losses,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,246,"take raw predictions as input (unlike the scorers), we",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,247,can optimize a bit and avoid repeating computing the,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,248,predictions of the previous trees. We'll re-use,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,249,raw_predictions (as it's needed for training anyway) for,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,250,"evaluating the training loss, and create",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,251,raw_predictions_val for storing the raw predictions of,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,252,the validation data.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,269,"scorer_ is a callable with signature (est, X, y) and",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,270,calls est.predict() or est.predict_proba() depending on,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,271,its nature.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,272,"Unfortunately, each call to scorer_() will compute",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,273,the predictions of all the trees. So we use a subset of,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,274,the training set to compute train scores.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,276,Compute the subsample set,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,290,warm start: this is not the first time fit was called,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,292,Check that the maximum number of iterations is not smaller,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,293,than the number of iterations from the previous fit,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,301,Convert array attributes to lists,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,305,Compute raw predictions,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,313,Compute the subsample set,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,320,Get the predictors from the previous fit,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,325,initialize gradients and hessians (empty arrays).,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,326,"shape = (n_trees_per_iteration, n_samples).",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,340,"Update gradients and hessians, inplace",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,345,Append a list since there may be more than 1 predictor per iter,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,348,Build `n_trees_per_iteration` trees.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,377,Update raw_predictions with the predictions of the newly,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,378,created tree.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,387,Update raw_predictions_val with the newest tree(s),
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,412,maybe we could also early stop if all the trees are stumps?,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,439,hard delete so we're sure it can't be used anymore,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,458,TODO: incorporate sample_weights here in `resample`,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,547,A higher score is always better. Higher tol means that it will be,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,548,harder for subsequent iteration to be considered an improvement upon,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,549,"the reference score, and therefore it is more likely to early stop",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,550,because of the lack of significant improvement.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,571,F-aligned array,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,573,F-aligned array,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,574,We convert the array to C-contiguous since predicting is faster,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,575,with this layout (training is faster on F-arrays though),
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,608,score_ arrays contain the negative loss,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,657,noqa,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,698,Note that the learning rate is already accounted for in the leaves,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,699,values.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,899,Return raw predictions after converting shape,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,900,"(n_samples, 1) to (n_samples,)",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,904,Just convert y to the expected dtype,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,1098,TODO: This could be done in parallel,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,1140,encode classes into 0 ... n_classes - 1 and sets attributes classes_,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,1141,and n_trees_per_iteration_,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,1148,"only 1 tree for binary classification. For multiclass classification,",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,1149,we build 1 tree per class.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,7,Author: Nicolas Hug,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,23,to avoid zero division errors,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,79,start and stop indices of the node in the splitter.partition,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,80,"array. Concretely,",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,81,self.sample_indices = view(self.splitter.partition[start:stop]),
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,82,Please see the comments about splitter.partition and,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,83,splitter.split_indices for more info about this design.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,84,"These 2 attributes are only used in _update_raw_prediction, because we",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,85,need to iterate over the leaves and I don't know how to efficiently,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,86,store the sample_indices views because they're all of different sizes.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,105,"These are bounds for the node's *children* values, not the node's",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,106,value. The bounds are used in the splitter when considering potential,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,107,left and right child.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,248,time spent finding the best splits,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,249,time spent computing histograms,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,250,time spent splitting nodes,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,327,Do not even bother computing any splitting statistics.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,352,no valid split,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,367,Consider the node with the highest loss reduction (a.k.a. gain),
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,401,set start and stop indices,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,408,"If no missing values are encountered at fit time, then samples",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,409,with missing values during predict() will go to whichever child,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,410,has the most samples.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,434,Set value bounds for respecting monotonic constraints,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,435,See test_nodes_values() for details,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,446,NEG,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,452,"Compute histograms of children, and compute their best possible split",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,453,(if needed),
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,458,We will compute the histograms of both nodes even if one of them,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,459,"is a leaf, since computing the second histogram is very cheap",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,460,(using histogram subtraction).,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,470,We use the brute O(n_samples) method on the child that has the,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,471,"smallest number of samples, and the subtraction trick O(n_bins)",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,472,on the other one.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,539,Leaf node,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,543,Decision node,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,551,"Split is on the last non-missing bin: it's a ""split on nans"". All",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,552,"nans go to the right, the rest go to the left.",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,19,"create gradients and hessians array, update inplace, and return",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,27,"create gradients and hessians array, update inplace, and return",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,34,hessians aren't updated because they're constant:,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,35,the value is 1 (and not 2) because the loss is actually an half,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,36,least squares loss.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,39,hessians aren't updated because they're constant,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,51,I don't understand why but y_true == 0 fails :/,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,52,"('binary_crossentropy', 0.3, 0),",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,61,Check that gradients are zero when the loss is minimized on 1D array,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,62,using Halley's method with the first and second order derivatives,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,63,computed by the Loss instance.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,94,"Make sure gradients and hessians computed in the loss are correct, by",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,95,comparing with their approximations computed with finite central,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,96,differences.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,97,See https://en.wikipedia.org/wiki/Finite_difference.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,111,only take gradients and hessians of first tree / class.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,115,Approximate gradients,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,116,"For multiclass loss, we should only change the predictions of one tree",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,117,"(here the first), hence the use of offset[:, 0] += eps",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,118,"As a softmax is computed, offsetting the whole array by a constant would",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,119,"have no effect on the probabilities, and thus on the loss",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,127,Approximate hessians,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,128,need big enough eps as we divide by its square,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,145,scalar,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,147,Make sure baseline prediction is the mean of all targets,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,159,scalar,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,161,Make sure baseline prediction is the median of all targets,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,178,"Make sure baseline prediction is equal to link_function(p), where p",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,179,"is the proba of the positive class. We want predict_proba() to return p,",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,180,and by definition,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,181,p = inverse_link_function(raw_prediction) = sigmoid(raw_prediction),
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,182,So we want raw_prediction = link_function(p) = log(p / (1 - p)),
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,185,scalar,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,203,Same logic as for above test. Here inverse_link_function = softmax and,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,204,link_function = log,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,222,Make sure that passing sample weights to the gradient and hessians,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,223,computation methods is equivalent to multiplying by the weights.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,267,Make sure that passing sample_weight to a loss correctly influences the,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,268,"hessians_are_constant attribute, and consequently the shape of the",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,269,hessians array.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,7,"To use this experimental feature, we need to explicitly ask for it:",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,8,noqa,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,24,Make sure sklearn has the same predictions as lightgbm for easy targets.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,25,,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,26,In particular when the size of the trees are bound and the number of,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,27,"samples is large enough, the structure of the prediction trees found by",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,28,LightGBM and sklearn should be exactly identical.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,29,,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,30,Notes:,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,31,- Several candidate splits may have equal gains when the number of,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,32,samples in a node is low (and because of float errors). Therefore the,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,33,predictions on the test set might differ if the structure of the tree,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,34,is not exactly the same. To avoid this issue we only compare the,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,35,predictions on the test set when the number of samples is large enough,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,36,and max_leaf_nodes is low enough.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,37,- To ignore  discrepancies caused by small differences the binning,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,38,"strategy, data is pre-binned if n_samples > 255.",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,39,- We don't check the least_absolute_deviation loss here. This is because,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,40,LightGBM's computation of the median (used for the initial value of,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,41,raw_prediction) is a bit off (they'll e.g. return midpoints when there,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,42,"is no need to.). Since these tests only run 1 iteration, the",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,43,discrepancy between the initial values leads to biggish differences in,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,44,the predictions. These differences are much smaller with more,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,45,iterations.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,57,bin data and convert it to float32 so that the estimator doesn't,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,58,treat it as pre-binned,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,75,"We need X to be treated an numerical data, not pre-binned data.",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,80,less than 1% of the predictions are different up to the 3rd decimal,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,86,less than 1% of the predictions are different up to the 4th decimal,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,98,Same as test_same_predictions_regression but for classification,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,110,bin data and convert it to float32 so that the estimator doesn't,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,111,treat it as pre-binned,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,129,"We need X to be treated an numerical data, not pre-binned data.",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,159,Same as test_same_predictions_regression but for classification,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,173,bin data and convert it to float32 so that the estimator doesn't,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,174,treat it as pre-binned,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,192,"We need X to be treated an numerical data, not pre-binned data.",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,201,assert more than 75% of the predicted probabilities are the same up to,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,202,the second decimal,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,217,assert more than 75% of the predicted probabilities are the same up,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,218,to the second decimal,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_predictor.py,23,Init gradients and hessians to that of least squares loss,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_predictor.py,49,Make sure infinite values and infinite thresholds are handled properly.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_predictor.py,50,"In particular, if a value is +inf and the threshold is ALMOST_INF the",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_predictor.py,51,sample should go to the right child. If the threshold is inf (split on,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_predictor.py,52,"nan), the +inf sample will go to the left child.",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_predictor.py,57,We just construct a simple tree with 1 root and 2 children,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_predictor.py,58,parent node,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_predictor.py,64,left child,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_predictor.py,68,right child,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,22,Just a redef to avoid having to pass arguments all the time (as the,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,23,function is private we don't use default values for parameters),
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,60,255 - 1,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,75,128 - 1,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,122,max_bins is the number of bins for non-missing values,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,138,Check that the binned data is approximately balanced across bins.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,155,max_bins is the number of bins for non-missing values,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,173,max_bins is the number of bins for non-missing values,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,196,Adding more bins to the mapper yields the same results (same thresholds),
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,211,max_bins is the number of bins for non-missing values,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,239,Check that n_bins_non_missing is n_unique_values when,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,240,"there are not a lot of unique values, else n_bins - 1.",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,251,Make sure bin thresholds are different when applying subsampling,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,263,255 <=> missing value,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,269,2 <=> missing value,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,276,check for missing values: make sure nans are mapped to the last bin,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,277,and that the _BinMapper attributes are correct,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,304,Make sure infinite values are properly handled.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,16,Generate some test data directly binned so as to test the grower code,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,17,independently of the binning logic.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,37,Assume a square loss applied to an initial model that always predicts 0,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,38,(hardcoded for this test):,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,47,Make sure the samples are correctly dispatched from a parent to its,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,48,children,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,52,each sample from the parent is propagated to one of the two children,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,59,"samples are sent either to the left or the right node, never to both",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,91,"The root node is not yet splitted, but the best possible split has",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,92,already been evaluated:,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,101,Calling split next applies the next split and computes the best split,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,102,for each of the two newly introduced children nodes.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,105,"All training samples have ben splitted in the two nodes, approximately",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,106,50%/50%,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,112,The left node is too pure: there is no gain to split it further.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,116,"The right node can still be splitted further, this time on feature #1",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,124,The right split has not been applied yet. Let's do it now:,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,134,"All the leafs are pure, it is not possible to split any further:",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,139,Check the values of the leaves:,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,147,Build a tree on the toy 3-leaf dataset to extract the predictor.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,155,(2 decision nodes + 3 leaves),
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,157,Check that the node structure can be converted into a predictor,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,158,object to perform predictions at scale,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,163,Probe some predictions for each leaf of the tree,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,164,each group of 3 samples corresponds to a condition in _make_training_data,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,183,Check that training set can be recovered exactly:,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,203,"data = linear target, 3 features, 1 irrelevant.",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,237,Make sure root node isn't split if n_samples is not at least twice,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,238,min_samples_leaf,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,243,"data = linear target, 3 features, 1 irrelevant.",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,263,To assert that stumps are created when max_depth=1,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,271,Make sure max_depth parameter works as expected,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,277,"data = linear target, 3 features, 1 irrelevant.",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,326,Make sure that missing values are supported at predict time even if they,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,327,were not encountered in the training data: the missing values are,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,328,assigned to whichever child has the most samples.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,344,"go from root to a leaf, always following node with the most samples.",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,345,That's the path nans are supposed to take,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,354,"now build X_test with only nans, and make sure all predictions are equal",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,355,to prediction_main_path,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,361,Make sure the split on nan situations are respected even when there are,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,362,samples with +inf values (we set the threshold to +inf when we have a,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,363,split on nan so this test makes sure this does not introduce edge-case,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,364,bugs). We need to use the private API so that we can also test,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,365,predict_binned().,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,368,the gradient values will force a split on nan situation,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,388,sanity check: this was a split on nan,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,392,Make sure in particular that the +inf sample is mapped to the left child,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,393,"Note that lightgbm ""fails"" here and will assign the inf sample to the",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,394,"right child, even though it's a ""split on nan"" situation.",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,13,noqa,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,27,make sure leaves values (from left to right) are either all increasing,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,28,or all decreasing (or neither) depending on the monotonic constraint.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,43,start at root (0),
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,49,"some increasing, some decreasing",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,52,all increasing,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,54,NEG,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,55,all decreasing,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,60,Make sure siblings values respect the monotonic constraints. Left should,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,61,be lower (resp greater) than right child if constraint is POS (resp.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,62,NEG).,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,63,"Note that this property alone isn't enough to ensure full monotonicity,",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,64,since we also need to guanrantee that all the descendents of the left,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,65,"child won't be greater (resp. lower) than the right child, or its",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,66,descendents. That's why we need to bound the predicted values (this is,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,67,tested in assert_children_values_bounded),
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,87,NEG,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,92,Make sure that the values of the children of a node are bounded by the,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,93,middle value between that node and its sibling (if there is a monotonic,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,94,constraint).,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,95,"As a bonus, we also check that the siblings values are properly ordered",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,96,which is slightly redundant with assert_children_values_monotonic (but,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,97,this check is done on the grower nodes whereas,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,98,assert_children_values_monotonic is done on the predictor nodes),
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,107,on the right,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,117,NEG,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,139,"Build a single tree with only one feature, and make sure the nodes",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,140,values respect the monotonic constraints.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,142,"Considering the following tree with a monotonic POS constraint, we",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,143,should have:,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,144,,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,145,root,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,146,/    \,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,147,5     10    # middle = 7.5,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,148,/ \   / \,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,149,a  b  c  d,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,150,,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,151,a <= b and c <= d  (assert_children_values_monotonic),
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,152,"a, b <= middle <= c, d (assert_children_values_bounded)",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,153,a <= b <= c <= d (assert_leaves_values_monotonic),
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,154,,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,155,"The last one is a consequence of the others, but can't hurt to check",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,172,grow() will shrink the leaves values at the very end. For our comparison,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,173,"tests, we need to revert the shrinkage of the leaves, else we would",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,174,compare the value of a leaf (shrunk) with a node (not shrunk) and the,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,175,test would not be correct.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,179,The consistency of the bounds can only be checked on the tree grower,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,180,as the node bounds are not copied into the predictor tree. The,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,181,consistency checks on the values of node children and leaves can be,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,182,done either on the grower tree or on the predictor tree. We only,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,183,do those checks on the predictor tree as the latter is derived from,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,184,the former.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,193,Train a model with a POS constraint on the first feature and a NEG,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,194,"constraint on the second feature, and make sure the constraints are",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,195,respected by checking the predictions.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,196,"test adapted from lightgbm's test_monotone_constraint(), itself inspired",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,197,by https://xgboost.readthedocs.io/en/latest/tutorials/monotonic.html,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,202,positive correlation with y,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,203,negative correslation with y,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,217,"We now assert the predictions properly respect the constraints, on each",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,218,feature. When testing for a feature we need to set the other one to a,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,219,"constant, because the monotonic constraints are only a ""all else being",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,220,"equal"" type of constraints:",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,221,a constraint on the first feature only means that,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,222,"x0 < x0' => f(x0, x1) < f(x0', x1)",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,223,while x1 stays constant.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,224,The constraint does not guanrantee that,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,225,"x0 < x0' => f(x0, x1) < f(x0', x1')",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,227,First feature (POS),
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,228,assert pred is all increasing when f_0 is all increasing,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,232,assert pred actually follows the variations of f_0,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,237,Second feature (NEG),
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,238,assert pred is all decreasing when f_1 is all increasing,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,242,assert pred actually follows the inverse variations of f_1,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,274,The purpose of this test is to show that when computing the gain at a,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,275,"given split, the value of the current node should be properly bounded to",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,276,"respect the monotonic constraints, because it strongly interacts with",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,277,"min_gain_to_split. We build a simple example where gradients are [1, 1,",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,278,"100, 1, 1] (hessians are all ones). The best split happens on the 3rd",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,279,"bin, and depending on whether the value of the node is bounded or not,",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,280,the min_gain_to_split constraint is or isn't satisfied.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,312,"Since the gradient array is [1, 1, 100, 1, 1]",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,313,the max possible gain happens on the 3rd bin (or equivalently in the 2nd),
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,314,"and is equal to about 1307, which less than min_gain_to_split = 2000, so",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,315,the node is considered unsplittable (gain = -1),
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,320,the unbounded value is equal to -sum_gradients / sum_hessians,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,326,min_gain_to_split not respected,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,328,here again the max possible gain is on the 3rd bin but we now cap the,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,329,"value of the node into [-10, inf].",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,330,This means the gain is now about 2430 which is more than the,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,331,min_gain_to_split constraint.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,10,"To use this experimental feature, we need to explicitly ask for it:",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,11,noqa,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,23,Check identical nodes for each tree,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,28,Check identical predictions,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,37,Check that a ValueError is raised when the maximum number of iterations,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,38,is smaller than the number of iterations from the previous fit when warm,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,39,start is True.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,56,Make sure that fitting 50 iterations and then 25 with warm start is,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,57,equivalent to fitting 75 iterations.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,70,Check that both predictors are equal,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,79,Test if possible to fit trees of different depth in ensemble.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,86,First 20 trees have max_depth == 2,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,89,Last 10 trees have max_depth == 3,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,100,Make sure that early stopping occurs after a small number of iterations,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,101,when fitting a second time with warm starting.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,120,Test if warm start with equal n_estimators does nothing,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,129,Check that both predictors are equal,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,138,Test if fit clears state.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,144,inits state,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,146,clears old state and equals est,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,148,Check that both predictors have the same train_score_ and,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,149,validation_score_ attributes,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,153,Check that both predictors are equal,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,163,Make sure the seeds for train/val split and small trainset subsampling,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,164,are correctly set in a warm start context.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,166,Helper to avoid consuming rngs,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,182,"clear the old state, different seed",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,188,inits state,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,190,clears old state and equals est,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,193,"Without warm starting, the seeds should be",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,194,* all different if random state is None,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,195,* all equal if random state is an integer,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,196,* different when refitting and equal with a new estimator (because,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,197,the random state is mutated),
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,205,"With warm starting, the seeds must be equal",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,76,Constant hessian: 1. per sample.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,83,This test checks that the values of gradients and hessians are,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,84,consistent in different places:,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,85,- in split_info: si.sum_gradient_left + si.sum_gradient_right must be,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,86,equal to the gradient at the node. Same for hessians.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,87,- in the histograms: summing 'sum_gradients' over the bins must be,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,88,"constant across all features, and those sums must be equal to the",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,89,node's gradient. Same for hessians.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,154,make sure that si.sum_gradient_left + si.sum_gradient_right have their,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,155,"expected value, same for hessians",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,171,"make sure sum of gradients in histograms are the same for all features,",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,172,and make sure they're equal to their expected value,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,180,"note: gradients and hessians have shape (n_features,),",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,181,we're comparing them to *scalars*. This has the benefit of also,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,182,making sure that all the entries are equal across features.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,183,"shape = (n_features,)",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,184,scalar,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,187,"0 is not the actual hessian, but it's not computed in this case",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,197,Check that split_indices returns the correct splits and that,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,198,splitter.partition is consistent with what is returned.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,208,split will happen on feature 1 and on bin 3,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,250,sanity checks for best split,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,262,Check that the resulting split indices sizes are consistent with the,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,263,count statistics anticipated when looking for the best split.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,269,"Try to split a pure node (all gradients are equal, same for hessians)",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,270,with min_gain_to_split = 0 and make sure that the node is not split (best,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,271,"possible gain = -1). Note: before the strict inequality comparison, this",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,272,test would fail because the node would be split with a gain of 0.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,316,basic sanity check with no missing values: given the gradient,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,317,"values, the split must occur on bin_idx=3",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,318,X_binned,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,319,gradients,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,320,no missing values,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,321,n_bins_non_missing,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,322,don't split on nans,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,323,expected_bin_idx,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,326,We replace 2 samples by NaNs (bin_idx=8),
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,327,"These 2 samples were mapped to the left node before, so they should",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,328,be mapped to left node again,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,329,Notice how the bin_idx threshold changes from 3 to 1.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,330,8 <=> missing,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,332,missing values,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,333,n_bins_non_missing,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,334,don't split on nans,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,335,cut on bin_idx=1,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,336,missing values go to left,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,338,"same as above, but with non-consecutive missing_values_bin",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,339,9 <=> missing,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,341,missing values,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,342,n_bins_non_missing,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,343,don't split on nans,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,344,cut on bin_idx=1,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,345,missing values go to left,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,347,this time replacing 2 samples that were on the right.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,348,8 <=> missing,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,350,missing values,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,351,n_bins_non_missing,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,352,don't split on nans,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,353,cut on bin_idx=3 (like in first case),
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,354,missing values go to right,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,356,"same as above, but with non-consecutive missing_values_bin",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,357,9 <=> missing,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,359,missing values,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,360,n_bins_non_missing,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,361,don't split on nans,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,362,cut on bin_idx=3 (like in first case),
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,363,missing values go to right,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,365,"For the following cases, split_on_nans is True (we replace all of",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,366,"the samples with nans, instead of just 2).",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,367,4 <=> missing,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,369,missing values,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,370,n_bins_non_missing,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,371,split on nans,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,372,cut on bin_idx=3,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,373,missing values go to right,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,375,"same as above, but with non-consecutive missing_values_bin",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,376,9 <=> missing,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,378,missing values,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,379,n_bins_non_missing,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,380,split on nans,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,381,cut on bin_idx=3,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,382,missing values go to right,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,384,6 <=> missing,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,386,missing values,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,387,n_bins_non_missing,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,388,split on nans,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,389,cut on bin_idx=5,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,390,missing values go to right,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,392,"same as above, but with non-consecutive missing_values_bin",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,393,9 <=> missing,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,395,missing values,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,396,n_bins_non_missing,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,397,split on nans,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,398,cut on bin_idx=5,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,399,missing values go to right,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,406,Make sure missing values are properly supported.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,407,we build an artificial example with gradients such that the best split,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,408,"is on bin_idx=3, when there are no missing values.",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,409,Then we introduce missing values and:,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,410,- make sure the chosen bin is correct (find_best_bin()): it's,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,411,"still the same split, even though the index of the bin may change",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,412,- make sure the missing values are mapped to the correct child,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,413,(split_indices()),
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,461,Make sure the split is properly computed.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,462,This also make sure missing values are properly assigned to the correct,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,463,child in split_indices(),
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,468,"When we don't split on nans, the split should always be the same.",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,472,"When we split on nans, samples with missing values are always mapped",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,473,to the right child.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,10,"To use this experimental feature, we need to explicitly ask for it:",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,11,noqa,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,71,use scorer,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,72,use scorer on train,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,73,same with default scorer,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,75,use loss,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,76,use loss on training data,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,77,no early stopping,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,87,just for coverage,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,88,easier to overfit fast,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,112,use scorer,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,113,use scorer on training data,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,114,same with default scorer,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,116,use loss,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,117,use loss on training data,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,118,no early stopping,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,128,just for coverage,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,129,easier to overfit fast,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,153,Test that early stopping is enabled by default if and only if there,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,154,are more than 10000 samples,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,166,not enough iterations,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,167,not enough iterations,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,168,not enough iterations,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,169,significant improvement,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,170,significant improvement,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,171,significant improvement,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,172,significant improvement,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,173,no significant improvement,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,174,no significant improvement,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,175,no significant improvement,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,187,For coverage only.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,196,Make sure training and validation data are binned separately.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,197,See issue 13926,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,209,Note that since the data is small there is no subsampling and the,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,210,random_state doesn't matter,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,222,sanity check for missing values support. With only one feature and,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,223,"y == isnan(X), the gbdt is supposed to reach perfect accuracy on the",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,224,training set.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,250,Make sure the estimators can deal with missing values and still yield,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,251,decent predictions,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,281,non regression test for issue #14018,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,282,make sure we avoid zero division errors when computing the leaves values.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,284,"If the learning rate is too high, the raw predictions are bad and will",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,285,saturate the softmax (or sigmoid in binary classif). This leads to,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,286,"probabilities being exactly 0 or 1, gradients being constant, and",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,287,hessians being zero.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,294,Make sure that the small trainset is stratified and has the expected,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,295,length (10k samples),
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,305,Compute the small training set,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,309,Compute the class distribution in the small training set,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,314,Test that the small training set has the expected length,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,318,Test that the class distributions in the whole dataset and in the small,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,319,training set are identical,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,324,Compare the buit-in missing value handling of Histogram GBC with an,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,325,a-priori missing value imputation strategy that should yield the same,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,326,results in terms of decision function.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,327,,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,328,Each feature (containing NaNs) is replaced by 2 features:,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,329,- one where the nans are replaced by min(feature) - 1,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,330,- one where the nans are replaced by max(feature) + 1,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,331,A split where nans go to the left has an equivalent split in the,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,332,"first (min) feature, and a split where nans go to the right has an",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,333,equivalent split in the second (max) feature.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,334,,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,335,Assuming the data is such that there is never a tie to select the best,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,336,"feature to split on during training, the learned decision trees should be",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,337,strictly equivalent (learn a sequence of splits that encode the same,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,338,decision function).,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,339,,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,340,The MinMaxImputer transformer is meant to be a toy implementation of the,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,341,"""Missing In Attributes"" (MIA) missing value handling for decision trees",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,342,https://www.sciencedirect.com/science/article/abs/pii/S0167865508000305,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,343,The implementation of MIA as an imputation transformer was suggested by,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,344,"""Remark 3"" in https://arxiv.org/abs/1902.06931",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,369,Pre-bin the data to ensure a deterministic handling by the 2,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,370,strategies and also make it easier to insert np.nan in a structured,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,371,way:,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,374,First feature has missing values completely at random:,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,378,Second and third features have missing values for extreme values,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,379,(censoring missingness):,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,386,Make the last feature nan pattern very informative:,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,392,Check that there is at least one missing value in each feature:,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,396,Let's use a test set to check that the learned decision function is,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,397,the same as evaluated on unseen data. Otherwise it could just be the,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,398,case that we find two independent ways to overfit the training set.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,401,n_samples need to be large enough to minimize the likelihood of having,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,402,several candidate splits with the same gain value in a given tree.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,406,Use a small number of leaf nodes and iterations so as to keep,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,407,under-fitting models to minimize the likelihood of ties when training the,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,408,model.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,417,Check that the model reach the same score:,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,424,Check the individual prediction match as a finer grained,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,425,decision function check.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,431,Basic test for infinite values,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,456,High level test making sure that inf and nan values are properly handled,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,457,when both are present. This is similar to,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,458,"test_split_on_nan_with_infinite_values() in test_grower.py, though we",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,459,cannot check the predictions for binned values here.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,473,categorical_crossentropy should only be used if there are more than two,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,474,classes present. PR #14869,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,485,Regression tests for #14709 where the targets need to be encoded before,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,486,to compute the score,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,495,Make sure setting a SW to zero amounts to ignoring the corresponding,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,496,sample,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,503,ignore the first 2 training samples by setting their weight to 0,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,511,Make sure setting a SW to zero amounts to ignoring the corresponding,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,512,sample,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,519,ignore the first 2 training samples by setting their weight to 0,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,532,ignore the first 2 training samples by setting their weight to 0,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,547,High level test to make sure that duplicating a sample is equivalent to,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,548,giving it weight of 2.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,550,fails for n_samples > 255 because binning does not take sample weights,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,551,into account. Keeping n_samples <= 255 makes,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,552,sure only unique values are used so SW have no effect on binning.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,567,This test can't pass if min_samples_leaf > 1 because that would force 2,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,568,"samples to be in the same node in est_sw, while these samples would be",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,569,free to be separate in est_dup: est_dup would just group together the,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,570,duplicated samples.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,573,Create dataset with duplicate and corresponding sample weights,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,586,checking raw_predict is stricter than just predict for classification,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,594,"For losses with constant hessians, the sum_hessians field of the",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,595,histograms must be equal to the sum of the sample weight of samples at,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,596,the corresponding bin.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,615,build sum_sample_weight which contains the sum of the sample weights at,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,616,each bin (for each feature). This must be equal to the sum_hessians,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,617,field of the corresponding histogram,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,624,Build histogram,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,637,Non regression test for,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,638,https://github.com/scikit-learn/scikit-learn/issues/16179,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,639,there was a bug when the max_depth and the max_leaf_nodes criteria were,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,640,"met at the same time, which would lead to max_leaf_nodes not being",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,641,respected.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,647,would be 4 prior to bug fix,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,651,Non regression test for #16661 where second fit fails with,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,652,"warm_start=True, early_stopping is on, and no validation set",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,659,does not raise on second call,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,667,Make sure it's still possible to build single-node trees. In that case,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,668,the value of the root is set to 0. That's a correct value: if the tree is,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,669,single-node that's because min_gain_to_split is not respected right from,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,670,"the root, so we don't want the tree to have any impact on the",
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,671,predictions.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,674,constant target will lead to a single root node,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,682,Still gives correct predictions thanks to the baseline prediction,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_histogram.py,25,Small sample_indices (below unrolling threshold),
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_histogram.py,38,Larger sample_indices (above unrolling threshold),
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_histogram.py,53,Make sure the order of the samples has no impact on the histogram,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_histogram.py,54,computations,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_histogram.py,100,Make sure the different unrolled histogram computations give the same,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_histogram.py,101,results as the naive one.,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_histogram.py,146,Make sure the histogram subtraction trick gives the same result as the,
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_histogram.py,147,classical method.,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,33,Common random state,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,36,Toy sample,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,38,test string class labels,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,44,Load the iris dataset and randomly permute it,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,49,Load the boston dataset and randomly permute it,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,56,Test the `_samme_proba` helper function.,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,58,Define some example (bad) `predict_proba` output.,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,65,_samme_proba calls estimator.predict_proba.,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,66,Make a mock object so I can control what gets returned.,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,78,Make sure that the correct elements come out as smallest --,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,79,`_samme_proba` should preserve the ordering in each example.,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,85,Test predict_proba robustness for one class label input.,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,86,In response to issue #7501,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,87,https://github.com/scikit-learn/scikit-learn/issues/7501,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,95,Check classification on a toy dataset.,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,105,Check classification on a toy dataset.,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,112,Check consistency on dataset iris.,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,132,Check we used multiple estimators,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,134,Check for distinct random states (see issue #7408),
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,138,Somewhat hacky regression test: prior to,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,139,"ae7adc880d624615a34bafdb1d75ef67051b8200,",
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,140,predict_proba returned SAMME.R values for SAMME.,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,148,Check consistency on dataset boston house prices.,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,154,Check we used multiple estimators,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,156,Check for distinct random states (see issue #7408),
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,163,Check staged predictions.,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,187,AdaBoost regression,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,205,Check that base trees can be grid-searched.,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,206,AdaBoost classification,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,214,AdaBoost regression,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,224,Check pickability.,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,227,Adaboost classifier,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,239,Adaboost regressor,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,252,Check variable importances.,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,272,Test that it gives proper exception on deficient input.,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,287,Test different base estimators.,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,290,XXX doesn't work with y_class because RF doesn't support classes_,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,291,Shouldn't AdaBoost run a LabelBinarizer?,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,306,"Check that an empty discrete ensemble fails in fit, not predict.",
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,315,Check classification with sparse input.,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,329,Flatten y to a 1d array,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,339,Trained on sparse format,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,346,Trained on dense format,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,353,predict,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,358,decision_function,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,363,predict_log_proba,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,368,predict_proba,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,373,score,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,378,staged_decision_function,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,385,staged_predict,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,391,staged_predict_proba,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,397,staged_score,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,404,Verify sparsity of data is maintained during training,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,412,Check regression with sparse input.,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,433,Trained on sparse format,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,439,Trained on dense format,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,445,predict,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,450,staged_predict,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,502,TODO: Remove in 0.24 when DummyClassifier's `strategy` default changes,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,518,check that giving weight will have an influence on the error computed,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,519,for a weak learner,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,525,add an arbitrary outlier,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,529,random_state=0 ensure that the underlying bootstrap will use the outlier,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,536,fit 3 models:,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,537,- a model containing the outlier,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,538,- a model without the outlier,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,539,- a model containing the outlier but with a null sample-weight,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,556,check that predict_proba and predict give consistent results,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,557,regression test for:,
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,558,https://github.com/scikit-learn/scikit-learn/issues/14084,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,5,Authors: Nicolas Goix <nicolas.goix@telecom-paristech.fr>,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,6,Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,7,License: BSD 3 clause,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,33,load the iris dataset,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,34,and randomly permute it,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,40,also load the boston dataset,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,41,and randomly permute it,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,77,Trained on sparse format,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,82,Trained on dense format,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,94,Test max_samples,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,101,"The dataset has less than 256 samples, explicitly setting",
scikit-learn/sklearn/ensemble/tests/test_iforest.py,102,max_samples > n_samples should result in a warning. If not set,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,103,explicitly there should be no warning,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,107,note that assert_no_warnings does not apply since it enables a,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,108,PendingDeprecationWarning triggered by scipy.sparse's use of,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,109,np.matrix. See issue #11251.,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,124,test X_test n_features match X_train one:,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,127,test that behaviour='old' will raise an error,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,183,Generate train/test data,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,189,Generate some abnormal novel observations,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,194,fit the model,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,197,"predict scores (the lower, the more normal)",
scikit-learn/sklearn/ensemble/tests/test_iforest.py,200,check that there is at most 6 errors (false positive or false negative),
scikit-learn/sklearn/ensemble/tests/test_iforest.py,206,toy sample (the last two samples are outliers),
scikit-learn/sklearn/ensemble/tests/test_iforest.py,209,Test IsolationForest,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,214,assert detect outliers:,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,220,Make sure validated max_samples in iforest and BaseBagging are identical,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,227,It tests non-regression for #5732 which failed at predict.,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,238,It tests non-regression for #8549 which used the wrong formula,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,239,"for average path length, strictly for the integer case",
scikit-learn/sklearn/ensemble/tests/test_iforest.py,240,Updated to check average path length when input is <= 2 (issue #11839),
scikit-learn/sklearn/ensemble/tests/test_iforest.py,252,_average_path_length is increasing,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,275,fit first 10 trees,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,279,remember the 1st tree,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,281,fit another 10 trees,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,284,expecting 20 fitted trees and no overwritten trees,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,289,mock get_chunk_n_rows to actually test more than one chunk (here one,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,290,chunk = 3 rows:,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,305,idem with chunk_size = 5 rows,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,330,2-d array of all 1s,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,342,2-d array where columns contain the same value across rows,
scikit-learn/sklearn/ensemble/tests/test_iforest.py,351,Single row,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,5,Author: Gilles Louppe,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,6,License: BSD 3 clause,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,40,also load the iris dataset,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,41,and randomly permute it,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,47,also load the boston dataset,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,48,and randomly permute it,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,55,TODO: Remove in 0.24 when DummyClassifier's `strategy` default updates,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,58,Check classification for various parameter settings.,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,81,Check classification for various parameter settings on sparse input.,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,117,Trained on sparse format,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,125,Trained on dense format,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,141,Check regression for various parameter settings.,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,163,Check regression for various parameter settings on sparse input.,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,199,Trained on sparse format,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,207,Trained on dense format,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,230,Test that bootstrapping samples generate non-perfect base estimators.,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,238,"without bootstrap, all trees are perfect on the training set",
scikit-learn/sklearn/ensemble/tests/test_bagging.py,247,"with bootstrap, trees are no longer perfect on the training set",
scikit-learn/sklearn/ensemble/tests/test_bagging.py,256,check that each sampling correspond to a complete bootstrap resample.,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,257,the size of each bootstrap should be the same as the input data but,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,258,the data should be different (checked using the hash of the data).,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,269,Test that bootstrapping features may generate duplicate features.,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,293,Predict probabilities.,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,300,Normal case,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,311,"Degenerate case, where some classes are missing",
scikit-learn/sklearn/ensemble/tests/test_bagging.py,325,Check that oob prediction is a good estimation of the generalization,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,326,error.,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,343,Test with few estimators,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,355,Check that oob prediction is a good estimation of the generalization,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,356,error.,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,372,Test with few estimators,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,384,Check singleton ensembles.,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,402,Test that it gives proper exception on deficient input.,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,406,Test max_samples,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,418,Test max_features,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,430,Test support of decision_function,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,435,Check parallel classification.,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,438,Classification,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,447,predict_proba,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,461,decision_function,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,488,Check parallel regression.,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,514,Check that bagging ensembles can be grid-searched.,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,515,Transform iris into a binary classification task,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,519,Grid search with scoring based on decision_function,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,529,Check base_estimator and its default values.,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,532,Classification,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,555,Regression,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,606,Test if fitting incrementally with warm start gives a forest of the,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,607,right size and the same results as a normal fit.,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,630,Test if warm start'ed second fit with smaller n_estimators raises error.,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,639,Test that nothing happens when fitting without increasing n_estimators,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,647,"modify X to nonsense values, this should not change anything",
scikit-learn/sklearn/ensemble/tests/test_bagging.py,657,warm started classifier with 5+5 estimators should be equivalent to,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,658,one classifier with 10 estimators,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,678,Check using oob_score and warm_start simultaneously fails,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,697,"Make sure OOB scores are identical when random_state, estimator, and",
scikit-learn/sklearn/ensemble/tests/test_bagging.py,698,training data are fixed and fitting is done twice,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,707,Check that format of estimators_samples_ is correct and that results,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,708,generated at fit time can be identically reproduced at a later time,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,709,using data saved in object attributes.,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,716,Get relevant attributes,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,721,Test for correct formatting,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,726,Re-fit single estimator to test for consistent sampling,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,743,This test is a regression test to check that with a random step,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,744,"(e.g. SparseRandomProjection) and a given random state, the results",
scikit-learn/sklearn/ensemble/tests/test_bagging.py,745,generated at fit time can be identically reproduced at a later time using,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,746,data saved in object attributes. Check issue #9524 for full discussion.,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,771,Make sure validated max_samples and original max_samples are identical,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,772,when valid integer max_samples supplied by user,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,783,Make sure the oob_score doesn't change when the labels change,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,784,See: https://github.com/scikit-learn/scikit-learn/issues/8933,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,806,Check that BaggingRegressor can accept X with missing/infinite data,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,834,Verify that exceptions can be raised by wrapper regressor,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,843,Check that BaggingClassifier can accept X with missing/infinite data,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,864,Verify that exceptions can be raised by wrapper classifier,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,873,Check that Bagging estimator can accept low fractional max_features,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,884,Check that Bagging estimator can generate sample indices properly,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,885,Non-regression test for:,
scikit-learn/sklearn/ensemble/tests/test_bagging.py,886,https://github.com/scikit-learn/scikit-learn/issues/16436,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,4,License: BSD 3 clause,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,60,prescale the data to avoid convergence warning without using a pipeline,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,61,for later assert,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,86,LogisticRegression has decision_function method,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,97,check that a column is dropped in binary classification,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,103,both classifiers implement 'predict_proba' and will both drop one column,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,112,LinearSVC does not implement 'predict_proba' and will not drop one column,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,122,prescale the data to avoid convergence warning without using a pipeline,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,123,for later assert,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,145,prescale the data to avoid convergence warning without using a pipeline,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,146,for later assert,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,178,prescale the data to avoid convergence warning without using a pipeline,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,179,for later assert,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,213,Check passthrough behavior on a sparse X matrix,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,232,Check passthrough behavior on a sparse X matrix,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,250,check that classifier will drop one of the probability column for,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,251,binary classification problem,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,253,Select only the 2 first classes,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,342,keep only classes 0 and 1,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,350,checking that fixing the random state of the CV will lead to the same,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,351,results,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,369,These warnings are raised due to _BaseComposition,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,388,check that we stratify the classes for the default CV,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,393,"since iris is not shuffled, a simple k-fold would not contain the",
scikit-learn/sklearn/ensemble/tests/test_stacking.py,394,3 classes during training,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,415,check that sample weights has an influence on the fitting,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,416,note: ConvergenceWarning are catch since we are not worrying about the,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,417,convergence here,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,444,check sample_weight is passed to all invocations of fit,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,472,check that the stacking affects the fit of the final estimator but not,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,473,the fit of the base estimators,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,474,note: ConvergenceWarning are catch since we are not worrying about the,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,475,convergence here,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,485,the base estimators should be identical,
scikit-learn/sklearn/ensemble/tests/test_stacking.py,490,the final estimator should be different,
scikit-learn/sklearn/ensemble/tests/test_common.py,40,"check that the behavior of `estimators`, `estimators_`,",
scikit-learn/sklearn/ensemble/tests/test_common.py,41,"`named_estimators`, `named_estimators_` is consistent across all",
scikit-learn/sklearn/ensemble/tests/test_common.py,42,ensemble classes and when using `set_params()`.,
scikit-learn/sklearn/ensemble/tests/test_common.py,44,before fit,
scikit-learn/sklearn/ensemble/tests/test_common.py,49,check fitted attributes,
scikit-learn/sklearn/ensemble/tests/test_common.py,56,check that set_params() does not add a new attribute,
scikit-learn/sklearn/ensemble/tests/test_common.py,66,check the behavior when setting an dropping an estimator,
scikit-learn/sklearn/ensemble/tests/test_common.py,76,check that the correspondence is correct,
scikit-learn/sklearn/ensemble/tests/test_common.py,79,check that we can set the parameters of the underlying classifier,
scikit-learn/sklearn/ensemble/tests/test_common.py,93,check that ensemble will fail during validation if the underlying,
scikit-learn/sklearn/ensemble/tests/test_common.py,94,estimators are not of the same type (i.e. classifier or regressor),
scikit-learn/sklearn/ensemble/tests/test_common.py,118,raise an error when the name contains dunder,
scikit-learn/sklearn/ensemble/tests/test_common.py,129,raise an error when the name is not unique,
scikit-learn/sklearn/ensemble/tests/test_common.py,142,raise an error when the name conflicts with the parameters,
scikit-learn/sklearn/ensemble/tests/test_common.py,168,check that we raise a consistent error when all estimators are,
scikit-learn/sklearn/ensemble/tests/test_common.py,169,dropped,
scikit-learn/sklearn/ensemble/tests/test_forest.py,5,"Authors: Gilles Louppe,",
scikit-learn/sklearn/ensemble/tests/test_forest.py,6,"Brian Holt,",
scikit-learn/sklearn/ensemble/tests/test_forest.py,7,"Andreas Mueller,",
scikit-learn/sklearn/ensemble/tests/test_forest.py,8,Arnaud Joly,
scikit-learn/sklearn/ensemble/tests/test_forest.py,9,License: BSD 3 clause,
scikit-learn/sklearn/ensemble/tests/test_forest.py,56,toy sample,
scikit-learn/sklearn/ensemble/tests/test_forest.py,62,Larger classification sample used for testing feature importances,
scikit-learn/sklearn/ensemble/tests/test_forest.py,67,also load the iris dataset,
scikit-learn/sklearn/ensemble/tests/test_forest.py,68,and randomly permute it,
scikit-learn/sklearn/ensemble/tests/test_forest.py,75,also load the boston dataset,
scikit-learn/sklearn/ensemble/tests/test_forest.py,76,and randomly permute it,
scikit-learn/sklearn/ensemble/tests/test_forest.py,82,also make a hastie_10_2 dataset,
scikit-learn/sklearn/ensemble/tests/test_forest.py,86,Get the default backend in joblib to test parallelism and interaction with,
scikit-learn/sklearn/ensemble/tests/test_forest.py,87,different backends,
scikit-learn/sklearn/ensemble/tests/test_forest.py,127,also test apply,
scikit-learn/sklearn/ensemble/tests/test_forest.py,138,Check consistency on dataset iris.,
scikit-learn/sklearn/ensemble/tests/test_forest.py,163,Check consistency on dataset boston house prices.,
scikit-learn/sklearn/ensemble/tests/test_forest.py,188,Regression models should not have a classes_ attribute.,
scikit-learn/sklearn/ensemble/tests/test_forest.py,204,Predict probabilities.,
scikit-learn/sklearn/ensemble/tests/test_forest.py,222,cast as dype,
scikit-learn/sklearn/ensemble/tests/test_forest.py,233,The forest estimator can detect that only the first 3 features of the,
scikit-learn/sklearn/ensemble/tests/test_forest.py,234,dataset are informative:,
scikit-learn/sklearn/ensemble/tests/test_forest.py,240,Check with parallel,
scikit-learn/sklearn/ensemble/tests/test_forest.py,246,Check with sample weights,
scikit-learn/sklearn/ensemble/tests/test_forest.py,276,Check whether variable importances of totally randomized trees,
scikit-learn/sklearn/ensemble/tests/test_forest.py,277,"converge towards their theoretical values (See Louppe et al,",
scikit-learn/sklearn/ensemble/tests/test_forest.py,278,"Understanding variable importances in forests of randomized trees, 2013).",
scikit-learn/sklearn/ensemble/tests/test_forest.py,304,Weight of each B of size k,
scikit-learn/sklearn/ensemble/tests/test_forest.py,307,For all B of size k,
scikit-learn/sklearn/ensemble/tests/test_forest.py,309,For all values B=b,
scikit-learn/sklearn/ensemble/tests/test_forest.py,327,P(B=b),
scikit-learn/sklearn/ensemble/tests/test_forest.py,348,Compute true importances,
scikit-learn/sklearn/ensemble/tests/test_forest.py,354,Estimate importances with totally randomized trees,
scikit-learn/sklearn/ensemble/tests/test_forest.py,363,Check correctness,
scikit-learn/sklearn/ensemble/tests/test_forest.py,378,Check that oob prediction is a good estimation of the generalization,
scikit-learn/sklearn/ensemble/tests/test_forest.py,379,error.,
scikit-learn/sklearn/ensemble/tests/test_forest.py,381,Proper behavior,
scikit-learn/sklearn/ensemble/tests/test_forest.py,394,Check warning if not enough estimators,
scikit-learn/sklearn/ensemble/tests/test_forest.py,405,csc matrix,
scikit-learn/sklearn/ensemble/tests/test_forest.py,408,non-contiguous targets in classification,
scikit-learn/sklearn/ensemble/tests/test_forest.py,416,csc matrix,
scikit-learn/sklearn/ensemble/tests/test_forest.py,431,Unfitted /  no bootstrap / no oob_score,
scikit-learn/sklearn/ensemble/tests/test_forest.py,438,No bootstrap,
scikit-learn/sklearn/ensemble/tests/test_forest.py,456,Check that base trees can be grid-searched.,
scikit-learn/sklearn/ensemble/tests/test_forest.py,486,Check pickability.,
scikit-learn/sklearn/ensemble/tests/test_forest.py,511,Check estimators on multi-output problems.,
scikit-learn/sklearn/ensemble/tests/test_forest.py,544,Check estimators on multi-output problems with string outputs.,
scikit-learn/sklearn/ensemble/tests/test_forest.py,573,Test that n_classes_ and classes_ have proper shape.,
scikit-learn/sklearn/ensemble/tests/test_forest.py,576,"Classification, single output",
scikit-learn/sklearn/ensemble/tests/test_forest.py,582,"Classification, multi-output",
scikit-learn/sklearn/ensemble/tests/test_forest.py,596,Test that the `sparse_output` parameter of RandomTreesEmbedding,
scikit-learn/sklearn/ensemble/tests/test_forest.py,597,works by returning a dense array.,
scikit-learn/sklearn/ensemble/tests/test_forest.py,599,Create the RTE with sparse=False,
scikit-learn/sklearn/ensemble/tests/test_forest.py,604,"Assert that type is ndarray, not scipy.sparse.csr.csr_matrix",
scikit-learn/sklearn/ensemble/tests/test_forest.py,609,Test that the `sparse_output` parameter of RandomTreesEmbedding,
scikit-learn/sklearn/ensemble/tests/test_forest.py,610,works by returning the same array for both argument values.,
scikit-learn/sklearn/ensemble/tests/test_forest.py,612,Create the RTEs,
scikit-learn/sklearn/ensemble/tests/test_forest.py,621,Assert that dense and sparse hashers have same array.,
scikit-learn/sklearn/ensemble/tests/test_forest.py,625,Ignore warnings from switching to more power iterations in randomized_svd,
scikit-learn/sklearn/ensemble/tests/test_forest.py,628,test random forest hashing on circles dataset,
scikit-learn/sklearn/ensemble/tests/test_forest.py,629,make sure that it is linearly separable.,
scikit-learn/sklearn/ensemble/tests/test_forest.py,630,even after projected to two SVD dimensions,
scikit-learn/sklearn/ensemble/tests/test_forest.py,631,Note: Not all random_states produce perfect results.,
scikit-learn/sklearn/ensemble/tests/test_forest.py,636,test fit and transform:,
scikit-learn/sklearn/ensemble/tests/test_forest.py,641,one leaf active per data point per forest,
scikit-learn/sklearn/ensemble/tests/test_forest.py,680,Single variable with 4 values,
scikit-learn/sklearn/ensemble/tests/test_forest.py,698,"On a single variable problem where X_0 has 4 equiprobable values, there",
scikit-learn/sklearn/ensemble/tests/test_forest.py,699,"are 5 ways to build a random tree. The more compact (0,1/0,0/--0,2/--) of",
scikit-learn/sklearn/ensemble/tests/test_forest.py,700,them has probability 1/3 while the 4 others have probability 1/6.,
scikit-learn/sklearn/ensemble/tests/test_forest.py,703,Rough approximation of 1/6.,
scikit-learn/sklearn/ensemble/tests/test_forest.py,710,"Two variables, one with 2 values, one with 3 values",
scikit-learn/sklearn/ensemble/tests/test_forest.py,733,Test precedence of max_leaf_nodes over max_depth.,
scikit-learn/sklearn/ensemble/tests/test_forest.py,753,test boundary value,
scikit-learn/sklearn/ensemble/tests/test_forest.py,787,Test if leaves contain more than leaf_count training examples,
scikit-learn/sklearn/ensemble/tests/test_forest.py,790,test boundary value,
scikit-learn/sklearn/ensemble/tests/test_forest.py,800,drop inner nodes,
scikit-learn/sklearn/ensemble/tests/test_forest.py,809,drop inner nodes,
scikit-learn/sklearn/ensemble/tests/test_forest.py,823,Test if leaves contain at least min_weight_fraction_leaf of the,
scikit-learn/sklearn/ensemble/tests/test_forest.py,824,training set,
scikit-learn/sklearn/ensemble/tests/test_forest.py,830,test both DepthFirstTreeBuilder and BestFirstTreeBuilder,
scikit-learn/sklearn/ensemble/tests/test_forest.py,831,by setting max_leaf_nodes,
scikit-learn/sklearn/ensemble/tests/test_forest.py,841,drop inner nodes,
scikit-learn/sklearn/ensemble/tests/test_forest.py,891,Check that it works no matter the memory layout,
scikit-learn/sklearn/ensemble/tests/test_forest.py,895,Nothing,
scikit-learn/sklearn/ensemble/tests/test_forest.py,900,C-order,
scikit-learn/sklearn/ensemble/tests/test_forest.py,905,F-order,
scikit-learn/sklearn/ensemble/tests/test_forest.py,910,Contiguous,
scikit-learn/sklearn/ensemble/tests/test_forest.py,916,csr matrix,
scikit-learn/sklearn/ensemble/tests/test_forest.py,921,csc_matrix,
scikit-learn/sklearn/ensemble/tests/test_forest.py,926,coo_matrix,
scikit-learn/sklearn/ensemble/tests/test_forest.py,931,Strided,
scikit-learn/sklearn/ensemble/tests/test_forest.py,967,Check class_weights resemble sample_weights behavior.,
scikit-learn/sklearn/ensemble/tests/test_forest.py,970,"Iris is balanced, so no effect expected for using 'balanced' weights",
scikit-learn/sklearn/ensemble/tests/test_forest.py,977,Make a multi-output problem with three copies of Iris,
scikit-learn/sklearn/ensemble/tests/test_forest.py,979,Create user-defined weights that should balance over the outputs,
scikit-learn/sklearn/ensemble/tests/test_forest.py,986,"Check against multi-output ""balanced"" which should also have no effect",
scikit-learn/sklearn/ensemble/tests/test_forest.py,991,"Inflate importance of class 1, check against user-defined weights",
scikit-learn/sklearn/ensemble/tests/test_forest.py,1001,Check that sample_weight and class_weight are multiplicative,
scikit-learn/sklearn/ensemble/tests/test_forest.py,1015,"Test class_weight works for multi-output""""""",
scikit-learn/sklearn/ensemble/tests/test_forest.py,1023,smoke test for balanced subsample,
scikit-learn/sklearn/ensemble/tests/test_forest.py,1034,Test if class_weight raises errors and warnings when expected.,
scikit-learn/sklearn/ensemble/tests/test_forest.py,1038,Invalid preset string,
scikit-learn/sklearn/ensemble/tests/test_forest.py,1043,Warning warm_start with preset,
scikit-learn/sklearn/ensemble/tests/test_forest.py,1049,Not a list or preset for multi-output,
scikit-learn/sklearn/ensemble/tests/test_forest.py,1053,Incorrect length list for multi-output,
scikit-learn/sklearn/ensemble/tests/test_forest.py,1064,Test if fitting incrementally with warm start gives a forest of the,
scikit-learn/sklearn/ensemble/tests/test_forest.py,1065,right size and the same results as a normal fit.,
scikit-learn/sklearn/ensemble/tests/test_forest.py,1096,Test if fit clears state and grows a new forest when warm_start==False.,
scikit-learn/sklearn/ensemble/tests/test_forest.py,1105,inits state,
scikit-learn/sklearn/ensemble/tests/test_forest.py,1107,clears old state and equals clf,
scikit-learn/sklearn/ensemble/tests/test_forest.py,1118,Test if warm start second fit with smaller n_estimators raises error.,
scikit-learn/sklearn/ensemble/tests/test_forest.py,1133,Test if warm start with equal n_estimators does nothing and returns the,
scikit-learn/sklearn/ensemble/tests/test_forest.py,1134,same forest and raises a warning.,
scikit-learn/sklearn/ensemble/tests/test_forest.py,1144,Now clf_2 equals clf.,
scikit-learn/sklearn/ensemble/tests/test_forest.py,1148,If we had fit the trees again we would have got a different forest as we,
scikit-learn/sklearn/ensemble/tests/test_forest.py,1149,changed the random state.,
scikit-learn/sklearn/ensemble/tests/test_forest.py,1159,Test that the warm start computes oob score when asked.,
scikit-learn/sklearn/ensemble/tests/test_forest.py,1162,Use 15 estimators to avoid 'some inputs do not have OOB scores' warning.,
scikit-learn/sklearn/ensemble/tests/test_forest.py,1177,Test that oob_score is computed even if we don't need to train,
scikit-learn/sklearn/ensemble/tests/test_forest.py,1178,additional trees.,
scikit-learn/sklearn/ensemble/tests/test_forest.py,1220,Assert that leaves index are correct,
scikit-learn/sklearn/ensemble/tests/test_forest.py,1234,Test if min_impurity_split of base estimators is set,
scikit-learn/sklearn/ensemble/tests/test_forest.py,1235,Regression test for #8006,
scikit-learn/sklearn/ensemble/tests/test_forest.py,1258,Simply check if the parameter is passed on correctly. Tree tests,
scikit-learn/sklearn/ensemble/tests/test_forest.py,1259,will suffice for the actual working of this param,
scikit-learn/sklearn/ensemble/tests/test_forest.py,1263,"mypy error: Variable ""DEFAULT_JOBLIB_BACKEND"" is not valid type",
scikit-learn/sklearn/ensemble/tests/test_forest.py,1264,type: ignore,
scikit-learn/sklearn/ensemble/tests/test_forest.py,1288,predict_proba requires shared memory. Ensure that's honored.,
scikit-learn/sklearn/ensemble/tests/test_forest.py,1304,build a forest of single node trees. See #13636,
scikit-learn/sklearn/ensemble/tests/test_forest.py,1335,Check invalid `max_samples` values,
scikit-learn/sklearn/ensemble/tests/test_forest.py,1359,First fit with no restriction on max samples,
scikit-learn/sklearn/ensemble/tests/test_forest.py,1366,Second fit with max samples restricted to just 2,
scikit-learn/sklearn/ensemble/tests/test_base.py,5,Authors: Gilles Louppe,
scikit-learn/sklearn/ensemble/tests/test_base.py,6,License: BSD 3 clause,
scikit-learn/sklearn/ensemble/tests/test_base.py,23,Check BaseEnsemble methods.,
scikit-learn/sklearn/ensemble/tests/test_base.py,29,empty the list and create estimators manually,
scikit-learn/sklearn/ensemble/tests/test_base.py,52,Check that instantiating a BaseEnsemble with n_estimators<=0 raises,
scikit-learn/sklearn/ensemble/tests/test_base.py,53,a ValueError.,
scikit-learn/sklearn/ensemble/tests/test_base.py,63,Check that instantiating a BaseEnsemble with a string as n_estimators,
scikit-learn/sklearn/ensemble/tests/test_base.py,64,raises a ValueError demanding n_estimators to be supplied as an integer.,
scikit-learn/sklearn/ensemble/tests/test_base.py,79,Linear Discriminant Analysis doesn't have random state: smoke test,
scikit-learn/sklearn/ensemble/tests/test_base.py,84,check random_state is None still sets,
scikit-learn/sklearn/ensemble/tests/test_base.py,88,check random_state fixes results in consistent initialisation,
scikit-learn/sklearn/ensemble/tests/test_base.py,95,nested random_state,
scikit-learn/sklearn/ensemble/tests/test_base.py,108,ensure multiple random_state parameters are invariant to get_params(),
scikit-learn/sklearn/ensemble/tests/test_base.py,109,iteration order,
scikit-learn/sklearn/ensemble/tests/test_voting.py,32,Load datasets,
scikit-learn/sklearn/ensemble/tests/test_voting.py,308,check that an error is raised and indicative if sample_weight is not,
scikit-learn/sklearn/ensemble/tests/test_voting.py,309,supported.,
scikit-learn/sklearn/ensemble/tests/test_voting.py,319,check that _fit_single_estimator will raise the right error,
scikit-learn/sklearn/ensemble/tests/test_voting.py,320,it should raise the original error if this is not linked to sample_weight,
scikit-learn/sklearn/ensemble/tests/test_voting.py,339,Should not raise an error.,
scikit-learn/sklearn/ensemble/tests/test_voting.py,344,check equivalence in the output when setting underlying estimators,
scikit-learn/sklearn/ensemble/tests/test_voting.py,361,TODO: Remove parametrization in 0.24 when None is removed in Voting*,
scikit-learn/sklearn/ensemble/tests/test_voting.py,366,Test predict,
scikit-learn/sklearn/ensemble/tests/test_voting.py,400,Test soft voting transform,
scikit-learn/sklearn/ensemble/tests/test_voting.py,426,Test estimator weights inputs as list and array,
scikit-learn/sklearn/ensemble/tests/test_voting.py,473,TODO: Remove drop=None in 0.24 when None is removed in Voting*,
scikit-learn/sklearn/ensemble/tests/test_voting.py,485,TODO: remove the parametrization on 'drop' when support for None is,
scikit-learn/sklearn/ensemble/tests/test_voting.py,486,removed.,
scikit-learn/sklearn/ensemble/tests/test_voting.py,487,check that an estimator can be set to 'drop' and passing some weight,
scikit-learn/sklearn/ensemble/tests/test_voting.py,488,regression test for,
scikit-learn/sklearn/ensemble/tests/test_voting.py,489,https://github.com/scikit-learn/scikit-learn/issues/13777,
scikit-learn/sklearn/ensemble/tests/test_voting.py,511,FIXME: to be removed when meta-estimators can specified themselves,
scikit-learn/sklearn/ensemble/tests/test_voting.py,512,their testing parameters (for required parameters).,
scikit-learn/sklearn/ensemble/tests/test_voting.py,560,TODO: Remove in 0.24 when None is removed in Voting*,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,47,toy sample,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,54,also load the boston dataset,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,55,and randomly permute it,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,61,also load the iris dataset,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,62,and randomly permute it,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,70,Check classification on a toy dataset.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,93,Check input parameter validation for GradientBoostingClassifier.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,141,test fit before feature importance,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,145,deviance requires ``n_classes >= 2``.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,153,Check input parameter validation for GradientBoostingRegressor,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,187,Test GradientBoostingClassifier on synthetic dataset used by,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,188,Hastie et al. in ESLII Example 12.7.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,216,Check consistency on dataset boston house prices with least squares,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,217,and least absolute deviation.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,251,Check consistency on dataset iris.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,273,"Test on synthetic regression datasets used in Leo Breiman,",
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,274,`Bagging Predictors?. Machine Learning 24(2): 123-140 (1996).,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,280,Friedman1,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,292,Friedman2,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,302,Friedman3,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,324,Predict probabilities.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,332,"check if probabilities are in [0, 1].",
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,337,derive predictions from probabilities,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,343,Test input checks (shape and type of X and y).,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,355,X has wrong shape,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,382,check that predict_stages through an error if the type of X is not,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,383,supported,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,401,test if max_features is valid.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,416,Test to make sure random state is set properly.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,450,The most important feature is the median income by far.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,453,The three subsequent features are the following. Their relative ordering,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,454,might change a bit depending on the randomness of the trees and the,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,455,train / test split.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,460,Test if max features is set properly for floats and str.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,494,Test whether staged decision function eventually gives,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,495,the same prediction.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,501,test raise ValueError if not fitted,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,508,test if prediction for last stage equals ``predict``,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,516,Test whether staged predict proba eventually gives,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,517,the same prediction.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,523,test raise NotFittedError if not fitted,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,529,test if prediction for last stage equals ``predict``,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,535,test if prediction for last stage equals ``predict_proba``,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,545,test that staged_functions make defensive copies,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,548,don't predict zeros,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,554,regressor has no staged_predict_proba,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,563,Check model serialization.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,583,Check if we can fit even though all targets are equal.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,586,classifier should raise exception,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,597,Check if quantile loss with alpha=0.5 equals lad.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,614,Test with non-integer class labels.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,625,Test with float class labels.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,637,Test with float class labels.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,643,This will raise a DataConversionWarning that we want to,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,644,"""always"" raise, elsewhere the warnings gets ignored in the",
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,645,"later tests, and the tests that check for this warning fail",
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,652,Test with different memory layouts of X and y,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,681,Test if oob improvement has correct shape and regression test.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,686,hard-coded regression test - change if modification in OOB computation,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,693,Test if oob improvement has correct shape.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,701,Check OOB improvement on multi-class dataset.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,708,hard-coded regression test - change if modification in OOB computation,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,709,FIXME: the following snippet does not yield the same results on 32 bits,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,710,"assert_array_almost_equal(clf.oob_improvement_[:5],",
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,711,"np.array([12.68, 10.45, 8.18, 6.43, 5.13]),",
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,712,decimal=2),
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,716,Check verbose=1 does not cause error.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,728,check output,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,731,with OOB,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,737,one for 1-10 and then 9 for 20-100,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,742,Check verbose=2 does not cause error.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,753,check output,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,756,no OOB,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,762,100 lines for n_estimators==100,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,768,Test if warm start equals fit.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,781,Random state is preserved and hence predict_proba must also be,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,782,same,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,790,Test if warm start equals fit - set n_estimators.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,805,Test if possible to fit trees of different depth in ensemble.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,812,last 10 trees have different depth,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,820,Test if fit clears state.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,826,inits state,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,828,clears old state and equals est,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,835,Test if warm start with zero n_estimators raises error,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,845,Test if warm start with smaller n_estimators raises error,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,855,Test if warm start with equal n_estimators does nothing,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,869,Test if oob can be turned on during warm start.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,877,the last 10 are not zeros,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,884,Test if warm start OOB equals fit.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,902,Test that all sparse matrix types are supported,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,931,Test that feeding a X in Fortran-ordered is giving the same results as,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,932,in C-ordered,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,959,Test if monitor return value works.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,964,this is not altered,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,969,try refit,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,984,try refit,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,994,Test greedy trees with max_depth + 1 leafs.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1010,Test greedy trees with max_depth + 1 leafs.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1024,Test if init='zero' works for regression.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1039,Test if init='zero' works for classification.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1049,binary clf,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1065,Test precedence of max_leaf_nodes over max_depth.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1081,Test if min_impurity_split of base estimators is set,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1082,Regression test for #8006,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1100,Simply check if the parameter is passed on correctly. Tree tests,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1101,will suffice for the actual working of this param,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1106,Test if warm_start does nothing if n_estimators is not changed.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1107,Regression test for #3513.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1116,Predict probabilities.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1125,"check if probabilities are in [0, 1].",
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1132,derive predictions from probabilities,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1143,ignore the first 2 training samples by setting their weight to 0,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1158,ignore the first 2 training samples by setting their weight to 0,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1225,Check if early_stopping works as expected,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1234,Without early stopping,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1265,Check if validation_fraction has an effect,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1274,Check if n_estimators_ increase monotonically with n_iter_no_change,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1275,Set validation,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1283,Make sure data splitting for early stopping is stratified,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1298,TODO: Remove in 0.24 when DummyClassifier's `strategy` default updates,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1307,Check that GradientBoostingRegressor works when init is a sklearn,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1308,estimator.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1309,Check that an error is raised if trying to fit with sample weight but,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1310,initial estimator does not support sample weight,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1315,init supports sample weights,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1319,init does not support sample weights,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1321,ok no sample weights,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1328,Check that the init estimator can be a pipeline (see issue #13466),
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1333,pipeline without sample_weight works fine,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1341,Passing sample_weight to a pipeline raises a ValueError. This test makes,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1342,sure we make the distinction between ValueError raised by a pipeline that,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1343,"was passed sample_weight, and a ValueError raised by a regular estimator",
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1344,whose input checking failed.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1348,Note that NuSVR properly supports sample_weight,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1359,Make sure error is raised if init estimators don't have the required,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1360,"methods (fit, predict, predict_proba)",
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1369,"when doing early stopping (_, , y_train, _ = train_test_split(X, y))",
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1370,there might be classes in y that are missing in y_train. As the init,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1371,"estimator will be trained on y_train, we need to raise an error if this",
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1372,happens.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1375,only 2 negative class over 10 samples,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1383,No error if we let training data be big enough,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1389,growing an ensemble of single node trees. See #13620,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,24,Check binomial deviance loss.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,25,Check against alternative definitions in ESLII.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,28,"pred has the same BD for y in {0, 1}",
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,38,check if same results as alternative definition of deviance (from ESLII),
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,51,check the gradient against the,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,62,least squares,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,70,Smoke test for init estimators with sample weights.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,88,skip multiclass,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,102,check if predictions match,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,137,Non regression test for the QuantileLossFunction object,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,138,There was a sign problem when evaluating the function,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,139,for negative values of 'ytrue - ypred',
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,147,Test if deviance supports sample weights.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,168,one-hot encoding,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,180,Make sure get_init_raw_predictions returns float64 arrays with shape,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,181,"(n_samples, K) where K is 1 for binary classification and regression, and",
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,182,K = n_classes for multiclass classification,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,215,Make sure the get_init_raw_predictions() returns the expected values for,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,216,each loss.,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,223,Least squares loss,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,227,Make sure baseline prediction is the mean of all targets,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,230,Least absolute and huber loss,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,235,Make sure baseline prediction is the median of all targets,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,238,Quantile loss,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,243,Make sure baseline prediction is the alpha-quantile of all targets,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,248,Binomial deviance,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,251,"Make sure baseline prediction is equal to link_function(p), where p",
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,252,"is the proba of the positive class. We want predict_proba() to return p,",
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,253,and by definition,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,254,p = inverse_link_function(raw_prediction) = sigmoid(raw_prediction),
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,255,So we want raw_prediction = link_function(p) = log(p / (1 - p)),
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,260,Exponential loss,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,267,Multinomial deviance loss,
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,280,Make sure quantile loss with alpha = .5 is equivalent to LAD,
scikit-learn/sklearn/__check_build/__init__.py,17,Raise a comprehensible error and list the contents of the,
scikit-learn/sklearn/__check_build/__init__.py,18,directory to help debugging on the mailing list.,
scikit-learn/sklearn/__check_build/__init__.py,22,Picking up the local install: this will work only if the,
scikit-learn/sklearn/__check_build/__init__.py,23,install is an 'inplace build',
scikit-learn/sklearn/__check_build/__init__.py,44,noqa,
scikit-learn/sklearn/__check_build/setup.py,1,Author: Virgile Fritsch <virgile.fritsch@inria.fr>,
scikit-learn/sklearn/__check_build/setup.py,2,License: BSD 3 clause,
scikit-learn/sklearn/datasets/_species_distributions.py,35,Authors: Peter Prettenhofer <peter.prettenhofer@gmail.com>,
scikit-learn/sklearn/datasets/_species_distributions.py,36,Jake Vanderplas <vanderplas@astro.washington.edu>,
scikit-learn/sklearn/datasets/_species_distributions.py,37,,
scikit-learn/sklearn/datasets/_species_distributions.py,38,License: BSD 3 clause,
scikit-learn/sklearn/datasets/_species_distributions.py,55,The original data can be found at:,
scikit-learn/sklearn/datasets/_species_distributions.py,56,https://biodiversityinformatics.amnh.org/open_source/maxent/samples.zip,
scikit-learn/sklearn/datasets/_species_distributions.py,63,The original data can be found at:,
scikit-learn/sklearn/datasets/_species_distributions.py,64,https://biodiversityinformatics.amnh.org/open_source/maxent/coverages.zip,
scikit-learn/sklearn/datasets/_species_distributions.py,126,"x,y coordinates for corner cells",
scikit-learn/sklearn/datasets/_species_distributions.py,132,x coordinates of the grid cells,
scikit-learn/sklearn/datasets/_species_distributions.py,134,y coordinates of the grid cells,
scikit-learn/sklearn/datasets/_species_distributions.py,214,Define parameters for the data files.  These should not be changed,
scikit-learn/sklearn/datasets/_species_distributions.py,215,unless the data model changes.  They will be saved in the npz file,
scikit-learn/sklearn/datasets/_species_distributions.py,216,with the downloaded data.,
scikit-learn/sklearn/datasets/_species_distributions.py,232,samples.zip is a valid npz,
scikit-learn/sklearn/datasets/_species_distributions.py,244,coverages.zip is a valid npz,
scikit-learn/sklearn/datasets/_twenty_newsgroups.py,24,Copyright (c) 2011 Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/sklearn/datasets/_twenty_newsgroups.py,25,License: BSD 3 clause,
scikit-learn/sklearn/datasets/_twenty_newsgroups.py,51,The original data can be found at:,
scikit-learn/sklearn/datasets/_twenty_newsgroups.py,52,https://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz,
scikit-learn/sklearn/datasets/_twenty_newsgroups.py,79,Store a zipped pickle,
scikit-learn/sklearn/datasets/_twenty_newsgroups.py,295,Sort the categories to have the ordering of the labels,
scikit-learn/sklearn/datasets/_twenty_newsgroups.py,301,searchsorted to have continuous labels,
scikit-learn/sklearn/datasets/_twenty_newsgroups.py,304,Use an object array to shuffle: avoids memory copy,
scikit-learn/sklearn/datasets/_twenty_newsgroups.py,315,Use an object array to shuffle: avoids memory copy,
scikit-learn/sklearn/datasets/_twenty_newsgroups.py,415,we shuffle but use a fixed seed for the memoization,
scikit-learn/sklearn/datasets/_twenty_newsgroups.py,440,the data is stored as int16 for compactness,
scikit-learn/sklearn/datasets/_twenty_newsgroups.py,441,but normalize needs floats,
scikit-learn/sklearn/datasets/_kddcup99.py,27,The original data can be found at:,
scikit-learn/sklearn/datasets/_kddcup99.py,28,https://archive.ics.uci.edu/ml/machine-learning-databases/kddcup99-mld/kddcup.data.gz,
scikit-learn/sklearn/datasets/_kddcup99.py,35,The original data can be found at:,
scikit-learn/sklearn/datasets/_kddcup99.py,36,https://archive.ics.uci.edu/ml/machine-learning-databases/kddcup99-mld/kddcup.data_10_percent.gz,
scikit-learn/sklearn/datasets/_kddcup99.py,130,selected abnormal samples:,
scikit-learn/sklearn/datasets/_kddcup99.py,140,select all samples with positive logged_in attribute:,
scikit-learn/sklearn/datasets/_kddcup99.py,288,XXX bug when compress!=0:,
scikit-learn/sklearn/datasets/_kddcup99.py,289,(error: 'Incorrect data length while decompressing[...] the file,
scikit-learn/sklearn/datasets/_kddcup99.py,290,could be corrupted.'),
scikit-learn/sklearn/datasets/_lfw.py,8,Copyright (c) 2011 Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/sklearn/datasets/_lfw.py,9,License: BSD 3 clause,
scikit-learn/sklearn/datasets/_lfw.py,26,The original data can be found in:,
scikit-learn/sklearn/datasets/_lfw.py,27,http://vis-www.cs.umass.edu/lfw/lfw.tgz,
scikit-learn/sklearn/datasets/_lfw.py,34,The original funneled data can be found in:,
scikit-learn/sklearn/datasets/_lfw.py,35,http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz,
scikit-learn/sklearn/datasets/_lfw.py,42,The original target data can be found in:,
scikit-learn/sklearn/datasets/_lfw.py,43,"http://vis-www.cs.umass.edu/lfw/pairsDevTrain.txt',",
scikit-learn/sklearn/datasets/_lfw.py,44,"http://vis-www.cs.umass.edu/lfw/pairsDevTest.txt',",
scikit-learn/sklearn/datasets/_lfw.py,45,"http://vis-www.cs.umass.edu/lfw/pairs.txt',",
scikit-learn/sklearn/datasets/_lfw.py,67,,
scikit-learn/sklearn/datasets/_lfw.py,68,Common private utilities for data fetching from the original LFW website,
scikit-learn/sklearn/datasets/_lfw.py,69,"local disk caching, and image decoding.",
scikit-learn/sklearn/datasets/_lfw.py,70,,
scikit-learn/sklearn/datasets/_lfw.py,118,import PIL only when needed,
scikit-learn/sklearn/datasets/_lfw.py,121,compute the portion of the images to load to respect the slice_ parameter,
scikit-learn/sklearn/datasets/_lfw.py,122,given by the caller,
scikit-learn/sklearn/datasets/_lfw.py,138,allocate some contiguous memory to host the decoded image slices,
scikit-learn/sklearn/datasets/_lfw.py,145,iterate over the collected file path to load the jpeg files as numpy,
scikit-learn/sklearn/datasets/_lfw.py,146,arrays,
scikit-learn/sklearn/datasets/_lfw.py,151,Checks if jpeg reading worked. Refer to issue #3594 for more,
scikit-learn/sklearn/datasets/_lfw.py,152,details.,
scikit-learn/sklearn/datasets/_lfw.py,160,"scale uint8 coded colors to the [0.0, 1.0] floats",
scikit-learn/sklearn/datasets/_lfw.py,164,average the color channels to compute a gray levels,
scikit-learn/sklearn/datasets/_lfw.py,165,representation,
scikit-learn/sklearn/datasets/_lfw.py,173,,
scikit-learn/sklearn/datasets/_lfw.py,174,Task #1:  Face Identification on picture with names,
scikit-learn/sklearn/datasets/_lfw.py,175,,
scikit-learn/sklearn/datasets/_lfw.py,183,scan the data folder content to retain people with more that,
scikit-learn/sklearn/datasets/_lfw.py,184,`min_faces_per_person` face pictures,
scikit-learn/sklearn/datasets/_lfw.py,207,shuffle the faces with a deterministic RNG scheme to avoid having,
scikit-learn/sklearn/datasets/_lfw.py,208,"all faces of the same person in a row, as it would break some",
scikit-learn/sklearn/datasets/_lfw.py,209,cross validation and learning algorithms such as SGD and online,
scikit-learn/sklearn/datasets/_lfw.py,210,k-means that make an IID assumption,
scikit-learn/sklearn/datasets/_lfw.py,303,wrap the loader in a memoizing function that will return memmaped data,
scikit-learn/sklearn/datasets/_lfw.py,304,arrays for optimal memory usage,
scikit-learn/sklearn/datasets/_lfw.py,306,Deal with change of API in joblib,
scikit-learn/sklearn/datasets/_lfw.py,312,load and memoize the pairs as np arrays,
scikit-learn/sklearn/datasets/_lfw.py,326,pack the results as a Bunch instance,
scikit-learn/sklearn/datasets/_lfw.py,332,,
scikit-learn/sklearn/datasets/_lfw.py,333,Task #2:  Face Verification on pairs of face pictures,
scikit-learn/sklearn/datasets/_lfw.py,334,,
scikit-learn/sklearn/datasets/_lfw.py,343,parse the index file to find the number of pairs to be able to allocate,
scikit-learn/sklearn/datasets/_lfw.py,344,the right amount of memory before starting to decode the jpeg files,
scikit-learn/sklearn/datasets/_lfw.py,350,iterating over the metadata lines for each pair to find the filename to,
scikit-learn/sklearn/datasets/_lfw.py,351,decode and load in memory,
scikit-learn/sklearn/datasets/_lfw.py,474,wrap the loader in a memoizing function that will return memmaped data,
scikit-learn/sklearn/datasets/_lfw.py,475,arrays for optimal memory usage,
scikit-learn/sklearn/datasets/_lfw.py,477,Deal with change of API in joblib,
scikit-learn/sklearn/datasets/_lfw.py,483,select the right metadata file according to the requested subset,
scikit-learn/sklearn/datasets/_lfw.py,494,load and memoize the pairs as np arrays,
scikit-learn/sklearn/datasets/_lfw.py,503,pack the results as a Bunch instance,
scikit-learn/sklearn/datasets/_california_housing.py,21,Authors: Peter Prettenhofer,
scikit-learn/sklearn/datasets/_california_housing.py,22,License: BSD 3 clause,
scikit-learn/sklearn/datasets/_california_housing.py,40,The original data can be found at:,
scikit-learn/sklearn/datasets/_california_housing.py,41,https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.tgz,
scikit-learn/sklearn/datasets/_california_housing.py,138,Columns are not in the same order compared to the previous,
scikit-learn/sklearn/datasets/_california_housing.py,139,URL resource on lib.stat.cmu.edu,
scikit-learn/sklearn/datasets/_california_housing.py,154,avg rooms = total rooms / households,
scikit-learn/sklearn/datasets/_california_housing.py,157,avg bed rooms = total bed rooms / households,
scikit-learn/sklearn/datasets/_california_housing.py,160,avg occupancy = population / households,
scikit-learn/sklearn/datasets/_california_housing.py,163,"target in units of 100,000",
scikit-learn/sklearn/datasets/_samples_generator.py,5,"Authors: B. Thirion, G. Varoquaux, A. Gramfort, V. Michel, O. Grisel,",
scikit-learn/sklearn/datasets/_samples_generator.py,6,"G. Louppe, J. Nothman",
scikit-learn/sklearn/datasets/_samples_generator.py,7,License: BSD 3 clause,
scikit-learn/sklearn/datasets/_samples_generator.py,157,"Count features, clusters and samples",
scikit-learn/sklearn/datasets/_samples_generator.py,162,Use log2 to avoid overflow errors,
scikit-learn/sklearn/datasets/_samples_generator.py,185,Distribute samples among clusters by weight,
scikit-learn/sklearn/datasets/_samples_generator.py,193,Initialize X and y,
scikit-learn/sklearn/datasets/_samples_generator.py,197,Build the polytope whose vertices become cluster centroids,
scikit-learn/sklearn/datasets/_samples_generator.py,206,Initially draw informative features from the standard normal,
scikit-learn/sklearn/datasets/_samples_generator.py,209,Create each cluster; a variant of make_blobs,
scikit-learn/sklearn/datasets/_samples_generator.py,213,assign labels,
scikit-learn/sklearn/datasets/_samples_generator.py,214,slice a view of the cluster,
scikit-learn/sklearn/datasets/_samples_generator.py,217,introduce random covariance,
scikit-learn/sklearn/datasets/_samples_generator.py,219,shift the cluster to a vertex,
scikit-learn/sklearn/datasets/_samples_generator.py,221,Create redundant features,
scikit-learn/sklearn/datasets/_samples_generator.py,227,Repeat some features,
scikit-learn/sklearn/datasets/_samples_generator.py,233,Fill useless features,
scikit-learn/sklearn/datasets/_samples_generator.py,237,Randomly replace labels,
scikit-learn/sklearn/datasets/_samples_generator.py,242,Randomly shift and scale,
scikit-learn/sklearn/datasets/_samples_generator.py,252,Randomly permute samples,
scikit-learn/sklearn/datasets/_samples_generator.py,255,Randomly permute features,
scikit-learn/sklearn/datasets/_samples_generator.py,366,pick a nonzero number of labels per document by rejection sampling,
scikit-learn/sklearn/datasets/_samples_generator.py,371,pick n classes,
scikit-learn/sklearn/datasets/_samples_generator.py,374,pick a class with probability P(c),
scikit-learn/sklearn/datasets/_samples_generator.py,380,pick a non-zero document length by rejection sampling,
scikit-learn/sklearn/datasets/_samples_generator.py,385,generate a document of length n_words,
scikit-learn/sklearn/datasets/_samples_generator.py,387,"if sample does not belong to any class, generate noise word",
scikit-learn/sklearn/datasets/_samples_generator.py,391,sample words with replacement from selected classes,
scikit-learn/sklearn/datasets/_samples_generator.py,412,return_indicator can be True due to backward compatibility,
scikit-learn/sklearn/datasets/_samples_generator.py,552,Randomly generate a well conditioned input set,
scikit-learn/sklearn/datasets/_samples_generator.py,556,"Randomly generate a low rank, fat tail input set",
scikit-learn/sklearn/datasets/_samples_generator.py,563,Generate a ground truth model with only n_informative features being non,
scikit-learn/sklearn/datasets/_samples_generator.py,564,zeros (the other features are not correlated to y and should be ignored,
scikit-learn/sklearn/datasets/_samples_generator.py,565,by a sparsifying regularizers such as L1 or elastic net),
scikit-learn/sklearn/datasets/_samples_generator.py,572,Add noise,
scikit-learn/sklearn/datasets/_samples_generator.py,576,Randomly permute samples and features,
scikit-learn/sklearn/datasets/_samples_generator.py,649,"so as not to have the first point = last point, we set endpoint=False",
scikit-learn/sklearn/datasets/_samples_generator.py,813,Set n_centers by looking at centers arg,
scikit-learn/sklearn/datasets/_samples_generator.py,828,Set n_centers by looking at [n_samples] arg,
scikit-learn/sklearn/datasets/_samples_generator.py,846,"stds: if cluster_std is given as list, it must be consistent",
scikit-learn/sklearn/datasets/_samples_generator.py,847,with the n_centers,
scikit-learn/sklearn/datasets/_samples_generator.py,1132,Random (ortho normal) vectors,
scikit-learn/sklearn/datasets/_samples_generator.py,1136,Index of the singular values,
scikit-learn/sklearn/datasets/_samples_generator.py,1139,Build the singular profile by assembling signal and noise components,
scikit-learn/sklearn/datasets/_samples_generator.py,1192,generate dictionary,
scikit-learn/sklearn/datasets/_samples_generator.py,1196,generate code,
scikit-learn/sklearn/datasets/_samples_generator.py,1204,encode signal,
scikit-learn/sklearn/datasets/_samples_generator.py,1350,Permute the lines: we don't want to have asymmetries in the final,
scikit-learn/sklearn/datasets/_samples_generator.py,1351,SPD matrix,
scikit-learn/sklearn/datasets/_samples_generator.py,1358,Form the diagonal vector into a row matrix,
scikit-learn/sklearn/datasets/_samples_generator.py,1528,Build multivariate normal distribution,
scikit-learn/sklearn/datasets/_samples_generator.py,1532,Sort by distance from origin,
scikit-learn/sklearn/datasets/_samples_generator.py,1536,Label by quantile,
scikit-learn/sklearn/datasets/_samples_generator.py,1616,row and column clusters of approximately equal sizes,
scikit-learn/sklearn/datasets/_samples_generator.py,1710,row and column clusters of approximately equal sizes,
scikit-learn/sklearn/datasets/_covtype.py,13,Author: Lars Buitinck,
scikit-learn/sklearn/datasets/_covtype.py,14,Peter Prettenhofer <peter.prettenhofer@gmail.com>,
scikit-learn/sklearn/datasets/_covtype.py,15,License: BSD 3 clause,
scikit-learn/sklearn/datasets/_covtype.py,32,The original data can be found in:,
scikit-learn/sklearn/datasets/_covtype.py,33,https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz,
scikit-learn/sklearn/datasets/_covtype.py,114,delete archive,
scikit-learn/sklearn/datasets/_openml.py,24,noqa,
scikit-learn/sklearn/datasets/_openml.py,100,"potentially, the directory has been created already",
scikit-learn/sklearn/datasets/_openml.py,116,"XXX: First time, decompression will not be necessary (by using fsrc), but",
scikit-learn/sklearn/datasets/_openml.py,117,it will happen nonetheless,
scikit-learn/sklearn/datasets/_openml.py,161,"412 is an OpenML specific error code, indicating a generic error",
scikit-learn/sklearn/datasets/_openml.py,162,"(e.g., data not found)",
scikit-learn/sklearn/datasets/_openml.py,166,"412 error, not in except for nicer traceback",
scikit-learn/sklearn/datasets/_openml.py,206,"turns the sparse data back into an array (can't use toarray() function,",
scikit-learn/sklearn/datasets/_openml.py,207,as this does only work on numeric data),
scikit-learn/sklearn/datasets/_openml.py,212,TODO: improve for efficiency,
scikit-learn/sklearn/datasets/_openml.py,266,This should never happen,
scikit-learn/sklearn/datasets/_openml.py,277,"only numeric, integer, real are left",
scikit-learn/sklearn/datasets/_openml.py,280,cast to floats when there are any missing values,
scikit-learn/sklearn/datasets/_openml.py,310,calculate chunksize,
scikit-learn/sklearn/datasets/_openml.py,317,read arff data with chunks,
scikit-learn/sklearn/datasets/_openml.py,362,situation in which we return the oldest active version,
scikit-learn/sklearn/datasets/_openml.py,375,an integer version has been provided,
scikit-learn/sklearn/datasets/_openml.py,380,we can do this in 1 function call if OpenML does not require the,
scikit-learn/sklearn/datasets/_openml.py,381,"specification of the dataset status (i.e., return datasets with a",
scikit-learn/sklearn/datasets/_openml.py,382,"given name / version regardless of active, deactivated, etc. )",
scikit-learn/sklearn/datasets/_openml.py,383,TODO: feature request OpenML.,
scikit-learn/sklearn/datasets/_openml.py,394,OpenML API function: https://www.openml.org/api_docs#!/data/get_data_id,
scikit-learn/sklearn/datasets/_openml.py,403,OpenML function:,
scikit-learn/sklearn/datasets/_openml.py,404,https://www.openml.org/api_docs#!/data/get_data_features_id,
scikit-learn/sklearn/datasets/_openml.py,413,OpenML API function:,
scikit-learn/sklearn/datasets/_openml.py,414,https://www.openml.org/api_docs#!/data/get_data_qualities_id,
scikit-learn/sklearn/datasets/_openml.py,422,"the qualities might not be available, but we still try to process",
scikit-learn/sklearn/datasets/_openml.py,423,the data,
scikit-learn/sklearn/datasets/_openml.py,441,"If the data qualities are unavailable, we return -1",
scikit-learn/sklearn/datasets/_openml.py,452,Accesses an ARFF file on the OpenML server. Documentation:,
scikit-learn/sklearn/datasets/_openml.py,453,https://www.openml.org/api_data_docs#!/data/get_download_id,
scikit-learn/sklearn/datasets/_openml.py,454,"encode_nominal argument is to ensure unit testing, do not alter in",
scikit-learn/sklearn/datasets/_openml.py,455,production!,
scikit-learn/sklearn/datasets/_openml.py,475,verifies the data type of the y array in case there are multiple targets,
scikit-learn/sklearn/datasets/_openml.py,476,(throws an error if these targets do not comply with sklearn support),
scikit-learn/sklearn/datasets/_openml.py,489,"note: we compare to a string, not boolean",
scikit-learn/sklearn/datasets/_openml.py,503,logic for determining on which columns can be learned. Note that from the,
scikit-learn/sklearn/datasets/_openml.py,504,OpenML guide follows that columns that have the `is_row_identifier` or,
scikit-learn/sklearn/datasets/_openml.py,505,"`is_ignore` flag, these can not be learned on. Also target columns are",
scikit-learn/sklearn/datasets/_openml.py,506,excluded.,
scikit-learn/sklearn/datasets/_openml.py,625,no caching will be applied,
scikit-learn/sklearn/datasets/_openml.py,628,"check valid function arguments. data_id XOR (name, version) should be",
scikit-learn/sklearn/datasets/_openml.py,629,provided,
scikit-learn/sklearn/datasets/_openml.py,631,"OpenML is case-insensitive, but the caching mechanism is not",
scikit-learn/sklearn/datasets/_openml.py,632,convert all data names (str) to lower case,
scikit-learn/sklearn/datasets/_openml.py,642,"from the previous if statement, it is given that name is None",
scikit-learn/sklearn/datasets/_openml.py,675,"download data features, meta-info about column types",
scikit-learn/sklearn/datasets/_openml.py,687,determines the default target based on the data feature results,
scikit-learn/sklearn/datasets/_openml.py,688,(which is currently more reliable than the data description;,
scikit-learn/sklearn/datasets/_openml.py,689,see issue: https://github.com/openml/OpenML/issues/768),
scikit-learn/sklearn/datasets/_openml.py,693,"for code-simplicity, make target_column by default a list",
scikit-learn/sklearn/datasets/_openml.py,706,prepare which columns and data types should be returned for the X and y,
scikit-learn/sklearn/datasets/_openml.py,709,XXX: col_slice_y should be all nominal or all numeric,
scikit-learn/sklearn/datasets/_openml.py,725,determine arff encoding to return,
scikit-learn/sklearn/datasets/_openml.py,727,The shape must include the ignored features to keep the right indexes,
scikit-learn/sklearn/datasets/_openml.py,728,during the arff data conversion.,
scikit-learn/sklearn/datasets/_openml.py,734,obtain the data,
scikit-learn/sklearn/datasets/_openml.py,754,nominal attributes is a dict mapping from the attribute name to the,
scikit-learn/sklearn/datasets/_openml.py,755,possible values. Includes also the target column (which will be,
scikit-learn/sklearn/datasets/_openml.py,756,"popped off below, before it will be packed in the Bunch object)",
scikit-learn/sklearn/datasets/_openml.py,767,No target,
scikit-learn/sklearn/datasets/_openml.py,780,"reshape y back to 1-D array, if there is only 1 target column; back",
scikit-learn/sklearn/datasets/_openml.py,781,to None if there are not target columns,
scikit-learn/sklearn/datasets/_olivetti_faces.py,13,Copyright (c) 2011 David Warde-Farley <wardefar at iro dot umontreal dot ca>,
scikit-learn/sklearn/datasets/_olivetti_faces.py,14,License: BSD 3 clause,
scikit-learn/sklearn/datasets/_olivetti_faces.py,29,The original data can be found at:,
scikit-learn/sklearn/datasets/_olivetti_faces.py,30,https://cs.nyu.edu/~roweis/data/olivettifaces.mat,
scikit-learn/sklearn/datasets/_olivetti_faces.py,111,delete raw .mat data,
scikit-learn/sklearn/datasets/_olivetti_faces.py,120,"We want floating point data, but float32 is enough (there is only",
scikit-learn/sklearn/datasets/_olivetti_faces.py,121,one byte of precision in the original uint8s anyway),
scikit-learn/sklearn/datasets/_olivetti_faces.py,126,"10 images per class, 400 images total, each class is contiguous.",
scikit-learn/sklearn/datasets/_rcv1.py,8,Author: Tom Dupre la Tour,
scikit-learn/sklearn/datasets/_rcv1.py,9,License: BSD 3 clause,
scikit-learn/sklearn/datasets/_rcv1.py,30,The original vectorized data can be found at:,
scikit-learn/sklearn/datasets/_rcv1.py,31,http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/a13-vector-files/lyrl2004_vectors_test_pt0.dat.gz,
scikit-learn/sklearn/datasets/_rcv1.py,32,http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/a13-vector-files/lyrl2004_vectors_test_pt1.dat.gz,
scikit-learn/sklearn/datasets/_rcv1.py,33,http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/a13-vector-files/lyrl2004_vectors_test_pt2.dat.gz,
scikit-learn/sklearn/datasets/_rcv1.py,34,http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/a13-vector-files/lyrl2004_vectors_test_pt3.dat.gz,
scikit-learn/sklearn/datasets/_rcv1.py,35,http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/a13-vector-files/lyrl2004_vectors_train.dat.gz,
scikit-learn/sklearn/datasets/_rcv1.py,36,while the original stemmed token files can be found,
scikit-learn/sklearn/datasets/_rcv1.py,37,"in the README, section B.12.i.:",
scikit-learn/sklearn/datasets/_rcv1.py,38,http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/lyrl2004_rcv1v2_README.htm,
scikit-learn/sklearn/datasets/_rcv1.py,67,The original data can be found at:,
scikit-learn/sklearn/datasets/_rcv1.py,68,http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a08-topic-qrels/rcv1-v2.topics.qrels.gz,
scikit-learn/sklearn/datasets/_rcv1.py,165,load data (X) and sample_id,
scikit-learn/sklearn/datasets/_rcv1.py,176,Training data is before testing data,
scikit-learn/sklearn/datasets/_rcv1.py,184,delete archives,
scikit-learn/sklearn/datasets/_rcv1.py,192,"load target (y), categories, and sample_id_bis",
scikit-learn/sklearn/datasets/_rcv1.py,199,parse the target file,
scikit-learn/sklearn/datasets/_rcv1.py,222,delete archive,
scikit-learn/sklearn/datasets/_rcv1.py,225,"Samples in X are ordered with sample_id,",
scikit-learn/sklearn/datasets/_rcv1.py,226,"whereas in y, they are ordered with sample_id_bis.",
scikit-learn/sklearn/datasets/_rcv1.py,230,"save category names in a list, with same order than y",
scikit-learn/sklearn/datasets/_rcv1.py,235,reorder categories in lexicographic order,
scikit-learn/sklearn/datasets/_rcv1.py,279,s[p] = i,
scikit-learn/sklearn/datasets/_svmlight_format_io.py,13,Authors: Mathieu Blondel <mathieu@mblondel.org>,
scikit-learn/sklearn/datasets/_svmlight_format_io.py,14,Lars Buitinck,
scikit-learn/sklearn/datasets/_svmlight_format_io.py,15,Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/sklearn/datasets/_svmlight_format_io.py,16,License: BSD 3 clause,
scikit-learn/sklearn/datasets/_svmlight_format_io.py,159,file descriptor,
scikit-learn/sklearn/datasets/_svmlight_format_io.py,187,"convert from array.array, give data the right dtype",
scikit-learn/sklearn/datasets/_svmlight_format_io.py,192,never empty,
scikit-learn/sklearn/datasets/_svmlight_format_io.py,195,"no-op for float{32,64}",
scikit-learn/sklearn/datasets/_svmlight_format_io.py,287,disable heuristic search to avoid getting inconsistent results on,
scikit-learn/sklearn/datasets/_svmlight_format_io.py,288,different segments of the file,
scikit-learn/sklearn/datasets/_svmlight_format_io.py,433,Convert comment string to list of lines in UTF-8.,
scikit-learn/sklearn/datasets/_svmlight_format_io.py,434,"If a byte string is passed, then check whether it's ASCII;",
scikit-learn/sklearn/datasets/_svmlight_format_io.py,435,"if a user wants to get fancy, they'll have to decode themselves.",
scikit-learn/sklearn/datasets/_svmlight_format_io.py,436,Avoid mention of str and unicode types for Python 3.x compat.,
scikit-learn/sklearn/datasets/_svmlight_format_io.py,438,just for the exception,
scikit-learn/sklearn/datasets/_svmlight_format_io.py,461,"We had some issues with CSR matrices with unsorted indices (e.g. #1501),",
scikit-learn/sklearn/datasets/_svmlight_format_io.py,462,"so sort them here, but first make sure we don't modify the user's X.",
scikit-learn/sklearn/datasets/_svmlight_format_io.py,463,TODO We can do this cheaper; sorted_indices copies the whole matrix.,
scikit-learn/sklearn/datasets/_base.py,5,Copyright (c) 2007 David Cournapeau <cournape@gmail.com>,
scikit-learn/sklearn/datasets/_base.py,6,2010 Fabian Pedregosa <fabian.pedregosa@inria.fr>,
scikit-learn/sklearn/datasets/_base.py,7,2010 Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/sklearn/datasets/_base.py,8,License: BSD 3 clause,
scikit-learn/sklearn/datasets/_base.py,198,convert to array for fancy indexing,
scikit-learn/sklearn/datasets/_base.py,896,Read data,
scikit-learn/sklearn/datasets/_base.py,900,Read header,
scikit-learn/sklearn/datasets/_base.py,999,names of features,
scikit-learn/sklearn/datasets/_base.py,1011,last column is target value,
scikit-learn/sklearn/datasets/_base.py,1050,import PIL only when needed,
scikit-learn/sklearn/datasets/_base.py,1059,Load image data for each image in the source folder.,
scikit-learn/sklearn/datasets/tests/test_olivetti_faces.py,25,test the return_X_y option,
scikit-learn/sklearn/datasets/tests/test_lfw.py,61,generate some random jpeg files for each person,
scikit-learn/sklearn/datasets/tests/test_lfw.py,78,add some random file pollution to test robustness,
scikit-learn/sklearn/datasets/tests/test_lfw.py,82,generate some pairing metadata files using the same format as LFW,
scikit-learn/sklearn/datasets/tests/test_lfw.py,126,The data is croped around the center as a rectangular bounding box,
scikit-learn/sklearn/datasets/tests/test_lfw.py,127,around the face. Colors are converted to gray levels:,
scikit-learn/sklearn/datasets/tests/test_lfw.py,131,the target is array of person integer ids,
scikit-learn/sklearn/datasets/tests/test_lfw.py,134,names of the persons can be found using the target_names array,
scikit-learn/sklearn/datasets/tests/test_lfw.py,138,It is possible to ask for the original data without any croping or color,
scikit-learn/sklearn/datasets/tests/test_lfw.py,139,conversion and not limit on the number of picture per person,
scikit-learn/sklearn/datasets/tests/test_lfw.py,145,the ids and class names are the same as previously,
scikit-learn/sklearn/datasets/tests/test_lfw.py,152,test return_X_y option,
scikit-learn/sklearn/datasets/tests/test_lfw.py,176,The data is croped around the center as a rectangular bounding box,
scikit-learn/sklearn/datasets/tests/test_lfw.py,177,around the face. Colors are converted to gray levels:,
scikit-learn/sklearn/datasets/tests/test_lfw.py,180,the target is whether the person is the same or not,
scikit-learn/sklearn/datasets/tests/test_lfw.py,183,names of the persons can be found using the target_names array,
scikit-learn/sklearn/datasets/tests/test_lfw.py,187,It is possible to ask for the original data without any croping or color,
scikit-learn/sklearn/datasets/tests/test_lfw.py,188,conversion,
scikit-learn/sklearn/datasets/tests/test_lfw.py,194,the ids and class names are the same as previously,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,32,test X's shape,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,38,test X's non-zero values,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,45,tests X's zero values,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,52,test can change X's values,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,56,test y,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,61,test loading from file descriptor,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,96,test X'shape,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,101,test X's non-zero values,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,107,21 features in file,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,116,necessary under windows,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,121,"because we ""close"" it manually and write to it,",
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,122,we need to remove it manually.,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,128,necessary under windows,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,133,"because we ""close"" it manually and write to it,",
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,134,we need to remove it manually.,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,172,load svmfile with qid attribute,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,207,in python 3 integers are valid file opening arguments (taken as unix,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,208,file descriptors),
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,223,"slicing a csr_matrix can unsort its .indices, so test that we sort",
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,224,those correctly,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,233,we need to pass a comment to get the version info in;,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,234,LibSVM doesn't grok comments so they're not put in by,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,235,default anymore.,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,238,"make sure y's shape is: (n_samples, n_labels)",
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,239,when it is sparse,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,242,"Note: with dtype=np.int32 we are performing unsafe casts,",
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,243,where X.astype(dtype) overflows. The result is,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,244,then platform dependent and X_dense.astype(dtype) may be,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,245,different from X_sparse.astype(dtype).asarray().,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,274,allow a rounding error at the last decimal place,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,280,allow a rounding error at the last decimal place,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,297,make sure it dumps multilabel correctly,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,308,loses the last decimal place,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,319,make sure it's using the most concise format possible,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,327,make sure it's correct too :),
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,346,XXX we have to update this to support Python 3.x,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,380,test dumping a file with query_id,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,395,load svmfile with longint qid attribute,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,458,put some marks that are likely to happen anywhere in a row,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,465,load the original sparse matrix into 3 independent CSR matrices,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,501,load the same data in 2 parts with all the possible byte offsets to,
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,502,locate the split so has to test for particular boundary cases,
scikit-learn/sklearn/datasets/tests/test_california_housing.py,15,test return_X_y option,
scikit-learn/sklearn/datasets/tests/test_california_housing.py,32,Check that pandas is imported lazily and that an informative error,
scikit-learn/sklearn/datasets/tests/test_california_housing.py,33,message is raised when pandas is missing:,
scikit-learn/sklearn/datasets/tests/test_rcv1.py,18,test sparsity,
scikit-learn/sklearn/datasets/tests/test_rcv1.py,24,test shapes,
scikit-learn/sklearn/datasets/tests/test_rcv1.py,30,test ordering of categories,
scikit-learn/sklearn/datasets/tests/test_rcv1.py,34,test number of sample for some categories,
scikit-learn/sklearn/datasets/tests/test_rcv1.py,41,test shuffling and subset,
scikit-learn/sklearn/datasets/tests/test_rcv1.py,46,test return_X_y option,
scikit-learn/sklearn/datasets/tests/test_rcv1.py,50,The first 23149 samples are the training samples,
scikit-learn/sklearn/datasets/tests/test_rcv1.py,53,test some precise values,
scikit-learn/sklearn/datasets/tests/test_covtype.py,23,test return_X_y option,
scikit-learn/sklearn/datasets/tests/test_20news.py,17,Extract a reduced dataset,
scikit-learn/sklearn/datasets/tests/test_20news.py,20,Check that the ordering of the target_names is the same,
scikit-learn/sklearn/datasets/tests/test_20news.py,21,as the ordering in the full dataset,
scikit-learn/sklearn/datasets/tests/test_20news.py,23,Assert that we have only 0 and 1 as labels,
scikit-learn/sklearn/datasets/tests/test_20news.py,26,Check that the number of filenames is consistent with data/target,
scikit-learn/sklearn/datasets/tests/test_20news.py,30,Check that the first entry of the reduced dataset corresponds to,
scikit-learn/sklearn/datasets/tests/test_20news.py,31,the first entry of the corresponding category in the full dataset,
scikit-learn/sklearn/datasets/tests/test_20news.py,38,check that return_X_y option,
scikit-learn/sklearn/datasets/tests/test_20news.py,49,Extract the full dataset,
scikit-learn/sklearn/datasets/tests/test_20news.py,57,test subset = train,
scikit-learn/sklearn/datasets/tests/test_20news.py,64,test subset = test,
scikit-learn/sklearn/datasets/tests/test_20news.py,71,test return_X_y option,
scikit-learn/sklearn/datasets/tests/test_20news.py,75,test subset = all,
scikit-learn/sklearn/datasets/tests/test_common.py,9,noqa,
scikit-learn/sklearn/datasets/tests/test_common.py,12,Check that pandas is imported lazily and that an informative error,
scikit-learn/sklearn/datasets/tests/test_common.py,13,message is raised when pandas is missing:,
scikit-learn/sklearn/datasets/tests/test_openml.py,30,"if True, urlopen will be monkey patched to only use local files",
scikit-learn/sklearn/datasets/tests/test_openml.py,35,XXX Test is intended to verify/ensure correct decoding behavior,
scikit-learn/sklearn/datasets/tests/test_openml.py,36,Not usable with sparse data or datasets that have columns marked as,
scikit-learn/sklearn/datasets/tests/test_openml.py,37,"{row_identifier, ignore}",
scikit-learn/sklearn/datasets/tests/test_openml.py,41,"XXX: This would be faster with np.take, although it does not",
scikit-learn/sklearn/datasets/tests/test_openml.py,42,handle missing values fast (also not with mode='wrap'),
scikit-learn/sklearn/datasets/tests/test_openml.py,48,non-nominal attribute,
scikit-learn/sklearn/datasets/tests/test_openml.py,53,also obtain decoded arff,
scikit-learn/sklearn/datasets/tests/test_openml.py,64,"XXX: Test per column, as this makes it easier to avoid problems with",
scikit-learn/sklearn/datasets/tests/test_openml.py,65,missing values,
scikit-learn/sklearn/datasets/tests/test_openml.py,77,"fetches a dataset in three various ways from OpenML, using the",
scikit-learn/sklearn/datasets/tests/test_openml.py,78,"fetch_openml function, and does various checks on the validity of the",
scikit-learn/sklearn/datasets/tests/test_openml.py,79,result. Note that this function can be mocked (by invoking,
scikit-learn/sklearn/datasets/tests/test_openml.py,80,_monkey_patch_webbased_functions before invoking this function),
scikit-learn/sklearn/datasets/tests/test_openml.py,85,"Please note that cache=False is crucial, as the monkey patched files are",
scikit-learn/sklearn/datasets/tests/test_openml.py,86,not consistent with reality,
scikit-learn/sklearn/datasets/tests/test_openml.py,88,"without specifying the version, there is no guarantee that the data id",
scikit-learn/sklearn/datasets/tests/test_openml.py,89,will be the same,
scikit-learn/sklearn/datasets/tests/test_openml.py,91,fetch with dataset id,
scikit-learn/sklearn/datasets/tests/test_openml.py,97,"single target, so target is vector",
scikit-learn/sklearn/datasets/tests/test_openml.py,101,"multi target, so target is array",
scikit-learn/sklearn/datasets/tests/test_openml.py,111,TODO: pass in a list of expected nominal features,
scikit-learn/sklearn/datasets/tests/test_openml.py,119,check whether the data by id and data by id target are equal,
scikit-learn/sklearn/datasets/tests/test_openml.py,132,np.isnan doesn't work on CSR matrix,
scikit-learn/sklearn/datasets/tests/test_openml.py,136,test return_X_y option,
scikit-learn/sklearn/datasets/tests/test_openml.py,146,monkey patches the urlopen function. Important note: Do NOT use this,
scikit-learn/sklearn/datasets/tests/test_openml.py,147,"in combination with a regular cache directory, as the files that are",
scikit-learn/sklearn/datasets/tests/test_openml.py,148,stored as cache should not be mixed up with real openml datasets,
scikit-learn/sklearn/datasets/tests/test_openml.py,225,"load the file itself, to simulate a http error",
scikit-learn/sklearn/datasets/tests/test_openml.py,254,XXX: Global variable,
scikit-learn/sklearn/datasets/tests/test_openml.py,285,classification dataset with numeric only columns,
scikit-learn/sklearn/datasets/tests/test_openml.py,326,as_frame = True returns the same underlying data as as_frame = False,
scikit-learn/sklearn/datasets/tests/test_openml.py,345,classification dataset with numeric only columns,
scikit-learn/sklearn/datasets/tests/test_openml.py,387,classification dataset with numeric and categorical columns,
scikit-learn/sklearn/datasets/tests/test_openml.py,424,regression dataset with numeric and categorical columns,
scikit-learn/sklearn/datasets/tests/test_openml.py,521,Check because of the numeric row attribute (issue #12329),
scikit-learn/sklearn/datasets/tests/test_openml.py,554,"JvR: very important check, as this dataset defined several row ids",
scikit-learn/sklearn/datasets/tests/test_openml.py,555,"and ignore attributes. Note that data_features json has 82 attributes,",
scikit-learn/sklearn/datasets/tests/test_openml.py,556,"and row id (1), ignore attributes (3) have been removed.",
scikit-learn/sklearn/datasets/tests/test_openml.py,594,classification dataset with multiple targets (natively),
scikit-learn/sklearn/datasets/tests/test_openml.py,632,dataset with strings,
scikit-learn/sklearn/datasets/tests/test_openml.py,689,classification dataset with numeric only columns,
scikit-learn/sklearn/datasets/tests/test_openml.py,726,classification dataset with numeric only columns,
scikit-learn/sklearn/datasets/tests/test_openml.py,745,classification dataset with numeric and categorical columns,
scikit-learn/sklearn/datasets/tests/test_openml.py,750,Not all original instances included for space reasons,
scikit-learn/sklearn/datasets/tests/test_openml.py,770,classification dataset with numeric and categorical columns,
scikit-learn/sklearn/datasets/tests/test_openml.py,775,Not all original instances included for space reasons,
scikit-learn/sklearn/datasets/tests/test_openml.py,789,regression dataset with numeric and categorical columns,
scikit-learn/sklearn/datasets/tests/test_openml.py,813,sparse dataset,
scikit-learn/sklearn/datasets/tests/test_openml.py,814,Australian is the only sparse dataset that is reasonably small,
scikit-learn/sklearn/datasets/tests/test_openml.py,815,"as it is inactive, we need to catch the warning. Due to mocking",
scikit-learn/sklearn/datasets/tests/test_openml.py,816,"framework, it is not deactivated in our tests",
scikit-learn/sklearn/datasets/tests/test_openml.py,821,Not all original instances included for space reasons,
scikit-learn/sklearn/datasets/tests/test_openml.py,839,numpy specific check,
scikit-learn/sklearn/datasets/tests/test_openml.py,845,Check because of the numeric row attribute (issue #12329),
scikit-learn/sklearn/datasets/tests/test_openml.py,850,Not all original instances included for space reasons,
scikit-learn/sklearn/datasets/tests/test_openml.py,864,"JvR: very important check, as this dataset defined several row ids",
scikit-learn/sklearn/datasets/tests/test_openml.py,865,"and ignore attributes. Note that data_features json has 82 attributes,",
scikit-learn/sklearn/datasets/tests/test_openml.py,866,"and row id (1), ignore attributes (3) have been removed (and target is",
scikit-learn/sklearn/datasets/tests/test_openml.py,867,stored in data.target),
scikit-learn/sklearn/datasets/tests/test_openml.py,872,Not all original instances included for space reasons,
scikit-learn/sklearn/datasets/tests/test_openml.py,886,classification dataset with multiple targets (natively),
scikit-learn/sklearn/datasets/tests/test_openml.py,918,first fill the cache,
scikit-learn/sklearn/datasets/tests/test_openml.py,920,assert file exists,
scikit-learn/sklearn/datasets/tests/test_openml.py,923,"redownload, to utilize cache",
scikit-learn/sklearn/datasets/tests/test_openml.py,963,The first call will raise an error since location exists,
scikit-learn/sklearn/datasets/tests/test_openml.py,1030,fetch inactive dataset by id,
scikit-learn/sklearn/datasets/tests/test_openml.py,1036,fetch inactive dataset by name and version,
scikit-learn/sklearn/datasets/tests/test_openml.py,1046,there is no active version of glass2,
scikit-learn/sklearn/datasets/tests/test_openml.py,1049,Note that we only want to search by name (not data id),
scikit-learn/sklearn/datasets/tests/test_openml.py,1059,Note that we only want to search by name (not data id),
scikit-learn/sklearn/datasets/tests/test_openml.py,1072,single column test,
scikit-learn/sklearn/datasets/tests/test_openml.py,1081,multi column test,
scikit-learn/sklearn/datasets/tests/test_openml.py,1096,single column test,
scikit-learn/sklearn/datasets/tests/test_openml.py,1167,Regression test for #14340,
scikit-learn/sklearn/datasets/tests/test_openml.py,1168,62 is the ID of the ZOO dataset,
scikit-learn/sklearn/datasets/tests/test_openml.py,1174,"The dataset has 17 features, including 1 ignored (animal),",
scikit-learn/sklearn/datasets/tests/test_openml.py,1175,so we assert that we don't have the ignored feature in the final Bunch,
scikit-learn/sklearn/datasets/tests/test_base.py,73,get_data_home will point to a pre-existing folder,
scikit-learn/sklearn/datasets/tests/test_base.py,78,clear_data_home will delete both the content and the folder it-self,
scikit-learn/sklearn/datasets/tests/test_base.py,82,if the folder is missing it will be created again,
scikit-learn/sklearn/datasets/tests/test_base.py,134,assert is china image,
scikit-learn/sklearn/datasets/tests/test_base.py,137,assert is flower image,
scikit-learn/sklearn/datasets/tests/test_base.py,150,test return_X_y option,
scikit-learn/sklearn/datasets/tests/test_base.py,184,test return_X_y option,
scikit-learn/sklearn/datasets/tests/test_base.py,197,test return_X_y option,
scikit-learn/sklearn/datasets/tests/test_base.py,209,test return_X_y option,
scikit-learn/sklearn/datasets/tests/test_base.py,220,test return_X_y option,
scikit-learn/sklearn/datasets/tests/test_base.py,232,test return_X_y option,
scikit-learn/sklearn/datasets/tests/test_base.py,271,test return_X_y option,
scikit-learn/sklearn/datasets/tests/test_base.py,284,This reproduces a problem when Bunch pickles have been created,
scikit-learn/sklearn/datasets/tests/test_base.py,285,with scikit-learn 0.16 and are read with 0.17. Basically there,
scikit-learn/sklearn/datasets/tests/test_base.py,286,is a surprising behaviour because reading bunch.key uses,
scikit-learn/sklearn/datasets/tests/test_base.py,287,bunch.__dict__ (which is non empty for 0.16 Bunch objects),
scikit-learn/sklearn/datasets/tests/test_base.py,288,whereas assigning into bunch.key uses bunch.__setattr__. See,
scikit-learn/sklearn/datasets/tests/test_base.py,289,https://github.com/scikit-learn/scikit-learn/issues/6196 for,
scikit-learn/sklearn/datasets/tests/test_base.py,290,more details,
scikit-learn/sklearn/datasets/tests/test_base.py,293,After loading from pickle the __dict__ should have been ignored,
scikit-learn/sklearn/datasets/tests/test_base.py,296,Making sure that changing the attr does change the value,
scikit-learn/sklearn/datasets/tests/test_base.py,297,associated with __getitem__ as well,
scikit-learn/sklearn/datasets/tests/test_base.py,304,check that dir (important for autocomplete) shows attributes,
scikit-learn/sklearn/datasets/tests/test_samples_generator.py,52,Test for n_features > 30,
scikit-learn/sklearn/datasets/tests/test_samples_generator.py,70,Create very separate clusters; check that vertices are unique and,
scikit-learn/sklearn/datasets/tests/test_samples_generator.py,71,correspond to classes,
scikit-learn/sklearn/datasets/tests/test_samples_generator.py,98,"Cluster by sign, viewed as strings to allow uniquing",
scikit-learn/sklearn/datasets/tests/test_samples_generator.py,121,Ensure on vertices of hypercube,
scikit-learn/sklearn/datasets/tests/test_samples_generator.py,201,Also test return_distributions and return_indicator with True,
scikit-learn/sklearn/datasets/tests/test_samples_generator.py,254,"Test that y ~= np.dot(X, c) + bias + N(0, 1.0).",
scikit-learn/sklearn/datasets/tests/test_samples_generator.py,257,Test with small number of features.,
scikit-learn/sklearn/datasets/tests/test_samples_generator.py,258,n_informative=3,
scikit-learn/sklearn/datasets/tests/test_samples_generator.py,272,"Test that y ~= np.dot(X, c) + bias + N(0, 1.0)",
scikit-learn/sklearn/datasets/tests/test_samples_generator.py,520,"Testing odd and even case, because in the past make_circles always",
scikit-learn/sklearn/datasets/tests/test_samples_generator.py,521,created an even number of samples.,
scikit-learn/sklearn/utils/_joblib.py,5,joblib imports may raise DeprecationWarning on certain Python,
scikit-learn/sklearn/utils/_joblib.py,6,versions,
scikit-learn/sklearn/utils/_pprint.py,4,"Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010,",
scikit-learn/sklearn/utils/_pprint.py,5,"2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018 Python Software Foundation;",
scikit-learn/sklearn/utils/_pprint.py,6,All Rights Reserved,
scikit-learn/sklearn/utils/_pprint.py,8,"Authors: Fred L. Drake, Jr. <fdrake@acm.org> (built-in CPython pprint module)",
scikit-learn/sklearn/utils/_pprint.py,9,Nicolas Hug (scikit-learn specific changes),
scikit-learn/sklearn/utils/_pprint.py,11,License: PSF License version 2 (see below),
scikit-learn/sklearn/utils/_pprint.py,13,PYTHON SOFTWARE FOUNDATION LICENSE VERSION 2,
scikit-learn/sklearn/utils/_pprint.py,14,--------------------------------------------,
scikit-learn/sklearn/utils/_pprint.py,16,"1. This LICENSE AGREEMENT is between the Python Software Foundation (""PSF""),",
scikit-learn/sklearn/utils/_pprint.py,17,"and the Individual or Organization (""Licensee"") accessing and otherwise",
scikit-learn/sklearn/utils/_pprint.py,18,"using this software (""Python"") in source or binary form and its associated",
scikit-learn/sklearn/utils/_pprint.py,19,documentation.,
scikit-learn/sklearn/utils/_pprint.py,21,"2. Subject to the terms and conditions of this License Agreement, PSF hereby",
scikit-learn/sklearn/utils/_pprint.py,22,"grants Licensee a nonexclusive, royalty-free, world-wide license to",
scikit-learn/sklearn/utils/_pprint.py,23,"reproduce, analyze, test, perform and/or display publicly, prepare",
scikit-learn/sklearn/utils/_pprint.py,24,"derivative works, distribute, and otherwise use Python alone or in any",
scikit-learn/sklearn/utils/_pprint.py,25,"derivative version, provided, however, that PSF's License Agreement and",
scikit-learn/sklearn/utils/_pprint.py,26,"PSF's notice of copyright, i.e., ""Copyright (c) 2001, 2002, 2003, 2004,",
scikit-learn/sklearn/utils/_pprint.py,27,"2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016,",
scikit-learn/sklearn/utils/_pprint.py,28,"2017, 2018 Python Software Foundation; All Rights Reserved"" are retained in",
scikit-learn/sklearn/utils/_pprint.py,29,Python alone or in any derivative version prepared by Licensee.,
scikit-learn/sklearn/utils/_pprint.py,31,3. In the event Licensee prepares a derivative work that is based on or,
scikit-learn/sklearn/utils/_pprint.py,32,"incorporates Python or any part thereof, and wants to make the derivative",
scikit-learn/sklearn/utils/_pprint.py,33,"work available to others as provided herein, then Licensee hereby agrees to",
scikit-learn/sklearn/utils/_pprint.py,34,include in any such work a brief summary of the changes made to Python.,
scikit-learn/sklearn/utils/_pprint.py,36,"4. PSF is making Python available to Licensee on an ""AS IS"" basis. PSF MAKES",
scikit-learn/sklearn/utils/_pprint.py,37,"NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR IMPLIED. BY WAY OF EXAMPLE, BUT",
scikit-learn/sklearn/utils/_pprint.py,38,"NOT LIMITATION, PSF MAKES NO AND DISCLAIMS ANY REPRESENTATION OR WARRANTY OF",
scikit-learn/sklearn/utils/_pprint.py,39,MERCHANTABILITY OR FITNESS FOR ANY PARTICULAR PURPOSE OR THAT THE USE OF,
scikit-learn/sklearn/utils/_pprint.py,40,PYTHON WILL NOT INFRINGE ANY THIRD PARTY RIGHTS.,
scikit-learn/sklearn/utils/_pprint.py,42,5. PSF SHALL NOT BE LIABLE TO LICENSEE OR ANY OTHER USERS OF PYTHON FOR ANY,
scikit-learn/sklearn/utils/_pprint.py,43,"INCIDENTAL, SPECIAL, OR CONSEQUENTIAL DAMAGES OR LOSS AS A RESULT OF",
scikit-learn/sklearn/utils/_pprint.py,44,"MODIFYING, DISTRIBUTING, OR OTHERWISE USING PYTHON, OR ANY DERIVATIVE",
scikit-learn/sklearn/utils/_pprint.py,45,"THEREOF, EVEN IF ADVISED OF THE POSSIBILITY THEREOF.",
scikit-learn/sklearn/utils/_pprint.py,47,6. This License Agreement will automatically terminate upon a material,
scikit-learn/sklearn/utils/_pprint.py,48,breach of its terms and conditions.,
scikit-learn/sklearn/utils/_pprint.py,50,7. Nothing in this License Agreement shall be deemed to create any,
scikit-learn/sklearn/utils/_pprint.py,51,"relationship of agency, partnership, or joint venture between PSF and",
scikit-learn/sklearn/utils/_pprint.py,52,Licensee. This License Agreement does not grant permission to use PSF,
scikit-learn/sklearn/utils/_pprint.py,53,trademarks or trade name in a trademark sense to endorse or promote products,
scikit-learn/sklearn/utils/_pprint.py,54,"or services of Licensee, or any third party.",
scikit-learn/sklearn/utils/_pprint.py,56,"8. By copying, installing or otherwise using Python, Licensee agrees to be",
scikit-learn/sklearn/utils/_pprint.py,57,bound by the terms and conditions of this License Agreement.,
scikit-learn/sklearn/utils/_pprint.py,60,Brief summary of changes to original code:,
scikit-learn/sklearn/utils/_pprint.py,61,"- ""compact"" parameter is supported for dicts, not just lists or tuples",
scikit-learn/sklearn/utils/_pprint.py,62,"- estimators have a custom handler, they're not just treated as objects",
scikit-learn/sklearn/utils/_pprint.py,63,"- long sequences (lists, tuples, dict items) with more than N elements are",
scikit-learn/sklearn/utils/_pprint.py,64,"shortened using ellipsis (', ...') at the end.",
scikit-learn/sklearn/utils/_pprint.py,78,needed for _dispatch[tuple.__repr__] not to be overridden,
scikit-learn/sklearn/utils/_pprint.py,161,ignore indent param,
scikit-learn/sklearn/utils/_pprint.py,163,"Max number of elements in a list, dict, tuple until we start using",
scikit-learn/sklearn/utils/_pprint.py,164,ellipsis. This also affects the number of arguments of an estimators,
scikit-learn/sklearn/utils/_pprint.py,165,(they are treated as dicts),
scikit-learn/sklearn/utils/_pprint.py,324,Note: need to copy _dispatch to prevent instances of the builtin,
scikit-learn/sklearn/utils/_pprint.py,325,PrettyPrinter class to call methods of _EstimatorPrettyPrinter (see issue,
scikit-learn/sklearn/utils/_pprint.py,326,12906),
scikit-learn/sklearn/utils/_pprint.py,327,"mypy error: ""Type[PrettyPrinter]"" has no attribute ""_dispatch""",
scikit-learn/sklearn/utils/_pprint.py,328,type: ignore,
scikit-learn/sklearn/utils/fixes.py,6,Authors: Emmanuelle Gouillart <emmanuelle.gouillart@normalesup.org>,
scikit-learn/sklearn/utils/fixes.py,7,Gael Varoquaux <gael.varoquaux@normalesup.org>,
scikit-learn/sklearn/utils/fixes.py,8,Fabian Pedregosa <fpedregosa@acm.org>,
scikit-learn/sklearn/utils/fixes.py,9,Lars Buitinck,
scikit-learn/sklearn/utils/fixes.py,10,,
scikit-learn/sklearn/utils/fixes.py,11,License: BSD 3 clause,
scikit-learn/sklearn/utils/fixes.py,19,noqa,
scikit-learn/sklearn/utils/fixes.py,28,x may be of the form dev-1ea1592,
scikit-learn/sklearn/utils/fixes.py,40,"Backport of lobpcg functionality from scipy 1.4.0, can be removed",
scikit-learn/sklearn/utils/fixes.py,41,"once support for sp_version < (1, 4) is dropped",
scikit-learn/sklearn/utils/fixes.py,42,mypy error: Name 'lobpcg' already defined (possibly by an import),
scikit-learn/sklearn/utils/fixes.py,43,type: ignore  # noqa,
scikit-learn/sklearn/utils/fixes.py,46,Preserves earlier default choice of pinvh cutoff `cond` value.,
scikit-learn/sklearn/utils/fixes.py,47,Can be removed once issue #14055 is fully addressed.,
scikit-learn/sklearn/utils/fixes.py,50,mypy error: Name 'pinvh' already defined (possibly by an import),
scikit-learn/sklearn/utils/fixes.py,51,type: ignore  # noqa,
scikit-learn/sklearn/utils/fixes.py,58,"TODO: replace by copy=False, when only scipy > 1.1 is supported.",
scikit-learn/sklearn/utils/estimator_checks.py,62,"if estimator doesn't have _get_tags, use _DEFAULT_TAGS",
scikit-learn/sklearn/utils/estimator_checks.py,63,"if estimator has tags but not key, use _DEFAULT_TAGS[key]",
scikit-learn/sklearn/utils/estimator_checks.py,88,Check that all estimator yield informative messages when,
scikit-learn/sklearn/utils/estimator_checks.py,89,trained on empty datasets,
scikit-learn/sklearn/utils/estimator_checks.py,96,"cross-decomposition's ""transform"" returns X and Y",
scikit-learn/sklearn/utils/estimator_checks.py,100,Test that all estimators check their input for NaN's and infs,
scikit-learn/sklearn/utils/estimator_checks.py,104,Check that pairwise estimator throws error on non-square input,
scikit-learn/sklearn/utils/estimator_checks.py,113,"Test that estimators can be pickled, and once pickled",
scikit-learn/sklearn/utils/estimator_checks.py,114,give the same answer as before.,
scikit-learn/sklearn/utils/estimator_checks.py,121,test classifiers can handle non-array data and pandas objects,
scikit-learn/sklearn/utils/estimator_checks.py,123,test classifiers trained on a single label always return this label,
scikit-learn/sklearn/utils/estimator_checks.py,129,basic consistency testing,
scikit-learn/sklearn/utils/estimator_checks.py,146,test if predict_proba is a monotonic transformation of decision_function,
scikit-learn/sklearn/utils/estimator_checks.py,152,Checks that the Estimator targets are not NaN.,
scikit-learn/sklearn/utils/estimator_checks.py,175,TODO: test with intercept,
scikit-learn/sklearn/utils/estimator_checks.py,176,TODO: test with multiple responses,
scikit-learn/sklearn/utils/estimator_checks.py,177,basic testing,
scikit-learn/sklearn/utils/estimator_checks.py,191,check that the regressor handles int input,
scikit-learn/sklearn/utils/estimator_checks.py,199,All transformers should either deal with sparse data or raise an,
scikit-learn/sklearn/utils/estimator_checks.py,200,exception with type TypeError and an intelligible error message,
scikit-learn/sklearn/utils/estimator_checks.py,203,"these don't actually fit the data, so don't raise errors",
scikit-learn/sklearn/utils/estimator_checks.py,208,Dependent on external solvers and hence accessing the iter,
scikit-learn/sklearn/utils/estimator_checks.py,209,param is non-trivial.,
scikit-learn/sklearn/utils/estimator_checks.py,219,this is clustering on the features,
scikit-learn/sklearn/utils/estimator_checks.py,220,let's not test that here.,
scikit-learn/sklearn/utils/estimator_checks.py,229,checks for outlier detectors that have a fit_predict method,
scikit-learn/sklearn/utils/estimator_checks.py,233,checks for estimators that can be used on a test set,
scikit-learn/sklearn/utils/estimator_checks.py,237,test outlier detectors can handle non-array data,
scikit-learn/sklearn/utils/estimator_checks.py,239,test if NotFittedError is raised,
scikit-learn/sklearn/utils/estimator_checks.py,364,"try to construct estimator to get tags, if it is unable to then",
scikit-learn/sklearn/utils/estimator_checks.py,365,return the estimator class,
scikit-learn/sklearn/utils/estimator_checks.py,453,got a class,
scikit-learn/sklearn/utils/estimator_checks.py,456,got an instance,
scikit-learn/sklearn/utils/estimator_checks.py,468,the only SkipTest thrown currently results from not,
scikit-learn/sklearn/utils/estimator_checks.py,469,being able to import pandas.,
scikit-learn/sklearn/utils/estimator_checks.py,491,set parameters to speed up some estimators and,
scikit-learn/sklearn/utils/estimator_checks.py,492,avoid deprecated behaviour,
scikit-learn/sklearn/utils/estimator_checks.py,500,"LinearSVR, LinearSVC",
scikit-learn/sklearn/utils/estimator_checks.py,503,NMF,
scikit-learn/sklearn/utils/estimator_checks.py,506,MLP,
scikit-learn/sklearn/utils/estimator_checks.py,510,randomized lasso,
scikit-learn/sklearn/utils/estimator_checks.py,515,RANSAC,
scikit-learn/sklearn/utils/estimator_checks.py,518,K-Means,
scikit-learn/sklearn/utils/estimator_checks.py,522,TruncatedSVD doesn't run with n_components = n_features,
scikit-learn/sklearn/utils/estimator_checks.py,523,This is ugly :-/,
scikit-learn/sklearn/utils/estimator_checks.py,533,be tolerant of noisy datasets (not actually speed),
scikit-learn/sklearn/utils/estimator_checks.py,540,"Due to the jl lemma and often very few samples, the number",
scikit-learn/sklearn/utils/estimator_checks.py,541,of components of the random matrix projection will be probably,
scikit-learn/sklearn/utils/estimator_checks.py,542,greater than the number of features.,
scikit-learn/sklearn/utils/estimator_checks.py,543,"So we impose a smaller number (avoid ""auto"" mode)",
scikit-learn/sklearn/utils/estimator_checks.py,547,SelectKBest has a default of k=10,
scikit-learn/sklearn/utils/estimator_checks.py,548,which is more feature than we have in most case.,
scikit-learn/sklearn/utils/estimator_checks.py,553,The default min_samples_leaf (20) isn't appropriate for small,
scikit-learn/sklearn/utils/estimator_checks.py,554,datasets (only very shallow trees are built) that the checks use.,
scikit-learn/sklearn/utils/estimator_checks.py,557,Speed-up by reducing the number of CV or splits for CV estimators,
scikit-learn/sklearn/utils/estimator_checks.py,593,TODO: remove in 0.24,
scikit-learn/sklearn/utils/estimator_checks.py,667,Generate large indices matrix only if its supported by scipy,
scikit-learn/sklearn/utils/estimator_checks.py,691,catch deprecation warnings,
scikit-learn/sklearn/utils/estimator_checks.py,696,catch deprecation warnings,
scikit-learn/sklearn/utils/estimator_checks.py,701,fit and predict,
scikit-learn/sklearn/utils/estimator_checks.py,740,check that estimators will accept a 'sample_weight' parameter of,
scikit-learn/sklearn/utils/estimator_checks.py,741,type pandas.Series in the 'fit' function.,
scikit-learn/sklearn/utils/estimator_checks.py,767,check that estimators will accept a 'sample_weight' parameter of,
scikit-learn/sklearn/utils/estimator_checks.py,768,type _NotAnArray in the 'fit' function.,
scikit-learn/sklearn/utils/estimator_checks.py,784,check that estimators will accept a 'sample_weight' parameter of,
scikit-learn/sklearn/utils/estimator_checks.py,785,type list in the 'fit' function.,
scikit-learn/sklearn/utils/estimator_checks.py,798,Test that estimators don't raise any exception,
scikit-learn/sklearn/utils/estimator_checks.py,804,check that estimators raise an error if sample_weight,
scikit-learn/sklearn/utils/estimator_checks.py,805,shape mismatches the input,
scikit-learn/sklearn/utils/estimator_checks.py,829,check that the estimators yield same results for,
scikit-learn/sklearn/utils/estimator_checks.py,830,unit weights and no weights,
scikit-learn/sklearn/utils/estimator_checks.py,834,We skip pairwise because the data is not pairwise,
scikit-learn/sklearn/utils/estimator_checks.py,867,check that estimators treat dtype object as numeric if possible,
scikit-learn/sklearn/utils/estimator_checks.py,897,Estimators supporting string will not call np.asarray to convert the,
scikit-learn/sklearn/utils/estimator_checks.py,898,"data to numeric and therefore, the error will not be raised.",
scikit-learn/sklearn/utils/estimator_checks.py,899,Checking for each element dtype in the input array will be costly.,
scikit-learn/sklearn/utils/estimator_checks.py,900,Refer to #11401 for full discussion.,
scikit-learn/sklearn/utils/estimator_checks.py,905,check that estimators raise an exception on providing complex data,
scikit-learn/sklearn/utils/estimator_checks.py,916,this estimator raises,
scikit-learn/sklearn/utils/estimator_checks.py,917,"ValueError: Found array with 0 feature(s) (shape=(23, 0))",
scikit-learn/sklearn/utils/estimator_checks.py,918,while a minimum of 1 is required.,
scikit-learn/sklearn/utils/estimator_checks.py,919,error,
scikit-learn/sklearn/utils/estimator_checks.py,966,check that fit method only changes or sets private attributes,
scikit-learn/sklearn/utils/estimator_checks.py,968,to not check deprecated classes,
scikit-learn/sklearn/utils/estimator_checks.py,996,check that fit doesn't add any public attribute,
scikit-learn/sklearn/utils/estimator_checks.py,1005,check that fit doesn't change any public attribute,
scikit-learn/sklearn/utils/estimator_checks.py,1021,check by fitting a 2d array and predicting with a 1d array,
scikit-learn/sklearn/utils/estimator_checks.py,1040,FIXME this is a bit loose,
scikit-learn/sklearn/utils/estimator_checks.py,1051,apply function on the whole set and on mini batches,
scikit-learn/sklearn/utils/estimator_checks.py,1057,func can output tuple (e.g. score_samples),
scikit-learn/sklearn/utils/estimator_checks.py,1071,check that method gives invariant results if applied,
scikit-learn/sklearn/utils/estimator_checks.py,1072,on mini batches or the whole set,
scikit-learn/sklearn/utils/estimator_checks.py,1105,Check that fitting a 2d array with only one sample either works or,
scikit-learn/sklearn/utils/estimator_checks.py,1106,returns an informative message. The error message should either mention,
scikit-learn/sklearn/utils/estimator_checks.py,1107,the number of samples or the number of classes.,
scikit-learn/sklearn/utils/estimator_checks.py,1123,min_cluster_size cannot be less than the data size for OPTICS.,
scikit-learn/sklearn/utils/estimator_checks.py,1139,check fitting a 2d array with only 1 feature either works or returns,
scikit-learn/sklearn/utils/estimator_checks.py,1140,informative message,
scikit-learn/sklearn/utils/estimator_checks.py,1152,ensure two labels in subsample for RandomizedLogisticRegression,
scikit-learn/sklearn/utils/estimator_checks.py,1155,ensure non skipped trials for RANSACRegressor,
scikit-learn/sklearn/utils/estimator_checks.py,1173,check fitting 1d X array raises a ValueError,
scikit-learn/sklearn/utils/estimator_checks.py,1180,FIXME this is a bit loose,
scikit-learn/sklearn/utils/estimator_checks.py,1212,"We need to make sure that we have non negative data, for things",
scikit-learn/sklearn/utils/estimator_checks.py,1213,like NMF,
scikit-learn/sklearn/utils/estimator_checks.py,1219,try the same with some list,
scikit-learn/sklearn/utils/estimator_checks.py,1240,fit,
scikit-learn/sklearn/utils/estimator_checks.py,1251,fit_transform method should work on non fitted estimator,
scikit-learn/sklearn/utils/estimator_checks.py,1259,check for consistent n_samples,
scikit-learn/sklearn/utils/estimator_checks.py,1299,raises error on malformed input for transform,
scikit-learn/sklearn/utils/estimator_checks.py,1304,"If it's not an array, it does not have a 'T' property",
scikit-learn/sklearn/utils/estimator_checks.py,1319,check that make_pipeline(est) gives same score as est,
scikit-learn/sklearn/utils/estimator_checks.py,1344,check that all estimators accept an optional y,
scikit-learn/sklearn/utils/estimator_checks.py,1345,in fit and score so they can be used in pipelines,
scikit-learn/sklearn/utils/estimator_checks.py,1365,if_delegate_has_method makes methods into functions,
scikit-learn/sklearn/utils/estimator_checks.py,1366,"with an explicit ""self"", so need to shift arguments",
scikit-learn/sklearn/utils/estimator_checks.py,1405,The precise message can change depending on whether X or y is,
scikit-learn/sklearn/utils/estimator_checks.py,1406,validated first. Let us test the type of exception only:,
scikit-learn/sklearn/utils/estimator_checks.py,1414,the following y should be accepted by both classifiers and regressors,
scikit-learn/sklearn/utils/estimator_checks.py,1415,and ignored by unsupervised models,
scikit-learn/sklearn/utils/estimator_checks.py,1424,Checks that Estimator X's do not contain NaN or inf.,
scikit-learn/sklearn/utils/estimator_checks.py,1441,catch deprecation warnings,
scikit-learn/sklearn/utils/estimator_checks.py,1445,try to fit,
scikit-learn/sklearn/utils/estimator_checks.py,1459,actually fit,
scikit-learn/sklearn/utils/estimator_checks.py,1462,predict,
scikit-learn/sklearn/utils/estimator_checks.py,1477,transform,
scikit-learn/sklearn/utils/estimator_checks.py,1515,some estimators can't do features less than 0,
scikit-learn/sklearn/utils/estimator_checks.py,1520,include NaN values when the estimator should deal with them,
scikit-learn/sklearn/utils/estimator_checks.py,1522,set randomly 10 elements to np.nan,
scikit-learn/sklearn/utils/estimator_checks.py,1539,pickle and unpickle!,
scikit-learn/sklearn/utils/estimator_checks.py,1557,check if number of features changes between calls to partial_fit.,
scikit-learn/sklearn/utils/estimator_checks.py,1675,catch deprecation and neighbors warnings,
scikit-learn/sklearn/utils/estimator_checks.py,1683,fit,
scikit-learn/sklearn/utils/estimator_checks.py,1685,with lists,
scikit-learn/sklearn/utils/estimator_checks.py,1698,fit_predict(X) and labels_ should be of type int,
scikit-learn/sklearn/utils/estimator_checks.py,1702,Add noise to X to test the possible values of the labels,
scikit-learn/sklearn/utils/estimator_checks.py,1705,There should be at least one sample in every cluster. Equivalently,
scikit-learn/sklearn/utils/estimator_checks.py,1706,labels_ should contain all the consecutive values between its,
scikit-learn/sklearn/utils/estimator_checks.py,1707,min and its max.,
scikit-learn/sklearn/utils/estimator_checks.py,1712,Labels are expected to start at 0 (no noise) or -1 (if noise),
scikit-learn/sklearn/utils/estimator_checks.py,1714,Labels should be less than n_clusters - 1,
scikit-learn/sklearn/utils/estimator_checks.py,1718,else labels should be less than max(labels_) which is necessarily true,
scikit-learn/sklearn/utils/estimator_checks.py,1729,MiniBatchKMeans,
scikit-learn/sklearn/utils/estimator_checks.py,1745,catch deprecation warnings,
scikit-learn/sklearn/utils/estimator_checks.py,1748,try to fit,
scikit-learn/sklearn/utils/estimator_checks.py,1762,predict,
scikit-learn/sklearn/utils/estimator_checks.py,1770,Warnings are raised by decision function,
scikit-learn/sklearn/utils/estimator_checks.py,1777,generate binary problem from multi-class one,
scikit-learn/sklearn/utils/estimator_checks.py,1803,raises error on malformed input for fit,
scikit-learn/sklearn/utils/estimator_checks.py,1814,fit,
scikit-learn/sklearn/utils/estimator_checks.py,1816,with lists,
scikit-learn/sklearn/utils/estimator_checks.py,1822,training set performance,
scikit-learn/sklearn/utils/estimator_checks.py,1826,raises error on malformed input for predict,
scikit-learn/sklearn/utils/estimator_checks.py,1845,decision_function agrees with predict,
scikit-learn/sklearn/utils/estimator_checks.py,1858,raises error on malformed input for decision_function,
scikit-learn/sklearn/utils/estimator_checks.py,1872,predict_proba agrees with predict,
scikit-learn/sklearn/utils/estimator_checks.py,1876,check that probas for all classes sum to one,
scikit-learn/sklearn/utils/estimator_checks.py,1880,raises error on malformed input for predict_proba,
scikit-learn/sklearn/utils/estimator_checks.py,1890,predict_log_proba is a transformation of predict_proba,
scikit-learn/sklearn/utils/estimator_checks.py,1897,Check for deviation from the precise given contamination level that may,
scikit-learn/sklearn/utils/estimator_checks.py,1898,be due to ties in the anomaly scores.,
scikit-learn/sklearn/utils/estimator_checks.py,1906,"ensure that all values in the 'critical area' are tied,",
scikit-learn/sklearn/utils/estimator_checks.py,1907,leading to the observed discrepancy between provided,
scikit-learn/sklearn/utils/estimator_checks.py,1908,and actual contamination levels.,
scikit-learn/sklearn/utils/estimator_checks.py,1928,fit,
scikit-learn/sklearn/utils/estimator_checks.py,1930,with lists,
scikit-learn/sklearn/utils/estimator_checks.py,1944,raises error on malformed input for predict,
scikit-learn/sklearn/utils/estimator_checks.py,1947,decision_function agrees with predict,
scikit-learn/sklearn/utils/estimator_checks.py,1952,raises error on malformed input for decision_function,
scikit-learn/sklearn/utils/estimator_checks.py,1955,decision_function is a translation of score_samples,
scikit-learn/sklearn/utils/estimator_checks.py,1959,raises error on malformed input for score_samples,
scikit-learn/sklearn/utils/estimator_checks.py,1962,contamination parameter (not for OneClassSVM which has the nu parameter),
scikit-learn/sklearn/utils/estimator_checks.py,1965,proportion of outliers equal to contamination parameter when not,
scikit-learn/sklearn/utils/estimator_checks.py,1966,set to 'auto'. This is true for the training set and cannot thus be,
scikit-learn/sklearn/utils/estimator_checks.py,1967,checked as follows for estimators with a novelty parameter such as,
scikit-learn/sklearn/utils/estimator_checks.py,1968,LocalOutlierFactor (tested in check_outliers_fit_predict),
scikit-learn/sklearn/utils/estimator_checks.py,1976,num_outliers should be equal to expected_outliers unless,
scikit-learn/sklearn/utils/estimator_checks.py,1977,there are ties in the decision_function values. this can,
scikit-learn/sklearn/utils/estimator_checks.py,1978,only be tested for estimators with a decision_function,
scikit-learn/sklearn/utils/estimator_checks.py,1979,"method, i.e. all estimators except LOF which is already",
scikit-learn/sklearn/utils/estimator_checks.py,1980,excluded from this if branch.,
scikit-learn/sklearn/utils/estimator_checks.py,1985,"raises error when contamination is a scalar and not in [0,1]",
scikit-learn/sklearn/utils/estimator_checks.py,2034,some want non-negative input,
scikit-learn/sklearn/utils/estimator_checks.py,2054,"Common test for Regressors, Classifiers and Outlier detection estimators",
scikit-learn/sklearn/utils/estimator_checks.py,2068,"These only work on 2d, so this test makes no sense",
scikit-learn/sklearn/utils/estimator_checks.py,2082,fit,
scikit-learn/sklearn/utils/estimator_checks.py,2087,"Check that when a 2D y is given, a DataConversionWarning is",
scikit-learn/sklearn/utils/estimator_checks.py,2088,raised,
scikit-learn/sklearn/utils/estimator_checks.py,2097,check that we warned if we don't support multi-output,
scikit-learn/sklearn/utils/estimator_checks.py,2135,training set performance,
scikit-learn/sklearn/utils/estimator_checks.py,2137,This is a pathological data set for ComplementNB.,
scikit-learn/sklearn/utils/estimator_checks.py,2138,For some specific cases 'ComplementNB' predicts less classes,
scikit-learn/sklearn/utils/estimator_checks.py,2139,than expected,
scikit-learn/sklearn/utils/estimator_checks.py,2148,TODO: remove in 0.24,
scikit-learn/sklearn/utils/estimator_checks.py,2165,"We need to make sure that we have non negative data, for things",
scikit-learn/sklearn/utils/estimator_checks.py,2166,like NMF,
scikit-learn/sklearn/utils/estimator_checks.py,2204,separate estimators to control random seeds,
scikit-learn/sklearn/utils/estimator_checks.py,2216,fit,
scikit-learn/sklearn/utils/estimator_checks.py,2230,X is already scaled,
scikit-learn/sklearn/utils/estimator_checks.py,2245,"linear regressors need to set alpha, but not generalized CV ones",
scikit-learn/sklearn/utils/estimator_checks.py,2250,raises error on malformed input for fit,
scikit-learn/sklearn/utils/estimator_checks.py,2257,fit,
scikit-learn/sklearn/utils/estimator_checks.py,2264,TODO: find out why PLS and CCA fail. RANSAC is random,
scikit-learn/sklearn/utils/estimator_checks.py,2265,"and furthermore assumes the presence of outliers, hence",
scikit-learn/sklearn/utils/estimator_checks.py,2266,skipped,
scikit-learn/sklearn/utils/estimator_checks.py,2273,checks whether regressors have decision_function or predict_proba,
scikit-learn/sklearn/utils/estimator_checks.py,2282,"FIXME CCA, PLS is not robust to rank 1 effects",
scikit-learn/sklearn/utils/estimator_checks.py,2290,doesn't have function,
scikit-learn/sklearn/utils/estimator_checks.py,2292,has function. Should raise deprecation warning,
scikit-learn/sklearn/utils/estimator_checks.py,2306,create a very noisy dataset,
scikit-learn/sklearn/utils/estimator_checks.py,2311,"can't use gram_if_pairwise() here, setting up gram matrix manually",
scikit-learn/sklearn/utils/estimator_checks.py,2337,"XXX: Generally can use 0.89 here. On Windows, LinearSVC gets",
scikit-learn/sklearn/utils/estimator_checks.py,2338,0.88 (Issue #9111),
scikit-learn/sklearn/utils/estimator_checks.py,2365,"this is run on classes, not instances, though this should be changed",
scikit-learn/sklearn/utils/estimator_checks.py,2373,"This is a very small dataset, default n_iter are likely to prevent",
scikit-learn/sklearn/utils/estimator_checks.py,2374,convergence,
scikit-learn/sklearn/utils/estimator_checks.py,2382,Let the model compute the class frequencies,
scikit-learn/sklearn/utils/estimator_checks.py,2386,Count each label occurrence to reweight manually,
scikit-learn/sklearn/utils/estimator_checks.py,2408,some want non-negative input,
scikit-learn/sklearn/utils/estimator_checks.py,2416,Make a physical copy of the original estimator parameters before fitting.,
scikit-learn/sklearn/utils/estimator_checks.py,2420,Fit the model,
scikit-learn/sklearn/utils/estimator_checks.py,2423,Compare the state of the model parameters with the original parameters,
scikit-learn/sklearn/utils/estimator_checks.py,2428,We should never change or mutate the internal state of input,
scikit-learn/sklearn/utils/estimator_checks.py,2429,parameters by default. To check this we use the joblib.hash function,
scikit-learn/sklearn/utils/estimator_checks.py,2430,that introspects recursively any subobjects to compute a checksum.,
scikit-learn/sklearn/utils/estimator_checks.py,2431,The only exception to this rule of immutable constructor parameters,
scikit-learn/sklearn/utils/estimator_checks.py,2432,is possible RandomState instance but in this check we explicitly,
scikit-learn/sklearn/utils/estimator_checks.py,2433,fixed the random_state params recursively to be integer seeds.,
scikit-learn/sklearn/utils/estimator_checks.py,2449,__init__ signature has additional objects in PyPy,
scikit-learn/sklearn/utils/estimator_checks.py,2458,Test for no setting apart from parameters during init,
scikit-learn/sklearn/utils/estimator_checks.py,2465,Ensure that each parameter is set in init,
scikit-learn/sklearn/utils/estimator_checks.py,2484,test sparsify with dense inputs,
scikit-learn/sklearn/utils/estimator_checks.py,2490,pickle and unpickle with sparse coef_,
scikit-learn/sklearn/utils/estimator_checks.py,2525,separate estimators to control random seeds,
scikit-learn/sklearn/utils/estimator_checks.py,2538,Here pandas objects (Series and DataFrame) are tested explicitly,
scikit-learn/sklearn/utils/estimator_checks.py,2539,because some estimators may handle them (especially their indexing),
scikit-learn/sklearn/utils/estimator_checks.py,2540,specially.,
scikit-learn/sklearn/utils/estimator_checks.py,2554,fit,
scikit-learn/sklearn/utils/estimator_checks.py,2563,"this check works on classes, not instances",
scikit-learn/sklearn/utils/estimator_checks.py,2564,test default-constructibility,
scikit-learn/sklearn/utils/estimator_checks.py,2565,get rid of deprecation warnings,
scikit-learn/sklearn/utils/estimator_checks.py,2568,test cloning,
scikit-learn/sklearn/utils/estimator_checks.py,2570,test __repr__,
scikit-learn/sklearn/utils/estimator_checks.py,2572,test that set_params returns self,
scikit-learn/sklearn/utils/estimator_checks.py,2575,test if init does nothing but set parameters,
scikit-learn/sklearn/utils/estimator_checks.py,2576,this is important for grid_search etc.,
scikit-learn/sklearn/utils/estimator_checks.py,2577,We get the default parameters from init and then,
scikit-learn/sklearn/utils/estimator_checks.py,2578,compare these against the actual values of the attributes.,
scikit-learn/sklearn/utils/estimator_checks.py,2580,this comes from getattr. Gets rid of deprecation decorator.,
scikit-learn/sklearn/utils/estimator_checks.py,2595,init is not a python function.,
scikit-learn/sklearn/utils/estimator_checks.py,2596,true for mixins,
scikit-learn/sklearn/utils/estimator_checks.py,2599,they can need a non-default argument,
scikit-learn/sklearn/utils/estimator_checks.py,2614,"deprecated parameter, not in get_params",
scikit-learn/sklearn/utils/estimator_checks.py,2623,Allows to set default parameters to np.nan,
scikit-learn/sklearn/utils/estimator_checks.py,2629,TODO: remove in 0.24,
scikit-learn/sklearn/utils/estimator_checks.py,2637,Estimators with a `requires_positive_y` tag only accept strictly positive,
scikit-learn/sklearn/utils/estimator_checks.py,2638,data,
scikit-learn/sklearn/utils/estimator_checks.py,2640,"Create strictly positive y. The minimal increment above 0 is 1, as",
scikit-learn/sklearn/utils/estimator_checks.py,2641,y could be of integer dtype.,
scikit-learn/sklearn/utils/estimator_checks.py,2643,Estimators in mono_output_task_error raise ValueError if y is of 1-D,
scikit-learn/sklearn/utils/estimator_checks.py,2644,Convert into a 2-D y for those estimators.,
scikit-learn/sklearn/utils/estimator_checks.py,2651,Estimators with a `_pairwise` tag only accept,
scikit-learn/sklearn/utils/estimator_checks.py,2652,"X of shape (`n_samples`, `n_samples`)",
scikit-learn/sklearn/utils/estimator_checks.py,2655,Estimators with `1darray` in `X_types` tag only accept,
scikit-learn/sklearn/utils/estimator_checks.py,2656,"X of shape (`n_samples`,)",
scikit-learn/sklearn/utils/estimator_checks.py,2659,Estimators with a `requires_positive_X` tag only accept,
scikit-learn/sklearn/utils/estimator_checks.py,2660,strictly positive data,
scikit-learn/sklearn/utils/estimator_checks.py,2668,Test that estimators that are not transformers with a parameter,
scikit-learn/sklearn/utils/estimator_checks.py,2669,"max_iter, return the attribute of n_iter_ at least 1.",
scikit-learn/sklearn/utils/estimator_checks.py,2671,These models are dependent on external solvers like,
scikit-learn/sklearn/utils/estimator_checks.py,2672,libsvm and accessing the iter parameter is non-trivial.,
scikit-learn/sklearn/utils/estimator_checks.py,2678,Tested in test_transformer_n_iter,
scikit-learn/sklearn/utils/estimator_checks.py,2683,LassoLars stops early for the default alpha=1.0 the iris dataset.,
scikit-learn/sklearn/utils/estimator_checks.py,2702,"Test that transformers with a parameter max_iter, return the",
scikit-learn/sklearn/utils/estimator_checks.py,2703,attribute of n_iter_ at least 1.,
scikit-learn/sklearn/utils/estimator_checks.py,2707,Check using default data,
scikit-learn/sklearn/utils/estimator_checks.py,2718,These return a n_iter per component.,
scikit-learn/sklearn/utils/estimator_checks.py,2728,Checks if get_params(deep=False) is a subset of get_params(deep=True),
scikit-learn/sklearn/utils/estimator_checks.py,2740,Check that get_params() returns the same thing,
scikit-learn/sklearn/utils/estimator_checks.py,2741,before and after set_params() with some fuzz,
scikit-learn/sklearn/utils/estimator_checks.py,2753,some fuzz values,
scikit-learn/sklearn/utils/estimator_checks.py,2765,"Exception occurred, possibly parameter validation",
scikit-learn/sklearn/utils/estimator_checks.py,2794,Check if classifier throws an exception when fed regression targets,
scikit-learn/sklearn/utils/estimator_checks.py,2805,Check whether an estimator having both decision_function and,
scikit-learn/sklearn/utils/estimator_checks.py,2806,predict_proba methods has outputs with perfect rank correlation.,
scikit-learn/sklearn/utils/estimator_checks.py,2818,Since the link function from decision_function() to predict_proba(),
scikit-learn/sklearn/utils/estimator_checks.py,2819,"is sometimes not precise enough (typically expit), we round to the",
scikit-learn/sklearn/utils/estimator_checks.py,2820,10th decimal to avoid numerical issues.,
scikit-learn/sklearn/utils/estimator_checks.py,2827,Check fit_predict for outlier detectors.,
scikit-learn/sklearn/utils/estimator_checks.py,2842,check fit_predict = fit.predict when the estimator has both a predict and,
scikit-learn/sklearn/utils/estimator_checks.py,2843,a fit_predict method. recall that it is already assumed here that the,
scikit-learn/sklearn/utils/estimator_checks.py,2844,estimator has a fit_predict method,
scikit-learn/sklearn/utils/estimator_checks.py,2850,proportion of outliers equal to contamination parameter when not,
scikit-learn/sklearn/utils/estimator_checks.py,2851,set to 'auto',
scikit-learn/sklearn/utils/estimator_checks.py,2858,num_outliers should be equal to expected_outliers unless,
scikit-learn/sklearn/utils/estimator_checks.py,2859,there are ties in the decision_function values. this can,
scikit-learn/sklearn/utils/estimator_checks.py,2860,only be tested for estimators with a decision_function,
scikit-learn/sklearn/utils/estimator_checks.py,2861,method,
scikit-learn/sklearn/utils/estimator_checks.py,2867,"raises error when contamination is a scalar and not in [0,1]",
scikit-learn/sklearn/utils/estimator_checks.py,2874,Check that proper warning is raised for non-negative X,
scikit-learn/sklearn/utils/estimator_checks.py,2875,when tag requires_positive_X is present,
scikit-learn/sklearn/utils/estimator_checks.py,2884,Check that est.fit(X) is the same as est.fit(X).fit(X). Ideally we would,
scikit-learn/sklearn/utils/estimator_checks.py,2885,check that the estimated parameters during training (e.g. coefs_) are,
scikit-learn/sklearn/utils/estimator_checks.py,2886,"the same, but having a universal comparison function for those",
scikit-learn/sklearn/utils/estimator_checks.py,2887,attributes is difficult and full of edge cases. So instead we check that,
scikit-learn/sklearn/utils/estimator_checks.py,2888,"predict(), predict_proba(), decision_function() and transform() return",
scikit-learn/sklearn/utils/estimator_checks.py,2889,the same results.,
scikit-learn/sklearn/utils/estimator_checks.py,2913,Fit for the first time,
scikit-learn/sklearn/utils/estimator_checks.py,2920,Fit again,
scikit-learn/sklearn/utils/estimator_checks.py,2939,Make sure that n_features_in_ attribute doesn't exist until fit is,
scikit-learn/sklearn/utils/estimator_checks.py,2940,"called, and that its value is correct.",
scikit-learn/sklearn/utils/estimator_checks.py,2971,noqa,
scikit-learn/sklearn/utils/_show_versions.py,6,License: BSD 3 clause,
scikit-learn/sklearn/utils/graph.py,8,Authors: Aric Hagberg <hagberg@lanl.gov>,
scikit-learn/sklearn/utils/graph.py,9,Gael Varoquaux <gael.varoquaux@normalesup.org>,
scikit-learn/sklearn/utils/graph.py,10,Jake Vanderplas <vanderplas@astro.washington.edu>,
scikit-learn/sklearn/utils/graph.py,11,License: BSD 3 clause,
scikit-learn/sklearn/utils/graph.py,15,noqa,
scikit-learn/sklearn/utils/graph.py,18,,
scikit-learn/sklearn/utils/graph.py,19,Path and connected component analysis.,
scikit-learn/sklearn/utils/graph.py,20,Code adapted from networkx,
scikit-learn/sklearn/utils/graph.py,55,level (number of hops) when seen in BFS,
scikit-learn/sklearn/utils/graph.py,56,the current level,
scikit-learn/sklearn/utils/graph.py,57,dict of nodes to check at next level,
scikit-learn/sklearn/utils/graph.py,59,advance to next level,
scikit-learn/sklearn/utils/graph.py,60,and start a new list (fringe),
scikit-learn/sklearn/utils/graph.py,63,set the level of vertex v,
scikit-learn/sklearn/utils/graph.py,68,return all path lengths as dictionary,
scikit-learn/sklearn/utils/deprecation.py,31,"Adapted from https://wiki.python.org/moin/PythonDecoratorLibrary,",
scikit-learn/sklearn/utils/deprecation.py,32,but with many changes.,
scikit-learn/sklearn/utils/deprecation.py,47,Note that this is only triggered properly if the `property`,
scikit-learn/sklearn/utils/deprecation.py,48,"decorator comes before the `deprecated` decorator, like so:",
scikit-learn/sklearn/utils/deprecation.py,49,,
scikit-learn/sklearn/utils/deprecation.py,50,@deprecated(msg),
scikit-learn/sklearn/utils/deprecation.py,51,@property,
scikit-learn/sklearn/utils/deprecation.py,52,def deprecated_attribute_(self):,
scikit-learn/sklearn/utils/deprecation.py,53,...,
scikit-learn/sklearn/utils/deprecation.py,63,FIXME: we should probably reset __new__ for full generality,
scikit-learn/sklearn/utils/deprecation.py,90,Add a reference to the wrapped function so that we can introspect,
scikit-learn/sklearn/utils/deprecation.py,91,on function arguments in Python 2 (already works in Python 3),
scikit-learn/sklearn/utils/deprecation.py,128,Raise a deprecation warning with standardized deprecation message.,
scikit-learn/sklearn/utils/deprecation.py,129,Useful because we are now deprecating # anything that isn't explicitly,
scikit-learn/sklearn/utils/deprecation.py,130,in an __init__ file.,
scikit-learn/sklearn/utils/deprecation.py,132,TODO: remove in 0.24 since this shouldn't be needed anymore.,
scikit-learn/sklearn/utils/metaestimators.py,2,Author: Joel Nothman,
scikit-learn/sklearn/utils/metaestimators.py,3,Andreas Mueller,
scikit-learn/sklearn/utils/metaestimators.py,4,License: BSD,
scikit-learn/sklearn/utils/metaestimators.py,40,Ensure strict ordering of parameter setting:,
scikit-learn/sklearn/utils/metaestimators.py,41,1. All steps,
scikit-learn/sklearn/utils/metaestimators.py,44,2. Step replacement,
scikit-learn/sklearn/utils/metaestimators.py,52,3. Step parameters and other initialisation arguments,
scikit-learn/sklearn/utils/metaestimators.py,57,assumes `name` is a valid estimator name,
scikit-learn/sklearn/utils/metaestimators.py,99,update the docstring of the descriptor,
scikit-learn/sklearn/utils/metaestimators.py,103,raise an AttributeError if the attribute is not present on the object,
scikit-learn/sklearn/utils/metaestimators.py,105,"delegate only on instances, not the classes.",
scikit-learn/sklearn/utils/metaestimators.py,106,this is to allow access to the docstrings.,
scikit-learn/sklearn/utils/metaestimators.py,118,"lambda, but not partial, allows help() to work with update_wrapper",
scikit-learn/sklearn/utils/metaestimators.py,120,update the docstring of the returned function,
scikit-learn/sklearn/utils/metaestimators.py,196,X is a precomputed square kernel matrix,
scikit-learn/sklearn/utils/sparsefuncs.py,1,Authors: Manoj Kumar,
scikit-learn/sklearn/utils/sparsefuncs.py,2,Thomas Unterthiner,
scikit-learn/sklearn/utils/sparsefuncs.py,3,Giorgio Patrini,
scikit-learn/sklearn/utils/sparsefuncs.py,4,,
scikit-learn/sklearn/utils/sparsefuncs.py,5,License: BSD 3 clause,
scikit-learn/sklearn/utils/sparsefuncs.py,261,The following swapping makes life easier since m is assumed to be the,
scikit-learn/sklearn/utils/sparsefuncs.py,262,smaller integer below.,
scikit-learn/sklearn/utils/sparsefuncs.py,275,Modify indptr first,
scikit-learn/sklearn/utils/sparsefuncs.py,345,"reduceat tries casts X.indptr to intp, which errors",
scikit-learn/sklearn/utils/sparsefuncs.py,346,if it is int64 on a 32 bit system.,
scikit-learn/sklearn/utils/sparsefuncs.py,347,"Reinitializing prevents this where possible, see #13737",
scikit-learn/sklearn/utils/sparsefuncs.py,463,We rely here on the fact that np.diff(Y.indptr) for a CSR,
scikit-learn/sklearn/utils/sparsefuncs.py,464,will return the number of nonzero entries in each row.,
scikit-learn/sklearn/utils/sparsefuncs.py,465,A bincount over Y.indices will return the number of nonzeros,
scikit-learn/sklearn/utils/sparsefuncs.py,466,in each column. See ``csr_matrix.getnnz`` in scipy >= 0.14.,
scikit-learn/sklearn/utils/sparsefuncs.py,475,astype here is for consistency with axis=0 dtype,
scikit-learn/sklearn/utils/sparsefuncs.py,541,Prevent modifying X in place,
scikit-learn/sklearn/utils/extmath.py,4,Authors: Gael Varoquaux,
scikit-learn/sklearn/utils/extmath.py,5,Alexandre Gramfort,
scikit-learn/sklearn/utils/extmath.py,6,Alexandre T. Passos,
scikit-learn/sklearn/utils/extmath.py,7,Olivier Grisel,
scikit-learn/sklearn/utils/extmath.py,8,Lars Buitinck,
scikit-learn/sklearn/utils/extmath.py,9,Stefan van der Walt,
scikit-learn/sklearn/utils/extmath.py,10,Kyle Kastner,
scikit-learn/sklearn/utils/extmath.py,11,Giorgio Patrini,
scikit-learn/sklearn/utils/extmath.py,12,License: BSD 3 clause,
scikit-learn/sklearn/utils/extmath.py,136,sparse is always 2D. Implies b is 3D+,
scikit-learn/sklearn/utils/extmath.py,137,"[i, j] @ [k, ..., l, m, n] -> [i, k, ..., l, n]",
scikit-learn/sklearn/utils/extmath.py,143,sparse is always 2D. Implies a is 3D+,
scikit-learn/sklearn/utils/extmath.py,144,"[k, ..., l, m] @ [i, j] -> [k, ..., l, j]",
scikit-learn/sklearn/utils/extmath.py,211,"Generating normal random vectors with shape: (A.shape[1], size)",
scikit-learn/sklearn/utils/extmath.py,214,Ensure f32 is preserved as f32,
scikit-learn/sklearn/utils/extmath.py,217,"Deal with ""auto"" mode",
scikit-learn/sklearn/utils/extmath.py,224,Perform power iterations with Q to further 'imprint' the top,
scikit-learn/sklearn/utils/extmath.py,225,singular vectors of A in Q,
scikit-learn/sklearn/utils/extmath.py,237,Sample the range of A using by linear projection of Q,
scikit-learn/sklearn/utils/extmath.py,238,Extract an orthonormal basis,
scikit-learn/sklearn/utils/extmath.py,335,Checks if the number of iterations is explicitly specified,
scikit-learn/sklearn/utils/extmath.py,336,Adjust n_iter. 7 was found a good compromise for PCA. See #5299,
scikit-learn/sklearn/utils/extmath.py,342,this implementation is a bit faster with smaller shape[1],
scikit-learn/sklearn/utils/extmath.py,348,project M to the (k + p) dimensional space using the basis vectors,
scikit-learn/sklearn/utils/extmath.py,351,compute the SVD on the thin matrix: (k + p) wide,
scikit-learn/sklearn/utils/extmath.py,361,In case of transpose u_based_decision=false,
scikit-learn/sklearn/utils/extmath.py,362,to actually flip based on u and not v.,
scikit-learn/sklearn/utils/extmath.py,366,transpose back the results according to the input convention,
scikit-learn/sklearn/utils/extmath.py,429,get ALL unique values,
scikit-learn/sklearn/utils/extmath.py,524,"columns of u, rows of v",
scikit-learn/sklearn/utils/extmath.py,530,"rows of v, columns of u",
scikit-learn/sklearn/utils/extmath.py,676,Use at least float64 for the accumulating functions to avoid precision issue,
scikit-learn/sklearn/utils/extmath.py,677,see https://github.com/numpy/numpy/issues/9393. The float64 is also retained,
scikit-learn/sklearn/utils/extmath.py,678,as it is in case the float overflows,
scikit-learn/sklearn/utils/extmath.py,754,old = stats until now,
scikit-learn/sklearn/utils/extmath.py,755,new = the current increment,
scikit-learn/sklearn/utils/extmath.py,756,updated = the aggregated stats,
scikit-learn/sklearn/utils/multiclass.py,1,"Author: Arnaud Joly, Joel Nothman, Hamzeh Alsalhi",
scikit-learn/sklearn/utils/multiclass.py,2,,
scikit-learn/sklearn/utils/multiclass.py,3,License: BSD 3 clause,
scikit-learn/sklearn/utils/multiclass.py,73,Check that we don't mix label format,
scikit-learn/sklearn/utils/multiclass.py,84,Check consistency for the indicator format,
scikit-learn/sklearn/utils/multiclass.py,91,Get the unique set of labels,
scikit-learn/sklearn/utils/multiclass.py,98,Check that we don't mix string type with number type,
scikit-learn/sklearn/utils/multiclass.py,146,"bool, int, uint",
scikit-learn/sklearn/utils/multiclass.py,151,"bool, int, uint",
scikit-learn/sklearn/utils/multiclass.py,253,Known to fail in numpy 1.3 for array of arrays,
scikit-learn/sklearn/utils/multiclass.py,256,The old sequence of sequences format,
scikit-learn/sklearn/utils/multiclass.py,268,Invalid inputs,
scikit-learn/sklearn/utils/multiclass.py,271,"[[[1, 2]]] or [obj_1] and not [""label_1""]",
scikit-learn/sklearn/utils/multiclass.py,274,[[]],
scikit-learn/sklearn/utils/multiclass.py,277,"[[1, 2], [1, 2]]",
scikit-learn/sklearn/utils/multiclass.py,279,"[1, 2, 3] or [[1], [2], [3]]",
scikit-learn/sklearn/utils/multiclass.py,281,check float and contains non-integer float values,
scikit-learn/sklearn/utils/multiclass.py,283,"[.1, .2, 3] or [[.1, .2, 3]] or [[1., .2]] and not [1., 2., 3.]",
scikit-learn/sklearn/utils/multiclass.py,288,"[1, 2, 3] or [[1., 2., 3]] or [[1, 2]]",
scikit-learn/sklearn/utils/multiclass.py,290,"[1, 2] or [[""a""], [""b""]]",
scikit-learn/sklearn/utils/multiclass.py,319,This is the first call to partial_fit,
scikit-learn/sklearn/utils/multiclass.py,323,classes is None and clf.classes_ has already previously been set:,
scikit-learn/sklearn/utils/multiclass.py,324,nothing to do,
scikit-learn/sklearn/utils/multiclass.py,365,separate sample weights for zero and non-zero elements,
scikit-learn/sklearn/utils/multiclass.py,378,"An explicit zero was found, combine its weight with the weight",
scikit-learn/sklearn/utils/multiclass.py,379,of the implicit zeros,
scikit-learn/sklearn/utils/multiclass.py,383,If an there is an implicit zero and it is not in classes and,
scikit-learn/sklearn/utils/multiclass.py,384,"class_prior, make an entry for it",
scikit-learn/sklearn/utils/multiclass.py,436,"Monotonically transform the sum_of_confidences to (-1/3, 1/3)",
scikit-learn/sklearn/utils/multiclass.py,437,and add it with votes. The monotonic transformation  is,
scikit-learn/sklearn/utils/multiclass.py,438,"f: x -> x / (3 * (|x| + 1)), it uses 1/3 instead of 1/2",
scikit-learn/sklearn/utils/multiclass.py,439,to ensure that we won't reach the limits and change vote order.,
scikit-learn/sklearn/utils/multiclass.py,440,The motivation is to use confidence levels as a way to break ties in,
scikit-learn/sklearn/utils/multiclass.py,441,the votes without switching any decision made based on a difference,
scikit-learn/sklearn/utils/multiclass.py,442,of 1 vote.,
scikit-learn/sklearn/utils/random.py,1,Author: Hamzeh Alsalhi <ha258@cornell.edu>,
scikit-learn/sklearn/utils/random.py,2,,
scikit-learn/sklearn/utils/random.py,3,License: BSD 3 clause,
scikit-learn/sklearn/utils/random.py,59,use uniform distribution if no class_probability is given,
scikit-learn/sklearn/utils/random.py,77,If 0 is not present in the classes insert it with a probability 0.0,
scikit-learn/sklearn/utils/random.py,82,If there are nonzero classes choose randomly using class_probability,
scikit-learn/sklearn/utils/random.py,92,Normalize probabilities for the nonzero elements,
scikit-learn/sklearn/utils/_testing.py,3,"Copyright (c) 2011, 2012",
scikit-learn/sklearn/utils/_testing.py,4,"Authors: Pietro Berkes,",
scikit-learn/sklearn/utils/_testing.py,5,Andreas Muller,
scikit-learn/sklearn/utils/_testing.py,6,Mathieu Blondel,
scikit-learn/sklearn/utils/_testing.py,7,Olivier Grisel,
scikit-learn/sklearn/utils/_testing.py,8,Arnaud Joly,
scikit-learn/sklearn/utils/_testing.py,9,Denis Engemann,
scikit-learn/sklearn/utils/_testing.py,10,Giorgio Patrini,
scikit-learn/sklearn/utils/_testing.py,11,Thierry Guillemot,
scikit-learn/sklearn/utils/_testing.py,12,License: BSD 3 clause,
scikit-learn/sklearn/utils/_testing.py,34,WindowsError only exist on Windows,
scikit-learn/sklearn/utils/_testing.py,83,assert_raises_regexp is deprecated in Python 3.4 in favor of,
scikit-learn/sklearn/utils/_testing.py,84,assert_raises_regex but lets keep the backward compat in scikit-learn with,
scikit-learn/sklearn/utils/_testing.py,85,the old name for now,
scikit-learn/sklearn/utils/_testing.py,111,Cause all warnings to always be triggered.,
scikit-learn/sklearn/utils/_testing.py,113,Trigger a warning.,
scikit-learn/sklearn/utils/_testing.py,116,Filter out numpy-specific warnings in numpy >= 1.9,
scikit-learn/sklearn/utils/_testing.py,120,Verify some things,
scikit-learn/sklearn/utils/_testing.py,133,very important to avoid uncontrolled state propagation,
scikit-learn/sklearn/utils/_testing.py,159,Cause all warnings to always be triggered.,
scikit-learn/sklearn/utils/_testing.py,162,Let's not catch the numpy internal DeprecationWarnings,
scikit-learn/sklearn/utils/_testing.py,164,Trigger a warning.,
scikit-learn/sklearn/utils/_testing.py,166,Verify some things,
scikit-learn/sklearn/utils/_testing.py,178,Checks the message of all warnings belong to warning_class,
scikit-learn/sklearn/utils/_testing.py,180,"substring will match, the entire message with typo won't",
scikit-learn/sklearn/utils/_testing.py,181,For Python 3 compatibility,
scikit-learn/sklearn/utils/_testing.py,183,add support for certain tests,
scikit-learn/sklearn/utils/_testing.py,216,This platform does not report numpy divide by zeros,
scikit-learn/sklearn/utils/_testing.py,223,To remove when we support numpy 1.7,
scikit-learn/sklearn/utils/_testing.py,232,very important to avoid uncontrolled state propagation,
scikit-learn/sklearn/utils/_testing.py,238,Filter out numpy-specific warnings in numpy >= 1.9,
scikit-learn/sklearn/utils/_testing.py,276,Avoid common pitfall of passing category as the first positional,
scikit-learn/sklearn/utils/_testing.py,277,argument which result in the test not being run,
scikit-learn/sklearn/utils/_testing.py,379,concatenate exception names,
scikit-learn/sklearn/utils/_testing.py,423,both dense,
scikit-learn/sklearn/utils/_testing.py,430,TODO: Remove in 0.24. This class is now in utils.__init__.,
scikit-learn/sklearn/utils/_testing.py,463,get parent folder,
scikit-learn/sklearn/utils/_testing.py,472,Ignore deprecation warnings triggered at import time.,
scikit-learn/sklearn/utils/_testing.py,483,get rid of abstract base classes,
scikit-learn/sklearn/utils/_testing.py,490,copy,
scikit-learn/sklearn/utils/_testing.py,508,"drop duplicates, sort for reproducibility",
scikit-learn/sklearn/utils/_testing.py,509,itemgetter is used to ensure the sort does not extend to the 2nd item of,
scikit-learn/sklearn/utils/_testing.py,510,the tuple,
scikit-learn/sklearn/utils/_testing.py,542,Decorator for tests involving both BLAS calls and multiprocessing.,
scikit-learn/sklearn/utils/_testing.py,543,,
scikit-learn/sklearn/utils/_testing.py,544,"Under POSIX (e.g. Linux or OSX), using multiprocessing in conjunction",
scikit-learn/sklearn/utils/_testing.py,545,with some implementation of BLAS (or other libraries that manage an,
scikit-learn/sklearn/utils/_testing.py,546,internal posix thread pool) can cause a crash or a freeze of the Python,
scikit-learn/sklearn/utils/_testing.py,547,process.,
scikit-learn/sklearn/utils/_testing.py,548,,
scikit-learn/sklearn/utils/_testing.py,549,In practice all known packaged distributions (from Linux distros or,
scikit-learn/sklearn/utils/_testing.py,550,Anaconda) of BLAS under Linux seems to be safe. So we this problem seems,
scikit-learn/sklearn/utils/_testing.py,551,to only impact OSX users.,
scikit-learn/sklearn/utils/_testing.py,552,,
scikit-learn/sklearn/utils/_testing.py,553,This wrapper makes it possible to skip tests that can possibly cause,
scikit-learn/sklearn/utils/_testing.py,554,this crash under OS X with.,
scikit-learn/sklearn/utils/_testing.py,555,,
scikit-learn/sklearn/utils/_testing.py,556,Under Python 3.4+ it is possible to use the `forkserver` start method,
scikit-learn/sklearn/utils/_testing.py,557,for multiprocessing to avoid this issue. However it can cause pickling,
scikit-learn/sklearn/utils/_testing.py,558,errors on interactively defined functions. It therefore not enabled by,
scikit-learn/sklearn/utils/_testing.py,559,default.,
scikit-learn/sklearn/utils/_testing.py,580,"This can fail under windows,",
scikit-learn/sklearn/utils/_testing.py,581,but will succeed when called by atexit,
scikit-learn/sklearn/utils/_testing.py,626,Utils to test docstrings,
scikit-learn/sklearn/utils/_testing.py,635,Error on builtin C function,
scikit-learn/sklearn/utils/_testing.py,700,Don't check docstring for property-functions,
scikit-learn/sklearn/utils/_testing.py,703,Don't check docstring for setup / teardown pytest functions,
scikit-learn/sklearn/utils/_testing.py,706,Dont check estimator_checks module,
scikit-learn/sklearn/utils/_testing.py,709,Get the arguments from the function signature,
scikit-learn/sklearn/utils/_testing.py,711,drop self,
scikit-learn/sklearn/utils/_testing.py,715,Analyze function's docstring,
scikit-learn/sklearn/utils/_testing.py,728,Type hints are empty only if parameter name ended with :,
scikit-learn/sklearn/utils/_testing.py,739,Create a list of parameters to compare with the parameters gotten,
scikit-learn/sklearn/utils/_testing.py,740,from the func signature,
scikit-learn/sklearn/utils/_testing.py,744,If one of the docstring's parameters had an error then return that,
scikit-learn/sklearn/utils/_testing.py,745,incorrect message,
scikit-learn/sklearn/utils/_testing.py,749,Remove the parameters that should be ignored from list,
scikit-learn/sklearn/utils/_testing.py,752,"The following is derived from pytest, Copyright (c) 2004-2017 Holger",
scikit-learn/sklearn/utils/_testing.py,753,"Krekel and others, Licensed under MIT License. See",
scikit-learn/sklearn/utils/_testing.py,754,https://github.com/pytest-dev/pytest,
scikit-learn/sklearn/utils/_testing.py,774,If there wasn't any difference in the parameters themselves between,
scikit-learn/sklearn/utils/_testing.py,775,docstring and signature including having the same length then return,
scikit-learn/sklearn/utils/_testing.py,776,empty list,
scikit-learn/sklearn/utils/_testing.py,795,Prepend function name,
scikit-learn/sklearn/utils/_testing.py,833,"If coverage is running, pass the config file to the subprocess",
scikit-learn/sklearn/utils/optimize.py,11,This is a modified file from scipy.optimize,
scikit-learn/sklearn/utils/optimize.py,12,"Original authors: Travis Oliphant, Eric Jones",
scikit-learn/sklearn/utils/optimize.py,13,"Modifications by Gael Varoquaux, Mathieu Blondel and Tom Dupre la Tour",
scikit-learn/sklearn/utils/optimize.py,14,License: BSD,
scikit-learn/sklearn/utils/optimize.py,46,line search failed: try different one.,
scikit-learn/sklearn/utils/optimize.py,92,check curvature,
scikit-learn/sklearn/utils/optimize.py,100,fall back to steepest descent direction,
scikit-learn/sklearn/utils/optimize.py,110,"update np.dot(ri,ri) for next time.",
scikit-learn/sklearn/utils/optimize.py,178,Outer loop: our Newton iteration,
scikit-learn/sklearn/utils/optimize.py,180,Compute a search direction pk by applying the CG method to,
scikit-learn/sklearn/utils/optimize.py,181,del2 f(xk) p = - fgrad f(xk) starting from 0.,
scikit-learn/sklearn/utils/optimize.py,192,"Inner loop: solve the Newton update by conjugate gradient, to",
scikit-learn/sklearn/utils/optimize.py,193,avoid inverting the Hessian,
scikit-learn/sklearn/utils/optimize.py,207,upcast if necessary,
scikit-learn/sklearn/utils/optimize.py,234,handle both scipy and scikit-learn solver names,
scikit-learn/sklearn/utils/optimize.py,248,"In scipy <= 1.0.0, nit may exceed maxiter for lbfgs.",
scikit-learn/sklearn/utils/optimize.py,249,See https://github.com/scipy/scipy/issues/7854,
scikit-learn/sklearn/utils/__init__.py,36,Do not deprecate parallel_backend and register_parallel_backend as they are,
scikit-learn/sklearn/utils/__init__.py,37,needed to tune `scikit-learn` behavior and have different effect if called,
scikit-learn/sklearn/utils/__init__.py,38,from the vendored version or or the site-package version. The other are,
scikit-learn/sklearn/utils/__init__.py,39,utilities that are independent of scikit-learn so they are not part of,
scikit-learn/sklearn/utils/__init__.py,40,scikit-learn public API.,
scikit-learn/sklearn/utils/__init__.py,99,Bunch pickles generated with scikit-learn 0.16.* have an non,
scikit-learn/sklearn/utils/__init__.py,100,empty __dict__. This causes a surprising behaviour when,
scikit-learn/sklearn/utils/__init__.py,101,loading these pickles scikit-learn 0.17: reading bunch.key,
scikit-learn/sklearn/utils/__init__.py,102,uses __dict__ but assigning to bunch.key use __setattr__ and,
scikit-learn/sklearn/utils/__init__.py,103,only changes bunch['key']. More details can be found at:,
scikit-learn/sklearn/utils/__init__.py,104,https://github.com/scikit-learn/scikit-learn/issues/6196.,
scikit-learn/sklearn/utils/__init__.py,105,Overriding __setstate__ to be a noop has the effect of,
scikit-learn/sklearn/utils/__init__.py,106,ignoring the pickled __dict__,
scikit-learn/sklearn/utils/__init__.py,173,FIXME: Remove the check for NumPy when using >= 1.12,
scikit-learn/sklearn/utils/__init__.py,174,check if we have an boolean array-likes to make the proper indexing,
scikit-learn/sklearn/utils/__init__.py,185,Work-around for indexing with read-only key in pandas,
scikit-learn/sklearn/utils/__init__.py,186,FIXME: solved in pandas 0.25,
scikit-learn/sklearn/utils/__init__.py,191,check whether we should index with loc or iloc,
scikit-learn/sklearn/utils/__init__.py,199,key is a slice or a scalar,
scikit-learn/sklearn/utils/__init__.py,202,key is a boolean array-like,
scikit-learn/sklearn/utils/__init__.py,204,key is a integer array-like of key,
scikit-learn/sklearn/utils/__init__.py,271,TODO: remove in 0.24,
scikit-learn/sklearn/utils/__init__.py,409,we get an empty list,
scikit-learn/sklearn/utils/__init__.py,412,Convert key into positive indexes,
scikit-learn/sklearn/utils/__init__.py,434,pandas indexing with strings is endpoint included,
scikit-learn/sklearn/utils/__init__.py,578,Code adapted from StratifiedShuffleSplit(),
scikit-learn/sklearn/utils/__init__.py,581,"for multi-label y, map each distinct row to a string repr",
scikit-learn/sklearn/utils/__init__.py,582,using join because str(row) uses an ellipsis if len(row) > 1000,
scikit-learn/sklearn/utils/__init__.py,590,Find the sorted list of instances for each class:,
scikit-learn/sklearn/utils/__init__.py,591,"(np.unique above performs a sort, so code is O(n logn) already)",
scikit-learn/sklearn/utils/__init__.py,607,convert sparse matrices to CSR for row-based indexing,
scikit-learn/sklearn/utils/__init__.py,611,syntactic sugar for the unit argument case,
scikit-learn/sklearn/utils/__init__.py,922,adapted from joblib.logger.short_format_time without the Windows -.1s,
scikit-learn/sklearn/utils/__init__.py,923,adjustment,
scikit-learn/sklearn/utils/__init__.py,1026,convert from numpy.bool_ to python bool to ensure that testing,
scikit-learn/sklearn/utils/__init__.py,1027,is_scalar_nan(x) is True does not fail.,
scikit-learn/sklearn/utils/__init__.py,1072,this computes a bad approximation to the mode of the,
scikit-learn/sklearn/utils/__init__.py,1073,multivariate hypergeometric given by class_counts and n_draws,
scikit-learn/sklearn/utils/__init__.py,1075,"floored means we don't overshoot n_samples, but probably undershoot",
scikit-learn/sklearn/utils/__init__.py,1077,"we add samples according to how much ""left over"" probability",
scikit-learn/sklearn/utils/__init__.py,1078,"they had, until we arrive at n_samples",
scikit-learn/sklearn/utils/__init__.py,1083,"add according to remainder, but break ties",
scikit-learn/sklearn/utils/__init__.py,1084,randomly to avoid biases,
scikit-learn/sklearn/utils/__init__.py,1087,if we need_to_add less than what's in inds,
scikit-learn/sklearn/utils/__init__.py,1088,we draw randomly from them.,
scikit-learn/sklearn/utils/__init__.py,1089,"if we need to add more, we add them all and",
scikit-learn/sklearn/utils/__init__.py,1090,go to the next value,
scikit-learn/sklearn/utils/__init__.py,1112,noqa,
scikit-learn/sklearn/utils/__init__.py,1133,noqa,
scikit-learn/sklearn/utils/__init__.py,1164,lazy import to avoid circular imports from sklearn.base,
scikit-learn/sklearn/utils/__init__.py,1178,sklearn package,
scikit-learn/sklearn/utils/__init__.py,1179,Ignore deprecation warnings triggered at import time and from walking,
scikit-learn/sklearn/utils/__init__.py,1180,packages,
scikit-learn/sklearn/utils/__init__.py,1193,TODO: Remove when FeatureHasher is implemented in PYPY,
scikit-learn/sklearn/utils/__init__.py,1194,Skips FeatureHasher for PYPY,
scikit-learn/sklearn/utils/__init__.py,1206,get rid of abstract base classes,
scikit-learn/sklearn/utils/__init__.py,1213,copy,
scikit-learn/sklearn/utils/__init__.py,1231,"drop duplicates, sort for reproducibility",
scikit-learn/sklearn/utils/__init__.py,1232,itemgetter is used to ensure the sort does not extend to the 2nd item of,
scikit-learn/sklearn/utils/__init__.py,1233,the tuple,
scikit-learn/sklearn/utils/setup.py,49,generate _seq_dataset from template,
scikit-learn/sklearn/utils/_mocking.py,26,have shape and length but don't support indexing.,
scikit-learn/sklearn/utils/_mocking.py,32,ugly hack to make iloc work.,
scikit-learn/sklearn/utils/_mocking.py,39,Pandas data frames also are array-like: we want to make sure that,
scikit-learn/sklearn/utils/_mocking.py,40,input validation in cross-validation does not try to call that,
scikit-learn/sklearn/utils/_mocking.py,41,method.,
scikit-learn/sklearn/utils/_mask.py,13,can't have NaNs in integer array.,
scikit-learn/sklearn/utils/_mask.py,16,np.isnan does not work on object dtypes.,
scikit-learn/sklearn/utils/stats.py,12,Find index of median prediction for each sample,
scikit-learn/sklearn/utils/stats.py,16,"in rare cases, percentile_idx equals to len(sorted_idx)",
scikit-learn/sklearn/utils/validation.py,3,Authors: Olivier Grisel,
scikit-learn/sklearn/utils/validation.py,4,Gael Varoquaux,
scikit-learn/sklearn/utils/validation.py,5,Andreas Mueller,
scikit-learn/sklearn/utils/validation.py,6,Lars Buitinck,
scikit-learn/sklearn/utils/validation.py,7,Alexandre Gramfort,
scikit-learn/sklearn/utils/validation.py,8,Nicolas Tresegnie,
scikit-learn/sklearn/utils/validation.py,9,Sylvain Marie,
scikit-learn/sklearn/utils/validation.py,10,License: BSD 3 clause,
scikit-learn/sklearn/utils/validation.py,34,Silenced by default to reduce verbosity. Turn on at runtime for,
scikit-learn/sklearn/utils/validation.py,35,performance profiling.,
scikit-learn/sklearn/utils/validation.py,41,validation is also imported in extmath,
scikit-learn/sklearn/utils/validation.py,47,"First try an O(n) time, O(1) space solution for the common case that",
scikit-learn/sklearn/utils/validation.py,48,everything is finite; fall back to O(n) space np.isfinite to prevent,
scikit-learn/sklearn/utils/validation.py,49,false positives from overflow in sum method. The sum is also calculated,
scikit-learn/sklearn/utils/validation.py,50,safely to reduce dtype induced overflows.,
scikit-learn/sklearn/utils/validation.py,64,"for object dtype data, we only check for NaNs (GH-13254)",
scikit-learn/sklearn/utils/validation.py,121,is numpy array,
scikit-learn/sklearn/utils/validation.py,142,Don't get num_samples from an ensembles length!,
scikit-learn/sklearn/utils/validation.py,155,Check that shape is returning an integer or default to len,
scikit-learn/sklearn/utils/validation.py,156,Dask dataframes may not return numeric shape[0] value,
scikit-learn/sklearn/utils/validation.py,304,Indices dtype validation,
scikit-learn/sklearn/utils/validation.py,316,ensure correct sparse format,
scikit-learn/sklearn/utils/validation.py,318,create new with correct sparse,
scikit-learn/sklearn/utils/validation.py,322,any other type,
scikit-learn/sklearn/utils/validation.py,328,convert dtype,
scikit-learn/sklearn/utils/validation.py,331,force copy,
scikit-learn/sklearn/utils/validation.py,438,store reference to original array to check if copy is needed when,
scikit-learn/sklearn/utils/validation.py,439,function returns,
scikit-learn/sklearn/utils/validation.py,442,store whether originally we wanted numeric dtype,
scikit-learn/sklearn/utils/validation.py,447,not a data type (e.g. a column named dtype in a pandas DataFrame),
scikit-learn/sklearn/utils/validation.py,450,check if the object contains several dtypes (typically a pandas,
scikit-learn/sklearn/utils/validation.py,451,"DataFrame), and store them. If not, store None.",
scikit-learn/sklearn/utils/validation.py,454,"throw warning if columns are sparse. If all columns are sparse, then",
scikit-learn/sklearn/utils/validation.py,455,array.sparse exists and sparsity will be perserved (later).,
scikit-learn/sklearn/utils/validation.py,466,pandas boolean dtype __array__ interface coerces bools to objects,
scikit-learn/sklearn/utils/validation.py,476,"if input is object, convert to float.",
scikit-learn/sklearn/utils/validation.py,483,no dtype conversion required,
scikit-learn/sklearn/utils/validation.py,486,dtype conversion required. Let's select the first element of the,
scikit-learn/sklearn/utils/validation.py,487,list of accepted types.,
scikit-learn/sklearn/utils/validation.py,503,"When all dataframe columns are sparse, convert to a sparse array",
scikit-learn/sklearn/utils/validation.py,505,DataFrame.sparse only supports `to_coo`,
scikit-learn/sklearn/utils/validation.py,515,"If np.array(..) gives ComplexWarning, then we convert the warning",
scikit-learn/sklearn/utils/validation.py,516,to an error. This is needed because specifying a non complex,
scikit-learn/sklearn/utils/validation.py,517,"dtype to the function converts complex to real dtype,",
scikit-learn/sklearn/utils/validation.py,518,thereby passing the test made in the lines following the scope,
scikit-learn/sklearn/utils/validation.py,519,of warnings context manager.,
scikit-learn/sklearn/utils/validation.py,524,Conversion float -> int should not contain NaN or,
scikit-learn/sklearn/utils/validation.py,525,inf (numpy#14412). We cannot use casting='safe' because,
scikit-learn/sklearn/utils/validation.py,526,then conversion float -> int would be disallowed.,
scikit-learn/sklearn/utils/validation.py,538,It is possible that the np.array(..) gave no warning. This happens,
scikit-learn/sklearn/utils/validation.py,539,"when no dtype conversion happened, for example dtype = None. The",
scikit-learn/sklearn/utils/validation.py,540,result is that np.array(..) produces an array of complex dtype,
scikit-learn/sklearn/utils/validation.py,541,and we need to catch and raise exception for such cases.,
scikit-learn/sklearn/utils/validation.py,545,If input is scalar raise error,
scikit-learn/sklearn/utils/validation.py,552,If input is 1D raise error,
scikit-learn/sklearn/utils/validation.py,560,in the future np.flexible dtypes will be handled like object dtypes,
scikit-learn/sklearn/utils/validation.py,571,make sure we actually converted to numeric:,
scikit-learn/sklearn/utils/validation.py,861,"only csr, csc, and coo have `data` attribute",
scikit-learn/sklearn/utils/validation.py,962,avoid X.min() on sparse matrix since it also sorts the indices,
scikit-learn/sklearn/utils/validation.py,1103,note: the minimum value available is,
scikit-learn/sklearn/utils/validation.py,1104,- single-precision: np.finfo('float32').eps = 1.2e-07,
scikit-learn/sklearn/utils/validation.py,1105,- double-precision: np.finfo('float64').eps = 2.2e-16,
scikit-learn/sklearn/utils/validation.py,1107,the various thresholds used for validation,
scikit-learn/sklearn/utils/validation.py,1108,we may wish to change the value according to precision.,
scikit-learn/sklearn/utils/validation.py,1114,Check that there are no significant imaginary parts,
scikit-learn/sklearn/utils/validation.py,1126,warn about imaginary parts being removed,
scikit-learn/sklearn/utils/validation.py,1136,Remove all imaginary parts (even if zero),
scikit-learn/sklearn/utils/validation.py,1139,Check that there are no significant negative eigenvalues,
scikit-learn/sklearn/utils/validation.py,1157,Remove all negative values and warn about it,
scikit-learn/sklearn/utils/validation.py,1168,Check for conditioning (small positive non-zeros),
scikit-learn/sklearn/utils/validation.py,1296,ignore first 'self' argument for instance methods,
scikit-learn/sklearn/utils/validation.py,1333,Non-indexable pass-through (for now for backward-compatibility).,
scikit-learn/sklearn/utils/validation.py,1334,https://github.com/scikit-learn/scikit-learn/issues/15805,
scikit-learn/sklearn/utils/validation.py,1337,Any other fit_params should support indexing,
scikit-learn/sklearn/utils/validation.py,1338,(e.g. for cross-validation).,
scikit-learn/sklearn/utils/class_weight.py,1,Authors: Andreas Mueller,
scikit-learn/sklearn/utils/class_weight.py,2,Manoj Kumar,
scikit-learn/sklearn/utils/class_weight.py,3,License: BSD 3 clause,
scikit-learn/sklearn/utils/class_weight.py,37,Import error caused by circular imports.,
scikit-learn/sklearn/utils/class_weight.py,44,uniform class weights,
scikit-learn/sklearn/utils/class_weight.py,47,Find the weight of each class as present in y.,
scikit-learn/sklearn/utils/class_weight.py,57,user-defined dictionary,
scikit-learn/sklearn/utils/class_weight.py,146,"Get class weights for the subsample, covering all classes in",
scikit-learn/sklearn/utils/class_weight.py,147,case some labels that were present in the original data are,
scikit-learn/sklearn/utils/class_weight.py,148,missing from the sample.,
scikit-learn/sklearn/utils/class_weight.py,168,Make missing classes' weight zero,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,184,Convert data,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,190,Function is only called after we verify that pandas is installed,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,214,Intentionally modify the balanced class_weight,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,215,to simulate a bug and raise an exception,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,219,Simply assigning coef_ to the class_weight,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,236,Convert data,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,245,return 1 if X has more than one element else return 0,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,309,"Toy classifier that only supports binary classification, will fail tests.",
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,318,Toy classifier that only supports binary classification.,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,342,always returns True,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,347,Tests that check_fit_score_takes_y works on a class with,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,348,a deprecated fit method,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,360,"tests that the estimator actually fails on ""bad"" estimators.",
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,361,"not a complete test of all checks, which are very extensive.",
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,363,check that we have a set_params and can clone,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,367,check that values returned by get_params match set_params,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,374,check that we have a fit method,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,378,check that fit does input validation,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,384,check that sample_weights in fit accepts pandas.Series type,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,386,noqa,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,393,check that predict does input validation (doesn't accept dicts in input),
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,398,check that estimator state does not change,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,399,at transform/predict/predict_proba time,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,402,check that `fit` only changes attribures that,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,403,are private (start with an _ or end with a _).,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,409,check that `fit` doesn't add any public attribute,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,416,check for invariant method,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,423,check for sparse matrix input handling,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,426,"the check for sparse input handling prints to the stdout,",
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,427,"instead of raising an error, so as not to remove the original traceback.",
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,428,that means we need to jump through some hoops to catch it.,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,440,Large indices test on bad estimator,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,446,does error on binary_only untagged estimator,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,451,non-regression test for estimators transforming to sparse data,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,454,doesn't error on actual estimator,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,460,doesn't error on binary_only tagged estimator,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,463,Check regressor with requires_positive_y estimator tag,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,470,should raise AssertionError,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,473,should pass,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,479,check that TransformerMixin is not required for transformer tests to run,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,485,check that check_estimator doesn't modify the estimator it receives,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,493,when 'est = SGDClassifier()',
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,497,without fitting,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,503,when 'est = SGDClassifier()',
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,507,with fitting,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,515,check that a ValueError/AttributeError is raised when calling predict,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,516,on an unfitted estimator,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,521,check that CorrectNotFittedError inherit from either ValueError,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,522,or AttributeError,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,553,check that check_estimator() works on estimator with _pairwise,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,554,kernel or metric,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,556,test precomputed kernel,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,560,test precomputed metric,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,608,check that ill-computed balanced weights raises an exception,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,618,all_estimator should not fail when pytest is not installed and return,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,619,only public estimators,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,626,This module is run as a script to check that we have no dependency on,
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,627,pytest for estimator checks.,
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,26,Sparsify the array a little bit,
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,58,Sparsify the array a little bit,
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,96,default params for incr_mean_variance,
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,101,Test errors,
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,112,Test _incr_mean_and_var with a 1 row input,
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,118,X.shape[axis] picks # samples,
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,127,Test _incremental_mean_and_var with whole data,
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,166,non-regression test for:,
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,167,https://github.com/scikit-learn/scikit-learn/issues/16448,
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,168,check that computing the incremental mean and variance is equivalent to,
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,169,computing the mean and variance on the stacked dataset.,
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,186,check the behaviour when we update the variance with an empty matrix,
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,195,update statistic with a column which should ignored,
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,222,we avoid creating specific data for axis 0 and 1: translating the data is,
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,223,enough.,
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,228,take a copy of the old statistics since they are modified in place.,
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,243,Sparsify the array a little bit,
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,526,Check dtypes with large sparse matrices too,
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,527,XXX: test fails on 32bit (Windows/Linux),
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,543,Test csc_row_median actually calculates the median.,
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,545,Test that it gives the same output when X is dense.,
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,553,Test that it gives the same output when X is sparse,
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,563,Test for toy data.,
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,571,Test that it raises an Error for non-csc matrices.,
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,586,"csr_matrix will use int32 indices by default,",
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,587,up-casting those to int64 when necessary,
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,602,checks that csr_row_norms returns the same output as,
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,603,"scipy.sparse.linalg.norm, and that the dype is the same as X.dtype.",
scikit-learn/sklearn/utils/tests/test_murmurhash.py,1,Author: Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/sklearn/utils/tests/test_murmurhash.py,2,,
scikit-learn/sklearn/utils/tests/test_murmurhash.py,3,License: BSD 3 clause,
scikit-learn/sklearn/utils/tests/test_fixes.py,1,Authors: Gael Varoquaux <gael.varoquaux@normalesup.org>,
scikit-learn/sklearn/utils/tests/test_fixes.py,2,Justin Vincent,
scikit-learn/sklearn/utils/tests/test_fixes.py,3,Lars Buitinck,
scikit-learn/sklearn/utils/tests/test_fixes.py,4,License: BSD 3 clause,
scikit-learn/sklearn/utils/tests/test_fixes.py,25,arguments are simply passed through,
scikit-learn/sklearn/utils/tests/test_fixes.py,31,arguments are mapped to the corresponding backend,
scikit-learn/sklearn/utils/tests/test_fixes.py,71,"Test the basics; right bounds, right size",
scikit-learn/sklearn/utils/tests/test_fixes.py,75,Test that it's actually (fairly) uniform,
scikit-learn/sklearn/utils/tests/test_fixes.py,81,Test that random_state works,
scikit-learn/sklearn/utils/tests/test_validation.py,57,Test function for as_float_array,
scikit-learn/sklearn/utils/tests/test_validation.py,62,Another test,
scikit-learn/sklearn/utils/tests/test_validation.py,65,Checking that the array wasn't overwritten,
scikit-learn/sklearn/utils/tests/test_validation.py,68,Test int dtypes <= 32bit,
scikit-learn/sklearn/utils/tests/test_validation.py,77,Test object dtype,
scikit-learn/sklearn/utils/tests/test_validation.py,82,"Here, X is of the right type, it shouldn't be modified",
scikit-learn/sklearn/utils/tests/test_validation.py,85,Test that if X is fortran ordered it stays,
scikit-learn/sklearn/utils/tests/test_validation.py,89,Test the copy parameter with some matrices,
scikit-learn/sklearn/utils/tests/test_validation.py,113,Confirm that input validation code does not return np.matrix,
scikit-learn/sklearn/utils/tests/test_validation.py,122,Confirm that input validation code doesn't copy memory mapped arrays,
scikit-learn/sklearn/utils/tests/test_validation.py,138,Check that ordering is enforced correctly by validation utilities.,
scikit-learn/sklearn/utils/tests/test_validation.py,139,"We need to check each validation utility, because a 'copy' without",
scikit-learn/sklearn/utils/tests/test_validation.py,140,'order=K' will kill the ordering.,
scikit-learn/sklearn/utils/tests/test_validation.py,220,casting a float array containing NaN or inf to int dtype should,
scikit-learn/sklearn/utils/tests/test_validation.py,221,raise an error irrespective of the force_all_finite parameter.,
scikit-learn/sklearn/utils/tests/test_validation.py,228,accept_sparse == False,
scikit-learn/sklearn/utils/tests/test_validation.py,229,raise error on sparse inputs,
scikit-learn/sklearn/utils/tests/test_validation.py,235,ensure_2d=False,
scikit-learn/sklearn/utils/tests/test_validation.py,238,ensure_2d=True with 1d array,
scikit-learn/sklearn/utils/tests/test_validation.py,243,ensure_2d=True with scalar array,
scikit-learn/sklearn/utils/tests/test_validation.py,248,don't allow ndim > 3,
scikit-learn/sklearn/utils/tests/test_validation.py,252,doesn't raise,
scikit-learn/sklearn/utils/tests/test_validation.py,254,dtype and order enforcement.,
scikit-learn/sklearn/utils/tests/test_validation.py,279,doesn't copy if it was already good,
scikit-learn/sklearn/utils/tests/test_validation.py,285,allowed sparse != None,
scikit-learn/sklearn/utils/tests/test_validation.py,300,XXX unreached code as of v0.22,
scikit-learn/sklearn/utils/tests/test_validation.py,312,no change if allowed,
scikit-learn/sklearn/utils/tests/test_validation.py,315,got converted,
scikit-learn/sklearn/utils/tests/test_validation.py,320,doesn't copy if it was already good,
scikit-learn/sklearn/utils/tests/test_validation.py,324,other input formats,
scikit-learn/sklearn/utils/tests/test_validation.py,325,convert lists to arrays,
scikit-learn/sklearn/utils/tests/test_validation.py,328,raise on too deep lists,
scikit-learn/sklearn/utils/tests/test_validation.py,331,doesn't raise,
scikit-learn/sklearn/utils/tests/test_validation.py,333,convert weird stuff to arrays,
scikit-learn/sklearn/utils/tests/test_validation.py,338,"deprecation warning if string-like array with dtype=""numeric""",
scikit-learn/sklearn/utils/tests/test_validation.py,345,"deprecation warning if byte-like array with dtype=""numeric""",
scikit-learn/sklearn/utils/tests/test_validation.py,353,test that data-frame like objects with dtype object,
scikit-learn/sklearn/utils/tests/test_validation.py,354,get converted,
scikit-learn/sklearn/utils/tests/test_validation.py,359,"smoke-test against dataframes with column named ""dtype""",
scikit-learn/sklearn/utils/tests/test_validation.py,365,test that data-frames with homogeneous dtype are not upcast,
scikit-learn/sklearn/utils/tests/test_validation.py,379,"float16, int16, float32 casts to float32",
scikit-learn/sklearn/utils/tests/test_validation.py,384,"float16, int16, float16 casts to float32",
scikit-learn/sklearn/utils/tests/test_validation.py,390,we're not using upcasting rules for determining,
scikit-learn/sklearn/utils/tests/test_validation.py,391,"the target type yet, so we cast to the default of float64",
scikit-learn/sklearn/utils/tests/test_validation.py,394,check that we handle pandas dtypes in a semi-reasonable way,
scikit-learn/sklearn/utils/tests/test_validation.py,395,this is actually tricky because we can't really know that this,
scikit-learn/sklearn/utils/tests/test_validation.py,396,should be integer ahead of converting it.,
scikit-learn/sklearn/utils/tests/test_validation.py,413,test that lists with ints don't get converted to floats,
scikit-learn/sklearn/utils/tests/test_validation.py,501,When large sparse are allowed,
scikit-learn/sklearn/utils/tests/test_validation.py,506,When large sparse are not allowed,
scikit-learn/sklearn/utils/tests/test_validation.py,514,empty list is considered 2D by default:,
scikit-learn/sklearn/utils/tests/test_validation.py,520,"If considered a 1D collection when ensure_2d=False, then the minimum",
scikit-learn/sklearn/utils/tests/test_validation.py,521,number of samples will break:,
scikit-learn/sklearn/utils/tests/test_validation.py,526,Invalid edge case when checking the default minimum sample of a scalar,
scikit-learn/sklearn/utils/tests/test_validation.py,532,Simulate a model that would need at least 2 samples to be well defined,
scikit-learn/sklearn/utils/tests/test_validation.py,540,The same message is raised if the data has 2 dimensions even if this is,
scikit-learn/sklearn/utils/tests/test_validation.py,541,not mandatory,
scikit-learn/sklearn/utils/tests/test_validation.py,545,Simulate a model that would require at least 3 features (e.g. SelectKBest,
scikit-learn/sklearn/utils/tests/test_validation.py,546,with k=3),
scikit-learn/sklearn/utils/tests/test_validation.py,554,Only the feature check is enabled whenever the number of dimensions is 2,
scikit-learn/sklearn/utils/tests/test_validation.py,555,even if allow_nd is enabled:,
scikit-learn/sklearn/utils/tests/test_validation.py,559,Simulate a case where a pipeline stage as trimmed all the features of a,
scikit-learn/sklearn/utils/tests/test_validation.py,560,2D dataset.,
scikit-learn/sklearn/utils/tests/test_validation.py,568,nd-data is not checked for any minimum number of features by default:,
scikit-learn/sklearn/utils/tests/test_validation.py,581,list of lists,
scikit-learn/sklearn/utils/tests/test_validation.py,586,tuple of tuples,
scikit-learn/sklearn/utils/tests/test_validation.py,591,list of np arrays,
scikit-learn/sklearn/utils/tests/test_validation.py,597,tuple of np arrays,
scikit-learn/sklearn/utils/tests/test_validation.py,603,dataframe,
scikit-learn/sklearn/utils/tests/test_validation.py,609,sparse matrix,
scikit-learn/sklearn/utils/tests/test_validation.py,644,check error for bad inputs,
scikit-learn/sklearn/utils/tests/test_validation.py,648,check that asymmetric arrays are properly symmetrized,
scikit-learn/sklearn/utils/tests/test_validation.py,650,Check for warnings and errors,
scikit-learn/sklearn/utils/tests/test_validation.py,665,Check is TypeError raised when non estimator instance passed,
scikit-learn/sklearn/utils/tests/test_validation.py,682,NotFittedError is a subclass of both ValueError and AttributeError,
scikit-learn/sklearn/utils/tests/test_validation.py,738,Does not raise,
scikit-learn/sklearn/utils/tests/test_validation.py,741,Raises when using attribute that is not defined,
scikit-learn/sklearn/utils/tests/test_validation.py,760,Despite ensembles having __len__ they must raise TypeError,
scikit-learn/sklearn/utils/tests/test_validation.py,763,"XXX: We should have a test with a string, but what is correct behaviour?",
scikit-learn/sklearn/utils/tests/test_validation.py,767,check pandas dataframe with 'fit' column does not raise error,
scikit-learn/sklearn/utils/tests/test_validation.py,768,https://github.com/scikit-learn/scikit-learn/issues/8415,
scikit-learn/sklearn/utils/tests/test_validation.py,790,regression test that check_array works on pandas Series,
scikit-learn/sklearn/utils/tests/test_validation.py,795,with categorical dtype (not a numpy dtype) (GH12699),
scikit-learn/sklearn/utils/tests/test_validation.py,802,"pandas dataframe will coerce a boolean into a object, this is a mismatch",
scikit-learn/sklearn/utils/tests/test_validation.py,803,with np.result_type which will return a float,
scikit-learn/sklearn/utils/tests/test_validation.py,804,check_array needs to explicitly check for bool dtype in a dataframe for,
scikit-learn/sklearn/utils/tests/test_validation.py,805,this situation,
scikit-learn/sklearn/utils/tests/test_validation.py,806,https://github.com/scikit-learn/scikit-learn/issues/15787,
scikit-learn/sklearn/utils/tests/test_validation.py,899,check that it gives a good error if there's no __len__,
scikit-learn/sklearn/utils/tests/test_validation.py,971,Test that ``_check_psd_eigenvalues`` returns the right output for valid,
scikit-learn/sklearn/utils/tests/test_validation.py,972,"input, possibly raising the right warning",
scikit-learn/sklearn/utils/tests/test_validation.py,1007,Test that ``_check_psd_eigenvalues`` raises the right error for invalid,
scikit-learn/sklearn/utils/tests/test_validation.py,1008,input,
scikit-learn/sklearn/utils/tests/test_validation.py,1015,check array order,
scikit-learn/sklearn/utils/tests/test_validation.py,1021,check None input,
scikit-learn/sklearn/utils/tests/test_validation.py,1025,check numbers input,
scikit-learn/sklearn/utils/tests/test_validation.py,1029,check wrong number of dimensions,
scikit-learn/sklearn/utils/tests/test_validation.py,1034,check incorrect n_samples,
scikit-learn/sklearn/utils/tests/test_validation.py,1039,float32 dtype is preserved,
scikit-learn/sklearn/utils/tests/test_validation.py,1045,int dtype will be converted to float64 instead,
scikit-learn/sklearn/utils/tests/test_validation.py,1100,The * is place before a keyword only argument without a default value,
scikit-learn/sklearn/utils/tests/test_validation.py,1169,check_array converts pandas dataframe with only sparse arrays into,
scikit-learn/sklearn/utils/tests/test_validation.py,1170,sparse matrix,
scikit-learn/sklearn/utils/tests/test_validation.py,1178,by default pandas converts to coo when accept_sparse is True,
scikit-learn/sklearn/utils/tests/test_utils.py,34,toy array,
scikit-learn/sklearn/utils/tests/test_utils.py,39,Check the check_random_state utility function behavior,
scikit-learn/sklearn/utils/tests/test_utils.py,57,Make sure gen_batches errors on invalid batch_size,
scikit-learn/sklearn/utils/tests/test_utils.py,73,Test whether the deprecated decorator issues appropriate warnings,
scikit-learn/sklearn/utils/tests/test_utils.py,74,Copied almost verbatim from https://docs.python.org/library/warnings.html,
scikit-learn/sklearn/utils/tests/test_utils.py,76,First a function...,
scikit-learn/sklearn/utils/tests/test_utils.py,86,function must remain usable,
scikit-learn/sklearn/utils/tests/test_utils.py,92,... then a class.,
scikit-learn/sklearn/utils/tests/test_utils.py,110,Border case not worth mentioning in doctests,
scikit-learn/sklearn/utils/tests/test_utils.py,113,Check that invalid arguments yield ValueError,
scikit-learn/sklearn/utils/tests/test_utils.py,121,"Issue:6581, n_samples can be more when replace is True (default).",
scikit-learn/sklearn/utils/tests/test_utils.py,126,Make sure resample can stratify,
scikit-learn/sklearn/utils/tests/test_utils.py,139,"all 1s, one 0",
scikit-learn/sklearn/utils/tests/test_utils.py,143,Make sure stratified resampling supports the replace parameter,
scikit-learn/sklearn/utils/tests/test_utils.py,156,make sure n_samples can be greater than X.shape[0] if we sample with,
scikit-learn/sklearn/utils/tests/test_utils.py,157,replacement,
scikit-learn/sklearn/utils/tests/test_utils.py,165,Make sure y can be 2d when stratifying,
scikit-learn/sklearn/utils/tests/test_utils.py,175,resample must be ndarray,
scikit-learn/sklearn/utils/tests/test_utils.py,302,validation of the indices,
scikit-learn/sklearn/utils/tests/test_utils.py,303,we make a copy because indices is mutable and shared between tests,
scikit-learn/sklearn/utils/tests/test_utils.py,427,sparse matrix are keeping the 2D shape,
scikit-learn/sklearn/utils/tests/test_utils.py,458,check that we are raising an error if the array-like passed is 1D and,
scikit-learn/sklearn/utils/tests/test_utils.py,459,we try to index on the 2nd dimension,
scikit-learn/sklearn/utils/tests/test_utils.py,510,to make the inner arrays hashable,
scikit-learn/sklearn/utils/tests/test_utils.py,513,"A.shape = (2,2,2)",
scikit-learn/sklearn/utils/tests/test_utils.py,515,shouldn't raise a ValueError for dim = 3,
scikit-learn/sklearn/utils/tests/test_utils.py,520,Check that shuffle does not try to convert to numpy arrays with float,
scikit-learn/sklearn/utils/tests/test_utils.py,521,dtypes can let any indexable datastructure pass-through.,
scikit-learn/sklearn/utils/tests/test_utils.py,553,check that gen_even_slices contains all samples,
scikit-learn/sklearn/utils/tests/test_utils.py,559,check that passing negative n_chunks raises an error,
scikit-learn/sklearn/utils/tests/test_utils.py,679,Only parallel_backend and register_parallel_backend are not deprecated in,
scikit-learn/sklearn/utils/tests/test_utils.py,680,sklearn.utils,
scikit-learn/sklearn/utils/tests/test_fast_dict.py,27,Test the argmin implementation on the IntFloatDict,
scikit-learn/sklearn/utils/tests/test_testing.py,43,0.24,
scikit-learn/sklearn/utils/tests/test_testing.py,51,0.24,
scikit-learn/sklearn/utils/tests/test_testing.py,59,0.24,
scikit-learn/sklearn/utils/tests/test_testing.py,68,0.24,
scikit-learn/sklearn/utils/tests/test_testing.py,79,Linear Discriminant Analysis doesn't have random state: smoke test,
scikit-learn/sklearn/utils/tests/test_testing.py,90,basic compare,
scikit-learn/sklearn/utils/tests/test_testing.py,132,multiple exceptions in a tuple,
scikit-learn/sklearn/utils/tests/test_testing.py,139,This check that ignore_warning decorateur and context manager are working,
scikit-learn/sklearn/utils/tests/test_testing.py,140,as expected,
scikit-learn/sklearn/utils/tests/test_testing.py,148,Check the function directly,
scikit-learn/sklearn/utils/tests/test_testing.py,164,Check the decorator,
scikit-learn/sklearn/utils/tests/test_testing.py,197,Check the context manager,
scikit-learn/sklearn/utils/tests/test_testing.py,229,Check that passing warning class as first positional argument,
scikit-learn/sklearn/utils/tests/test_testing.py,254,test that assert_warns doesn't have side effects on warnings,
scikit-learn/sklearn/utils/tests/test_testing.py,255,filters,
scikit-learn/sklearn/utils/tests/test_testing.py,269,Should raise an AssertionError,
scikit-learn/sklearn/utils/tests/test_testing.py,271,"assert_warns has a special handling of ""FutureWarning"" that",
scikit-learn/sklearn/utils/tests/test_testing.py,272,pytest.warns does not have,
scikit-learn/sklearn/utils/tests/test_testing.py,284,Tests for docstrings:,
scikit-learn/sklearn/utils/tests/test_testing.py,665,0.24,
scikit-learn/sklearn/utils/tests/test_pprint.py,14,Ignore flake8 (lots of line too long issues),
scikit-learn/sklearn/utils/tests/test_pprint.py,15,flake8: noqa,
scikit-learn/sklearn/utils/tests/test_pprint.py,17,Constructors excerpted to test pprinting,
scikit-learn/sklearn/utils/tests/test_pprint.py,178,Basic pprint test,
scikit-learn/sklearn/utils/tests/test_pprint.py,187,remove first \n,
scikit-learn/sklearn/utils/tests/test_pprint.py,192,Make sure the changed_only param is correctly used,
scikit-learn/sklearn/utils/tests/test_pprint.py,198,Check with a repr that doesn't fit on a single line,
scikit-learn/sklearn/utils/tests/test_pprint.py,204,remove first \n,
scikit-learn/sklearn/utils/tests/test_pprint.py,211,"Defaults to np.NaN, trying with float('NaN')",
scikit-learn/sklearn/utils/tests/test_pprint.py,216,make sure array parameters don't throw error (see #13583),
scikit-learn/sklearn/utils/tests/test_pprint.py,223,Render a pipeline object,
scikit-learn/sklearn/utils/tests/test_pprint.py,239,remove first \n,
scikit-learn/sklearn/utils/tests/test_pprint.py,244,Render a deeply nested estimator,
scikit-learn/sklearn/utils/tests/test_pprint.py,276,remove first \n,
scikit-learn/sklearn/utils/tests/test_pprint.py,281,render a gridsearch,
scikit-learn/sklearn/utils/tests/test_pprint.py,301,remove first \n,
scikit-learn/sklearn/utils/tests/test_pprint.py,306,render a pipeline inside a gridsearch,
scikit-learn/sklearn/utils/tests/test_pprint.py,367,remove first \n,
scikit-learn/sklearn/utils/tests/test_pprint.py,369,Remove address of '<function chi2 at 0x.....>' for reproducibility,
scikit-learn/sklearn/utils/tests/test_pprint.py,383,No ellipsis,
scikit-learn/sklearn/utils/tests/test_pprint.py,400,remove first \n,
scikit-learn/sklearn/utils/tests/test_pprint.py,403,Now with ellipsis,
scikit-learn/sklearn/utils/tests/test_pprint.py,420,remove first \n,
scikit-learn/sklearn/utils/tests/test_pprint.py,423,Also test with lists,
scikit-learn/sklearn/utils/tests/test_pprint.py,440,remove first \n,
scikit-learn/sklearn/utils/tests/test_pprint.py,443,Now with ellipsis,
scikit-learn/sklearn/utils/tests/test_pprint.py,460,remove first \n,
scikit-learn/sklearn/utils/tests/test_pprint.py,465,Check that the bruteforce ellipsis (used when the number of non-blank,
scikit-learn/sklearn/utils/tests/test_pprint.py,466,characters exceeds N_CHAR_MAX) renders correctly.,
scikit-learn/sklearn/utils/tests/test_pprint.py,470,test when the left and right side of the ellipsis aren't on the same,
scikit-learn/sklearn/utils/tests/test_pprint.py,471,line.,
scikit-learn/sklearn/utils/tests/test_pprint.py,479,remove first \n,
scikit-learn/sklearn/utils/tests/test_pprint.py,482,test with very small N_CHAR_MAX,
scikit-learn/sklearn/utils/tests/test_pprint.py,483,"Note that N_CHAR_MAX is not strictly enforced, but it's normal: to avoid",
scikit-learn/sklearn/utils/tests/test_pprint.py,484,weird reprs we still keep the whole line of the right part (after the,
scikit-learn/sklearn/utils/tests/test_pprint.py,485,ellipsis).,
scikit-learn/sklearn/utils/tests/test_pprint.py,490,remove first \n,
scikit-learn/sklearn/utils/tests/test_pprint.py,493,test with N_CHAR_MAX == number of non-blank characters: In this case we,
scikit-learn/sklearn/utils/tests/test_pprint.py,494,don't want ellipsis,
scikit-learn/sklearn/utils/tests/test_pprint.py,500,test with N_CHAR_MAX == number of non-blank characters - 10: the left and,
scikit-learn/sklearn/utils/tests/test_pprint.py,501,right side of the ellispsis are on different lines. In this case we,
scikit-learn/sklearn/utils/tests/test_pprint.py,502,want to expend the whole line of the right side,
scikit-learn/sklearn/utils/tests/test_pprint.py,509,remove first \n,
scikit-learn/sklearn/utils/tests/test_pprint.py,512,test with N_CHAR_MAX == number of non-blank characters - 10: the left and,
scikit-learn/sklearn/utils/tests/test_pprint.py,513,right side of the ellispsis are on the same line. In this case we don't,
scikit-learn/sklearn/utils/tests/test_pprint.py,514,"want to expend the whole line of the right side, just add the ellispsis",
scikit-learn/sklearn/utils/tests/test_pprint.py,515,between the 2 sides.,
scikit-learn/sklearn/utils/tests/test_pprint.py,522,remove first \n,
scikit-learn/sklearn/utils/tests/test_pprint.py,525,test with N_CHAR_MAX == number of non-blank characters - 2: the left and,
scikit-learn/sklearn/utils/tests/test_pprint.py,526,"right side of the ellispsis are on the same line, but adding the ellipsis",
scikit-learn/sklearn/utils/tests/test_pprint.py,527,would actually make the repr longer. So we don't add the ellipsis.,
scikit-learn/sklearn/utils/tests/test_pprint.py,534,remove first \n,
scikit-learn/sklearn/utils/tests/test_pprint.py,539,non regression test than ensures we can still use the builtin,
scikit-learn/sklearn/utils/tests/test_pprint.py,540,PrettyPrinter class for estimators (as done e.g. by joblib).,
scikit-learn/sklearn/utils/tests/test_pprint.py,541,Used to be a bug,
scikit-learn/sklearn/utils/tests/test_random.py,11,,
scikit-learn/sklearn/utils/tests/test_random.py,12,test custom sampling without replacement algorithm,
scikit-learn/sklearn/utils/tests/test_random.py,13,,
scikit-learn/sklearn/utils/tests/test_random.py,36,n_population < n_sample,
scikit-learn/sklearn/utils/tests/test_random.py,42,n_population == n_samples,
scikit-learn/sklearn/utils/tests/test_random.py,47,n_population >= n_samples,
scikit-learn/sklearn/utils/tests/test_random.py,51,n_population < 0 or n_samples < 0,
scikit-learn/sklearn/utils/tests/test_random.py,59,This test is heavily inspired from test_random.py of python-core.,
scikit-learn/sklearn/utils/tests/test_random.py,60,,
scikit-learn/sklearn/utils/tests/test_random.py,61,"For the entire allowable range of 0 <= k <= N, validate that",
scikit-learn/sklearn/utils/tests/test_random.py,62,the sample is of the correct length and contains only unique items,
scikit-learn/sklearn/utils/tests/test_random.py,72,test edge case n_population == n_samples == 0,
scikit-learn/sklearn/utils/tests/test_random.py,77,This test is heavily inspired from test_random.py of python-core.,
scikit-learn/sklearn/utils/tests/test_random.py,78,,
scikit-learn/sklearn/utils/tests/test_random.py,79,"For the entire allowable range of 0 <= k <= N, validate that",
scikit-learn/sklearn/utils/tests/test_random.py,80,sample generates all possible permutations,
scikit-learn/sklearn/utils/tests/test_random.py,83,a large number of trials prevents false negatives without slowing normal,
scikit-learn/sklearn/utils/tests/test_random.py,84,case,
scikit-learn/sklearn/utils/tests/test_random.py,88,Counting the number of combinations is not as good as counting the,
scikit-learn/sklearn/utils/tests/test_random.py,89,"the number of permutations. However, it works with sampling algorithm",
scikit-learn/sklearn/utils/tests/test_random.py,90,that does not provide a random permutation of the subset of integer.,
scikit-learn/sklearn/utils/tests/test_random.py,107,Explicit class probabilities,
scikit-learn/sklearn/utils/tests/test_random.py,119,Implicit class probabilities,
scikit-learn/sklearn/utils/tests/test_random.py,120,test for array-like support,
scikit-learn/sklearn/utils/tests/test_random.py,132,Edge case probabilities 1.0 and 0.0,
scikit-learn/sklearn/utils/tests/test_random.py,145,One class target data,
scikit-learn/sklearn/utils/tests/test_random.py,146,test for array-like support,
scikit-learn/sklearn/utils/tests/test_random.py,160,the length of an array in classes and class_probabilities is mismatched,
scikit-learn/sklearn/utils/tests/test_random.py,166,the class dtype is not supported,
scikit-learn/sklearn/utils/tests/test_random.py,172,the class dtype is not supported,
scikit-learn/sklearn/utils/tests/test_random.py,178,Given probabilities don't sum to 1,
scikit-learn/sklearn/utils/tests/test_class_weight.py,14,Test (and demo) compute_class_weight.,
scikit-learn/sklearn/utils/tests/test_class_weight.py,19,total effect of samples is preserved,
scikit-learn/sklearn/utils/tests/test_class_weight.py,26,Raise error when y does not contain all class labels,
scikit-learn/sklearn/utils/tests/test_class_weight.py,31,Fix exception in error message formatting when missing label is a string,
scikit-learn/sklearn/utils/tests/test_class_weight.py,32,https://github.com/scikit-learn/scikit-learn/issues/8312,
scikit-learn/sklearn/utils/tests/test_class_weight.py,36,Raise error when y has items not in classes,
scikit-learn/sklearn/utils/tests/test_class_weight.py,50,"When the user specifies class weights, compute_class_weights should just",
scikit-learn/sklearn/utils/tests/test_class_weight.py,51,return them.,
scikit-learn/sklearn/utils/tests/test_class_weight.py,54,"When a class weight is specified that isn't in classes, a ValueError",
scikit-learn/sklearn/utils/tests/test_class_weight.py,55,should get raised,
scikit-learn/sklearn/utils/tests/test_class_weight.py,68,"Test that results with class_weight=""balanced"" is invariant wrt",
scikit-learn/sklearn/utils/tests/test_class_weight.py,69,class imbalance if the number of samples is identical.,
scikit-learn/sklearn/utils/tests/test_class_weight.py,70,The test uses a balanced two class dataset with 100 datapoints.,
scikit-learn/sklearn/utils/tests/test_class_weight.py,71,"It creates three versions, one where class 1 is duplicated",
scikit-learn/sklearn/utils/tests/test_class_weight.py,72,"resulting in 150 points of class 1 and 50 of class 0,",
scikit-learn/sklearn/utils/tests/test_class_weight.py,73,"one where there are 50 points in class 1 and 150 in class 0,",
scikit-learn/sklearn/utils/tests/test_class_weight.py,74,and one where there are 100 points of each class (this one is balanced,
scikit-learn/sklearn/utils/tests/test_class_weight.py,75,again).,
scikit-learn/sklearn/utils/tests/test_class_weight.py,76,"With balancing class weights, all three should give the same model.",
scikit-learn/sklearn/utils/tests/test_class_weight.py,78,create dataset where class 1 is duplicated twice,
scikit-learn/sklearn/utils/tests/test_class_weight.py,81,create dataset where class 0 is duplicated twice,
scikit-learn/sklearn/utils/tests/test_class_weight.py,84,duplicate everything,
scikit-learn/sklearn/utils/tests/test_class_weight.py,87,results should be identical,
scikit-learn/sklearn/utils/tests/test_class_weight.py,96,Test compute_class_weight when labels are negative,
scikit-learn/sklearn/utils/tests/test_class_weight.py,97,Test with balanced class labels.,
scikit-learn/sklearn/utils/tests/test_class_weight.py,105,Test with unbalanced class labels.,
scikit-learn/sklearn/utils/tests/test_class_weight.py,116,Test compute_class_weight when classes are unordered,
scikit-learn/sklearn/utils/tests/test_class_weight.py,127,Test for the case where no weight is given for a present class.,
scikit-learn/sklearn/utils/tests/test_class_weight.py,128,Current behaviour is to assign the unweighted classes a weight of 1.,
scikit-learn/sklearn/utils/tests/test_class_weight.py,133,Test for non specified weights,
scikit-learn/sklearn/utils/tests/test_class_weight.py,138,Tests for partly specified weights,
scikit-learn/sklearn/utils/tests/test_class_weight.py,149,Test (and demo) compute_sample_weight.,
scikit-learn/sklearn/utils/tests/test_class_weight.py,150,Test with balanced classes,
scikit-learn/sklearn/utils/tests/test_class_weight.py,155,Test with user-defined weights,
scikit-learn/sklearn/utils/tests/test_class_weight.py,159,Test with column vector of balanced classes,
scikit-learn/sklearn/utils/tests/test_class_weight.py,164,Test with unbalanced classes,
scikit-learn/sklearn/utils/tests/test_class_weight.py,171,Test with `None` weights,
scikit-learn/sklearn/utils/tests/test_class_weight.py,175,Test with multi-output of balanced classes,
scikit-learn/sklearn/utils/tests/test_class_weight.py,180,Test with multi-output with user-defined weights,
scikit-learn/sklearn/utils/tests/test_class_weight.py,185,Test with multi-output of unbalanced classes,
scikit-learn/sklearn/utils/tests/test_class_weight.py,192,Test compute_sample_weight with subsamples specified.,
scikit-learn/sklearn/utils/tests/test_class_weight.py,193,Test with balanced classes and all samples present,
scikit-learn/sklearn/utils/tests/test_class_weight.py,198,Test with column vector of balanced classes and all samples present,
scikit-learn/sklearn/utils/tests/test_class_weight.py,203,Test with a subsample,
scikit-learn/sklearn/utils/tests/test_class_weight.py,209,Test with a bootstrap subsample,
scikit-learn/sklearn/utils/tests/test_class_weight.py,215,Test with a bootstrap subsample for multi-output,
scikit-learn/sklearn/utils/tests/test_class_weight.py,220,Test with a missing class,
scikit-learn/sklearn/utils/tests/test_class_weight.py,225,Test with a missing class for multi-output,
scikit-learn/sklearn/utils/tests/test_class_weight.py,232,Test compute_sample_weight raises errors expected.,
scikit-learn/sklearn/utils/tests/test_class_weight.py,233,Invalid preset string,
scikit-learn/sklearn/utils/tests/test_class_weight.py,246,"Not ""balanced"" for subsample",
scikit-learn/sklearn/utils/tests/test_class_weight.py,250,Not a list or preset for multi-output,
scikit-learn/sklearn/utils/tests/test_class_weight.py,254,Incorrect length list for multi-output,
scikit-learn/sklearn/utils/tests/test_class_weight.py,260,Non-regression smoke test for #12146,
scikit-learn/sklearn/utils/tests/test_class_weight.py,261,more than 32 distinct classes,
scikit-learn/sklearn/utils/tests/test_class_weight.py,262,use subsampling,
scikit-learn/sklearn/utils/tests/test_shortest_path.py,12,set nonzero entries to infinity,
scikit-learn/sklearn/utils/tests/test_shortest_path.py,15,set diagonal to zero,
scikit-learn/sklearn/utils/tests/test_shortest_path.py,32,sparse grid of distances,
scikit-learn/sklearn/utils/tests/test_shortest_path.py,36,make symmetric: distances are not direction-dependent,
scikit-learn/sklearn/utils/tests/test_shortest_path.py,39,make graph sparse,
scikit-learn/sklearn/utils/tests/test_shortest_path.py,43,set diagonal to zero,
scikit-learn/sklearn/utils/tests/test_shortest_path.py,71,We compare path length and not costs (-> set distances to 0 or 1),
scikit-learn/sklearn/utils/tests/test_shortest_path.py,80,Non-reachable nodes have distance 0 in graph_py,
scikit-learn/sklearn/utils/tests/test_multiclass.py,35,"valid when the data is formatted as sparse or dense, identified",
scikit-learn/sklearn/utils/tests/test_multiclass.py,36,by CSR format when the testing takes place,
scikit-learn/sklearn/utils/tests/test_multiclass.py,48,Only valid when data is dense,
scikit-learn/sklearn/utils/tests/test_multiclass.py,119,sequence of sequences that weren't supported even before deprecation,
scikit-learn/sklearn/utils/tests/test_multiclass.py,125,and also confusable as sequences of sequences,
scikit-learn/sklearn/utils/tests/test_multiclass.py,128,empty second dimension,
scikit-learn/sklearn/utils/tests/test_multiclass.py,131,3d,
scikit-learn/sklearn/utils/tests/test_multiclass.py,154,Empty iterable,
scikit-learn/sklearn/utils/tests/test_multiclass.py,158,Multiclass problem,
scikit-learn/sklearn/utils/tests/test_multiclass.py,163,Multilabel indicator,
scikit-learn/sklearn/utils/tests/test_multiclass.py,173,Several arrays passed,
scikit-learn/sklearn/utils/tests/test_multiclass.py,179,Border line case with binary indicator matrix,
scikit-learn/sklearn/utils/tests/test_multiclass.py,190,Test unique_labels with a variety of collected examples,
scikit-learn/sklearn/utils/tests/test_multiclass.py,192,Smoke test for all supported format,
scikit-learn/sklearn/utils/tests/test_multiclass.py,197,We don't support those format at the moment,
scikit-learn/sklearn/utils/tests/test_multiclass.py,210,Mix with binary or multiclass and multilabel,
scikit-learn/sklearn/utils/tests/test_multiclass.py,242,Only mark explicitly defined sparse examples as valid sparse,
scikit-learn/sklearn/utils/tests/test_multiclass.py,243,multilabel-indicators,
scikit-learn/sklearn/utils/tests/test_multiclass.py,265,Densify sparse examples before testing,
scikit-learn/sklearn/utils/tests/test_multiclass.py,286,@ignore_warnings,
scikit-learn/sklearn/utils/tests/test_multiclass.py,328,Define the sparse matrix with a mix of implicit and explicit zeros,
scikit-learn/sklearn/utils/tests/test_multiclass.py,355,Test again with explicit sample weights,
scikit-learn/sklearn/utils/tests/test_multiclass.py,400,test properties for ovr decision function,
scikit-learn/sklearn/utils/tests/test_multiclass.py,416,check that the decision values are within 0.5 range of the votes,
scikit-learn/sklearn/utils/tests/test_multiclass.py,424,check that the prediction are what we expect,
scikit-learn/sklearn/utils/tests/test_multiclass.py,425,highest vote or highest confidence if there is a tie.,
scikit-learn/sklearn/utils/tests/test_multiclass.py,426,for the second sample we have a tie (should be won by 1),
scikit-learn/sklearn/utils/tests/test_multiclass.py,430,third and fourth sample have the same vote but third sample,
scikit-learn/sklearn/utils/tests/test_multiclass.py,431,"has higher confidence, this should reflect on the decision values",
scikit-learn/sklearn/utils/tests/test_multiclass.py,434,assert subset invariance.,
scikit-learn/sklearn/utils/tests/test_optimize.py,10,Test that newton_cg gives same result as scipy's fmin_ncg,
scikit-learn/sklearn/utils/tests/test_deprecation.py,1,Authors: Raghav RV <rvraghav93@gmail.com>,
scikit-learn/sklearn/utils/tests/test_deprecation.py,2,License: BSD 3 clause,
scikit-learn/sklearn/utils/tests/test_deprecation.py,49,Test if _is_deprecated helper identifies wrapping via deprecated,
scikit-learn/sklearn/utils/tests/test_deprecation.py,50,NOTE it works only for class methods and functions,
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,19,This file tests the utils that are deprecated,
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,22,TODO: remove in 0.24,
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,29,TODO: remove in 0.24,
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,36,TODO: remove in 0.24,
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,43,TODO: remove in 0.24,
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,50,TODO: remove in 0.24,
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,57,TODO: remove in 0.24,
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,64,TODO: remove in 0.24,
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,85,TODO: remove in 0.24,
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,92,TODO: remove in 0.24,
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,99,TODO: remove in 0.24,
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,101,Non-regression test for:,
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,102,https://github.com/scikit-learn/scikit-learn/issues/15842,
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,105,noqa,
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,107,Calling all_estimators() also triggers a recursive import of all,
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,108,"submodules, including deprecated ones.",
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,115,TODO: remove in 0.24,
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,117,Non-regression test for:,
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,118,https://github.com/scikit-learn/scikit-learn/issues/15842,
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,121,noqa,
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,123,Calling all_estimators() also triggers a recursive import of all,
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,124,"submodules, including deprecated ones.",
scikit-learn/sklearn/utils/tests/test_seq_dataset.py,1,Author: Tom Dupre la Tour,
scikit-learn/sklearn/utils/tests/test_seq_dataset.py,2,Joan Massich <mailsik@gmail.com>,
scikit-learn/sklearn/utils/tests/test_seq_dataset.py,3,,
scikit-learn/sklearn/utils/tests/test_seq_dataset.py,4,License: BSD 3 clause,
scikit-learn/sklearn/utils/tests/test_seq_dataset.py,67,next sample,
scikit-learn/sklearn/utils/tests/test_seq_dataset.py,75,random sample,
scikit-learn/sklearn/utils/tests/test_seq_dataset.py,90,not shuffled,
scikit-learn/sklearn/utils/tests/test_seq_dataset.py,129,next sample,
scikit-learn/sklearn/utils/tests/test_extmath.py,1,Authors: Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/sklearn/utils/tests/test_extmath.py,2,Mathieu Blondel <mathieu@mblondel.org>,
scikit-learn/sklearn/utils/tests/test_extmath.py,3,Denis Engemann <denis-alexander.engemann@inria.fr>,
scikit-learn/sklearn/utils/tests/test_extmath.py,4,,
scikit-learn/sklearn/utils/tests/test_extmath.py,5,License: BSD 3 clause,
scikit-learn/sklearn/utils/tests/test_extmath.py,55,"with uniform weights, results should be identical to stats.mode",
scikit-learn/sklearn/utils/tests/test_extmath.py,69,"set this up so that each row should have a weighted mode of 6,",
scikit-learn/sklearn/utils/tests/test_extmath.py,70,with a score that is easily reproduced,
scikit-learn/sklearn/utils/tests/test_extmath.py,87,Check that extmath.randomized_svd is consistent with linalg.svd,
scikit-learn/sklearn/utils/tests/test_extmath.py,95,generate a matrix X of approximate effective rank `rank` and no noise,
scikit-learn/sklearn/utils/tests/test_extmath.py,96,component (very structured signal):,
scikit-learn/sklearn/utils/tests/test_extmath.py,102,compute the singular values of X using the slow exact method,
scikit-learn/sklearn/utils/tests/test_extmath.py,105,Convert the singular values to the specific dtype,
scikit-learn/sklearn/utils/tests/test_extmath.py,110,'none' would not be stable,
scikit-learn/sklearn/utils/tests/test_extmath.py,111,compute the singular values of X using the fast approximate method,
scikit-learn/sklearn/utils/tests/test_extmath.py,115,"If the input dtype is float, then the output dtype is float of the",
scikit-learn/sklearn/utils/tests/test_extmath.py,116,same bit size (f32 is not upcast to f64),
scikit-learn/sklearn/utils/tests/test_extmath.py,117,"But if the input dtype is int, the output dtype is float64",
scikit-learn/sklearn/utils/tests/test_extmath.py,131,ensure that the singular values of both methods are equal up to the,
scikit-learn/sklearn/utils/tests/test_extmath.py,132,real rank of the matrix,
scikit-learn/sklearn/utils/tests/test_extmath.py,135,check the singular vectors too (while not checking the sign),
scikit-learn/sklearn/utils/tests/test_extmath.py,139,check the sparse matrix representation,
scikit-learn/sklearn/utils/tests/test_extmath.py,142,compute the singular values of X using the fast approximate method,
scikit-learn/sklearn/utils/tests/test_extmath.py,182,"csr_matrix will use int32 indices by default,",
scikit-learn/sklearn/utils/tests/test_extmath.py,183,up-casting those to int64 when necessary,
scikit-learn/sklearn/utils/tests/test_extmath.py,196,Check that extmath.randomized_svd can handle noisy matrices,
scikit-learn/sklearn/utils/tests/test_extmath.py,202,generate a matrix X wity structure approximate rank `rank` and an,
scikit-learn/sklearn/utils/tests/test_extmath.py,203,important noisy component,
scikit-learn/sklearn/utils/tests/test_extmath.py,209,compute the singular values of X using the slow exact method,
scikit-learn/sklearn/utils/tests/test_extmath.py,213,compute the singular values of X using the fast approximate,
scikit-learn/sklearn/utils/tests/test_extmath.py,214,method without the iterated power method,
scikit-learn/sklearn/utils/tests/test_extmath.py,219,the approximation does not tolerate the noise:,
scikit-learn/sklearn/utils/tests/test_extmath.py,222,compute the singular values of X using the fast approximate,
scikit-learn/sklearn/utils/tests/test_extmath.py,223,method with iterated power method,
scikit-learn/sklearn/utils/tests/test_extmath.py,228,the iterated power method is helping getting rid of the noise:,
scikit-learn/sklearn/utils/tests/test_extmath.py,233,Check that extmath.randomized_svd can handle noisy matrices,
scikit-learn/sklearn/utils/tests/test_extmath.py,239,let us try again without 'low_rank component': just regularly but slowly,
scikit-learn/sklearn/utils/tests/test_extmath.py,240,decreasing singular values: the rank of the data matrix is infinite,
scikit-learn/sklearn/utils/tests/test_extmath.py,246,compute the singular values of X using the slow exact method,
scikit-learn/sklearn/utils/tests/test_extmath.py,249,compute the singular values of X using the fast approximate method,
scikit-learn/sklearn/utils/tests/test_extmath.py,250,without the iterated power method,
scikit-learn/sklearn/utils/tests/test_extmath.py,254,the approximation does not tolerate the noise:,
scikit-learn/sklearn/utils/tests/test_extmath.py,257,compute the singular values of X using the fast approximate method,
scikit-learn/sklearn/utils/tests/test_extmath.py,258,with iterated power method,
scikit-learn/sklearn/utils/tests/test_extmath.py,262,the iterated power method is still managing to get most of the,
scikit-learn/sklearn/utils/tests/test_extmath.py,263,structure at the requested rank,
scikit-learn/sklearn/utils/tests/test_extmath.py,268,Check that transposing the design matrix has limited impact,
scikit-learn/sklearn/utils/tests/test_extmath.py,296,in this case 'auto' is equivalent to transpose,
scikit-learn/sklearn/utils/tests/test_extmath.py,301,randomized_svd with power_iteration_normalized='none' diverges for,
scikit-learn/sklearn/utils/tests/test_extmath.py,302,large number of power iterations on this dataset,
scikit-learn/sklearn/utils/tests/test_extmath.py,308,Check that it diverges with many (non-normalized) power iterations,
scikit-learn/sklearn/utils/tests/test_extmath.py,336,randomized_svd throws a warning for lil and dok matrix,
scikit-learn/sklearn/utils/tests/test_extmath.py,351,"Check that svd_flip works in both situations, and reconstructs input.",
scikit-learn/sklearn/utils/tests/test_extmath.py,357,Check matrix reconstruction,
scikit-learn/sklearn/utils/tests/test_extmath.py,362,Check transposed matrix reconstruction,
scikit-learn/sklearn/utils/tests/test_extmath.py,368,Check that different flip methods are equivalent under reconstruction,
scikit-learn/sklearn/utils/tests/test_extmath.py,388,Check if the randomized_svd sign flipping is always done based on u,
scikit-learn/sklearn/utils/tests/test_extmath.py,389,irrespective of transpose.,
scikit-learn/sklearn/utils/tests/test_extmath.py,390,See https://github.com/scikit-learn/scikit-learn/issues/5608,
scikit-learn/sklearn/utils/tests/test_extmath.py,391,for more details.,
scikit-learn/sklearn/utils/tests/test_extmath.py,403,Without transpose,
scikit-learn/sklearn/utils/tests/test_extmath.py,409,With transpose,
scikit-learn/sklearn/utils/tests/test_extmath.py,419,Check if cartesian product delivers the right results,
scikit-learn/sklearn/utils/tests/test_extmath.py,439,check single axis,
scikit-learn/sklearn/utils/tests/test_extmath.py,445,Check correctness and robustness of logistic sigmoid implementation,
scikit-learn/sklearn/utils/tests/test_extmath.py,457,Test Youngs and Cramer incremental variance formulas.,
scikit-learn/sklearn/utils/tests/test_extmath.py,458,Doggie data from https://www.mathsisfun.com/data/standard-deviation.html,
scikit-learn/sklearn/utils/tests/test_extmath.py,504,Test Youngs and Cramer incremental variance formulas.,
scikit-learn/sklearn/utils/tests/test_extmath.py,509,Naive one pass variance computation - not numerically stable,
scikit-learn/sklearn/utils/tests/test_extmath.py,510,https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance,
scikit-learn/sklearn/utils/tests/test_extmath.py,517,"Two-pass algorithm, stable.",
scikit-learn/sklearn/utils/tests/test_extmath.py,518,We use it as a benchmark. It is not an online algorithm,
scikit-learn/sklearn/utils/tests/test_extmath.py,519,https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Two-pass_algorithm,
scikit-learn/sklearn/utils/tests/test_extmath.py,525,Naive online implementation,
scikit-learn/sklearn/utils/tests/test_extmath.py,526,https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online_algorithm,
scikit-learn/sklearn/utils/tests/test_extmath.py,527,This works only for chunks for size 1,
scikit-learn/sklearn/utils/tests/test_extmath.py,537,We want to show a case when one_pass_var has error > 1e-3 while,
scikit-learn/sklearn/utils/tests/test_extmath.py,538,_batch_mean_variance_update has less.,
scikit-learn/sklearn/utils/tests/test_extmath.py,548,Naive one pass var: >tol (=1063),
scikit-learn/sklearn/utils/tests/test_extmath.py,551,Starting point for online algorithms: after A0,
scikit-learn/sklearn/utils/tests/test_extmath.py,553,Naive implementation: >tol (436),
scikit-learn/sklearn/utils/tests/test_extmath.py,559,the mean is also slightly unstable,
scikit-learn/sklearn/utils/tests/test_extmath.py,563,Robust implementation: <tol (177),
scikit-learn/sklearn/utils/tests/test_extmath.py,576,Test that degrees of freedom parameter for calculations are correct.,
scikit-learn/sklearn/utils/tests/test_extmath.py,590,Assign this twice so that the test logic is consistent,
scikit-learn/sklearn/utils/tests/test_extmath.py,611,Testing that sign flip is working & largest value has positive sign,
scikit-learn/sklearn/utils/tests/test_extmath.py,634,test axis parameter,
scikit-learn/sklearn/utils/tests/test_extmath.py,669,dense ND / sparse,
scikit-learn/sklearn/utils/tests/test_extmath.py,677,sparse / dense ND,
scikit-learn/sklearn/utils/tests/test_extmath.py,693,2D @ 1D,
scikit-learn/sklearn/utils/tests/test_extmath.py,700,1D @ 2D,
scikit-learn/sklearn/inspection/_partial_dependence.py,3,Authors: Peter Prettenhofer,
scikit-learn/sklearn/inspection/_partial_dependence.py,4,Trevor Stephens,
scikit-learn/sklearn/inspection/_partial_dependence.py,5,Nicolas Hug,
scikit-learn/sklearn/inspection/_partial_dependence.py,6,License: BSD 3 clause,
scikit-learn/sklearn/inspection/_partial_dependence.py,18,noqa,
scikit-learn/sklearn/inspection/_partial_dependence.py,84,feature has low resolution use unique vals,
scikit-learn/sklearn/inspection/_partial_dependence.py,87,create axis based on percentiles and grid resolution,
scikit-learn/sklearn/inspection/_partial_dependence.py,108,"reshape to (1, n_points) for consistency with",
scikit-learn/sklearn/inspection/_partial_dependence.py,109,_partial_dependence_brute,
scikit-learn/sklearn/inspection/_partial_dependence.py,118,"define the prediction_method (predict, predict_proba, decision_function).",
scikit-learn/sklearn/inspection/_partial_dependence.py,125,"try predict_proba, then decision_function if it doesn't exist",
scikit-learn/sklearn/inspection/_partial_dependence.py,156,Note: predictions is of shape,
scikit-learn/sklearn/inspection/_partial_dependence.py,157,"(n_points,) for non-multioutput regressors",
scikit-learn/sklearn/inspection/_partial_dependence.py,158,"(n_points, n_tasks) for multioutput regressors",
scikit-learn/sklearn/inspection/_partial_dependence.py,159,"(n_points, 1) for the regressors in cross_decomposition (I think)",
scikit-learn/sklearn/inspection/_partial_dependence.py,160,"(n_points, 2) for binary classification",
scikit-learn/sklearn/inspection/_partial_dependence.py,161,"(n_points, n_classes) for multiclass classification",
scikit-learn/sklearn/inspection/_partial_dependence.py,163,average over samples,
scikit-learn/sklearn/inspection/_partial_dependence.py,166,"reshape to (n_targets, n_points) where n_targets is:",
scikit-learn/sklearn/inspection/_partial_dependence.py,167,- 1 for non-multioutput regression and binary classification (shape is,
scikit-learn/sklearn/inspection/_partial_dependence.py,168,already correct in those cases),
scikit-learn/sklearn/inspection/_partial_dependence.py,169,- n_tasks for multi-output regression,
scikit-learn/sklearn/inspection/_partial_dependence.py,170,- n_classes for multiclass classification.,
scikit-learn/sklearn/inspection/_partial_dependence.py,173,"non-multioutput regression, shape is (n_points,)",
scikit-learn/sklearn/inspection/_partial_dependence.py,176,"Binary classification, shape is (2, n_points).",
scikit-learn/sklearn/inspection/_partial_dependence.py,177,we output the effect of **positive** class,
scikit-learn/sklearn/inspection/_partial_dependence.py,311,TODO: to be removed if/when pipeline get a `steps_` attributes,
scikit-learn/sklearn/inspection/_partial_dependence.py,312,assuming Pipeline is the only estimator that does not store a new,
scikit-learn/sklearn/inspection/_partial_dependence.py,313,attribute,
scikit-learn/sklearn/inspection/_partial_dependence.py,315,FIXME: remove the None option when it will be deprecated,
scikit-learn/sklearn/inspection/_partial_dependence.py,327,Use check_array only on lists and other non-array-likes / sparse. Do not,
scikit-learn/sklearn/inspection/_partial_dependence.py,328,convert DataFrame into a NumPy array.,
scikit-learn/sklearn/inspection/_partial_dependence.py,388,"_get_column_indices() supports negative indexing. Here, we limit",
scikit-learn/sklearn/inspection/_partial_dependence.py,389,the indexing to be positive. The upper bound will be checked,
scikit-learn/sklearn/inspection/_partial_dependence.py,390,by _get_column_indices(),
scikit-learn/sklearn/inspection/_partial_dependence.py,414,reshape averaged_predictions to,
scikit-learn/sklearn/inspection/_partial_dependence.py,415,"(n_outputs, n_values_feature_0, n_values_feature_1, ...)",
scikit-learn/sklearn/inspection/_permutation_importance.py,17,Work on a copy of X to to ensure thread-safety in case of threading based,
scikit-learn/sklearn/inspection/_permutation_importance.py,18,"parallelism. Furthermore, making a copy is also useful when the joblib",
scikit-learn/sklearn/inspection/_permutation_importance.py,19,"backend is 'loky' (default) or the old 'multiprocessing': in those cases,",
scikit-learn/sklearn/inspection/_permutation_importance.py,20,if X is large it will be automatically be backed by a readonly memory map,
scikit-learn/sklearn/inspection/_permutation_importance.py,21,(memmap). X.copy() on the other hand is always guaranteed to return a,
scikit-learn/sklearn/inspection/_permutation_importance.py,22,writable data-structure whose columns can be shuffled inplace.,
scikit-learn/sklearn/inspection/_permutation_importance.py,122,Precompute random seed from the random state to be used,
scikit-learn/sklearn/inspection/_permutation_importance.py,123,to get a fresh independent RandomState instance for each,
scikit-learn/sklearn/inspection/_permutation_importance.py,124,"parallel call to _calculate_permutation_scores, irrespective of",
scikit-learn/sklearn/inspection/_permutation_importance.py,125,the fact that variables are shared or not depending on the active,
scikit-learn/sklearn/inspection/_permutation_importance.py,126,"joblib backend (sequential, thread-based or process-based).",
scikit-learn/sklearn/inspection/__init__.py,3,TODO: remove me in 0.24 (as well as the noqa markers) and,
scikit-learn/sklearn/inspection/__init__.py,4,import the partial_dependence func directly from the,
scikit-learn/sklearn/inspection/__init__.py,5,._partial_dependence module instead.,
scikit-learn/sklearn/inspection/__init__.py,6,Pre-cache the import of the deprecated module so that import,
scikit-learn/sklearn/inspection/__init__.py,7,sklearn.inspection.partial_dependence returns the function as in,
scikit-learn/sklearn/inspection/__init__.py,8,"0.21, instead of the module",
scikit-learn/sklearn/inspection/__init__.py,9,https://github.com/scikit-learn/scikit-learn/issues/15842,
scikit-learn/sklearn/inspection/__init__.py,15,noqa,
scikit-learn/sklearn/inspection/__init__.py,17,noqa,
scikit-learn/sklearn/inspection/__init__.py,18,noqa,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,18,noqa,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,45,toy sample,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,50,"(X, y), n_targets  <-- as expected in the output of partial_dep()",
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,61,iris,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,83,Check that partial_dependence has consistent output shape for different,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,84,kinds of estimators:,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,85,- classifiers with binary and multiclass settings,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,86,- regressors,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,87,- multi-task regressors,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,91,n_target corresponds to the number of classes (1 for binary classif) or,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,92,the number of tasks / outputs in multi task settings. It's equal to 1 for,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,93,classical regression_data.,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,111,"tests for _grid_from_X: sanity check for output, and for shapes.",
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,113,Make sure that the grid is a cartesian product of the input (it will use,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,114,the unique values instead of the percentiles),
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,126,test shapes of returned objects depending on the number of unique values,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,127,for a feature.,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,131,n_unique_values > grid_resolution,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,137,"n_unique_values < grid_resolution, will use actual values",
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,140,just to make sure the order is irrelevant,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,143,axes is a list of arrays of different shapes,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,175,Check that what is returned by _partial_dependence_brute or,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,176,_partial_dependence_recursion is equivalent to manually setting a target,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,177,"feature to a given value, and computing the average prediction over all",
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,178,samples.,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,179,This also checks that the brute and recursion methods give the same,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,180,output.,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,181,"Note that even on the trainset, the brute and the recursion methods",
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,182,"aren't always strictly equivalent, in particular when the slow method",
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,183,generates unrealistic samples that have low mass in the joint,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,184,"distribution of the input features, and when some of the features are",
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,185,dependent. Hence the high tolerance on the checks.,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,188,The 'init' estimator for GBDT (here the average prediction) isn't taken,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,189,"into account with the recursion method, for technical reasons. We set",
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,190,the mean to 0 to that this 'bug' doesn't have any effect.,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,194,target feature will be set to .5 and then to 123,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,211,"(shape is (1, 2) so make it (2,))",
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,213,allow for greater margin for error with recursion method,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,220,Make sure that the recursion method gives the same results on a,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,221,DecisionTreeRegressor and a GradientBoostingRegressor or a,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,222,RandomForestRegressor with 1 tree and equivalent parameters.,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,226,Purely random dataset to avoid correlated features,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,232,The 'init' estimator for GBDT (here the average prediction) isn't taken,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,233,"into account with the recursion method, for technical reasons. We set",
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,234,the mean to 0 to that this 'bug' doesn't have any effect.,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,237,set max_depth not too high to avoid splits with same gain but different,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,238,features,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,245,The forest will use ensemble.base._set_random_states to set the,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,246,random_state of the tree sub-estimator. We simulate this here to have,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,247,equivalent estimators.,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,260,"sanity check: if the trees aren't the same, the PD values won't be equal",
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,265,"For some reason the trees aren't exactly equal on 32bits, so the PDs",
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,266,cannot be equal either. See,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,267,https://github.com/scikit-learn/scikit-learn/issues/8853,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,289,Make sure the recursion method (implicitly uses decision_function) has,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,290,the same result as using brute method with,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,291,response_method=decision_function,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,295,make sure the init estimator predicts 0 anyway,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,318,If the target y only depends on one feature in an obvious way (linear or,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,319,quadratic) then the partial dependence for that feature should reflect,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,320,it.,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,321,We here fit a linear regression_data model (with polynomial features if,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,322,needed) and compute r_squared to check that the partial dependence,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,323,correctly reflects the target.,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,338,add polynomial features if needed,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,355,Make sure error is raised for multiclass-multioutput classifiers,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,357,make multiclass-multioutput dataset,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,373,simulate that we have some classes,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,471,check that array-like objects are accepted,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,477,TODO: Remove in 0.24 when DummyClassifier's `strategy` default updates,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,480,make sure that passing a non-constant init parameter to a GBDT and using,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,481,recursion method yields a warning.,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,498,Test near perfect correlation between partial dependence and diagonal,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,499,when sample weights emphasize y = x predictions,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,500,non-regression test for #13193,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,501,TODO: extend to HistGradientBoosting once sample_weight is supported,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,507,set y = x on mask and y = -x outside,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,511,sample weights to emphasize data points where y = x,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,524,TODO: remove/fix when PDP supports HGBT with sample weights,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,533,TODO: Remove in 0.24 when DummyClassifier's `strategy` default updates,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,536,check that the partial dependence support pipeline,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,584,check that the partial dependence support dataframe and pipeline,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,585,including a column transformer,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,595,the column transformer will reorder the column when transforming,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,596,we mixed the index to be sure that we are computing the partial,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,597,dependence of the right columns,
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,631,check all possible features type supported in PDP,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,31,Make sure that feature highly correlated to the target have a higher,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,32,importance,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,50,the correlated feature with y was added as the last column and should,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,51,have the highest importance,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,60,Make sure that feature highly correlated to the target have a higher,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,61,importance,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,70,Adds feature correlated with y as the last column,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,82,the correlated feature with y was added as the last column and should,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,83,have the highest importance,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,89,Permutation variable importance should not be affected by the high,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,90,"cardinality bias of traditional feature importances, especially when",
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,91,computed on a held-out test set:,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,100,Generate a multiclass classification dataset and a set of informative,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,101,binary features that can be used to predict some classes of y exactly,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,102,while leaving some classes unexplained to make the problem harder.,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,109,Not all target classes are explained by the binary class indicator,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,110,features:,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,113,Add 10 other noisy features with high cardinality (numerical) values,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,114,that can be used to overfit the training data.,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,118,Split the dataset to be able to evaluate on a held-out test set. The,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,119,Test size should be large enough for importance measurements to be,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,120,stable:,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,126,Variable importances computed by impurity decrease on the tree node,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,127,splits often use the noisy features in splits. This can give misleading,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,128,impression that high cardinality noisy variables are the most important:,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,134,Let's check that permutation-based feature importances do not have this,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,135,problem.,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,141,Split the importances between informative and noisy features,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,145,"Because we do not have a binary variable explaining each target classes,",
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,146,the RF model will have to use the random variable to make some,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,147,(overfitting) splits (as max_depth is not set). Therefore the noisy,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,148,variables will be non-zero but with small values oscillating around,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,149,zero:,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,153,The binary features correlated with y should have a higher importance,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,154,than the high cardinality noisy features.,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,155,"The maximum test accuracy is 2 / 5 == 0.4, each informative feature",
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,156,contributing approximately a bit more than 0.2 of accuracy.,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,164,Last column is correlated with y,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,175,the correlated feature with y is the last column and should,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,176,have the highest importance,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,179,use another random state,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,187,the correlated feature with y is the last column and should,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,188,have the highest importance,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,197,Last column is correlated with y,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,214,the correlated feature with y is the last column and should,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,215,have the highest importance,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,227,this relationship can be computed in closed form,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,237,regression test to make sure that sequential and parallel calls will,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,238,output the same results.,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,246,First check that the problem is structured enough and that the model is,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,247,"complex enough to not yield trivial, constant importances:",
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,252,The actually check that parallelism does not impact the results,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,253,either with shared memory (threading) or without isolated memory,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,254,via process-based parallelism using the default backend,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,255,('loky' or 'multiprocessing') depending on the joblib version:,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,257,process-based parallelism (by default):,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,265,thread-based parallelism:,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,278,This test checks that the column shuffling logic has the same behavior,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,279,both a dataframe and a simple numpy array.,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,282,regression test to make sure that sequential and parallel calls will,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,283,output the same results.,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,287,Add a categorical feature that is statistically linked to y:,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,291,Concatenate the extra column to the numpy array: integers will be,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,292,cast to float values,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,296,Insert extra column as a non-numpy-native dtype (while keeping backward,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,297,compat for old pandas versions):,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,306,Stich an aribtrary index to the dataframe:,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,317,First check that the problem is structured enough and that the model is,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,318,"complex enough to not yield trivial, constant importances:",
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,323,Now check that importances computed on dataframe matche the values,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,324,of those computed on the array with the same data.,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,336,"Smoke, non-regression test for:",
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,337,https://github.com/scikit-learn/scikit-learn/issues/15810,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,341,trigger joblib memmaping,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,346,Actual smoke test: should not raise any error:,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,350,Auxiliary check: DummyClassifier is feature independent:,
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,351,permutating feature should not change the predictions,
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,14,noqa,
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,199,noqa,
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,200,noqa,
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,201,noqa,
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,202,noqa,
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,203,noqa,
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,205,set target_idx for multi-class estimators,
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,215,regression and binary classification,
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,218,Use check_array only on lists and other non-array-likes / sparse. Do not,
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,219,convert DataFrame into a NumPy array.,
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,224,convert feature_names to list,
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,227,get the column names for a pandas dataframe,
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,230,define a list of numbered indices for a numpy array,
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,233,convert numpy array or pandas index to a list,
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,246,convert features into a seq of int tuples,
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,264,Early exit if the axes does not have the correct number of axes,
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,277,compute averaged predictions,
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,286,"For multioutput regression, we can only check the validity of target",
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,287,now that we have the predictions.,
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,288,"Also note: as multiclass-multioutput classifiers are not supported,",
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,289,multiclass and multioutput scenario are mutually exclusive. So there is,
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,290,no risk of overwriting target_idx here.,
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,291,checking the first result is enough,
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,301,get global min and max average predictions of PD grouped by plot type,
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,451,noqa,
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,452,noqa,
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,453,noqa,
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,454,noqa,
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,455,noqa,
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,471,"If ax was set off, it has most likely been set to off",
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,472,by a previous call to plot.,
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,496,array-like,
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,513,create contour levels for two-way plots,
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,528,contour plot,
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,547,Set xlabel if it is not already set,
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,558,contour plot,
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,564,hline erases xlim,
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,18,TODO: Remove when https://github.com/numpy/numpy/issues/14397 is resolved,
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,38,Test partial dependence plot function.,
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,86,two feature position,
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,121,check with str features and array feature names and single column,
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,139,line,
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,153,contour,
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,188,contour,
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,247,with axes object,
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,253,The first call to plot_partial_dependence will create two new axes to,
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,254,"place in the space of the passed in axes, which results in a total of",
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,255,three axes in the figure.,
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,256,Currently the API does not allow for the second call to,
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,257,"plot_partial_dependence to use the same axes again, because it will",
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,258,create two new axes in the space resulting in five axes. To get the,
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,259,expected behavior one needs to pass the generated axes into the second,
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,260,call:,
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,261,disp1 = plot_partial_dependence(...),
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,262,"disp2 = plot_partial_dependence(..., ax=disp1.axes_)",
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,282,second call to plot does not change the feature names from the first,
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,283,call,
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,303,Test partial dependence plot function on multi-class input.,
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,315,now with symbol labels,
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,336,check that the pd plots are different for another target,
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,351,Test partial dependence plot function on multi-output input.,
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,441,Make sure fig object is correctly used if not None,
scikit-learn/sklearn/cluster/_birch.py,1,Authors: Manoj Kumar <manojkumarsivaraj334@gmail.com>,
scikit-learn/sklearn/cluster/_birch.py,2,Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>,
scikit-learn/sklearn/cluster/_birch.py,3,Joel Nothman <joel.nothman@gmail.com>,
scikit-learn/sklearn/cluster/_birch.py,4,License: BSD 3 clause,
scikit-learn/sklearn/cluster/_birch.py,145,"The list of subclusters, centroids and squared norms",
scikit-learn/sklearn/cluster/_birch.py,146,to manipulate throughout.,
scikit-learn/sklearn/cluster/_birch.py,160,Keep centroids and squared norm as views. In this way,
scikit-learn/sklearn/cluster/_birch.py,161,"if we change init_centroids and init_sq_norm_, it is",
scikit-learn/sklearn/cluster/_birch.py,162,"sufficient,",
scikit-learn/sklearn/cluster/_birch.py,185,We need to find the closest subcluster among all the,
scikit-learn/sklearn/cluster/_birch.py,186,subclusters so that we can insert our new subcluster.,
scikit-learn/sklearn/cluster/_birch.py,193,"If the subcluster has a child, we need a recursive strategy.",
scikit-learn/sklearn/cluster/_birch.py,199,"If it is determined that the child need not be split, we",
scikit-learn/sklearn/cluster/_birch.py,200,can just update the closest_subcluster,
scikit-learn/sklearn/cluster/_birch.py,208,things not too good. we need to redistribute the subclusters in,
scikit-learn/sklearn/cluster/_birch.py,209,"our child node, and add a new subcluster in the parent",
scikit-learn/sklearn/cluster/_birch.py,210,subcluster to accommodate the new child.,
scikit-learn/sklearn/cluster/_birch.py,221,good to go!,
scikit-learn/sklearn/cluster/_birch.py,232,"not close to any other subclusters, and we still",
scikit-learn/sklearn/cluster/_birch.py,233,"have space, so add.",
scikit-learn/sklearn/cluster/_birch.py,238,We do not have enough space nor is it closer to an,
scikit-learn/sklearn/cluster/_birch.py,239,other subcluster. We need to split.,
scikit-learn/sklearn/cluster/_birch.py,474,"If partial_fit is called for the first time or fit is called, we",
scikit-learn/sklearn/cluster/_birch.py,475,start a new tree.,
scikit-learn/sklearn/cluster/_birch.py,479,The first root is the leaf. Manipulate this object throughout.,
scikit-learn/sklearn/cluster/_birch.py,485,To enable getting back subclusters.,
scikit-learn/sklearn/cluster/_birch.py,492,Cannot vectorize. Enough to convince to use cython.,
scikit-learn/sklearn/cluster/_birch.py,556,Perform just the final global clustering step.,
scikit-learn/sklearn/cluster/_birch.py,625,Preprocessing for the global clustering.,
scikit-learn/sklearn/cluster/_birch.py,630,There is no need to perform the global clustering step.,
scikit-learn/sklearn/cluster/_birch.py,638,To use in predict to avoid recalculation.,
scikit-learn/sklearn/cluster/_birch.py,650,The global clustering step that clusters the subclusters of,
scikit-learn/sklearn/cluster/_birch.py,651,the leaves. It assumes the centroids of the subclusters as,
scikit-learn/sklearn/cluster/_birch.py,652,samples and finds the final centroids.,
scikit-learn/sklearn/cluster/_agglomerative.py,30,,
scikit-learn/sklearn/cluster/_agglomerative.py,31,For non fully-connected graphs,
scikit-learn/sklearn/cluster/_agglomerative.py,48,Make the connectivity matrix symmetric:,
scikit-learn/sklearn/cluster/_agglomerative.py,51,Convert connectivity matrix to LIL,
scikit-learn/sklearn/cluster/_agglomerative.py,58,Compute the number of nodes,
scikit-learn/sklearn/cluster/_agglomerative.py,66,XXX: Can we do without completing the matrix?,
scikit-learn/sklearn/cluster/_agglomerative.py,92,explicitly cast connectivity to ensure safety,
scikit-learn/sklearn/cluster/_agglomerative.py,96,"Ensure zero distances aren't ignored by setting them to ""epsilon""",
scikit-learn/sklearn/cluster/_agglomerative.py,100,Use scipy.sparse.csgraph to generate a minimum spanning tree,
scikit-learn/sklearn/cluster/_agglomerative.py,103,Convert the graph to scipy.cluster.hierarchy array format,
scikit-learn/sklearn/cluster/_agglomerative.py,106,Undo the epsilon values,
scikit-learn/sklearn/cluster/_agglomerative.py,111,Sort edges of the min_spanning_tree by weight,
scikit-learn/sklearn/cluster/_agglomerative.py,114,Convert edge list into standard hierarchical clustering format,
scikit-learn/sklearn/cluster/_agglomerative.py,118,Compute parents,
scikit-learn/sklearn/cluster/_agglomerative.py,134,,
scikit-learn/sklearn/cluster/_agglomerative.py,135,Hierarchical tree building functions,
scikit-learn/sklearn/cluster/_agglomerative.py,224,imports PIL,
scikit-learn/sklearn/cluster/_agglomerative.py,256,create inertia matrix,
scikit-learn/sklearn/cluster/_agglomerative.py,262,We keep only the upper triangular for the moments,
scikit-learn/sklearn/cluster/_agglomerative.py,263,Generator expressions are faster than arrays on the following,
scikit-learn/sklearn/cluster/_agglomerative.py,271,build moments as a list,
scikit-learn/sklearn/cluster/_agglomerative.py,282,prepare the main fields,
scikit-learn/sklearn/cluster/_agglomerative.py,291,recursive merge loop,
scikit-learn/sklearn/cluster/_agglomerative.py,293,identify the merge,
scikit-learn/sklearn/cluster/_agglomerative.py,301,store inertia value,
scikit-learn/sklearn/cluster/_agglomerative.py,304,update the moments,
scikit-learn/sklearn/cluster/_agglomerative.py,308,update the structure matrix A and the inertia matrix,
scikit-learn/sklearn/cluster/_agglomerative.py,314,List comprehension is faster than a for loop,
scikit-learn/sklearn/cluster/_agglomerative.py,326,List comprehension is faster than a for loop,
scikit-learn/sklearn/cluster/_agglomerative.py,330,Separate leaves in children (empty lists up to now),
scikit-learn/sklearn/cluster/_agglomerative.py,332,sort children to get consistent output with unstructured version,
scikit-learn/sklearn/cluster/_agglomerative.py,334,return numpy array for efficient caching,
scikit-learn/sklearn/cluster/_agglomerative.py,337,2 is scaling factor to compare w/ unstructured version,
scikit-learn/sklearn/cluster/_agglomerative.py,344,single average and complete linkage,
scikit-learn/sklearn/cluster/_agglomerative.py,429,Single linkage is handled differently,
scikit-learn/sklearn/cluster/_agglomerative.py,442,imports PIL,
scikit-learn/sklearn/cluster/_agglomerative.py,454,for the linkage function of hierarchy to work on precomputed,
scikit-learn/sklearn/cluster/_agglomerative.py,455,"data, provide as first argument an ndarray of the shape returned",
scikit-learn/sklearn/cluster/_agglomerative.py,456,by sklearn.metrics.pairwise_distances.,
scikit-learn/sklearn/cluster/_agglomerative.py,465,Translate to something understood by scipy,
scikit-learn/sklearn/cluster/_agglomerative.py,478,We need the fast cythonized metric from neighbors,
scikit-learn/sklearn/cluster/_agglomerative.py,481,The Cython routines used require contiguous arrays,
scikit-learn/sklearn/cluster/_agglomerative.py,485,Sort edges of the min_spanning_tree by weight,
scikit-learn/sklearn/cluster/_agglomerative.py,488,Convert edge list into standard hierarchical clustering format,
scikit-learn/sklearn/cluster/_agglomerative.py,503,Put the diagonal to zero,
scikit-learn/sklearn/cluster/_agglomerative.py,514,"FIXME We compute all the distances, while we could have only computed",
scikit-learn/sklearn/cluster/_agglomerative.py,515,"the ""interesting"" distances",
scikit-learn/sklearn/cluster/_agglomerative.py,534,create inertia heap and connection matrix,
scikit-learn/sklearn/cluster/_agglomerative.py,538,"LIL seems to the best format to access the rows quickly,",
scikit-learn/sklearn/cluster/_agglomerative.py,539,without the numpy overhead of slicing CSR indices and data.,
scikit-learn/sklearn/cluster/_agglomerative.py,541,We are storing the graph in a list of IntFloatDict,
scikit-learn/sklearn/cluster/_agglomerative.py,546,We keep only the upper triangular for the heap,
scikit-learn/sklearn/cluster/_agglomerative.py,547,Generator expressions are faster than arrays on the following,
scikit-learn/sklearn/cluster/_agglomerative.py,554,prepare the main fields,
scikit-learn/sklearn/cluster/_agglomerative.py,559,recursive merge loop,
scikit-learn/sklearn/cluster/_agglomerative.py,561,identify the merge,
scikit-learn/sklearn/cluster/_agglomerative.py,570,store distances,
scikit-learn/sklearn/cluster/_agglomerative.py,575,Keep track of the number of elements per cluster,
scikit-learn/sklearn/cluster/_agglomerative.py,581,update the structure matrix A and the inertia matrix,
scikit-learn/sklearn/cluster/_agglomerative.py,582,"a clever 'min', or 'max' operation between A[i] and A[j]",
scikit-learn/sklearn/cluster/_agglomerative.py,586,Here we use the information from coord_col (containing the,
scikit-learn/sklearn/cluster/_agglomerative.py,587,distances) to update the heap,
scikit-learn/sklearn/cluster/_agglomerative.py,590,Clear A[i] and A[j] to save memory,
scikit-learn/sklearn/cluster/_agglomerative.py,593,Separate leaves in children (empty lists up to now),
scikit-learn/sklearn/cluster/_agglomerative.py,596,# return numpy array for efficient caching,
scikit-learn/sklearn/cluster/_agglomerative.py,604,Matching names to tree-building strategies,
scikit-learn/sklearn/cluster/_agglomerative.py,627,,
scikit-learn/sklearn/cluster/_agglomerative.py,628,Functions for cutting hierarchical clustering tree,
scikit-learn/sklearn/cluster/_agglomerative.py,659,"In this function, we store nodes as a heap to avoid recomputing",
scikit-learn/sklearn/cluster/_agglomerative.py,660,the max of the nodes: the first element is always the smallest,
scikit-learn/sklearn/cluster/_agglomerative.py,661,"We use negated indices as heaps work on smallest elements, and we",
scikit-learn/sklearn/cluster/_agglomerative.py,662,are interested in largest elements,
scikit-learn/sklearn/cluster/_agglomerative.py,663,children[-1] is the root of the tree,
scikit-learn/sklearn/cluster/_agglomerative.py,666,"As we have a heap, nodes[0] is the smallest element",
scikit-learn/sklearn/cluster/_agglomerative.py,668,Insert the 2 children and remove the largest node,
scikit-learn/sklearn/cluster/_agglomerative.py,677,,
scikit-learn/sklearn/cluster/_agglomerative.py,855,Early stopping is likely to give a speed up only for,
scikit-learn/sklearn/cluster/_agglomerative.py,856,a large number of clusters. The actual threshold,
scikit-learn/sklearn/cluster/_agglomerative.py,857,implemented here is heuristic,
scikit-learn/sklearn/cluster/_agglomerative.py,863,Construct the tree,
scikit-learn/sklearn/cluster/_agglomerative.py,888,Cut the tree,
scikit-learn/sklearn/cluster/_agglomerative.py,894,copy to avoid holding a reference on the original array,
scikit-learn/sklearn/cluster/_agglomerative.py,896,Reassign cluster numbers,
scikit-learn/sklearn/cluster/_agglomerative.py,1060,"save n_features_in_ attribute here to reset it after, because it will",
scikit-learn/sklearn/cluster/_agglomerative.py,1061,be overridden in AgglomerativeClustering since we passed it X.T.,
scikit-learn/sklearn/cluster/_feature_agglomeration.py,5,"Author: V. Michel, A. Gramfort",
scikit-learn/sklearn/cluster/_feature_agglomeration.py,6,License: BSD 3 clause,
scikit-learn/sklearn/cluster/_feature_agglomeration.py,15,,
scikit-learn/sklearn/cluster/_feature_agglomeration.py,16,Mixin class for feature agglomeration.,
scikit-learn/sklearn/cluster/_feature_agglomeration.py,48,a fast way to compute the mean of grouped features,
scikit-learn/sklearn/cluster/_bicluster.py,2,Authors : Kemal Eren,
scikit-learn/sklearn/cluster/_bicluster.py,3,License: BSD 3 clause,
scikit-learn/sklearn/cluster/_bicluster.py,56,"According to paper, this can also be done more efficiently with",
scikit-learn/sklearn/cluster/_bicluster.py,57,deviation reduction and balancing algorithms.,
scikit-learn/sklearn/cluster/_bicluster.py,144,"some eigenvalues of A * A.T are negative, causing",
scikit-learn/sklearn/cluster/_bicluster.py,145,sqrt() to be np.nan. This causes some vectors in vt,
scikit-learn/sklearn/cluster/_bicluster.py,146,to be np.nan.,
scikit-learn/sklearn/cluster/_bicluster.py,149,"initialize with [-1,1] as in ARPACK",
scikit-learn/sklearn/cluster/_bicluster.py,156,"initialize with [-1,1] as in ARPACK",
scikit-learn/sklearn/cluster/_dbscan.py,1,-*- coding: utf-8 -*-,
scikit-learn/sklearn/cluster/_dbscan.py,6,Author: Robert Layton <robertlayton@gmail.com>,
scikit-learn/sklearn/cluster/_dbscan.py,7,Joel Nothman <joel.nothman@gmail.com>,
scikit-learn/sklearn/cluster/_dbscan.py,8,Lars Buitinck,
scikit-learn/sklearn/cluster/_dbscan.py,9,,
scikit-learn/sklearn/cluster/_dbscan.py,10,License: BSD 3 clause,
scikit-learn/sklearn/cluster/_dbscan.py,319,Calculate neighborhood for all samples. This leaves the original,
scikit-learn/sklearn/cluster/_dbscan.py,320,"point in, which needs to be considered later (i.e. point i is in the",
scikit-learn/sklearn/cluster/_dbscan.py,321,"neighborhood of point i. While True, its useless information)",
scikit-learn/sklearn/cluster/_dbscan.py,323,"set the diagonal to explicit values, as a point is its own",
scikit-learn/sklearn/cluster/_dbscan.py,324,neighbor,
scikit-learn/sklearn/cluster/_dbscan.py,327,XXX: modifies X's internals in-place,
scikit-learn/sklearn/cluster/_dbscan.py,334,This has worst case O(n^2) memory complexity,
scikit-learn/sklearn/cluster/_dbscan.py,345,"Initially, all samples are noise.",
scikit-learn/sklearn/cluster/_dbscan.py,348,A list of all core samples found.,
scikit-learn/sklearn/cluster/_dbscan.py,357,fix for scipy sparse indexing issue,
scikit-learn/sklearn/cluster/_dbscan.py,360,no core samples,
scikit-learn/sklearn/cluster/_affinity_propagation.py,3,Author: Alexandre Gramfort alexandre.gramfort@inria.fr,
scikit-learn/sklearn/cluster/_affinity_propagation.py,4,Gael Varoquaux gael.varoquaux@normalesup.org,
scikit-learn/sklearn/cluster/_affinity_propagation.py,6,License: BSD 3 clause,
scikit-learn/sklearn/cluster/_affinity_propagation.py,24,Create mask to ignore diagonal of S,
scikit-learn/sklearn/cluster/_affinity_propagation.py,123,"It makes no sense to run the algorithm in this case, so return 1 or",
scikit-learn/sklearn/cluster/_affinity_propagation.py,124,"n_samples clusters, depending on preferences",
scikit-learn/sklearn/cluster/_affinity_propagation.py,138,Place preference on the diagonal of S,
scikit-learn/sklearn/cluster/_affinity_propagation.py,142,Initialize messages,
scikit-learn/sklearn/cluster/_affinity_propagation.py,143,Intermediate results,
scikit-learn/sklearn/cluster/_affinity_propagation.py,146,Remove degeneracies,
scikit-learn/sklearn/cluster/_affinity_propagation.py,150,Execute parallel affinity propagation updates,
scikit-learn/sklearn/cluster/_affinity_propagation.py,156,tmp = A + S; compute responsibilities,
scikit-learn/sklearn/cluster/_affinity_propagation.py,159,"np.max(A + S, axis=1)",
scikit-learn/sklearn/cluster/_affinity_propagation.py,163,tmp = Rnew,
scikit-learn/sklearn/cluster/_affinity_propagation.py,167,Damping,
scikit-learn/sklearn/cluster/_affinity_propagation.py,172,tmp = Rp; compute availabilities,
scikit-learn/sklearn/cluster/_affinity_propagation.py,176,tmp = -Anew,
scikit-learn/sklearn/cluster/_affinity_propagation.py,182,Damping,
scikit-learn/sklearn/cluster/_affinity_propagation.py,187,Check for convergence,
scikit-learn/sklearn/cluster/_affinity_propagation.py,207,Identify exemplars,
scikit-learn/sklearn/cluster/_affinity_propagation.py,211,Identify clusters,
scikit-learn/sklearn/cluster/_affinity_propagation.py,212,Refine the final set of exemplars and clusters and return results,
scikit-learn/sklearn/cluster/_affinity_propagation.py,221,"Reduce labels to a sorted, gapless, list",
scikit-learn/sklearn/cluster/_affinity_propagation.py,236,,
scikit-learn/sklearn/cluster/_mean_shift.py,12,Authors: Conrad Lee <conradlee@gmail.com>,
scikit-learn/sklearn/cluster/_mean_shift.py,13,Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/sklearn/cluster/_mean_shift.py,14,Gael Varoquaux <gael.varoquaux@normalesup.org>,
scikit-learn/sklearn/cluster/_mean_shift.py,15,Martino Sorbaro <martino.sorbaro@ed.ac.uk>,
scikit-learn/sklearn/cluster/_mean_shift.py,72,cannot fit NearestNeighbors with n_neighbors = 0,
scikit-learn/sklearn/cluster/_mean_shift.py,86,separate function for each seed's iterative loop,
scikit-learn/sklearn/cluster/_mean_shift.py,88,"For each seed, climb gradient until convergence or max_iter",
scikit-learn/sklearn/cluster/_mean_shift.py,90,when mean has converged,
scikit-learn/sklearn/cluster/_mean_shift.py,93,Find mean of points within bandwidth,
scikit-learn/sklearn/cluster/_mean_shift.py,98,Depending on seeding strategy this condition may occur,
scikit-learn/sklearn/cluster/_mean_shift.py,99,save the old mean,
scikit-learn/sklearn/cluster/_mean_shift.py,101,"If converged or at max_iter, adds the cluster",
scikit-learn/sklearn/cluster/_mean_shift.py,220,Bin points,
scikit-learn/sklearn/cluster/_mean_shift.py,226,Select only those bins as seeds which have enough members,
scikit-learn/sklearn/cluster/_mean_shift.py,388,We use n_jobs=1 because this will be used in nested calls under,
scikit-learn/sklearn/cluster/_mean_shift.py,389,parallel calls to _mean_shift_single_seed so there is no need for,
scikit-learn/sklearn/cluster/_mean_shift.py,390,for further parallelism.,
scikit-learn/sklearn/cluster/_mean_shift.py,393,execute iterations on all seeds in parallel,
scikit-learn/sklearn/cluster/_mean_shift.py,397,copy results in a dictionary,
scikit-learn/sklearn/cluster/_mean_shift.py,399,i.e. len(points_within) > 0,
scikit-learn/sklearn/cluster/_mean_shift.py,405,nothing near seeds,
scikit-learn/sklearn/cluster/_mean_shift.py,411,POST PROCESSING: remove near duplicate points,
scikit-learn/sklearn/cluster/_mean_shift.py,412,"If the distance between two kernels is less than the bandwidth,",
scikit-learn/sklearn/cluster/_mean_shift.py,413,then we have to remove one because it is a duplicate. Remove the,
scikit-learn/sklearn/cluster/_mean_shift.py,414,one with fewer points.,
scikit-learn/sklearn/cluster/_mean_shift.py,428,leave the current point as unique,
scikit-learn/sklearn/cluster/_mean_shift.py,431,ASSIGN LABELS: a point belongs to the cluster that it is closest to,
scikit-learn/sklearn/cluster/_spectral.py,1,-*- coding: utf-8 -*-,
scikit-learn/sklearn/cluster/_spectral.py,4,Author: Gael Varoquaux gael.varoquaux@normalesup.org,
scikit-learn/sklearn/cluster/_spectral.py,5,Brian Cheung,
scikit-learn/sklearn/cluster/_spectral.py,6,Wei LI <kuantkid@gmail.com>,
scikit-learn/sklearn/cluster/_spectral.py,7,License: BSD 3 clause,
scikit-learn/sklearn/cluster/_spectral.py,84,Normalize the eigenvectors to an equal length of a vector of ones.,
scikit-learn/sklearn/cluster/_spectral.py,85,Reorient the eigenvectors to point in the negative direction with respect,
scikit-learn/sklearn/cluster/_spectral.py,86,to the first element.  This may have to do with constraining the,
scikit-learn/sklearn/cluster/_spectral.py,87,eigenvectors to lie in a specific quadrant to make the discretization,
scikit-learn/sklearn/cluster/_spectral.py,88,search easier.,
scikit-learn/sklearn/cluster/_spectral.py,96,Normalize the rows of the eigenvectors.  Samples should lie on the unit,
scikit-learn/sklearn/cluster/_spectral.py,97,hypersphere centered at the origin.  This transforms the samples in the,
scikit-learn/sklearn/cluster/_spectral.py,98,embedding space to the space of partition matrices.,
scikit-learn/sklearn/cluster/_spectral.py,104,If there is an exception we try to randomize and rerun SVD again,
scikit-learn/sklearn/cluster/_spectral.py,105,do this max_svd_restarts times.,
scikit-learn/sklearn/cluster/_spectral.py,108,Initialize first column of rotation matrix with a row of the,
scikit-learn/sklearn/cluster/_spectral.py,109,eigenvectors,
scikit-learn/sklearn/cluster/_spectral.py,113,"To initialize the rest of the rotation matrix, find the rows",
scikit-learn/sklearn/cluster/_spectral.py,114,of the eigenvectors that are as orthogonal to each other as,
scikit-learn/sklearn/cluster/_spectral.py,115,possible,
scikit-learn/sklearn/cluster/_spectral.py,118,Accumulate c to ensure row is as orthogonal as possible to,
scikit-learn/sklearn/cluster/_spectral.py,119,previous picks as well as current one,
scikit-learn/sklearn/cluster/_spectral.py,150,otherwise calculate rotation and continue,
scikit-learn/sklearn/cluster/_spectral.py,258,The first eigen vector is constant only for fully connected graphs,
scikit-learn/sklearn/cluster/_spectral.py,259,and should be kept for spectral clustering (drop_first = False),
scikit-learn/sklearn/cluster/_spectral.py,260,See spectral_embedding documentation.,
scikit-learn/sklearn/cluster/setup.py,1,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/sklearn/cluster/setup.py,2,License: BSD 3 clause,
scikit-learn/sklearn/cluster/_kmeans.py,3,Authors: Gael Varoquaux <gael.varoquaux@normalesup.org>,
scikit-learn/sklearn/cluster/_kmeans.py,4,Thomas Rueckstiess <ruecksti@in.tum.de>,
scikit-learn/sklearn/cluster/_kmeans.py,5,James Bergstra <james.bergstra@umontreal.ca>,
scikit-learn/sklearn/cluster/_kmeans.py,6,Jan Schlueter <scikit-learn@jan-schlueter.de>,
scikit-learn/sklearn/cluster/_kmeans.py,7,Nelle Varoquaux,
scikit-learn/sklearn/cluster/_kmeans.py,8,Peter Prettenhofer <peter.prettenhofer@gmail.com>,
scikit-learn/sklearn/cluster/_kmeans.py,9,Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/sklearn/cluster/_kmeans.py,10,Mathieu Blondel <mathieu@mblondel.org>,
scikit-learn/sklearn/cluster/_kmeans.py,11,Robert Layton <robertlayton@gmail.com>,
scikit-learn/sklearn/cluster/_kmeans.py,12,License: BSD 3 clause,
scikit-learn/sklearn/cluster/_kmeans.py,42,,
scikit-learn/sklearn/cluster/_kmeans.py,43,Initialization heuristic,
scikit-learn/sklearn/cluster/_kmeans.py,87,Set the number of local seeding trials if none is given,
scikit-learn/sklearn/cluster/_kmeans.py,89,"This is what Arthur/Vassilvitskii tried, but did not report",
scikit-learn/sklearn/cluster/_kmeans.py,90,specific results for other than mentioning in the conclusion,
scikit-learn/sklearn/cluster/_kmeans.py,91,that it helped.,
scikit-learn/sklearn/cluster/_kmeans.py,94,Pick first center randomly,
scikit-learn/sklearn/cluster/_kmeans.py,101,Initialize list of closest distances and calculate current potential,
scikit-learn/sklearn/cluster/_kmeans.py,107,Pick the remaining n_clusters-1 points,
scikit-learn/sklearn/cluster/_kmeans.py,109,Choose center candidates by sampling with probability proportional,
scikit-learn/sklearn/cluster/_kmeans.py,110,to the squared distance to the closest existing center,
scikit-learn/sklearn/cluster/_kmeans.py,114,XXX: numerical imprecision can result in a candidate_id out of range,
scikit-learn/sklearn/cluster/_kmeans.py,118,Compute distances to center candidates,
scikit-learn/sklearn/cluster/_kmeans.py,122,update closest distances squared and potential for each candidate,
scikit-learn/sklearn/cluster/_kmeans.py,127,Decide which candidate is the best,
scikit-learn/sklearn/cluster/_kmeans.py,133,Permanently add best center candidate found in local tries,
scikit-learn/sklearn/cluster/_kmeans.py,142,,
scikit-learn/sklearn/cluster/_kmeans.py,143,K-means batch estimation by EM (expectation maximization),
scikit-learn/sklearn/cluster/_kmeans.py,176,normalize the weights to sum up to n_samples,
scikit-learn/sklearn/cluster/_kmeans.py,177,an array of 1 (i.e. samples_weight is None) is already normalized,
scikit-learn/sklearn/cluster/_kmeans.py,402,init,
scikit-learn/sklearn/cluster/_kmeans.py,438,compute new pairwise distances between centers and closest other,
scikit-learn/sklearn/cluster/_kmeans.py,439,center of each center for next iterations,
scikit-learn/sklearn/cluster/_kmeans.py,459,rerun E-step so that predicted labels match cluster centers,
scikit-learn/sklearn/cluster/_kmeans.py,548,init,
scikit-learn/sklearn/cluster/_kmeans.py,586,rerun E-step so that predicted labels match cluster centers,
scikit-learn/sklearn/cluster/_kmeans.py,721,ensure that the centers have the same dtype as X,
scikit-learn/sklearn/cluster/_kmeans.py,722,this is a requirement of fused types of cython,
scikit-learn/sklearn/cluster/_kmeans.py,984,verify that the number of samples given is larger than k,
scikit-learn/sklearn/cluster/_kmeans.py,991,Validate init array,
scikit-learn/sklearn/cluster/_kmeans.py,1004,subtract of mean of x for more accurate distance computations,
scikit-learn/sklearn/cluster/_kmeans.py,1007,The copy was already done above,
scikit-learn/sklearn/cluster/_kmeans.py,1013,precompute squared norms of data points,
scikit-learn/sklearn/cluster/_kmeans.py,1035,seeds for the initializations of the kmeans runs.,
scikit-learn/sklearn/cluster/_kmeans.py,1039,run a k-means once,
scikit-learn/sklearn/cluster/_kmeans.py,1045,determine if these results are the best so far,
scikit-learn/sklearn/cluster/_kmeans.py,1118,"Currently, this just skips a copy of the data if it is not in",
scikit-learn/sklearn/cluster/_kmeans.py,1119,np.array or CSR format already.,
scikit-learn/sklearn/cluster/_kmeans.py,1120,"XXX This skips _check_test_data, which may change the dtype;",
scikit-learn/sklearn/cluster/_kmeans.py,1121,we should refactor the input validation.,
scikit-learn/sklearn/cluster/_kmeans.py,1274,Perform label assignment to nearest centers,
scikit-learn/sklearn/cluster/_kmeans.py,1280,Reassign clusters that have very low weight,
scikit-learn/sklearn/cluster/_kmeans.py,1282,pick at most .5 * batch_size samples as new centers,
scikit-learn/sklearn/cluster/_kmeans.py,1289,Pick new clusters amongst observations with uniform probability,
scikit-learn/sklearn/cluster/_kmeans.py,1303,"reset counts of reassigned centers, but don't reset them too small",
scikit-learn/sklearn/cluster/_kmeans.py,1304,to avoid instant reassignment. This is a pretty dirty hack as it,
scikit-learn/sklearn/cluster/_kmeans.py,1305,also modifies the learning rates.,
scikit-learn/sklearn/cluster/_kmeans.py,1308,implementation for the sparse CSR representation completely written in,
scikit-learn/sklearn/cluster/_kmeans.py,1309,cython,
scikit-learn/sklearn/cluster/_kmeans.py,1315,dense variant in mostly numpy (not as memory efficient though),
scikit-learn/sklearn/cluster/_kmeans.py,1319,find points from minibatch that are assigned to this center,
scikit-learn/sklearn/cluster/_kmeans.py,1327,inplace remove previous count scaling,
scikit-learn/sklearn/cluster/_kmeans.py,1330,inplace sum with new points members of this cluster,
scikit-learn/sklearn/cluster/_kmeans.py,1335,update the count statistics for this center,
scikit-learn/sklearn/cluster/_kmeans.py,1338,inplace rescale to compute mean of all points (old and new),
scikit-learn/sklearn/cluster/_kmeans.py,1339,Note: numpy >= 1.10 does not support '/=' for the following,
scikit-learn/sklearn/cluster/_kmeans.py,1340,expression for a mixture of int and float (see numpy issue #6464),
scikit-learn/sklearn/cluster/_kmeans.py,1343,update the squared diff if necessary,
scikit-learn/sklearn/cluster/_kmeans.py,1355,Normalize inertia to be able to compare values when,
scikit-learn/sklearn/cluster/_kmeans.py,1356,batch_size changes,
scikit-learn/sklearn/cluster/_kmeans.py,1360,Compute an Exponentially Weighted Average of the squared,
scikit-learn/sklearn/cluster/_kmeans.py,1361,diff to monitor the convergence while discarding,
scikit-learn/sklearn/cluster/_kmeans.py,1362,minibatch-local stochastic variability:,
scikit-learn/sklearn/cluster/_kmeans.py,1363,https://en.wikipedia.org/wiki/Moving_average,
scikit-learn/sklearn/cluster/_kmeans.py,1375,Log progress to be able to monitor convergence,
scikit-learn/sklearn/cluster/_kmeans.py,1384,Early stopping based on absolute tolerance on squared change of,
scikit-learn/sklearn/cluster/_kmeans.py,1385,centers position (using EWA smoothing),
scikit-learn/sklearn/cluster/_kmeans.py,1392,Early stopping heuristic due to lack of improvement on smoothed inertia,
scikit-learn/sklearn/cluster/_kmeans.py,1409,update the convergence context to maintain state across successive calls:,
scikit-learn/sklearn/cluster/_kmeans.py,1620,using tol-based early stopping needs the allocation of a,
scikit-learn/sklearn/cluster/_kmeans.py,1621,dedicated before which can be expensive for high dim data:,
scikit-learn/sklearn/cluster/_kmeans.py,1622,hence we allocate it outside of the main loop,
scikit-learn/sklearn/cluster/_kmeans.py,1626,no need for the center buffer if tol-based early stopping is,
scikit-learn/sklearn/cluster/_kmeans.py,1627,disabled,
scikit-learn/sklearn/cluster/_kmeans.py,1646,perform several inits with random sub-sets,
scikit-learn/sklearn/cluster/_kmeans.py,1654,TODO: once the `k_means` function works with sparse input we,
scikit-learn/sklearn/cluster/_kmeans.py,1655,should refactor the following init to use it instead.,
scikit-learn/sklearn/cluster/_kmeans.py,1657,Initialize the centers using only a fraction of the data as we,
scikit-learn/sklearn/cluster/_kmeans.py,1658,expect n_samples to be very large when using MiniBatchKMeans,
scikit-learn/sklearn/cluster/_kmeans.py,1665,Compute the label assignment on the init dataset,
scikit-learn/sklearn/cluster/_kmeans.py,1672,Keep only the best cluster centers across independent inits on,
scikit-learn/sklearn/cluster/_kmeans.py,1673,the common validation set,
scikit-learn/sklearn/cluster/_kmeans.py,1685,Empty context to be used inplace by the convergence check routine,
scikit-learn/sklearn/cluster/_kmeans.py,1688,Perform the iterative optimization until the final convergence,
scikit-learn/sklearn/cluster/_kmeans.py,1689,criterion,
scikit-learn/sklearn/cluster/_kmeans.py,1691,Sample a minibatch from the full dataset,
scikit-learn/sklearn/cluster/_kmeans.py,1695,Perform the actual update step on the minibatch data,
scikit-learn/sklearn/cluster/_kmeans.py,1701,Here we randomly choose whether to perform,
scikit-learn/sklearn/cluster/_kmeans.py,1702,random reassignment: the choice is done as a function,
scikit-learn/sklearn/cluster/_kmeans.py,1703,"of the iteration index, and the minimum number of",
scikit-learn/sklearn/cluster/_kmeans.py,1704,"counts, in order to force this reassignment to happen",
scikit-learn/sklearn/cluster/_kmeans.py,1705,every once in a while,
scikit-learn/sklearn/cluster/_kmeans.py,1712,Monitor convergence and do early stopping if necessary,
scikit-learn/sklearn/cluster/_kmeans.py,1796,this is the first call partial_fit on this object:,
scikit-learn/sklearn/cluster/_kmeans.py,1797,initialize the cluster centers,
scikit-learn/sklearn/cluster/_kmeans.py,1808,"The lower the minimum count is, the more we do random",
scikit-learn/sklearn/cluster/_kmeans.py,1809,"reassignment, however, we don't want to do random",
scikit-learn/sklearn/cluster/_kmeans.py,1810,"reassignment too often, to allow for building up counts",
scikit-learn/sklearn/cluster/_kmeans.py,1815,Raise error if partial_fit called on data with different number,
scikit-learn/sklearn/cluster/_kmeans.py,1816,of features.,
scikit-learn/sklearn/cluster/_optics.py,1,-*- coding: utf-8 -*-,
scikit-learn/sklearn/cluster/_optics.py,261,Extract clusters from the calculated orders and reachability,
scikit-learn/sklearn/cluster/_optics.py,304,OPTICS helper functions,
scikit-learn/sklearn/cluster/_optics.py,453,Start all points as 'unprocessed',
scikit-learn/sklearn/cluster/_optics.py,468,"Here we first do a kNN query for each point, this differs from",
scikit-learn/sklearn/cluster/_optics.py,469,the original OPTICS that only used epsilon range queries.,
scikit-learn/sklearn/cluster/_optics.py,470,TODO: handle working_memory somehow?,
scikit-learn/sklearn/cluster/_optics.py,474,"OPTICS puts an upper limit on these, use inf for undefined.",
scikit-learn/sklearn/cluster/_optics.py,477,Main OPTICS loop. Not parallelizable. The order that entries are,
scikit-learn/sklearn/cluster/_optics.py,478,written to the 'ordering_' list is important!,
scikit-learn/sklearn/cluster/_optics.py,479,"Note that this implementation is O(n^2) theoretically, but",
scikit-learn/sklearn/cluster/_optics.py,480,supposedly with very low constant factors.,
scikit-learn/sklearn/cluster/_optics.py,484,Choose next based on smallest reachability distance,
scikit-learn/sklearn/cluster/_optics.py,485,"(And prefer smaller ids on ties, possibly np.inf!)",
scikit-learn/sklearn/cluster/_optics.py,510,Assume that radius_neighbors is faster without distances,
scikit-learn/sklearn/cluster/_optics.py,511,"and we don't need all distances, nevertheless, this means",
scikit-learn/sklearn/cluster/_optics.py,512,we may be doing some work twice.,
scikit-learn/sklearn/cluster/_optics.py,516,Getting indices of neighbors that have not been processed,
scikit-learn/sklearn/cluster/_optics.py,518,Neighbors of current point are already processed.,
scikit-learn/sklearn/cluster/_optics.py,522,Only compute distances to unprocessed neighbors:,
scikit-learn/sklearn/cluster/_optics.py,528,"the same logic as neighbors, p is ignored if explicitly set",
scikit-learn/sklearn/cluster/_optics.py,529,in the dict params,
scikit-learn/sklearn/cluster/_optics.py,693,find a maximal area,
scikit-learn/sklearn/cluster/_optics.py,699,"it's not a steep point, but still goes up.",
scikit-learn/sklearn/cluster/_optics.py,701,region should include no more than min_samples consecutive,
scikit-learn/sklearn/cluster/_optics.py,702,non steep xward points.,
scikit-learn/sklearn/cluster/_optics.py,786,Our implementation adds an inf to the end of reachability plot,
scikit-learn/sklearn/cluster/_optics.py,787,this helps to find potential clusters at the end of the,
scikit-learn/sklearn/cluster/_optics.py,788,reachability plot even if there's no upward region at the end of it.,
scikit-learn/sklearn/cluster/_optics.py,792,"steep down areas, introduced in section 4.3.2 of the paper",
scikit-learn/sklearn/cluster/_optics.py,795,"maximum in between, section 4.3.2",
scikit-learn/sklearn/cluster/_optics.py,797,Our implementation corrects a mistake in the original,
scikit-learn/sklearn/cluster/_optics.py,798,"paper, i.e., in Definition 9 steep downward point,",
scikit-learn/sklearn/cluster/_optics.py,799,r(p) * (1 - x1) <= r(p + 1) should be,
scikit-learn/sklearn/cluster/_optics.py,800,r(p) * (1 - x1) >= r(p + 1),
scikit-learn/sklearn/cluster/_optics.py,808,the following loop is is almost exactly as Figure 19 of the paper.,
scikit-learn/sklearn/cluster/_optics.py,809,it jumps over the areas which are not either steep down or up areas,
scikit-learn/sklearn/cluster/_optics.py,811,just continue if steep_index has been a part of a discovered xward,
scikit-learn/sklearn/cluster/_optics.py,812,area.,
scikit-learn/sklearn/cluster/_optics.py,818,steep downward areas,
scikit-learn/sklearn/cluster/_optics.py,830,steep upward areas,
scikit-learn/sklearn/cluster/_optics.py,845,"line (**), sc2*",
scikit-learn/sklearn/cluster/_optics.py,849,Definition 11: criterion 4,
scikit-learn/sklearn/cluster/_optics.py,852,Find the first index from the left side which is almost,
scikit-learn/sklearn/cluster/_optics.py,853,at the same level as the end of the detected cluster.,
scikit-learn/sklearn/cluster/_optics.py,859,Find the first index from the right side which is almost,
scikit-learn/sklearn/cluster/_optics.py,860,at the same level as the beginning of the detected,
scikit-learn/sklearn/cluster/_optics.py,861,cluster.,
scikit-learn/sklearn/cluster/_optics.py,862,Our implementation corrects a mistake in the original,
scikit-learn/sklearn/cluster/_optics.py,863,"paper, i.e., in Definition 11 4c, r(x) < r(sD) should be",
scikit-learn/sklearn/cluster/_optics.py,864,r(x) > r(sD).,
scikit-learn/sklearn/cluster/_optics.py,869,predecessor correction,
scikit-learn/sklearn/cluster/_optics.py,879,Definition 11: criterion 3.a,
scikit-learn/sklearn/cluster/_optics.py,883,Definition 11: criterion 1,
scikit-learn/sklearn/cluster/_optics.py,887,Definition 11: criterion 2,
scikit-learn/sklearn/cluster/_optics.py,893,add smaller clusters first.,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,5,"Authors: Vincent Michel, 2010, Gael Varoquaux 2012,",
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,6,Matteo Visconti di Oleggio Castello 2014,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,7,License: BSD 3 clause,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,41,Misc tests on linkage,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,53,Smoke test FeatureAgglomeration,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,56,test hierarchical clustering on a precomputed distances matrix,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,62,test hierarchical clustering on a precomputed distances matrix,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,68,Check that we obtain the correct solution for structured linkage trees.,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,71,Avoiding a mask with only 'True' entries,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,80,Check that ward_tree raises a ValueError with a connectivity matrix,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,81,of the wrong shape,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,84,Check that fitting with no samples raises an error,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,90,Check that we obtain the correct solution for unstructured linkage trees.,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,94,With specified a number of clusters just for the sake of,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,95,raising a warning and testing the warning code,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,113,Check that the height of the results of linkage tree is sorted.,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,125,Test either if an error is raised when memory is not,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,126,either a str or a joblib.Memory instance,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,137,Check that zero vectors in X produce an error when,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,138,'cosine' affinity is used,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,146,Check that we obtain the correct number of clusters with,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,147,agglomerative clustering.,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,158,test caching,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,170,Turn caching off now,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,173,Check that we obtain the same solution with early-stopping of the,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,174,tree building,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,182,Check that we raise a TypeError on dense matrices,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,191,Test that using ward with another metric than euclidean raises an,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,192,exception,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,201,Test using another metric than euclidean works with linkage complete,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,203,Compare our (structured) implementation to scipy,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,220,Test that using a distance matrix (affinity = 'precomputed') has same,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,221,results (with connectivity constraints),
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,236,Check that we obtain the correct solution in a simplistic case,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,251,Check that fitting with no samples raises a ValueError,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,257,Check that we get the correct result in two emblematic cases,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,285,Test scikit linkage with full connectivity (i.e. unstructured) vs scipy,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,289,"Not using a lil_matrix here, just to check that non sparse",
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,290,matrices are well handled,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,303,Sort the order of child nodes per row for consistency,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,313,Test error management in _hc_cut,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,318,Make sure our custom mst_linkage_core gives,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,319,the same results as scipy's builtin,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,333,Sort the order of child nodes per row for consistency,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,346,Ensure identical points are handled correctly when using mst with,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,347,a sparse connectivity matrix,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,369,Check that connectivity in the ward tree is propagated correctly during,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,370,merging.,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,379,"If changes are not propagated correctly, fit crashes with an",
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,380,IndexError,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,385,Check that children are ordered in the same way for both structured and,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,386,unstructured versions of ward_tree.,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,388,test on five random datasets,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,405,Test return_distance option on linkage and ward trees,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,407,"test that return_distance when set true, gives same",
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,408,output on both structured and unstructured clustering.,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,422,get children,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,426,check if we got the same clusters,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,429,check if the distances are the same,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,449,test on the following dataset where we know the truth,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,450,taken from scipy/cluster/tests/hierarchy_test_data.py,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,457,truth,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,485,check that the labels are the same,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,489,check that the distances are correct,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,502,check that the labels are the same,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,506,check that the distances are correct,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,512,Check non regression of a bug if a non item assignable connectivity is,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,513,provided with more than one component.,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,514,create dummy data,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,516,create a mask with several components to force connectivity fixing,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,535,Complete smoke test,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,566,Test that the full tree is computed if n_clusters is small,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,571,"When n_clusters is less, the full tree should be built",
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,572,that is the number of merges should be n_samples - 1,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,579,"When n_clusters is large, greater than max of 100 and 0.02 * n_samples.",
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,580,we should stop when there are n_clusters.,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,593,"Test n_components returned by linkage, average and ward tree",
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,597,Connectivity matrix having five components.,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,605,Test that an error is raised when n_clusters <= 0,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,617,Test that the affinity parameter is actually passed to the pairwise,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,618,function,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,645,Check that we obtain the correct number of clusters with,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,646,agglomerative clustering with distance_threshold.,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,652,test when distance threshold is set to 10,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,662,test if the clusters produced match the point in the linkage tree,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,663,where the distance exceeds the threshold,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,670,test number of clusters produced,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,672,test clusters produced,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,684,"this should result in all data in their own clusters, given that",
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,685,their pairwise distances are bigger than .1 (which may not be the case,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,686,with a different random seed).,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,691,check that the pairwise distances are indeed all larger than .1,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,702,check the distances within the clusters and with other clusters,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,710,to avoid taking the 0 diagonal in min(),
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,718,single data point clusters only have that inf diagonal here,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,729,test boundary case of distance_threshold matching the distance,
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,757,Check that an error is raised when affinity='precomputed',
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,758,and a non square matrix is passed (PR #16257).,
scikit-learn/sklearn/cluster/tests/test_spectral.py,24,noqa,
scikit-learn/sklearn/cluster/tests/test_spectral.py,60,Test that SpectralClustering fails with an unknown mode set.,
scikit-learn/sklearn/cluster/tests/test_spectral.py,68,Distance matrix,
scikit-learn/sklearn/cluster/tests/test_spectral.py,69,Similarity matrix,
scikit-learn/sklearn/cluster/tests/test_spectral.py,77,Test that SpectralClustering fails with an unknown assign_labels set.,
scikit-learn/sklearn/cluster/tests/test_spectral.py,85,Distance matrix,
scikit-learn/sklearn/cluster/tests/test_spectral.py,86,Similarity matrix,
scikit-learn/sklearn/cluster/tests/test_spectral.py,107,Test precomputed graph filtering when containing too many neighbors,
scikit-learn/sklearn/cluster/tests/test_spectral.py,126,"Note: in the following, random_state has been selected to have",
scikit-learn/sklearn/cluster/tests/test_spectral.py,127,a dataset that yields a stable eigen decomposition both when built,
scikit-learn/sklearn/cluster/tests/test_spectral.py,128,on OSX and Linux,
scikit-learn/sklearn/cluster/tests/test_spectral.py,131,nearest neighbors affinity,
scikit-learn/sklearn/cluster/tests/test_spectral.py,145,Additive chi^2 gives a negative similarity matrix which,
scikit-learn/sklearn/cluster/tests/test_spectral.py,146,doesn't make sense for spectral clustering,
scikit-learn/sklearn/cluster/tests/test_spectral.py,159,Histogram kernel implemented as a callable.,
scikit-learn/sklearn/cluster/tests/test_spectral.py,160,no kernel_params that we didn't ask for,
scikit-learn/sklearn/cluster/tests/test_spectral.py,167,raise error on unknown affinity,
scikit-learn/sklearn/cluster/tests/test_spectral.py,175,Test the discretize using a noise assignment matrix,
scikit-learn/sklearn/cluster/tests/test_spectral.py,178,random class labels,
scikit-learn/sklearn/cluster/tests/test_spectral.py,181,noise class assignment matrix,
scikit-learn/sklearn/cluster/tests/test_spectral.py,194,TODO: Remove when pyamg does replaces sp.rand call with np.random.rand,
scikit-learn/sklearn/cluster/tests/test_spectral.py,195,https://github.com/scikit-learn/scikit-learn/issues/15913,
scikit-learn/sklearn/cluster/tests/test_spectral.py,199,Test that spectral_clustering is the same for arpack and amg solver,
scikit-learn/sklearn/cluster/tests/test_spectral.py,200,Based on toy example from plot_segmentation_toy.py,
scikit-learn/sklearn/cluster/tests/test_spectral.py,202,a small two coin image,
scikit-learn/sklearn/cluster/tests/test_spectral.py,234,"Test that after adding n_components, result is different and",
scikit-learn/sklearn/cluster/tests/test_spectral.py,235,n_components = n_clusters by default,
scikit-learn/sklearn/cluster/tests/test_spectral.py,240,set n_components = n_cluster and test if result is the same,
scikit-learn/sklearn/cluster/tests/test_spectral.py,243,test that n_components=n_clusters by default,
scikit-learn/sklearn/cluster/tests/test_spectral.py,246,test that n_components affect result,
scikit-learn/sklearn/cluster/tests/test_spectral.py,247,"n_clusters=8 by default, and set n_components=2",
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,30,Test estimate_bandwidth,
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,36,"Test estimate_bandwidth when n_samples=1 and quantile<1, so that",
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,37,n_neighbors is set to 1.,
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,46,Test MeanShift algorithm,
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,71,Test estimate_bandwidth with sparse matrix,
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,93,Test MeanShift.predict,
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,101,"init away from the data, crash with a sensible warning",
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,108,"Non-regression: before fit, there should be not fitted attributes.",
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,127,Test the bin seeding technique which can be used in the mean shift,
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,128,algorithm,
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,129,Data is just 6 points in the plane,
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,133,"With a bin coarseness of 1.0 and min_bin_freq of 1, 3 bins should be",
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,134,found,
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,140,"With a bin coarseness of 1.0 and min_bin_freq of 2, 2 bins should be",
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,141,found,
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,147,"With a bin size of 0.01 and min_bin_freq of 1, 6 bins should be found",
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,148,we bail and use the whole data here.,
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,153,"tight clusters around [0, 0] and [1, 1], only get two bins",
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,30,Affinity Propagation algorithm,
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,31,Compute similarities,
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,34,Compute Affinity Propagation,
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,56,Test also with no copy,
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,61,Test input validation,
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,74,Test AffinityPropagation.predict,
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,82,Test exception in AffinityPropagation.predict,
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,83,Not fitted.,
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,88,"Predict not supported when affinity=""precomputed"".",
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,97,"In case of non-convergence of affinity_propagation(), the cluster",
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,98,centers should be an empty array and training samples should be labelled,
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,99,as noise (-1),
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,102,Force non-convergence by allowing only a single iteration,
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,114,setting preference > similarity,
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,118,expect every sample to become an exemplar,
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,122,setting preference < similarity,
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,126,"expect one cluster, with arbitrary (first) sample as exemplar",
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,130,setting different preferences,
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,134,"expect one cluster, with highest-preference sample as exemplar",
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,140,"In case of non-convergence of affinity_propagation(), the cluster",
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,141,centers should be an empty array,
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,144,Force non-convergence by allowing only a single iteration,
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,148,"At prediction time, consider new samples as noise since there are no",
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,149,clusters,
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,164,Unequal distances,
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,172,Equal distances,
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,176,Different preferences,
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,179,Same preferences,
scikit-learn/sklearn/cluster/tests/test_bicluster.py,27,Mock object for testing get_submatrix.,
scikit-learn/sklearn/cluster/tests/test_bicluster.py,32,Overridden to reproduce old get_submatrix test.,
scikit-learn/sklearn/cluster/tests/test_bicluster.py,55,Test get_shape and get_indices on fitted model.,
scikit-learn/sklearn/cluster/tests/test_bicluster.py,64,Test Dhillon's Spectral CoClustering on a simple problem.,
scikit-learn/sklearn/cluster/tests/test_bicluster.py,73,needs to be nonnegative before making it sparse,
scikit-learn/sklearn/cluster/tests/test_bicluster.py,74,threshold some values,
scikit-learn/sklearn/cluster/tests/test_bicluster.py,92,Test Kluger methods on a checkerboard dataset.,
scikit-learn/sklearn/cluster/tests/test_bicluster.py,114,cannot take log of sparse matrix,
scikit-learn/sklearn/cluster/tests/test_bicluster.py,175,adding any constant to a log-scaled matrix should make it,
scikit-learn/sklearn/cluster/tests/test_bicluster.py,176,bistochastic,
scikit-learn/sklearn/cluster/tests/test_bicluster.py,208,XXX Previously failed on build bot (not reproducible),
scikit-learn/sklearn/cluster/tests/test_bicluster.py,270,FIXME: remove in 0.25,
scikit-learn/sklearn/cluster/tests/test_birch.py,24,Sanity check for the number of samples in leaves and roots,
scikit-learn/sklearn/cluster/tests/test_birch.py,36,Test that fit is equivalent to calling partial_fit multiple times,
scikit-learn/sklearn/cluster/tests/test_birch.py,46,Test that same global labels are obtained after calling partial_fit,
scikit-learn/sklearn/cluster/tests/test_birch.py,47,with None,
scikit-learn/sklearn/cluster/tests/test_birch.py,54,Test the predict method predicts the nearest centroid.,
scikit-learn/sklearn/cluster/tests/test_birch.py,59,n_samples * n_samples_per_cluster,
scikit-learn/sklearn/cluster/tests/test_birch.py,72,Test that n_clusters param works properly,
scikit-learn/sklearn/cluster/tests/test_birch.py,79,Test that n_clusters = Agglomerative Clustering gives,
scikit-learn/sklearn/cluster/tests/test_birch.py,80,the same results.,
scikit-learn/sklearn/cluster/tests/test_birch.py,87,Test that the wrong global clustering step raises an Error.,
scikit-learn/sklearn/cluster/tests/test_birch.py,93,Test that a small number of clusters raises a warning.,
scikit-learn/sklearn/cluster/tests/test_birch.py,99,Test that sparse and dense data give same results,
scikit-learn/sklearn/cluster/tests/test_birch.py,122,Test that nodes have at max branching_factor number of subclusters,
scikit-learn/sklearn/cluster/tests/test_birch.py,126,Purposefully set a low threshold to maximize the subclusters.,
scikit-learn/sklearn/cluster/tests/test_birch.py,136,Raises error when branching_factor is set to one.,
scikit-learn/sklearn/cluster/tests/test_birch.py,153,Test that the leaf subclusters have a threshold lesser than radius,
scikit-learn/sklearn/cluster/tests/test_birch.py,165,"Check that birch supports n_clusters with np.int64 dtype, for instance",
scikit-learn/sklearn/cluster/tests/test_birch.py,166,coming from np.arange. #16484,
scikit-learn/sklearn/cluster/tests/test_optics.py,1,Authors: Shane Grigsby <refuge@rocktalus.com>,
scikit-learn/sklearn/cluster/tests/test_optics.py,2,Adrin Jalali <adrin.jalali@gmail.com>,
scikit-learn/sklearn/cluster/tests/test_optics.py,3,License: BSD 3 clause,
scikit-learn/sklearn/cluster/tests/test_optics.py,81,small and easy test (no clusters around other clusters),
scikit-learn/sklearn/cluster/tests/test_optics.py,82,but with a clear noise data.,
scikit-learn/sklearn/cluster/tests/test_optics.py,103,check float min_samples and min_cluster_size,
scikit-learn/sklearn/cluster/tests/test_optics.py,117,this may fail if the predecessor correction is not at work!,
scikit-learn/sklearn/cluster/tests/test_optics.py,148,in 'auto' mode,
scikit-learn/sklearn/cluster/tests/test_optics.py,152,Parameters chosen specifically for this task.,
scikit-learn/sklearn/cluster/tests/test_optics.py,153,Compute OPTICS,
scikit-learn/sklearn/cluster/tests/test_optics.py,156,"number of clusters, ignoring noise if present",
scikit-learn/sklearn/cluster/tests/test_optics.py,160,check attribute types and sizes,
scikit-learn/sklearn/cluster/tests/test_optics.py,176,test that we check a minimum number of samples,
scikit-learn/sklearn/cluster/tests/test_optics.py,179,Compute OPTICS,
scikit-learn/sklearn/cluster/tests/test_optics.py,183,Run the fit,
scikit-learn/sklearn/cluster/tests/test_optics.py,188,Test an extraction of eps too close to original eps,
scikit-learn/sklearn/cluster/tests/test_optics.py,194,Compute OPTICS,
scikit-learn/sklearn/cluster/tests/test_optics.py,213,Test extract where extraction eps is close to scaled max_eps,
scikit-learn/sklearn/cluster/tests/test_optics.py,219,Compute OPTICS,
scikit-learn/sklearn/cluster/tests/test_optics.py,222,Cluster ordering starts at 0; max cluster label = 2 is 3 clusters,
scikit-learn/sklearn/cluster/tests/test_optics.py,229,Test that OPTICS clustering labels are <= 5% difference of DBSCAN,
scikit-learn/sklearn/cluster/tests/test_optics.py,235,calculate optics with dbscan extract at 0.3 epsilon,
scikit-learn/sklearn/cluster/tests/test_optics.py,239,calculate dbscan labels,
scikit-learn/sklearn/cluster/tests/test_optics.py,249,verify label mismatch is <= 5% labels,
scikit-learn/sklearn/cluster/tests/test_optics.py,279,try arbitrary minimum sizes,
scikit-learn/sklearn/cluster/tests/test_optics.py,282,reduce for speed,
scikit-learn/sklearn/cluster/tests/test_optics.py,287,check behaviour is the same when min_cluster_size is a fraction,
scikit-learn/sklearn/cluster/tests/test_optics.py,308,"Ensure that we consider all unprocessed points,",
scikit-learn/sklearn/cluster/tests/test_optics.py,309,not only direct neighbors. when picking the next point.,
scikit-learn/sklearn/cluster/tests/test_optics.py,318,"Expected values, computed with (future) ELKI 0.7.5 using:",
scikit-learn/sklearn/cluster/tests/test_optics.py,319,java -jar elki.jar cli -dbc.in csv -dbc.filter FixedDBIDsFilter,
scikit-learn/sklearn/cluster/tests/test_optics.py,320,-algorithm clustering.optics.OPTICSHeap -optics.minpts 5,
scikit-learn/sklearn/cluster/tests/test_optics.py,321,where the FixedDBIDsFilter gives 0-indexed ids.,
scikit-learn/sklearn/cluster/tests/test_optics.py,351,Tests against known extraction array,
scikit-learn/sklearn/cluster/tests/test_optics.py,352,"Does NOT work with metric='euclidean', because sklearn euclidean has",
scikit-learn/sklearn/cluster/tests/test_optics.py,353,worse numeric precision. 'minkowski' is slower but more accurate.,
scikit-learn/sklearn/cluster/tests/test_optics.py,359,ELKI currently does not print the core distances (which are not used much,
scikit-learn/sklearn/cluster/tests/test_optics.py,360,"in literature, but we can at least ensure to have this consistency:",
scikit-learn/sklearn/cluster/tests/test_optics.py,365,"Expected values, computed with (future) ELKI 0.7.5 using",
scikit-learn/sklearn/cluster/tests/test_optics.py,406,testing an easy dbscan case. Not including clusters with different,
scikit-learn/sklearn/cluster/tests/test_optics.py,407,densities.,
scikit-learn/sklearn/cluster/tests/test_feature_agglomeration.py,4,Authors: Sergul Aydore 2017,
scikit-learn/sklearn/cluster/tests/test_feature_agglomeration.py,13,"(n_samples, n_features)",
scikit-learn/sklearn/cluster/tests/test_feature_agglomeration.py,26,Test transform,
scikit-learn/sklearn/cluster/tests/test_feature_agglomeration.py,34,Test inverse transform,
scikit-learn/sklearn/cluster/tests/test_k_means.py,39,"non centered, sparse centers to check the",
scikit-learn/sklearn/cluster/tests/test_k_means.py,56,cheks that kmeans works as intended,
scikit-learn/sklearn/cluster/tests/test_k_means.py,59,"will be rescaled to [1.5, 0.5, 0.5, 1.5]",
scikit-learn/sklearn/cluster/tests/test_k_means.py,81,check that empty clusters are relocated as expected,
scikit-learn/sklearn/cluster/tests/test_k_means.py,84,second center too far from others points will be empty at first iter,
scikit-learn/sklearn/cluster/tests/test_k_means.py,103,test for the _relocate_empty_clusters_(dense/sparse) helpers,
scikit-learn/sklearn/cluster/tests/test_k_means.py,105,Synthetic dataset with 3 obvious clusters of different sizes,
scikit-learn/sklearn/cluster/tests/test_k_means.py,112,centers all initialized to the first point of X,
scikit-learn/sklearn/cluster/tests/test_k_means.py,115,"With this initialization, all points will be assigned to the first center",
scikit-learn/sklearn/cluster/tests/test_k_means.py,116,At this point a center in centers_new is the weighted sum of the points,
scikit-learn/sklearn/cluster/tests/test_k_means.py,117,"it contains if it's not empty, otherwise it is the same as before.",
scikit-learn/sklearn/cluster/tests/test_k_means.py,131,The relocation scheme will take the 2 points farthest from the center and,
scikit-learn/sklearn/cluster/tests/test_k_means.py,132,"assign them to the 2 empty clusters, i.e. points at 10 and at 9.9. The",
scikit-learn/sklearn/cluster/tests/test_k_means.py,133,first center will be updated to contain the other 8 points.,
scikit-learn/sklearn/cluster/tests/test_k_means.py,141,check that results are identical between lloyd and elkan algorithms,
scikit-learn/sklearn/cluster/tests/test_k_means.py,164,Check that KMeans stops when convergence is reached when tol=0. (#16075),
scikit-learn/sklearn/cluster/tests/test_k_means.py,176,check that results are identical between lloyd and elkan algorithms,
scikit-learn/sklearn/cluster/tests/test_k_means.py,177,with sparse input,
scikit-learn/sklearn/cluster/tests/test_k_means.py,197,pure numpy implementation as easily auditable reference gold,
scikit-learn/sklearn/cluster/tests/test_k_means.py,198,implementation,
scikit-learn/sklearn/cluster/tests/test_k_means.py,214,perform label assignment using the dense array input,
scikit-learn/sklearn/cluster/tests/test_k_means.py,221,perform label assignment using the sparse CSR input,
scikit-learn/sklearn/cluster/tests/test_k_means.py,230,Check that dense and sparse minibatch update give the same results,
scikit-learn/sklearn/cluster/tests/test_k_means.py,246,extract a small minibatch,
scikit-learn/sklearn/cluster/tests/test_k_means.py,254,step 1: compute the dense minibatch update,
scikit-learn/sklearn/cluster/tests/test_k_means.py,260,compute the new inertia on the same batch to check that it decreased,
scikit-learn/sklearn/cluster/tests/test_k_means.py,266,check that the incremental difference computation is matching the,
scikit-learn/sklearn/cluster/tests/test_k_means.py,267,final observed value,
scikit-learn/sklearn/cluster/tests/test_k_means.py,271,step 2: compute the sparse minibatch update,
scikit-learn/sklearn/cluster/tests/test_k_means.py,277,compute the new inertia on the same batch to check that it decreased,
scikit-learn/sklearn/cluster/tests/test_k_means.py,283,check that the incremental difference computation is matching the,
scikit-learn/sklearn/cluster/tests/test_k_means.py,284,final observed value,
scikit-learn/sklearn/cluster/tests/test_k_means.py,288,step 3: check that sparse and dense updates lead to the same results,
scikit-learn/sklearn/cluster/tests/test_k_means.py,297,check that the number of clusters centers and distinct labels match,
scikit-learn/sklearn/cluster/tests/test_k_means.py,298,the expectation,
scikit-learn/sklearn/cluster/tests/test_k_means.py,305,check that the labels assignment are perfect (up to a permutation),
scikit-learn/sklearn/cluster/tests/test_k_means.py,309,check error on dataset being too small,
scikit-learn/sklearn/cluster/tests/test_k_means.py,315,Explore the part of the code where a new center is reassigned,
scikit-learn/sklearn/cluster/tests/test_k_means.py,332,"Reorder the labels so that the first instance is in cluster 0,",
scikit-learn/sklearn/cluster/tests/test_k_means.py,333,"the second in cluster 1, ...",
scikit-learn/sklearn/cluster/tests/test_k_means.py,350,two regression tests on bad n_init argument,
scikit-learn/sklearn/cluster/tests/test_k_means.py,351,previous bug: n_init <= 0 threw non-informative TypeError (#3858),
scikit-learn/sklearn/cluster/tests/test_k_means.py,360,test for sensible errors when giving explicit init,
scikit-learn/sklearn/cluster/tests/test_k_means.py,361,with wrong number of features or clusters,
scikit-learn/sklearn/cluster/tests/test_k_means.py,365,mismatch of number of features,
scikit-learn/sklearn/cluster/tests/test_k_means.py,370,for callable init,
scikit-learn/sklearn/cluster/tests/test_k_means.py,376,mismatch of number of clusters,
scikit-learn/sklearn/cluster/tests/test_k_means.py,381,for callable init,
scikit-learn/sklearn/cluster/tests/test_k_means.py,390,"Check the KMeans will work well, even if X is a fortran-aligned data.",
scikit-learn/sklearn/cluster/tests/test_k_means.py,404,strict non-convergence,
scikit-learn/sklearn/cluster/tests/test_k_means.py,405,loose non-convergence,
scikit-learn/sklearn/cluster/tests/test_k_means.py,406,strict convergence,
scikit-learn/sklearn/cluster/tests/test_k_means.py,407,loose convergence,
scikit-learn/sklearn/cluster/tests/test_k_means.py,410,check that fit.predict gives same result as fit_predict,
scikit-learn/sklearn/cluster/tests/test_k_means.py,411,There's a very small chance of failure with elkan on unstructured dataset,
scikit-learn/sklearn/cluster/tests/test_k_means.py,412,because predict method uses fast euclidean distances computation which,
scikit-learn/sklearn/cluster/tests/test_k_means.py,413,may cause small numerical instabilities.,
scikit-learn/sklearn/cluster/tests/test_k_means.py,414,NB: This test is largely redundant with respect to test_predict and,
scikit-learn/sklearn/cluster/tests/test_k_means.py,415,test_predict_equal_labels.  This test has the added effect of,
scikit-learn/sklearn/cluster/tests/test_k_means.py,416,testing idempotence of the fittng procesdure which appears to,
scikit-learn/sklearn/cluster/tests/test_k_means.py,417,be where it fails on some MacOS setups.,
scikit-learn/sklearn/cluster/tests/test_k_means.py,435,Due to randomness in the order in which chunks of data are processed when,
scikit-learn/sklearn/cluster/tests/test_k_means.py,436,"using more than one thread, the absolute values of the labels can be",
scikit-learn/sklearn/cluster/tests/test_k_means.py,437,different between the 2 strategies but they should correspond to the same,
scikit-learn/sklearn/cluster/tests/test_k_means.py,438,clustering.,
scikit-learn/sklearn/cluster/tests/test_k_means.py,455,"Check that a warning is raised, as the number clusters is larger",
scikit-learn/sklearn/cluster/tests/test_k_means.py,456,than the init_size,
scikit-learn/sklearn/cluster/tests/test_k_means.py,476,check if identical initial clusters are reassigned,
scikit-learn/sklearn/cluster/tests/test_k_means.py,477,also a regression test for when there are more desired reassignments than,
scikit-learn/sklearn/cluster/tests/test_k_means.py,478,samples.,
scikit-learn/sklearn/cluster/tests/test_k_means.py,485,there should not be too many exact zero cluster centers,
scikit-learn/sklearn/cluster/tests/test_k_means.py,488,do the same with batch-size > X.shape[0] (regression test),
scikit-learn/sklearn/cluster/tests/test_k_means.py,492,there should not be too many exact zero cluster centers,
scikit-learn/sklearn/cluster/tests/test_k_means.py,503,there should not be too many exact zero cluster centers,
scikit-learn/sklearn/cluster/tests/test_k_means.py,508,"Give a perfect initialization, but a large reassignment_ratio,",
scikit-learn/sklearn/cluster/tests/test_k_means.py,509,as a result all the centers should be reassigned and the model,
scikit-learn/sklearn/cluster/tests/test_k_means.py,510,should no longer be good,
scikit-learn/sklearn/cluster/tests/test_k_means.py,521,Turn on verbosity to smoke test the display code,
scikit-learn/sklearn/cluster/tests/test_k_means.py,533,"Give a perfect initialization, with a small reassignment_ratio,",
scikit-learn/sklearn/cluster/tests/test_k_means.py,534,no center should be reassigned,
scikit-learn/sklearn/cluster/tests/test_k_means.py,541,Turn on verbosity to smoke test the display code,
scikit-learn/sklearn/cluster/tests/test_k_means.py,553,Test for the case that the number of clusters to reassign is bigger,
scikit-learn/sklearn/cluster/tests/test_k_means.py,554,than the batch_size,
scikit-learn/sklearn/cluster/tests/test_k_means.py,558,Check that the fit works if n_clusters is bigger than the batch_size.,
scikit-learn/sklearn/cluster/tests/test_k_means.py,559,"Run the test with 550 clusters and 550 samples, because it turned out",
scikit-learn/sklearn/cluster/tests/test_k_means.py,560,that this values ensure that the number of clusters to reassign,
scikit-learn/sklearn/cluster/tests/test_k_means.py,561,is always bigger than the batch_size,
scikit-learn/sklearn/cluster/tests/test_k_means.py,574,Small test to check that giving the wrong number of centers,
scikit-learn/sklearn/cluster/tests/test_k_means.py,575,raises a meaningful error,
scikit-learn/sklearn/cluster/tests/test_k_means.py,580,Now check that the fit actually works,
scikit-learn/sklearn/cluster/tests/test_k_means.py,589,use the partial_fit API for online learning,
scikit-learn/sklearn/cluster/tests/test_k_means.py,593,compute the labeling on the complete dataset,
scikit-learn/sklearn/cluster/tests/test_k_means.py,629,Check if copy_x=False returns nearly equal X after de-centering.,
scikit-learn/sklearn/cluster/tests/test_k_means.py,635,check if my_X is centered,
scikit-learn/sklearn/cluster/tests/test_k_means.py,640,Check k_means with a bad initialization does not yield a singleton,
scikit-learn/sklearn/cluster/tests/test_k_means.py,641,Starting with bad centers that are quickly ignored should not,
scikit-learn/sklearn/cluster/tests/test_k_means.py,642,result in a repositioning of the centers to the center of mass that,
scikit-learn/sklearn/cluster/tests/test_k_means.py,643,would lead to collapsed centers which in turns make the clustering,
scikit-learn/sklearn/cluster/tests/test_k_means.py,644,dependent of the numerical unstabilities.,
scikit-learn/sklearn/cluster/tests/test_k_means.py,650,centers must not been collapsed,
scikit-learn/sklearn/cluster/tests/test_k_means.py,661,Check that fitting k-means with multiple inits gives better score,
scikit-learn/sklearn/cluster/tests/test_k_means.py,678,sanity check: re-predict labeling for training set samples,
scikit-learn/sklearn/cluster/tests/test_k_means.py,681,sanity check: predict centroid labels,
scikit-learn/sklearn/cluster/tests/test_k_means.py,685,re-predict labels for training set using fit_predict,
scikit-learn/sklearn/cluster/tests/test_k_means.py,692,check that models trained on sparse input also works for dense input at,
scikit-learn/sklearn/cluster/tests/test_k_means.py,693,predict time,
scikit-learn/sklearn/cluster/tests/test_k_means.py,710,mini batch kmeans is very unstable on such a small dataset hence,
scikit-learn/sklearn/cluster/tests/test_k_means.py,711,we use many inits,
scikit-learn/sklearn/cluster/tests/test_k_means.py,764,Check that increasing the number of init increases the quality,
scikit-learn/sklearn/cluster/tests/test_k_means.py,782,test calling the k_means function directly,
scikit-learn/sklearn/cluster/tests/test_k_means.py,783,catch output,
scikit-learn/sklearn/cluster/tests/test_k_means.py,798,check that the labels assignment are perfect (up to a permutation),
scikit-learn/sklearn/cluster/tests/test_k_means.py,802,check warning when centers are passed,
scikit-learn/sklearn/cluster/tests/test_k_means.py,806,to many clusters desired,
scikit-learn/sklearn/cluster/tests/test_k_means.py,812,Test that x_squared_norms can be None in _init_centroids,
scikit-learn/sklearn/cluster/tests/test_k_means.py,845,dtype of cluster centers has to be the dtype of the input,
scikit-learn/sklearn/cluster/tests/test_k_means.py,846,data,
scikit-learn/sklearn/cluster/tests/test_k_means.py,851,ensure the extracted row is a 2d array,
scikit-learn/sklearn/cluster/tests/test_k_means.py,855,dtype of cluster centers has to stay the same after,
scikit-learn/sklearn/cluster/tests/test_k_means.py,856,partial_fit,
scikit-learn/sklearn/cluster/tests/test_k_means.py,859,compare arrays with low precision since the difference between,
scikit-learn/sklearn/cluster/tests/test_k_means.py,860,32 and 64 bit sometimes makes a difference up to the 4th decimal,
scikit-learn/sklearn/cluster/tests/test_k_means.py,861,place,
scikit-learn/sklearn/cluster/tests/test_k_means.py,871,This test is used to check KMeans won't mutate the user provided input,
scikit-learn/sklearn/cluster/tests/test_k_means.py,872,array silently even if input data and init centers have the same type,
scikit-learn/sklearn/cluster/tests/test_k_means.py,887,Get a local optimum,
scikit-learn/sklearn/cluster/tests/test_k_means.py,890,Fit starting from a local optimum shouldn't change the solution,
scikit-learn/sklearn/cluster/tests/test_k_means.py,902,Get a local optimum,
scikit-learn/sklearn/cluster/tests/test_k_means.py,905,Test that a ValueError is raised for validate_center_shape,
scikit-learn/sklearn/cluster/tests/test_k_means.py,918,last point is duplicated,
scikit-learn/sklearn/cluster/tests/test_k_means.py,922,"only three distinct points, so only three clusters",
scikit-learn/sklearn/cluster/tests/test_k_means.py,923,can have points assigned to them,
scikit-learn/sklearn/cluster/tests/test_k_means.py,926,k_means should warn that fewer labels than cluster,
scikit-learn/sklearn/cluster/tests/test_k_means.py,927,centers have been used,
scikit-learn/sklearn/cluster/tests/test_k_means.py,939,a sample weight of N should yield the same result as an N-fold,
scikit-learn/sklearn/cluster/tests/test_k_means.py,940,repetition of the sample,
scikit-learn/sklearn/cluster/tests/test_k_means.py,964,not passing any sample weights should be equivalent,
scikit-learn/sklearn/cluster/tests/test_k_means.py,965,to all weights equal to one,
scikit-learn/sklearn/cluster/tests/test_k_means.py,977,scaling all sample weights by a common factor,
scikit-learn/sklearn/cluster/tests/test_k_means.py,978,shouldn't change the result,
scikit-learn/sklearn/cluster/tests/test_k_means.py,990,check that an error is raised when passing sample weights,
scikit-learn/sklearn/cluster/tests/test_k_means.py,991,with an incompatible shape,
scikit-learn/sklearn/cluster/tests/test_k_means.py,1008,Regression test on bad n_iter_ value. Previous bug n_iter_ was one off,
scikit-learn/sklearn/cluster/tests/test_k_means.py,1009,it's right value (#11340).,
scikit-learn/sklearn/cluster/tests/test_k_means.py,1016,check that empty clusters are correctly relocated when using sample,
scikit-learn/sklearn/cluster/tests/test_k_means.py,1017,weights (#13486),
scikit-learn/sklearn/cluster/tests/test_k_means.py,1030,Issue GH #14314,
scikit-learn/sklearn/cluster/tests/test_k_means.py,1038,Check that KMeans gives the same results in parallel mode than in,
scikit-learn/sklearn/cluster/tests/test_k_means.py,1039,sequential mode.,
scikit-learn/sklearn/cluster/tests/test_k_means.py,1054,FIXME: remove in 0.25,
scikit-learn/sklearn/cluster/tests/test_k_means.py,1067,FIXME: remove in 0.25,
scikit-learn/sklearn/cluster/tests/test_k_means.py,1104,check the results after a single iteration (E-step M-step E-step) by,
scikit-learn/sklearn/cluster/tests/test_k_means.py,1105,comparing against a pure python implementation.,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,27,Tests the DBSCAN algorithm with a similarity array.,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,28,Parameters chosen specifically for this task.,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,31,Compute similarities,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,34,Compute DBSCAN,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,37,"number of clusters, ignoring noise if present",
scikit-learn/sklearn/cluster/tests/test_dbscan.py,50,Tests the DBSCAN algorithm with a feature vector array.,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,51,Parameters chosen specifically for this task.,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,52,"Different eps to other test, because distance is not normalised.",
scikit-learn/sklearn/cluster/tests/test_dbscan.py,56,Compute DBSCAN,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,57,parameters chosen for task,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,61,"number of clusters, ignoring noise if present",
scikit-learn/sklearn/cluster/tests/test_dbscan.py,86,Ensure it is sparse not merely on diagonals:,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,99,test that precomputed neighbors graph is filtered if computed with,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,100,a radius larger than DBSCAN's eps.,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,118,test that the input is not modified by dbscan,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,143,Tests the DBSCAN algorithm with a callable metric.,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,144,Parameters chosen specifically for this task.,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,145,"Different eps to other test, because distance is not normalised.",
scikit-learn/sklearn/cluster/tests/test_dbscan.py,148,"metric is the function reference, not the string key.",
scikit-learn/sklearn/cluster/tests/test_dbscan.py,150,Compute DBSCAN,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,151,parameters chosen for task,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,156,"number of clusters, ignoring noise if present",
scikit-learn/sklearn/cluster/tests/test_dbscan.py,169,Tests that DBSCAN works with the metrics_params argument.,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,174,Compute DBSCAN with metric_params arg,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,179,Test that sample labels are the same as passing Minkowski 'p' directly,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,187,Minkowski with p=1 should be equivalent to Manhattan distance,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,197,Tests the DBSCAN algorithm with balltree for neighbor calculation.,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,205,"number of clusters, ignoring noise if present",
scikit-learn/sklearn/cluster/tests/test_dbscan.py,236,DBSCAN.fit should accept a list of lists.,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,238,must not raise exception,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,247,Test bad argument values: these should all raise ValueErrors,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,259,ensure min_samples is inclusive of core point,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,262,ensure eps is inclusive of circumference,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,270,ensure sample_weight is validated,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,276,ensure sample_weight has an effect,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,286,points within eps of each other:,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,289,and effect of non-positive and non-integer sample_weight:,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,299,"for non-negative sample_weight, cores should be identical to repetition",
scikit-learn/sklearn/cluster/tests/test_dbscan.py,313,sample_weight should work with precomputed distance matrix,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,320,sample_weight should work with estimator,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,340,"Degenerate case: every sample is a core sample, either with its own",
scikit-learn/sklearn/cluster/tests/test_dbscan.py,341,cluster or including other close core samples.,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,347,With eps=1 and min_samples=2 only the 3 samples from the denser area,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,348,are core samples. All other points are isolated and considered noise.,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,354,Only the sample in the middle of the dense area is core. Its two,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,355,neighbors are edge samples. Remaining samples are noise.,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,361,It's no longer possible to extract core samples with eps=1:,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,362,everything is noise.,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,370,see https://github.com/scikit-learn/scikit-learn/issues/4641 for,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,371,more details,
scikit-learn/sklearn/cluster/tests/test_dbscan.py,382,sample matrix with initial two row all zero,
scikit-learn/sklearn/cluster/tests/common.py,9,,
scikit-learn/sklearn/cluster/tests/common.py,10,Generate sample data,
scikit-learn/sklearn/cluster/tests/common.py,16,the data is voluntary shifted away from zero to check clustering,
scikit-learn/sklearn/cluster/tests/common.py,17,algorithm robustness with regards to non centered data,
scikit-learn/sklearn/neural_network/_stochastic_optimizers.py,4,Authors: Jiyuan Qian <jq401@nyu.edu>,
scikit-learn/sklearn/neural_network/_stochastic_optimizers.py,5,License: BSD 3 clause,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,4,Authors: Issam H. Laradji <issam.laradji@gmail.com>,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,5,Andreas Mueller,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,6,Jiyuan Qian,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,7,License: BSD 3 clause,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,102,Iterate over the hidden layers,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,108,For the hidden layers,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,112,For the last layer,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,221,Forward propagate,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,224,Get loss,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,229,Add L2 regularization term to loss,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,234,Backward propagate,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,237,The calculation of delta[last] here works with following,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,238,combinations of output activation and loss function:,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,239,"sigmoid and binary cross entropy, softmax and categorical cross",
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,240,"entropy, and identity with squared loss",
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,243,Compute gradient for the last layer,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,247,Iterate over the hidden layers,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,260,"set all attributes, allocate weights etc for first call",
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,261,Initialize parameters,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,266,Compute the number of layers,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,269,Output for regression,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,272,Output for multi class,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,275,Output for binary class and multi-label,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,279,Initialize coefficient and intercept layers,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,299,Use the initialization method recommended by,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,300,Glorot et al.,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,306,Generate weights and bias:,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,314,Make sure self.hidden_layer_sizes is a list,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,320,Validate input parameters.,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,329,Ensure y is 2D,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,338,check random state,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,343,First time training the model,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,346,lbfgs does not support mini-batches,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,357,Initialize lists,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,368,Run the Stochastic optimization solver,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,373,Run the LBFGS solver,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,417,raise ValueError if not registered,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,433,Store meta information for the parameters,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,438,Save sizes and indices of coefficients for faster unpacking,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,446,Save sizes and indices of intercepts for faster unpacking,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,452,Run LBFGS,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,490,early_stopping in partial_fit doesn't make sense,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,493,don't stratify in multilabel classification,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,517,Only shuffle the sample indices instead of X and y to,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,518,reduce the memory footprint. These indices will be used,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,519,to slice the X and y.,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,539,update weights,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,552,update no_improvement_count based on training loss or,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,553,validation score according to early_stopping,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,556,for learning rate that needs to be updated at iteration end,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,560,not better than last `n_iter_no_change` iterations by tol,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,561,stop or decrease learning rate,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,590,restore best weights,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,596,"compute validation score, use that for stopping",
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,601,update best parameters,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,602,"use validation_scores_, not loss_curve_",
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,603,let's hope no-one overloads .score with mse,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,683,Make sure self.hidden_layer_sizes is a list,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,692,Initialize layers,
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,698,forward propagate,
scikit-learn/sklearn/neural_network/_rbm.py,4,Authors: Yann N. Dauphin <dauphiya@iro.umontreal.ca>,
scikit-learn/sklearn/neural_network/_rbm.py,5,Vlad Niculae,
scikit-learn/sklearn/neural_network/_rbm.py,6,Gabriel Synnaeve,
scikit-learn/sklearn/neural_network/_rbm.py,7,Lars Buitinck,
scikit-learn/sklearn/neural_network/_rbm.py,8,License: BSD 3 clause,
scikit-learn/sklearn/neural_network/_rbm.py,14,logistic function,
scikit-learn/sklearn/neural_network/_rbm.py,292,sample binomial,
scikit-learn/sklearn/neural_network/_rbm.py,319,Randomly corrupt one feature in each sample in v.,
scikit-learn/sklearn/neural_network/__init__.py,6,License: BSD 3 clause,
scikit-learn/sklearn/neural_network/_base.py,4,Author: Issam H. Laradji <issam.laradji@gmail.com>,
scikit-learn/sklearn/neural_network/_base.py,5,License: BSD 3 clause,
scikit-learn/sklearn/neural_network/_base.py,115,Nothing to do,
scikit-learn/sklearn/neural_network/tests/test_rbm.py,27,in-place tricks shouldn't have modified X,
scikit-learn/sklearn/neural_network/tests/test_rbm.py,60,BernoulliRBM should work on small sparse matrices.,
scikit-learn/sklearn/neural_network/tests/test_rbm.py,62,no exception,
scikit-learn/sklearn/neural_network/tests/test_rbm.py,97,"Gibbs on the RBM hidden layer should be able to recreate [[0], [1]]",
scikit-learn/sklearn/neural_network/tests/test_rbm.py,98,from the same input,
scikit-learn/sklearn/neural_network/tests/test_rbm.py,103,you need that much iters,
scikit-learn/sklearn/neural_network/tests/test_rbm.py,112,"Gibbs on the RBM hidden layer should be able to recreate [[0], [1]] from",
scikit-learn/sklearn/neural_network/tests/test_rbm.py,113,"the same input even when the input is sparse, and test against non-sparse",
scikit-learn/sklearn/neural_network/tests/test_rbm.py,128,Check if we don't get NaNs sampling the full digits dataset.,
scikit-learn/sklearn/neural_network/tests/test_rbm.py,129,Also check that sampling again will yield different results.,
scikit-learn/sklearn/neural_network/tests/test_rbm.py,141,Test score_samples (pseudo-likelihood) method.,
scikit-learn/sklearn/neural_network/tests/test_rbm.py,142,Assert that pseudo-likelihood is computed without clipping.,
scikit-learn/sklearn/neural_network/tests/test_rbm.py,143,"See Fabian's blog, http://bit.ly/1iYefRk",
scikit-learn/sklearn/neural_network/tests/test_rbm.py,151,Sparse vs. dense should not affect the output. Also test sparse input,
scikit-learn/sklearn/neural_network/tests/test_rbm.py,152,validation.,
scikit-learn/sklearn/neural_network/tests/test_rbm.py,159,Test numerical stability (#2785): would previously generate infinities,
scikit-learn/sklearn/neural_network/tests/test_rbm.py,160,and crash with an exception.,
scikit-learn/sklearn/neural_network/tests/test_rbm.py,176,Make sure RBM works with sparse input when verbose=True,
scikit-learn/sklearn/neural_network/tests/test_rbm.py,186,make sure output is sound,
scikit-learn/sklearn/neural_network/tests/test_base.py,9,y_proba is equal to one should result in a finite logloss,
scikit-learn/sklearn/neural_network/tests/test_base.py,24,y_proba is equal to 1 should result in a finite logloss,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,5,Author: Issam H. Laradji,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,6,License: BSD 3 clause,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,59,Test that larger alpha yields weights closer to zero,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,79,Test that the algorithm solution is equal to a worked out example.,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,85,set weights,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,96,Initialize parameters,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,100,Compute the number of layers,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,103,Pre-allocate gradient matrices,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,119,Manually worked out example,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,120,h1 = g(X1 * W_i1 + b11) = g(0.6 * 0.1 + 0.8 * 0.3 + 0.7 * 0.5 + 0.1),
scikit-learn/sklearn/neural_network/tests/test_mlp.py,121,=  0.679178699175393,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,122,h2 = g(X2 * W_i2 + b12) = g(0.6 * 0.2 + 0.8 * 0.1 + 0.7 * 0 + 0.1),
scikit-learn/sklearn/neural_network/tests/test_mlp.py,123,= 0.574442516811659,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,124,o1 = g(h * W2 + b21) = g(0.679 * 0.1 + 0.574 * 0.2 + 1),
scikit-learn/sklearn/neural_network/tests/test_mlp.py,125,= 0.7654329236196236,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,126,d21 = -(0 - 0.765) = 0.765,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,127,d11 = (1 - 0.679) * 0.679 * 0.765 * 0.1 = 0.01667,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,128,d12 = (1 - 0.574) * 0.574 * 0.765 * 0.2 = 0.0374,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,129,W1grad11 = X1 * d11 + alpha * W11 = 0.6 * 0.01667 + 0.1 * 0.1 = 0.0200,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,130,W1grad11 = X1 * d12 + alpha * W12 = 0.6 * 0.0374 + 0.1 * 0.2 = 0.04244,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,131,W1grad21 = X2 * d11 + alpha * W13 = 0.8 * 0.01667 + 0.1 * 0.3 = 0.043336,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,132,W1grad22 = X2 * d12 + alpha * W14 = 0.8 * 0.0374 + 0.1 * 0.1 = 0.03992,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,133,W1grad31 = X3 * d11 + alpha * W15 = 0.6 * 0.01667 + 0.1 * 0.5 = 0.060002,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,134,W1grad32 = X3 * d12 + alpha * W16 = 0.6 * 0.0374 + 0.1 * 0 = 0.02244,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,135,W2grad1 = h1 * d21 + alpha * W21 = 0.679 * 0.765 + 0.1 * 0.1 = 0.5294,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,136,W2grad2 = h2 * d21 + alpha * W22 = 0.574 * 0.765 + 0.1 * 0.2 = 0.45911,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,137,b1grad1 = d11 = 0.01667,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,138,b1grad2 = d12 = 0.0374,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,139,b2grad = d21 = 0.765,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,140,"W1 = W1 - eta * [W1grad11, .., W1grad32] = [[0.1, 0.2], [0.3, 0.1],",
scikit-learn/sklearn/neural_network/tests/test_mlp.py,141,"[0.5, 0]] - 0.1 * [[0.0200, 0.04244], [0.043336, 0.03992],",
scikit-learn/sklearn/neural_network/tests/test_mlp.py,142,"[0.060002, 0.02244]] = [[0.098, 0.195756], [0.2956664,",
scikit-learn/sklearn/neural_network/tests/test_mlp.py,143,"0.096008], [0.4939998, -0.002244]]",
scikit-learn/sklearn/neural_network/tests/test_mlp.py,144,"W2 = W2 - eta * [W2grad1, W2grad2] = [[0.1], [0.2]] - 0.1 *",
scikit-learn/sklearn/neural_network/tests/test_mlp.py,145,"[[0.5294], [0.45911]] = [[0.04706], [0.154089]]",
scikit-learn/sklearn/neural_network/tests/test_mlp.py,146,"b1 = b1 - eta * [b1grad1, b1grad2] = 0.1 - 0.1 * [0.01667, 0.0374]",
scikit-learn/sklearn/neural_network/tests/test_mlp.py,147,"= [0.098333, 0.09626]",
scikit-learn/sklearn/neural_network/tests/test_mlp.py,148,b2 = b2 - eta * b2grad = 1.0 - 0.1 * 0.765 = 0.9235,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,158,Testing output,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,159,h1 = g(X1 * W_i1 + b11) = g(0.6 * 0.098 + 0.8 * 0.2956664 +,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,160,0.7 * 0.4939998 + 0.098333) = 0.677,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,161,h2 = g(X2 * W_i2 + b12) = g(0.6 * 0.195756 + 0.8 * 0.096008 +,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,162,0.7 * -0.002244 + 0.09626) = 0.572,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,163,o1 = h * W2 + b21 = 0.677 * 0.04706 +,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,164,0.572 * 0.154089 + 0.9235 = 1.043,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,165,prob = sigmoid(o1) = 0.739,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,170,Test gradient.,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,172,This makes sure that the activation functions and their derivatives,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,173,are correct. The numerical and analytical computation of the gradient,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,174,should be close.,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,213,analytically compute the gradients,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,223,numerically compute the gradients,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,234,Test lbfgs on classification.,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,235,It should achieve a score higher than 0.95 for the binary and multi-class,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,236,versions of the digits dataset.,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,255,"Test lbfgs on the boston dataset, a regression problems.",
scikit-learn/sklearn/neural_network/tests/test_mlp.py,264,Non linear models perform much better than linear bottleneck:,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,270,Test lbfgs parameter max_fun.,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,271,It should independently limit the number of iterations for lbfgs.,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,273,classification tests,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,285,Test lbfgs parameter max_fun.,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,286,It should independently limit the number of iterations for lbfgs.,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,288,regression tests,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,303,Tests that warm_start reuse past solutions.,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,324,Test that multi-label classification works as expected.,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,325,test fit method,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,334,test partial fit method,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,342,Make sure early stopping still work now that spliting is stratified by,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,343,default (it is disabled for multilabel classification),
scikit-learn/sklearn/neural_network/tests/test_mlp.py,349,Test that multi-output regression works as expected,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,358,Tests that passing different classes to partial_fit raises an error,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,368,Test partial_fit on classification.,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,369,`partial_fit` should yield the same results as 'fit' for binary and,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,370,multi-class classification.,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,390,Non regression test for bug 6994,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,391,Tests for labeling errors in partial fit,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,401,Test partial_fit on regression.,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,402,`partial_fit` should yield the same results as 'fit' for regression.,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,411,catch convergence warning,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,427,Test partial_fit error handling.,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,431,no classes passed,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,435,lbfgs doesn't support partial_fit,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,463,Test that invalid parameters raise value error,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,473,Test that predict_proba works as expected for binary class.,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,497,Test that predict_proba works as expected for multi class.,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,518,Test that predict_proba works as expected for multilabel.,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,519,Multilabel should not use softmax which makes probabilities sum to 1,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,542,Test that the shuffle parameter affects the training process (it should),
scikit-learn/sklearn/neural_network/tests/test_mlp.py,546,The coefficients will be identical if both do or do not shuffle,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,557,The coefficients will be slightly different if shuffle=True,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,569,Test that sparse and dense input matrices output the same results.,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,586,Test tolerance.,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,587,It should force the solver to exit the loop when it converges.,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,596,Test verbose.,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,649,No error raised,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,666,test n_iter_no_change using binary data set,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,667,the classifying fitting process is not prone to loss curve fluctuations,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,673,test multiple n_iter_no_change,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,679,validate n_iter_no_change,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,686,test n_iter_no_change using binary data set,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,687,the fitting process should go to max_iter iterations,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,691,set a ridiculous tolerance,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,692,this should always trigger _update_no_improvement_count(),
scikit-learn/sklearn/neural_network/tests/test_mlp.py,695,fit,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,702,validate n_iter_no_change doesn't cause early stopping,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,705,validate _update_no_improvement_count() was always triggered,
scikit-learn/sklearn/neural_network/tests/test_mlp.py,710,Make sure data splitting for early stopping is stratified,
scikit-learn/sklearn/tree/_reingold_tilford.py,1,Authors: William Mill (bill@billmill.org),
scikit-learn/sklearn/tree/_reingold_tilford.py,2,License: BSD 3 clause,
scikit-learn/sklearn/tree/_reingold_tilford.py,21,this is the number of the node in its group of siblings 1..n,
scikit-learn/sklearn/tree/_reingold_tilford.py,84,"print(""finished v ="", v.tree, ""children"")",
scikit-learn/sklearn/tree/_reingold_tilford.py,101,in buchheim notation:,
scikit-learn/sklearn/tree/_reingold_tilford.py,102,i == inner; o == outer; r == right; l == left; r = +; l = -,
scikit-learn/sklearn/tree/_reingold_tilford.py,137,"print(wl.tree, ""is conflicted with"", wr.tree, 'moving', subtrees,",
scikit-learn/sklearn/tree/_reingold_tilford.py,138,"'shift', shift)",
scikit-learn/sklearn/tree/_reingold_tilford.py,139,"print wl, wr, wr.number, wl.number, shift, subtrees, shift/subtrees",
scikit-learn/sklearn/tree/_reingold_tilford.py,150,"print(""shift:"", w, shift, w.change)",
scikit-learn/sklearn/tree/_reingold_tilford.py,158,the relevant text is at the bottom of page 7 of,
scikit-learn/sklearn/tree/_reingold_tilford.py,159,"""Improving Walker's Algorithm to Run in Linear Time"" by Buchheim et al,",
scikit-learn/sklearn/tree/_reingold_tilford.py,160,(2002),
scikit-learn/sklearn/tree/_reingold_tilford.py,161,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.16.8757&rep=rep1&type=pdf,
scikit-learn/sklearn/tree/_classes.py,6,Authors: Gilles Louppe <g.louppe@gmail.com>,
scikit-learn/sklearn/tree/_classes.py,7,Peter Prettenhofer <peter.prettenhofer@gmail.com>,
scikit-learn/sklearn/tree/_classes.py,8,Brian Holt <bdholt1@gmail.com>,
scikit-learn/sklearn/tree/_classes.py,9,Noel Dawe <noel@dawe.me>,
scikit-learn/sklearn/tree/_classes.py,10,Satrajit Gosh <satrajit.ghosh@gmail.com>,
scikit-learn/sklearn/tree/_classes.py,11,Joly Arnaud <arnaud.v.joly@gmail.com>,
scikit-learn/sklearn/tree/_classes.py,12,Fares Hedayati <fares.hedayati@gmail.com>,
scikit-learn/sklearn/tree/_classes.py,13,Nelson Liu <nelson@nelsonliu.me>,
scikit-learn/sklearn/tree/_classes.py,14,,
scikit-learn/sklearn/tree/_classes.py,15,License: BSD 3 clause,
scikit-learn/sklearn/tree/_classes.py,55,=============================================================================,
scikit-learn/sklearn/tree/_classes.py,56,Types and constants,
scikit-learn/sklearn/tree/_classes.py,57,=============================================================================,
scikit-learn/sklearn/tree/_classes.py,72,=============================================================================,
scikit-learn/sklearn/tree/_classes.py,73,Base decision tree,
scikit-learn/sklearn/tree/_classes.py,74,=============================================================================,
scikit-learn/sklearn/tree/_classes.py,158,Determine output settings,
scikit-learn/sklearn/tree/_classes.py,166,reshape is necessary to preserve the data contiguity against vs,
scikit-learn/sklearn/tree/_classes.py,167,"[:, np.newaxis] that does not.",
scikit-learn/sklearn/tree/_classes.py,199,Check parameters,
scikit-learn/sklearn/tree/_classes.py,211,float,
scikit-learn/sklearn/tree/_classes.py,225,float,
scikit-learn/sklearn/tree/_classes.py,254,float,
scikit-learn/sklearn/tree/_classes.py,288,Set min_weight_leaf from min_weight_fraction_leaf,
scikit-learn/sklearn/tree/_classes.py,321,Build tree,
scikit-learn/sklearn/tree/_classes.py,346,TODO: tree should't need this in this case,
scikit-learn/sklearn/tree/_classes.py,350,Use BestFirst if max_leaf_nodes given; use DepthFirst otherwise,
scikit-learn/sklearn/tree/_classes.py,423,Classification,
scikit-learn/sklearn/tree/_classes.py,439,Regression,
scikit-learn/sklearn/tree/_classes.py,510,build pruned tree,
scikit-learn/sklearn/tree/_classes.py,516,TODO: the tree shouldn't need this param,
scikit-learn/sklearn/tree/_classes.py,585,=============================================================================,
scikit-learn/sklearn/tree/_classes.py,586,Public estimators,
scikit-learn/sklearn/tree/_classes.py,587,=============================================================================,
scikit-learn/sklearn/tree/_classes.py,1241,TODO: Remove method in 0.24,
scikit-learn/sklearn/tree/_classes.py,1249,TODO: Remove method in 0.24,
scikit-learn/sklearn/tree/_export.py,5,Authors: Gilles Louppe <g.louppe@gmail.com>,
scikit-learn/sklearn/tree/_export.py,6,Peter Prettenhofer <peter.prettenhofer@gmail.com>,
scikit-learn/sklearn/tree/_export.py,7,Brian Holt <bdholt1@gmail.com>,
scikit-learn/sklearn/tree/_export.py,8,Noel Dawe <noel@dawe.me>,
scikit-learn/sklearn/tree/_export.py,9,Satrajit Gosh <satrajit.ghosh@gmail.com>,
scikit-learn/sklearn/tree/_export.py,10,Trevor Stephens <trev.stephens@gmail.com>,
scikit-learn/sklearn/tree/_export.py,11,Li Li <aiki.nogard@gmail.com>,
scikit-learn/sklearn/tree/_export.py,12,Giuseppe Vettigli <vettigli@gmail.com>,
scikit-learn/sklearn/tree/_export.py,13,License: BSD 3 clause,
scikit-learn/sklearn/tree/_export.py,45,Initialize saturation & value; calculate chroma & value shift,
scikit-learn/sklearn/tree/_export.py,51,Calculate some intermediate values,
scikit-learn/sklearn/tree/_export.py,54,Initialize RGB with same hue & chroma as our color,
scikit-learn/sklearn/tree/_export.py,63,Shift the initial RGB values to match value and store,
scikit-learn/sklearn/tree/_export.py,214,Find the appropriate color & intensity for a node,
scikit-learn/sklearn/tree/_export.py,216,Classification tree,
scikit-learn/sklearn/tree/_export.py,225,Regression tree or multi-output,
scikit-learn/sklearn/tree/_export.py,229,unpack numpy scalars,
scikit-learn/sklearn/tree/_export.py,231,compute the color as alpha against white,
scikit-learn/sklearn/tree/_export.py,233,Return html color code in #RRGGBB format,
scikit-learn/sklearn/tree/_export.py,237,Fetch appropriate color for node,
scikit-learn/sklearn/tree/_export.py,239,Initialize colors and bounds if required,
scikit-learn/sklearn/tree/_export.py,242,Find max and min impurities for multi-output,
scikit-learn/sklearn/tree/_export.py,247,Find max and min values in leaf nodes for regression,
scikit-learn/sklearn/tree/_export.py,254,Regression,
scikit-learn/sklearn/tree/_export.py,257,If multi-output color node by impurity,
scikit-learn/sklearn/tree/_export.py,262,Generate the node content string,
scikit-learn/sklearn/tree/_export.py,268,Should labels be shown?,
scikit-learn/sklearn/tree/_export.py,274,Write node ID,
scikit-learn/sklearn/tree/_export.py,280,Write decision criteria,
scikit-learn/sklearn/tree/_export.py,282,"Always write node decision criteria, except for leaves",
scikit-learn/sklearn/tree/_export.py,295,Write impurity,
scikit-learn/sklearn/tree/_export.py,306,Write node sample count,
scikit-learn/sklearn/tree/_export.py,318,Write node class distribution / regression value,
scikit-learn/sklearn/tree/_export.py,320,For classification this will show the proportion of samples,
scikit-learn/sklearn/tree/_export.py,325,Regression,
scikit-learn/sklearn/tree/_export.py,328,Classification,
scikit-learn/sklearn/tree/_export.py,331,Classification without floating-point weights,
scikit-learn/sklearn/tree/_export.py,334,Classification with floating-point weights,
scikit-learn/sklearn/tree/_export.py,336,Strip whitespace,
scikit-learn/sklearn/tree/_export.py,344,Write node majority class,
scikit-learn/sklearn/tree/_export.py,348,Only done for single-output classification trees,
scikit-learn/sklearn/tree/_export.py,359,Clean up any trailing newlines,
scikit-learn/sklearn/tree/_export.py,384,PostScript compatibility for special characters,
scikit-learn/sklearn/tree/_export.py,391,validate,
scikit-learn/sklearn/tree/_export.py,400,The depth of each node for plotting with 'leaf' option,
scikit-learn/sklearn/tree/_export.py,402,The colors to render each node with,
scikit-learn/sklearn/tree/_export.py,406,Check length of feature_names before getting into the tree node,
scikit-learn/sklearn/tree/_export.py,407,Raise error if length of feature_names does not match,
scikit-learn/sklearn/tree/_export.py,408,n_features_ in the decision_tree,
scikit-learn/sklearn/tree/_export.py,415,each part writes to out_file,
scikit-learn/sklearn/tree/_export.py,417,Now recurse the tree and add node & edge attributes,
scikit-learn/sklearn/tree/_export.py,427,"If required, draw leaf nodes at same depth as each other",
scikit-learn/sklearn/tree/_export.py,438,Specify node aesthetics,
scikit-learn/sklearn/tree/_export.py,453,Specify graph & edge aesthetics,
scikit-learn/sklearn/tree/_export.py,469,Add node with description,
scikit-learn/sklearn/tree/_export.py,472,Collect ranks for 'leaf' option in plot_options,
scikit-learn/sklearn/tree/_export.py,490,Add edge to parent,
scikit-learn/sklearn/tree/_export.py,493,Draw True/False labels if parent is root node,
scikit-learn/sklearn/tree/_export.py,515,color cropped nodes grey,
scikit-learn/sklearn/tree/_export.py,520,Add edge to parent,
scikit-learn/sklearn/tree/_export.py,538,validate,
scikit-learn/sklearn/tree/_export.py,547,The depth of each node for plotting with 'leaf' option,
scikit-learn/sklearn/tree/_export.py,549,The colors to render each node with,
scikit-learn/sklearn/tree/_export.py,561,"traverses _tree.Tree recursively, builds intermediate",
scikit-learn/sklearn/tree/_export.py,562,"""_reingold_tilford.Tree"" object",
scikit-learn/sklearn/tree/_export.py,586,important to make sure we're still,
scikit-learn/sklearn/tree/_export.py,587,inside the axis after drawing the box,
scikit-learn/sklearn/tree/_export.py,588,this makes sense because the width of a box,
scikit-learn/sklearn/tree/_export.py,589,is about the same as the distance between boxes,
scikit-learn/sklearn/tree/_export.py,603,update sizes of all bboxes,
scikit-learn/sklearn/tree/_export.py,610,get figure to data transform,
scikit-learn/sklearn/tree/_export.py,611,adjust fontsize to avoid overlap,
scikit-learn/sklearn/tree/_export.py,612,get max box width and height,
scikit-learn/sklearn/tree/_export.py,617,width should be around scale_x in axis coordinates,
scikit-learn/sklearn/tree/_export.py,632,offset things by .5 to center them in plot,
scikit-learn/sklearn/tree/_export.py,640,root,
scikit-learn/sklearn/tree/_export.py,951,leaf,
scikit-learn/sklearn/tree/tests/test_export.py,17,toy sample,
scikit-learn/sklearn/tree/tests/test_export.py,26,Check correctness of export_graphviz,
scikit-learn/sklearn/tree/tests/test_export.py,33,Test export code,
scikit-learn/sklearn/tree/tests/test_export.py,49,Test with feature_names,
scikit-learn/sklearn/tree/tests/test_export.py,66,Test with class_names,
scikit-learn/sklearn/tree/tests/test_export.py,84,Test plot_options,
scikit-learn/sklearn/tree/tests/test_export.py,106,Test max_depth,
scikit-learn/sklearn/tree/tests/test_export.py,121,Test max_depth with plot_options,
scikit-learn/sklearn/tree/tests/test_export.py,136,Test multi-output with weighted samples,
scikit-learn/sklearn/tree/tests/test_export.py,169,Test regression output with plot_options,
scikit-learn/sklearn/tree/tests/test_export.py,200,Test classifier with degraded learning set,
scikit-learn/sklearn/tree/tests/test_export.py,213,Check for errors of export_graphviz,
scikit-learn/sklearn/tree/tests/test_export.py,216,Check not-fitted decision tree error,
scikit-learn/sklearn/tree/tests/test_export.py,223,Check if it errors when length of feature_names,
scikit-learn/sklearn/tree/tests/test_export.py,224,mismatches with number of features,
scikit-learn/sklearn/tree/tests/test_export.py,235,Check error when argument is not an estimator,
scikit-learn/sklearn/tree/tests/test_export.py,240,Check class_names error,
scikit-learn/sklearn/tree/tests/test_export.py,245,Check precision error,
scikit-learn/sklearn/tree/tests/test_export.py,286,"With the current random state, the impurity and the threshold",
scikit-learn/sklearn/tree/tests/test_export.py,287,will have the number of precision set in the export_graphviz,
scikit-learn/sklearn/tree/tests/test_export.py,288,function. We will check the number of precision with a strict,
scikit-learn/sklearn/tree/tests/test_export.py,289,equality. The value reported will have only 2 precision and,
scikit-learn/sklearn/tree/tests/test_export.py,290,"therefore, only a less equal comparison will be done.",
scikit-learn/sklearn/tree/tests/test_export.py,292,check value,
scikit-learn/sklearn/tree/tests/test_export.py,297,check impurity,
scikit-learn/sklearn/tree/tests/test_export.py,303,check impurity,
scikit-learn/sklearn/tree/tests/test_export.py,307,check threshold,
scikit-learn/sklearn/tree/tests/test_export.py,343,testing that leaves at level 1 are not truncated,
scikit-learn/sklearn/tree/tests/test_export.py,345,testing that the rest of the tree is truncated,
scikit-learn/sklearn/tree/tests/test_export.py,416,mostly smoke tests,
scikit-learn/sklearn/tree/tests/test_export.py,417,Check correctness of export_graphviz for criterion = entropy,
scikit-learn/sklearn/tree/tests/test_export.py,424,Test export code,
scikit-learn/sklearn/tree/tests/test_export.py,435,mostly smoke tests,
scikit-learn/sklearn/tree/tests/test_export.py,436,Check correctness of export_graphviz for criterion = gini,
scikit-learn/sklearn/tree/tests/test_export.py,443,Test export code,
scikit-learn/sklearn/tree/tests/test_export.py,453,FIXME: to be removed in 0.25,
scikit-learn/sklearn/tree/tests/test_export.py,457,test that a warning is raised when rotate is used.,
scikit-learn/sklearn/tree/tests/test_export.py,466,Testing if not fitted tree throws the correct error,
scikit-learn/sklearn/tree/tests/test_reingold_tilford.py,29,parents higher than children:,
scikit-learn/sklearn/tree/tests/test_reingold_tilford.py,33,these trees are always binary,
scikit-learn/sklearn/tree/tests/test_reingold_tilford.py,34,parents are centered above children,
scikit-learn/sklearn/tree/tests/test_reingold_tilford.py,42,test that x values are unique per depth / level,
scikit-learn/sklearn/tree/tests/test_reingold_tilford.py,43,we could also do it quicker using defaultdicts..,
scikit-learn/sklearn/tree/tests/test_reingold_tilford.py,49,reached all leafs,
scikit-learn/sklearn/tree/tests/test_tree.py,97,toy sample,
scikit-learn/sklearn/tree/tests/test_tree.py,103,also load the iris dataset,
scikit-learn/sklearn/tree/tests/test_tree.py,104,and randomly permute it,
scikit-learn/sklearn/tree/tests/test_tree.py,111,also load the boston dataset,
scikit-learn/sklearn/tree/tests/test_tree.py,112,and randomly permute it,
scikit-learn/sklearn/tree/tests/test_tree.py,127,NB: despite their names X_sparse_* are numpy arrays (and not sparse matrices),
scikit-learn/sklearn/tree/tests/test_tree.py,183,Check classification on a toy dataset.,
scikit-learn/sklearn/tree/tests/test_tree.py,197,Check classification on a weighted toy dataset.,
scikit-learn/sklearn/tree/tests/test_tree.py,211,Check regression on a toy dataset.,
scikit-learn/sklearn/tree/tests/test_tree.py,225,Check on a XOR problem,
scikit-learn/sklearn/tree/tests/test_tree.py,246,Check consistency on dataset iris.,
scikit-learn/sklearn/tree/tests/test_tree.py,264,Check consistency on dataset boston house prices.,
scikit-learn/sklearn/tree/tests/test_tree.py,274,"using fewer features reduces the learning ability of this tree,",
scikit-learn/sklearn/tree/tests/test_tree.py,275,but reduces training time.,
scikit-learn/sklearn/tree/tests/test_tree.py,285,Predict probabilities using DecisionTreeClassifier.,
scikit-learn/sklearn/tree/tests/test_tree.py,304,Check the array representation.,
scikit-learn/sklearn/tree/tests/test_tree.py,305,Check resize,
scikit-learn/sklearn/tree/tests/test_tree.py,315,Check when y is pure.,
scikit-learn/sklearn/tree/tests/test_tree.py,333,Check numerical stability.,
scikit-learn/sklearn/tree/tests/test_tree.py,356,Check variable importances.,
scikit-learn/sklearn/tree/tests/test_tree.py,375,Check on iris that importances are the same for all builders,
scikit-learn/sklearn/tree/tests/test_tree.py,387,Check if variable importance before fit raises ValueError.,
scikit-learn/sklearn/tree/tests/test_tree.py,394,Check that gini is equivalent to mse for binary output variable,
scikit-learn/sklearn/tree/tests/test_tree.py,404,The gini index and the mean square error (variance) might differ due,
scikit-learn/sklearn/tree/tests/test_tree.py,405,to numerical instability. Since those instabilities mainly occurs at,
scikit-learn/sklearn/tree/tests/test_tree.py,406,"high tree depth, we restrict this maximal depth.",
scikit-learn/sklearn/tree/tests/test_tree.py,420,Check max_features.,
scikit-learn/sklearn/tree/tests/test_tree.py,467,use values of max_features that are invalid,
scikit-learn/sklearn/tree/tests/test_tree.py,490,Test that it gives proper exception on deficient input.,
scikit-learn/sklearn/tree/tests/test_tree.py,492,predict before fit,
scikit-learn/sklearn/tree/tests/test_tree.py,498,wrong feature shape for sample,
scikit-learn/sklearn/tree/tests/test_tree.py,527,min_impurity_split warning,
scikit-learn/sklearn/tree/tests/test_tree.py,534,Wrong dimensions,
scikit-learn/sklearn/tree/tests/test_tree.py,540,Test with arrays that are non-contiguous.,
scikit-learn/sklearn/tree/tests/test_tree.py,546,predict before fitting,
scikit-learn/sklearn/tree/tests/test_tree.py,551,predict on vector with different dims,
scikit-learn/sklearn/tree/tests/test_tree.py,557,wrong sample shape,
scikit-learn/sklearn/tree/tests/test_tree.py,574,apply before fitting,
scikit-learn/sklearn/tree/tests/test_tree.py,585,test both DepthFirstTreeBuilder and BestFirstTreeBuilder,
scikit-learn/sklearn/tree/tests/test_tree.py,586,by setting max_leaf_nodes,
scikit-learn/sklearn/tree/tests/test_tree.py,590,test for integer parameter,
scikit-learn/sklearn/tree/tests/test_tree.py,595,"count samples on nodes, -1 means it is a leaf",
scikit-learn/sklearn/tree/tests/test_tree.py,600,test for float parameter,
scikit-learn/sklearn/tree/tests/test_tree.py,605,"count samples on nodes, -1 means it is a leaf",
scikit-learn/sklearn/tree/tests/test_tree.py,612,Test if leaves contain more than leaf_count training examples,
scikit-learn/sklearn/tree/tests/test_tree.py,616,test both DepthFirstTreeBuilder and BestFirstTreeBuilder,
scikit-learn/sklearn/tree/tests/test_tree.py,617,by setting max_leaf_nodes,
scikit-learn/sklearn/tree/tests/test_tree.py,621,test integer parameter,
scikit-learn/sklearn/tree/tests/test_tree.py,628,drop inner nodes,
scikit-learn/sklearn/tree/tests/test_tree.py,632,test float parameter,
scikit-learn/sklearn/tree/tests/test_tree.py,639,drop inner nodes,
scikit-learn/sklearn/tree/tests/test_tree.py,658,test both DepthFirstTreeBuilder and BestFirstTreeBuilder,
scikit-learn/sklearn/tree/tests/test_tree.py,659,by setting max_leaf_nodes,
scikit-learn/sklearn/tree/tests/test_tree.py,673,drop inner nodes,
scikit-learn/sklearn/tree/tests/test_tree.py,681,test case with no weights passed in,
scikit-learn/sklearn/tree/tests/test_tree.py,696,drop inner nodes,
scikit-learn/sklearn/tree/tests/test_tree.py,728,test integer min_samples_leaf,
scikit-learn/sklearn/tree/tests/test_tree.py,741,drop inner nodes,
scikit-learn/sklearn/tree/tests/test_tree.py,752,test float min_samples_leaf,
scikit-learn/sklearn/tree/tests/test_tree.py,765,drop inner nodes,
scikit-learn/sklearn/tree/tests/test_tree.py,789,test if min_impurity_split creates leaves with impurity,
scikit-learn/sklearn/tree/tests/test_tree.py,790,"[0, min_impurity_split) when min_samples_leaf = 1 and",
scikit-learn/sklearn/tree/tests/test_tree.py,791,min_samples_split = 2.,
scikit-learn/sklearn/tree/tests/test_tree.py,795,test both DepthFirstTreeBuilder and BestFirstTreeBuilder,
scikit-learn/sklearn/tree/tests/test_tree.py,796,by setting max_leaf_nodes,
scikit-learn/sklearn/tree/tests/test_tree.py,801,verify leaf nodes without min_impurity_split less than,
scikit-learn/sklearn/tree/tests/test_tree.py,802,impurity 1e-7,
scikit-learn/sklearn/tree/tests/test_tree.py,820,"verify leaf nodes have impurity [0,min_impurity_split] when using",
scikit-learn/sklearn/tree/tests/test_tree.py,821,min_impurity_split,
scikit-learn/sklearn/tree/tests/test_tree.py,842,test if min_impurity_decrease ensure that a split is made only if,
scikit-learn/sklearn/tree/tests/test_tree.py,843,if the impurity decrease is atleast that value,
scikit-learn/sklearn/tree/tests/test_tree.py,846,test both DepthFirstTreeBuilder and BestFirstTreeBuilder,
scikit-learn/sklearn/tree/tests/test_tree.py,847,by setting max_leaf_nodes,
scikit-learn/sklearn/tree/tests/test_tree.py,851,"Check default value of min_impurity_decrease, 1e-7",
scikit-learn/sklearn/tree/tests/test_tree.py,853,Check with explicit value of 0.05,
scikit-learn/sklearn/tree/tests/test_tree.py,856,Check with a much lower value of 0.0001,
scikit-learn/sklearn/tree/tests/test_tree.py,859,Check with a much lower value of 0.1,
scikit-learn/sklearn/tree/tests/test_tree.py,871,"If current node is a not leaf node, check if the split was",
scikit-learn/sklearn/tree/tests/test_tree.py,872,justified w.r.t the min_impurity_decrease,
scikit-learn/sklearn/tree/tests/test_tree.py,930,Check estimators on multi-output problems.,
scikit-learn/sklearn/tree/tests/test_tree.py,960,toy classification problem,
scikit-learn/sklearn/tree/tests/test_tree.py,977,toy regression problem,
scikit-learn/sklearn/tree/tests/test_tree.py,986,Test that n_classes_ and classes_ have proper shape.,
scikit-learn/sklearn/tree/tests/test_tree.py,988,"Classification, single output",
scikit-learn/sklearn/tree/tests/test_tree.py,995,"Classification, multi-output",
scikit-learn/sklearn/tree/tests/test_tree.py,1006,Check class rebalancing.,
scikit-learn/sklearn/tree/tests/test_tree.py,1018,Check that it works no matter the memory layout,
scikit-learn/sklearn/tree/tests/test_tree.py,1023,Nothing,
scikit-learn/sklearn/tree/tests/test_tree.py,1028,C-order,
scikit-learn/sklearn/tree/tests/test_tree.py,1033,F-order,
scikit-learn/sklearn/tree/tests/test_tree.py,1038,Contiguous,
scikit-learn/sklearn/tree/tests/test_tree.py,1043,csr matrix,
scikit-learn/sklearn/tree/tests/test_tree.py,1048,csc_matrix,
scikit-learn/sklearn/tree/tests/test_tree.py,1053,Strided,
scikit-learn/sklearn/tree/tests/test_tree.py,1060,Check sample weighting.,
scikit-learn/sklearn/tree/tests/test_tree.py,1061,Test that zero-weighted samples are not taken into account,
scikit-learn/sklearn/tree/tests/test_tree.py,1073,Test that low weighted samples are not taken into account at low depth,
scikit-learn/sklearn/tree/tests/test_tree.py,1082,Samples of class '2' are still weightier,
scikit-learn/sklearn/tree/tests/test_tree.py,1087,Samples of class '2' are no longer weightier,
scikit-learn/sklearn/tree/tests/test_tree.py,1090,Threshold should have moved,
scikit-learn/sklearn/tree/tests/test_tree.py,1092,Test that sample weighting is the same as having duplicates,
scikit-learn/sklearn/tree/tests/test_tree.py,1111,Check sample weighting raises errors.,
scikit-learn/sklearn/tree/tests/test_tree.py,1132,"Iris is balanced, so no effect expected for using 'balanced' weights",
scikit-learn/sklearn/tree/tests/test_tree.py,1139,Make a multi-output problem with three copies of Iris,
scikit-learn/sklearn/tree/tests/test_tree.py,1141,Create user-defined weights that should balance over the outputs,
scikit-learn/sklearn/tree/tests/test_tree.py,1148,"Check against multi-output ""auto"" which should also have no effect",
scikit-learn/sklearn/tree/tests/test_tree.py,1153,"Inflate importance of class 1, check against user-defined weights",
scikit-learn/sklearn/tree/tests/test_tree.py,1163,Check that sample_weight and class_weight are multiplicative,
scikit-learn/sklearn/tree/tests/test_tree.py,1177,Test if class_weight raises errors and warnings when expected.,
scikit-learn/sklearn/tree/tests/test_tree.py,1181,Invalid preset string,
scikit-learn/sklearn/tree/tests/test_tree.py,1188,Not a list or preset for multi-output,
scikit-learn/sklearn/tree/tests/test_tree.py,1193,Incorrect length list for multi-output,
scikit-learn/sklearn/tree/tests/test_tree.py,1205,Test greedy trees with max_depth + 1 leafs.,
scikit-learn/sklearn/tree/tests/test_tree.py,1212,"max_leaf_nodes in (0, 1) should raise ValueError",
scikit-learn/sklearn/tree/tests/test_tree.py,1225,Test precedence of max_leaf_nodes over max_depth.,
scikit-learn/sklearn/tree/tests/test_tree.py,1234,Ensure property arrays' memory stays alive when tree disappears,
scikit-learn/sklearn/tree/tests/test_tree.py,1235,non-regression for #2726,
scikit-learn/sklearn/tree/tests/test_tree.py,1240,"if pointing to freed memory, contents may be arbitrary",
scikit-learn/sklearn/tree/tests/test_tree.py,1260,do not check extra random trees,
scikit-learn/sklearn/tree/tests/test_tree.py,1287,Test if the warning for too large inputs is appropriate.,
scikit-learn/sklearn/tree/tests/test_tree.py,1308,Sanity check: we cannot request more memory than the size of the address,
scikit-learn/sklearn/tree/tests/test_tree.py,1309,space. Currently raises OverflowError.,
scikit-learn/sklearn/tree/tests/test_tree.py,1315,Non-regression test: MemoryError used to be dropped by Cython,
scikit-learn/sklearn/tree/tests/test_tree.py,1316,"because of missing ""except *"".",
scikit-learn/sklearn/tree/tests/test_tree.py,1329,Gain testing time,
scikit-learn/sklearn/tree/tests/test_tree.py,1339,Check the default (depth first search),
scikit-learn/sklearn/tree/tests/test_tree.py,1380,"Due to numerical instability of MSE and too strict test, we limit the",
scikit-learn/sklearn/tree/tests/test_tree.py,1381,maximal depth,
scikit-learn/sklearn/tree/tests/test_tree.py,1391,Check max_features,
scikit-learn/sklearn/tree/tests/test_tree.py,1400,Check min_samples_split,
scikit-learn/sklearn/tree/tests/test_tree.py,1410,Check min_samples_leaf,
scikit-learn/sklearn/tree/tests/test_tree.py,1420,Check best-first search,
scikit-learn/sklearn/tree/tests/test_tree.py,1435,Check various criterion,
scikit-learn/sklearn/tree/tests/test_tree.py,1462,n_samples set n_feature to ease construction of a simultaneous,
scikit-learn/sklearn/tree/tests/test_tree.py,1463,construction of a csr and csc matrix,
scikit-learn/sklearn/tree/tests/test_tree.py,1467,"Generate X, y",
scikit-learn/sklearn/tree/tests/test_tree.py,1492,"Ensure that X_sparse_test owns its data, indices and indptr array",
scikit-learn/sklearn/tree/tests/test_tree.py,1495,Ensure that we have explicit zeros,
scikit-learn/sklearn/tree/tests/test_tree.py,1499,Perform the comparison,
scikit-learn/sklearn/tree/tests/test_tree.py,1614,TODO: remove in v0.24,
scikit-learn/sklearn/tree/tests/test_tree.py,1644,Assert that leaves index are correct,
scikit-learn/sklearn/tree/tests/test_tree.py,1649,Ensure only one leave node per sample,
scikit-learn/sklearn/tree/tests/test_tree.py,1654,Ensure max depth is consistent with sum of indicator,
scikit-learn/sklearn/tree/tests/test_tree.py,1673,Currently we don't support sparse y,
scikit-learn/sklearn/tree/tests/test_tree.py,1754,Test MAE where sample weights are non-uniform (as illustrated above):,
scikit-learn/sklearn/tree/tests/test_tree.py,1760,Test MAE where all sample weights are uniform:,
scikit-learn/sklearn/tree/tests/test_tree.py,1766,Test MAE where a `sample_weight` is not explicitly provided.,
scikit-learn/sklearn/tree/tests/test_tree.py,1767,"This is equivalent to providing uniform sample weights, though",
scikit-learn/sklearn/tree/tests/test_tree.py,1768,the internal logic is different:,
scikit-learn/sklearn/tree/tests/test_tree.py,1775,Let's check whether copy of our criterion has the same type,
scikit-learn/sklearn/tree/tests/test_tree.py,1776,and properties as original,
scikit-learn/sklearn/tree/tests/test_tree.py,1802,try to make empty leaf by using near infinite value.,
scikit-learn/sklearn/tree/tests/test_tree.py,1857,single node tree,
scikit-learn/sklearn/tree/tests/test_tree.py,1861,pruned single node tree,
scikit-learn/sklearn/tree/tests/test_tree.py,1869,generate trees with increasing alphas,
scikit-learn/sklearn/tree/tests/test_tree.py,1876,A pruned tree must be a subtree of the previous tree (which had a,
scikit-learn/sklearn/tree/tests/test_tree.py,1877,smaller ccp_alpha),
scikit-learn/sklearn/tree/tests/test_tree.py,1905,is a leaf,
scikit-learn/sklearn/tree/tests/test_tree.py,1909,not a leaf,
scikit-learn/sklearn/preprocessing/_discretization.py,1,-*- coding: utf-8 -*-,
scikit-learn/sklearn/preprocessing/_discretization.py,3,Author: Henry Lin <hlin117@gmail.com>,
scikit-learn/sklearn/preprocessing/_discretization.py,4,Tom Dupré la Tour,
scikit-learn/sklearn/preprocessing/_discretization.py,6,License: BSD,
scikit-learn/sklearn/preprocessing/_discretization.py,176,fixes import loops,
scikit-learn/sklearn/preprocessing/_discretization.py,178,Deterministic initialization with uniform spacing,
scikit-learn/sklearn/preprocessing/_discretization.py,182,1D k-means procedure,
scikit-learn/sklearn/preprocessing/_discretization.py,185,"Must sort, centers may be unsorted even with sorted init",
scikit-learn/sklearn/preprocessing/_discretization.py,190,"Remove bins whose width are too small (i.e., <= 1e-8)",
scikit-learn/sklearn/preprocessing/_discretization.py,207,Fit the OneHotEncoder with toy datasets,
scikit-learn/sklearn/preprocessing/_discretization.py,208,so that it's ready for use after the KBinsDiscretizer is fitted,
scikit-learn/sklearn/preprocessing/_discretization.py,271,Values which are close to a bin edge are susceptible to numeric,
scikit-learn/sklearn/preprocessing/_discretization.py,272,instability. Add eps to X so these values are binned correctly,
scikit-learn/sklearn/preprocessing/_discretization.py,273,with respect to their decimal truncation. See documentation of,
scikit-learn/sklearn/preprocessing/_discretization.py,274,numpy.isclose for an explanation of ``rtol`` and ``atol``.,
scikit-learn/sklearn/preprocessing/_encoders.py,1,Authors: Andreas Mueller <amueller@ais.uni-bonn.de>,
scikit-learn/sklearn/preprocessing/_encoders.py,2,Joris Van den Bossche <jorisvandenbossche@gmail.com>,
scikit-learn/sklearn/preprocessing/_encoders.py,3,License: BSD 3 clause,
scikit-learn/sklearn/preprocessing/_encoders.py,41,"if not a dataframe, do normal check_array validation",
scikit-learn/sklearn/preprocessing/_encoders.py,50,"pandas dataframe, do validation later column by column, in order",
scikit-learn/sklearn/preprocessing/_encoders.py,51,to keep the dtype information to be used in the encoder.,
scikit-learn/sklearn/preprocessing/_encoders.py,67,pandas dataframes,
scikit-learn/sklearn/preprocessing/_encoders.py,69,"numpy arrays, sparse arrays",
scikit-learn/sklearn/preprocessing/_encoders.py,125,Set the problematic rows to an acceptable value and,
scikit-learn/sklearn/preprocessing/_encoders.py,126,continue `The rows are marked `X_mask` and will be,
scikit-learn/sklearn/preprocessing/_encoders.py,127,removed later.,
scikit-learn/sklearn/preprocessing/_encoders.py,129,cast Xi into the largest string type necessary,
scikit-learn/sklearn/preprocessing/_encoders.py,130,to handle different lengths of numpy strings,
scikit-learn/sklearn/preprocessing/_encoders.py,138,"We use check_unknown=False, since _encode_check_unknown was",
scikit-learn/sklearn/preprocessing/_encoders.py,139,already called above.,
scikit-learn/sklearn/preprocessing/_encoders.py,308,If we have both dropped columns and ignored unknown,
scikit-learn/sklearn/preprocessing/_encoders.py,309,"values, there will be ambiguous cells. This creates difficulties",
scikit-learn/sklearn/preprocessing/_encoders.py,310,in interpreting the model.,
scikit-learn/sklearn/preprocessing/_encoders.py,423,validation of X happens in _check_X called by _transform,
scikit-learn/sklearn/preprocessing/_encoders.py,430,"We remove all the dropped categories from mask, and decrement all",
scikit-learn/sklearn/preprocessing/_encoders.py,431,categories that occur after them to avoid an empty column.,
scikit-learn/sklearn/preprocessing/_encoders.py,437,drop='if_binary' but feature isn't binary,
scikit-learn/sklearn/preprocessing/_encoders.py,439,set to cardinality to not drop from X_int,
scikit-learn/sklearn/preprocessing/_encoders.py,442,dropped,
scikit-learn/sklearn/preprocessing/_encoders.py,500,validate shape of passed X,
scikit-learn/sklearn/preprocessing/_encoders.py,506,create resulting array of appropriate dtype,
scikit-learn/sklearn/preprocessing/_encoders.py,520,Only happens if there was a column with a unique,
scikit-learn/sklearn/preprocessing/_encoders.py,521,category. In this case we just fill the column with this,
scikit-learn/sklearn/preprocessing/_encoders.py,522,unique category value.,
scikit-learn/sklearn/preprocessing/_encoders.py,528,"for sparse X argmax returns 2D matrix, ensure 1D array",
scikit-learn/sklearn/preprocessing/_encoders.py,533,ignored unknown categories: we have a row of all zero,
scikit-learn/sklearn/preprocessing/_encoders.py,536,drop will either be None or handle_unknown will be error. If,
scikit-learn/sklearn/preprocessing/_encoders.py,537,"self.drop_idx_ is not None, then we can safely assume that all of",
scikit-learn/sklearn/preprocessing/_encoders.py,538,the nulls in each column are the dropped value,
scikit-learn/sklearn/preprocessing/_encoders.py,546,if ignored are found: potentially need to upcast result to,
scikit-learn/sklearn/preprocessing/_encoders.py,547,insert None values,
scikit-learn/sklearn/preprocessing/_encoders.py,718,validate shape of passed X,
scikit-learn/sklearn/preprocessing/_encoders.py,724,create resulting array of appropriate dtype,
scikit-learn/sklearn/preprocessing/_data.py,1,Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/sklearn/preprocessing/_data.py,2,Mathieu Blondel <mathieu@mblondel.org>,
scikit-learn/sklearn/preprocessing/_data.py,3,Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/sklearn/preprocessing/_data.py,4,Andreas Mueller <amueller@ais.uni-bonn.de>,
scikit-learn/sklearn/preprocessing/_data.py,5,Eric Martin <eric@ericmart.in>,
scikit-learn/sklearn/preprocessing/_data.py,6,Giorgio Patrini <giorgio.patrini@anu.edu.au>,
scikit-learn/sklearn/preprocessing/_data.py,7,Eric Chang <ericchang2017@u.northwestern.edu>,
scikit-learn/sklearn/preprocessing/_data.py,8,License: BSD 3 clause,
scikit-learn/sklearn/preprocessing/_data.py,68,"if we are fitting on 1D arrays, scale might be a scalar",
scikit-learn/sklearn/preprocessing/_data.py,75,New array to avoid side-effects,
scikit-learn/sklearn/preprocessing/_data.py,139,noqa,
scikit-learn/sklearn/preprocessing/_data.py,161,Xr is a view on the original array that enables easy use of,
scikit-learn/sklearn/preprocessing/_data.py,162,broadcasting on the axis in which we are interested in,
scikit-learn/sklearn/preprocessing/_data.py,167,Verify that mean_1 is 'close to zero'. If X contains very,
scikit-learn/sklearn/preprocessing/_data.py,168,"large values, mean_1 can also be very large, due to a lack of",
scikit-learn/sklearn/preprocessing/_data.py,169,"precision of mean_. In this case, a pre-scaling of the",
scikit-learn/sklearn/preprocessing/_data.py,170,"concerned feature is efficient, for instance by its mean or",
scikit-learn/sklearn/preprocessing/_data.py,171,maximum.,
scikit-learn/sklearn/preprocessing/_data.py,184,"If mean_2 is not 'close to zero', it comes from the fact that",
scikit-learn/sklearn/preprocessing/_data.py,185,"scale_ is very small so that mean_2 = mean_1/scale_ > 0, even",
scikit-learn/sklearn/preprocessing/_data.py,186,if mean_1 was close to zero. The problem is thus essentially,
scikit-learn/sklearn/preprocessing/_data.py,187,due to the lack of precision of mean_. A solution is then to,
scikit-learn/sklearn/preprocessing/_data.py,188,subtract the mean again:,
scikit-learn/sklearn/preprocessing/_data.py,304,"Checking one attribute is enough, becase they are all set together",
scikit-learn/sklearn/preprocessing/_data.py,305,in partial_fit,
scikit-learn/sklearn/preprocessing/_data.py,332,Reset internal state before fitting,
scikit-learn/sklearn/preprocessing/_data.py,492,noqa,
scikit-learn/sklearn/preprocessing/_data.py,493,"Unlike the scaler object, this function allows 1d input.",
scikit-learn/sklearn/preprocessing/_data.py,494,"If copy is required, it will be done inside the scaler object.",
scikit-learn/sklearn/preprocessing/_data.py,627,noqa,
scikit-learn/sklearn/preprocessing/_data.py,640,"Checking one attribute is enough, becase they are all set together",
scikit-learn/sklearn/preprocessing/_data.py,641,in partial_fit,
scikit-learn/sklearn/preprocessing/_data.py,661,Reset internal state before fitting,
scikit-learn/sklearn/preprocessing/_data.py,696,"Even in the case of `with_mean=False`, we update the mean anyway",
scikit-learn/sklearn/preprocessing/_data.py,697,This is needed for the incremental computation of the var,
scikit-learn/sklearn/preprocessing/_data.py,698,See incr_mean_variance_axis and _incremental_mean_variance_axis,
scikit-learn/sklearn/preprocessing/_data.py,700,"if n_samples_seen_ is an integer (i.e. no missing values), we need to",
scikit-learn/sklearn/preprocessing/_data.py,701,"transform it to a NumPy array of shape (n_features,) required by",
scikit-learn/sklearn/preprocessing/_data.py,702,incr_mean_variance_axis and _incremental_variance_axis,
scikit-learn/sklearn/preprocessing/_data.py,725,First pass,
scikit-learn/sklearn/preprocessing/_data.py,728,Next passes,
scikit-learn/sklearn/preprocessing/_data.py,744,First pass,
scikit-learn/sklearn/preprocessing/_data.py,761,"for backward-compatibility, reduce n_samples_seen_ to an integer",
scikit-learn/sklearn/preprocessing/_data.py,762,if the number of samples is the same for each feature (i.e. no,
scikit-learn/sklearn/preprocessing/_data.py,763,missing values),
scikit-learn/sklearn/preprocessing/_data.py,920,"Checking one attribute is enough, becase they are all set together",
scikit-learn/sklearn/preprocessing/_data.py,921,in partial_fit,
scikit-learn/sklearn/preprocessing/_data.py,937,Reset internal state before fitting,
scikit-learn/sklearn/preprocessing/_data.py,1062,noqa,
scikit-learn/sklearn/preprocessing/_data.py,1063,"Unlike the scaler object, this function allows 1d input.",
scikit-learn/sklearn/preprocessing/_data.py,1065,"If copy is required, it will be done inside the scaler object.",
scikit-learn/sklearn/preprocessing/_data.py,1192,"at fit, convert sparse matrices to csc for optimized computation of",
scikit-learn/sklearn/preprocessing/_data.py,1193,the quantiles,
scikit-learn/sklearn/preprocessing/_data.py,1588,What follows is a faster implementation of:,
scikit-learn/sklearn/preprocessing/_data.py,1589,"for i, comb in enumerate(combinations):",
scikit-learn/sklearn/preprocessing/_data.py,1590,"XP[:, i] = X[:, comb].prod(1)",
scikit-learn/sklearn/preprocessing/_data.py,1591,This implementation uses two optimisations.,
scikit-learn/sklearn/preprocessing/_data.py,1592,"First one is broadcasting,",
scikit-learn/sklearn/preprocessing/_data.py,1593,"multiply ([X1, ..., Xn], X1) -> [X1 X1, ..., Xn X1]",
scikit-learn/sklearn/preprocessing/_data.py,1594,"multiply ([X2, ..., Xn], X2) -> [X2 X2, ..., Xn X2]",
scikit-learn/sklearn/preprocessing/_data.py,1595,...,
scikit-learn/sklearn/preprocessing/_data.py,1596,"multiply ([X[:, start:end], X[:, start]) -> ...",
scikit-learn/sklearn/preprocessing/_data.py,1597,Second optimisation happens for degrees >= 3.,
scikit-learn/sklearn/preprocessing/_data.py,1598,Xi^3 is computed reusing previous computation:,
scikit-learn/sklearn/preprocessing/_data.py,1599,Xi^3 = Xi^2 * Xi.,
scikit-learn/sklearn/preprocessing/_data.py,1607,d = 0,
scikit-learn/sklearn/preprocessing/_data.py,1614,d >= 1,
scikit-learn/sklearn/preprocessing/_data.py,1627,"XP[:, start:end] are terms of degree d - 1",
scikit-learn/sklearn/preprocessing/_data.py,1628,that exclude feature #feature_idx.,
scikit-learn/sklearn/preprocessing/_data.py,2012,Needed for backported inspect.signature compatibility with PyPy,
scikit-learn/sklearn/preprocessing/_data.py,2106,Shift columns to the right.,
scikit-learn/sklearn/preprocessing/_data.py,2108,Column indices of dummy feature are 0 everywhere.,
scikit-learn/sklearn/preprocessing/_data.py,2110,"Row indices of dummy feature are 0, ..., n_samples-1.",
scikit-learn/sklearn/preprocessing/_data.py,2112,Prepend the dummy feature n_samples times.,
scikit-learn/sklearn/preprocessing/_data.py,2116,Shift index pointers since we need to add n_samples elements.,
scikit-learn/sklearn/preprocessing/_data.py,2118,indptr[0] must be 0.,
scikit-learn/sklearn/preprocessing/_data.py,2120,"Row indices of dummy feature are 0, ..., n_samples-1.",
scikit-learn/sklearn/preprocessing/_data.py,2122,Prepend the dummy feature n_samples times.,
scikit-learn/sklearn/preprocessing/_data.py,2265,"Due to floating-point precision error in `np.nanpercentile`,",
scikit-learn/sklearn/preprocessing/_data.py,2266,make sure that quantiles are monotonically increasing.,
scikit-learn/sklearn/preprocessing/_data.py,2267,Upstream issue in numpy:,
scikit-learn/sklearn/preprocessing/_data.py,2268,https://github.com/numpy/numpy/issues/14685,
scikit-learn/sklearn/preprocessing/_data.py,2306,"if no nnz, an error will be raised for computing the",
scikit-learn/sklearn/preprocessing/_data.py,2307,quantiles. Force the quantiles to be zeros.,
scikit-learn/sklearn/preprocessing/_data.py,2313,"due to floating-point precision error in `np.nanpercentile`,",
scikit-learn/sklearn/preprocessing/_data.py,2314,make sure the quantiles are monotonically increasing,
scikit-learn/sklearn/preprocessing/_data.py,2315,Upstream issue in numpy:,
scikit-learn/sklearn/preprocessing/_data.py,2316,https://github.com/numpy/numpy/issues/14685,
scikit-learn/sklearn/preprocessing/_data.py,2362,Create the quantiles of reference,
scikit-learn/sklearn/preprocessing/_data.py,2387,"for inverse transform, match a uniform distribution",
scikit-learn/sklearn/preprocessing/_data.py,2388,hide NaN comparison warnings,
scikit-learn/sklearn/preprocessing/_data.py,2391,else output distribution is already a uniform distribution,
scikit-learn/sklearn/preprocessing/_data.py,2393,find index for lower and higher bounds,
scikit-learn/sklearn/preprocessing/_data.py,2394,hide NaN comparison warnings,
scikit-learn/sklearn/preprocessing/_data.py,2407,Interpolate in one direction and in the other and take the,
scikit-learn/sklearn/preprocessing/_data.py,2408,mean. This is in case of repeated values in the features,
scikit-learn/sklearn/preprocessing/_data.py,2409,and hence repeated quantiles,
scikit-learn/sklearn/preprocessing/_data.py,2410,,
scikit-learn/sklearn/preprocessing/_data.py,2411,"If we don't do this, only one extreme of the duplicated is",
scikit-learn/sklearn/preprocessing/_data.py,2412,"used (the upper when we do ascending, and the",
scikit-learn/sklearn/preprocessing/_data.py,2413,lower for descending). We take the mean of these two,
scikit-learn/sklearn/preprocessing/_data.py,2424,"for forward transform, match the output distribution",
scikit-learn/sklearn/preprocessing/_data.py,2426,hide NaN comparison warnings,
scikit-learn/sklearn/preprocessing/_data.py,2429,find the value to clip the data to avoid mapping to,
scikit-learn/sklearn/preprocessing/_data.py,2430,infinity. Clip such that the inverse transform will be,
scikit-learn/sklearn/preprocessing/_data.py,2431,consistent,
scikit-learn/sklearn/preprocessing/_data.py,2436,else output distribution is uniform and the ppf is the,
scikit-learn/sklearn/preprocessing/_data.py,2437,identity function so we let X_col unchanged,
scikit-learn/sklearn/preprocessing/_data.py,2444,"In theory reset should be equal to `in_fit`, but there are tests",
scikit-learn/sklearn/preprocessing/_data.py,2445,checking the input number of feature and they expect a specific,
scikit-learn/sklearn/preprocessing/_data.py,2446,"string, which is not the same one raised by check_n_features. So we",
scikit-learn/sklearn/preprocessing/_data.py,2447,don't check n_features_in_ here for now (it's done with adhoc code in,
scikit-learn/sklearn/preprocessing/_data.py,2448,the estimator anyway).,
scikit-learn/sklearn/preprocessing/_data.py,2449,TODO: set reset=in_fit when addressing reset in,
scikit-learn/sklearn/preprocessing/_data.py,2450,predict/transform/etc.,
scikit-learn/sklearn/preprocessing/_data.py,2457,we only accept positive sparse matrix when ignore_implicit_zeros is,
scikit-learn/sklearn/preprocessing/_data.py,2458,false and that we call fit or transform.,
scikit-learn/sklearn/preprocessing/_data.py,2459,hide NaN comparison warnings,
scikit-learn/sklearn/preprocessing/_data.py,2465,check the output distribution,
scikit-learn/sklearn/preprocessing/_data.py,2476,check that the dimension of X are adequate with the fitted data,
scikit-learn/sklearn/preprocessing/_data.py,2799,if call from fit(),
scikit-learn/sklearn/preprocessing/_data.py,2800,force copy so that fit does not change X inplace,
scikit-learn/sklearn/preprocessing/_data.py,2805,hide NaN warnings,
scikit-learn/sklearn/preprocessing/_data.py,2813,hide NaN warnings,
scikit-learn/sklearn/preprocessing/_data.py,2846,hide NaN warnings,
scikit-learn/sklearn/preprocessing/_data.py,2895,hide NaN warnings,
scikit-learn/sklearn/preprocessing/_data.py,2918,when x >= 0,
scikit-learn/sklearn/preprocessing/_data.py,2921,lmbda != 0,
scikit-learn/sklearn/preprocessing/_data.py,2924,when x < 0,
scikit-learn/sklearn/preprocessing/_data.py,2928,lmbda == 2,
scikit-learn/sklearn/preprocessing/_data.py,2939,binary mask,
scikit-learn/sklearn/preprocessing/_data.py,2941,when x >= 0,
scikit-learn/sklearn/preprocessing/_data.py,2944,lmbda != 0,
scikit-learn/sklearn/preprocessing/_data.py,2947,when x < 0,
scikit-learn/sklearn/preprocessing/_data.py,2950,lmbda == 2,
scikit-learn/sklearn/preprocessing/_data.py,2961,the computation of lambda is influenced by NaNs so we need to,
scikit-learn/sklearn/preprocessing/_data.py,2962,get rid of them,
scikit-learn/sklearn/preprocessing/_data.py,2985,the computation of lambda is influenced by NaNs so we need to,
scikit-learn/sklearn/preprocessing/_data.py,2986,get rid of them,
scikit-learn/sklearn/preprocessing/_data.py,2988,"choosing bracket -2, 2 like for boxcox",
scikit-learn/sklearn/preprocessing/_label.py,1,Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/sklearn/preprocessing/_label.py,2,Mathieu Blondel <mathieu@mblondel.org>,
scikit-learn/sklearn/preprocessing/_label.py,3,Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/sklearn/preprocessing/_label.py,4,Andreas Mueller <amueller@ais.uni-bonn.de>,
scikit-learn/sklearn/preprocessing/_label.py,5,Joel Nothman <joel.nothman@gmail.com>,
scikit-learn/sklearn/preprocessing/_label.py,6,Hamzeh Alsalhi <ha258@cornell.edu>,
scikit-learn/sklearn/preprocessing/_label.py,7,License: BSD 3 clause,
scikit-learn/sklearn/preprocessing/_label.py,37,"only used in _encode below, see docstring there for details",
scikit-learn/sklearn/preprocessing/_label.py,43,unique sorts,
scikit-learn/sklearn/preprocessing/_label.py,58,"only used in _encode below, see docstring there for details",
scikit-learn/sklearn/preprocessing/_label.py,272,transform of empty array is empty array,
scikit-learn/sklearn/preprocessing/_label.py,293,inverse transform of empty array is empty array,
scikit-learn/sklearn/preprocessing/_label.py,604,XXX Workaround that will be removed when list of list format is,
scikit-learn/sklearn/preprocessing/_label.py,605,dropped,
scikit-learn/sklearn/preprocessing/_label.py,620,To account for pos_label == 0 in the dense case,
scikit-learn/sklearn/preprocessing/_label.py,658,pick out the known labels from y,
scikit-learn/sklearn/preprocessing/_label.py,690,preserve label ordering,
scikit-learn/sklearn/preprocessing/_label.py,712,Find the argmax for each row in y where y is a CSR matrix,
scikit-learn/sklearn/preprocessing/_label.py,721,picks out all indices obtaining the maximum per row,
scikit-learn/sklearn/preprocessing/_label.py,724,For corner case where last row has a max of 0,
scikit-learn/sklearn/preprocessing/_label.py,728,Gets the index of the first argmax in each row from y_i_all_argmax,
scikit-learn/sklearn/preprocessing/_label.py,730,first argmax of each row,
scikit-learn/sklearn/preprocessing/_label.py,733,Handle rows of all 0,
scikit-learn/sklearn/preprocessing/_label.py,736,Handles rows with max of 0 that contain negative numbers,
scikit-learn/sklearn/preprocessing/_label.py,761,Perform thresholding,
scikit-learn/sklearn/preprocessing/_label.py,773,Inverse transform data,
scikit-learn/sklearn/preprocessing/_label.py,907,Automatically increment on new class,
scikit-learn/sklearn/preprocessing/_label.py,912,sort classes and reorder columns,
scikit-learn/sklearn/preprocessing/_label.py,915,(make safe for tuples),
scikit-learn/sklearn/preprocessing/_label.py,920,ensure yt.indices keeps its current dtype,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1,Authors:,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2,,
scikit-learn/sklearn/preprocessing/tests/test_data.py,3,Giorgio Patrini,
scikit-learn/sklearn/preprocessing/tests/test_data.py,4,,
scikit-learn/sklearn/preprocessing/tests/test_data.py,5,License: BSD 3 clause,
scikit-learn/sklearn/preprocessing/tests/test_data.py,63,Make some data to be used many times,
scikit-learn/sklearn/preprocessing/tests/test_data.py,96,Test Polynomial Features,
scikit-learn/sklearn/preprocessing/tests/test_data.py,142,test some unicode,
scikit-learn/sklearn/preprocessing/tests/test_data.py,248,This degree should always be one more than the highest degree supported by,
scikit-learn/sklearn/preprocessing/tests/test_data.py,249,_csr_expansion.,
scikit-learn/sklearn/preprocessing/tests/test_data.py,292,Test scaling of dataset along single axis,
scikit-learn/sklearn/preprocessing/tests/test_data.py,299,cast only after scaling done,
scikit-learn/sklearn/preprocessing/tests/test_data.py,317,check inverse transform,
scikit-learn/sklearn/preprocessing/tests/test_data.py,321,Constant feature,
scikit-learn/sklearn/preprocessing/tests/test_data.py,333,Ensure scaling does not affect dtype,
scikit-learn/sklearn/preprocessing/tests/test_data.py,347,1-d inputs,
scikit-learn/sklearn/preprocessing/tests/test_data.py,360,Test numerical stability of scaling,
scikit-learn/sklearn/preprocessing/tests/test_data.py,361,np.log(1e-5) is taken because of its floating point representation,
scikit-learn/sklearn/preprocessing/tests/test_data.py,362,was empirically found to cause numerical problems with np.mean & np.std.,
scikit-learn/sklearn/preprocessing/tests/test_data.py,365,This does not raise a warning as the number of samples is too low,
scikit-learn/sklearn/preprocessing/tests/test_data.py,366,to trigger the problem in recent numpy,
scikit-learn/sklearn/preprocessing/tests/test_data.py,370,"with 2 more samples, the std computation run into numerical issues:",
scikit-learn/sklearn/preprocessing/tests/test_data.py,380,Large values can cause (often recoverable) numerical stability issues:,
scikit-learn/sklearn/preprocessing/tests/test_data.py,394,Test scaling of 2d array along first axis,
scikit-learn/sklearn/preprocessing/tests/test_data.py,399,first feature is always of zero,
scikit-learn/sklearn/preprocessing/tests/test_data.py,408,Check that X has been copied,
scikit-learn/sklearn/preprocessing/tests/test_data.py,411,check inverse transform,
scikit-learn/sklearn/preprocessing/tests/test_data.py,424,Check that the data hasn't been modified,
scikit-learn/sklearn/preprocessing/tests/test_data.py,431,Check that X has not been copied,
scikit-learn/sklearn/preprocessing/tests/test_data.py,435,"first feature is a constant, non zero feature",
scikit-learn/sklearn/preprocessing/tests/test_data.py,441,Check that X has not been copied,
scikit-learn/sklearn/preprocessing/tests/test_data.py,446,Test if the scaler will not overflow on float16 numpy arrays,
scikit-learn/sklearn/preprocessing/tests/test_data.py,448,float16 has a maximum of 65500.0. On the worst case 5 * 200000 is 100000,
scikit-learn/sklearn/preprocessing/tests/test_data.py,449,which is enough to overflow the data type,
scikit-learn/sklearn/preprocessing/tests/test_data.py,456,Calculate the float64 equivalent to verify result,
scikit-learn/sklearn/preprocessing/tests/test_data.py,459,"Overflow calculations may cause -inf, inf, or nan. Since there is no nan",
scikit-learn/sklearn/preprocessing/tests/test_data.py,460,"input, all of the outputs should be finite. This may be redundant since a",
scikit-learn/sklearn/preprocessing/tests/test_data.py,461,FloatingPointError exception will be thrown on overflow above.,
scikit-learn/sklearn/preprocessing/tests/test_data.py,464,The normal distribution is very unlikely to go above 4. At 4.0-8.0 the,
scikit-learn/sklearn/preprocessing/tests/test_data.py,465,float16 precision is 2^-8 which is around 0.004. Thus only 2 decimals are,
scikit-learn/sklearn/preprocessing/tests/test_data.py,466,checked to account for precision differences.,
scikit-learn/sklearn/preprocessing/tests/test_data.py,480,Test if partial_fit run over many batches of size 1 and 50,
scikit-learn/sklearn/preprocessing/tests/test_data.py,481,gives the same results as fit,
scikit-learn/sklearn/preprocessing/tests/test_data.py,486,Test mean at the end of the process,
scikit-learn/sklearn/preprocessing/tests/test_data.py,503,Test std after 1 step,
scikit-learn/sklearn/preprocessing/tests/test_data.py,518,"Test std until the end of partial fits, and",
scikit-learn/sklearn/preprocessing/tests/test_data.py,520,Clean estimator,
scikit-learn/sklearn/preprocessing/tests/test_data.py,530,Test if partial_fit run over many batches of size 1 and 50,
scikit-learn/sklearn/preprocessing/tests/test_data.py,531,gives the same results as fit,
scikit-learn/sklearn/preprocessing/tests/test_data.py,536,Test mean at the end of the process,
scikit-learn/sklearn/preprocessing/tests/test_data.py,544,Nones,
scikit-learn/sklearn/preprocessing/tests/test_data.py,547,Test std after 1 step,
scikit-learn/sklearn/preprocessing/tests/test_data.py,559,no constants,
scikit-learn/sklearn/preprocessing/tests/test_data.py,561,"Test std until the end of partial fits, and",
scikit-learn/sklearn/preprocessing/tests/test_data.py,563,Clean estimator,
scikit-learn/sklearn/preprocessing/tests/test_data.py,576,Test if the incremental computation introduces significative errors,
scikit-learn/sklearn/preprocessing/tests/test_data.py,577,for large datasets with values of large magniture,
scikit-learn/sklearn/preprocessing/tests/test_data.py,590,"Regardless of abs values, they must not be more diff 6 significant digits",
scikit-learn/sklearn/preprocessing/tests/test_data.py,595,NOTE Be aware that for much larger offsets std is very unstable (last,
scikit-learn/sklearn/preprocessing/tests/test_data.py,596,assert) while mean is OK.,
scikit-learn/sklearn/preprocessing/tests/test_data.py,598,Sparse input,
scikit-learn/sklearn/preprocessing/tests/test_data.py,606,with_mean=False is required with sparse input,
scikit-learn/sklearn/preprocessing/tests/test_data.py,611,chunk = sparse.csr_matrix(data_chunks),
scikit-learn/sklearn/preprocessing/tests/test_data.py,614,"Regardless of magnitude, they must not differ more than of 6 digits",
scikit-learn/sklearn/preprocessing/tests/test_data.py,622,Check that sparsity is not destroyed,
scikit-learn/sklearn/preprocessing/tests/test_data.py,638,Check some postconditions after applying partial_fit and transform,
scikit-learn/sklearn/preprocessing/tests/test_data.py,652,No change,
scikit-learn/sklearn/preprocessing/tests/test_data.py,658,as less or equal,
scikit-learn/sklearn/preprocessing/tests/test_data.py,660,(i+1) because the Scaler has been already fitted,
scikit-learn/sklearn/preprocessing/tests/test_data.py,667,default params,
scikit-learn/sklearn/preprocessing/tests/test_data.py,674,"not default params: min=1, max=2",
scikit-learn/sklearn/preprocessing/tests/test_data.py,682,"min=-.5, max=.6",
scikit-learn/sklearn/preprocessing/tests/test_data.py,690,raises on invalid range,
scikit-learn/sklearn/preprocessing/tests/test_data.py,697,Check min max scaler on toy data with zero variance features,
scikit-learn/sklearn/preprocessing/tests/test_data.py,706,default params,
scikit-learn/sklearn/preprocessing/tests/test_data.py,722,not default params,
scikit-learn/sklearn/preprocessing/tests/test_data.py,730,function interface,
scikit-learn/sklearn/preprocessing/tests/test_data.py,745,Test scaling of dataset along single axis,
scikit-learn/sklearn/preprocessing/tests/test_data.py,752,cast only after scaling done,
scikit-learn/sklearn/preprocessing/tests/test_data.py,764,check inverse transform,
scikit-learn/sklearn/preprocessing/tests/test_data.py,768,Constant feature,
scikit-learn/sklearn/preprocessing/tests/test_data.py,776,Function interface,
scikit-learn/sklearn/preprocessing/tests/test_data.py,787,first feature is always of zero,
scikit-learn/sklearn/preprocessing/tests/test_data.py,830,Check that X has not been modified (copy),
scikit-learn/sklearn/preprocessing/tests/test_data.py,880,test that the scaler return identity when with_mean and with_std are,
scikit-learn/sklearn/preprocessing/tests/test_data.py,881,False,
scikit-learn/sklearn/preprocessing/tests/test_data.py,930,test that scaler converts integer input to floating,
scikit-learn/sklearn/preprocessing/tests/test_data.py,931,for both sparse and dense matrices,
scikit-learn/sklearn/preprocessing/tests/test_data.py,934,first feature is always of zero,
scikit-learn/sklearn/preprocessing/tests/test_data.py,978,Check that X has not been modified (copy),
scikit-learn/sklearn/preprocessing/tests/test_data.py,999,Check that StandardScaler.fit does not change input,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1002,first feature is always of zero,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1025,check scaling and fit with direct calls on sparse data,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1036,check transform and inverse_transform after a fit on a dense array,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1053,Check if non finite inputs raise ValueError,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1073,check consistent type of attributes,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1092,check that the scaler is working when there is not data materialized in a,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1093,column of a sparse matrix,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1107,Test robust scaling of 2d array along first axis,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1110,first feature is always of zero,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1123,Check the equivalence of the fitting with dense and sparse matrices,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1143,Check RobustScaler on transforming csr matrix with one row,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1182,uniform output distribution,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1187,normal output distribution,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1193,make sure it is possible to take the inverse of a sparse matrix,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1194,which contain negative value; this is the case in the iris dataset,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1245,check that an error is raised at fit time,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1250,check that an error is raised at transform time,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1259,check that an error is raised at inverse_transform time,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1264,check that an error is raised if input is scalar,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1268,check that a warning is raised is n_quantiles > n_samples,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1287,dense case -> warning raise,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1300,consider the case where sparse entries are missing values and user-given,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1301,zeros are to be considered,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1335,check in conjunction with subsampling,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1355,"using the a uniform output, each entry of X should be map between 0 and 1",
scikit-learn/sklearn/preprocessing/tests/test_data.py,1356,and equally spaced,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1376,Test that subsampling the input yield to a consistent results We check,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1377,"that the computed quantiles are almost mapped to a [0, 1] vector where",
scikit-learn/sklearn/preprocessing/tests/test_data.py,1378,values are equally spaced. The infinite norm is checked to be smaller,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1379,than a given threshold. This is repeated 5 times.,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1381,dense support,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1397,each random subsampling yield a unique approximation to the expected,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1398,linspace CDF,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1401,sparse support,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1415,each random subsampling yield a unique approximation to the expected,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1416,linspace CDF,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1466,Lower and upper bounds are manually mapped. We checked that in the case,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1467,"of a constant feature and binary feature, the bounds are properly mapped.",
scikit-learn/sklearn/preprocessing/tests/test_data.py,1473,check sparse and dense are consistent,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1482,check the consistency of the bounds by learning on 1 matrix,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1483,and transforming another,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1494,check that values outside of the range learned will be mapped properly.,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1528,check that the quantile of the first column is all NaN,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1530,all other column should not contain NaN,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1536,Non-regression test for:,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1537,https://github.com/scikit-learn/scikit-learn/issues/15733,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1538,Taken from upstream bug report:,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1539,https://github.com/numpy/numpy/issues/14685,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1547,Check that the estimated quantile threasholds are monotically,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1548,increasing:,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1571,first feature is always of zero,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1580,test csc has same outcome,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1584,raises value error on axis != 0,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1591,Check that X has not been copied,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1598,null scale,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1622,Check RobustScaler on toy data with zero variance features,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1630,"NOTE: for such a small sample size, what we expect in the third column",
scikit-learn/sklearn/preprocessing/tests/test_data.py,1631,depends HEAVILY on the method used to calculate quantiles. The values,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1632,here were calculated to fit the quantiles produces by np.percentile,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1633,using numpy 1.9 Calculating quantiles with,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1634,scipy.stats.mstats.scoreatquantile or scipy.stats.mstats.mquantiles,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1635,would yield very different results!,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1643,make sure new data gets transformed correctly,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1655,Check MaxAbsScaler on toy data with zero variance features,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1671,make sure new data gets transformed correctly,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1682,function interface,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1686,sparse data,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1704,Check MaxAbsScaler on toy data with a large negative value,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1720,Check MaxAbsScaler on transforming csr matrix with one row,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1732,Test scaling of dataset along single axis,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1739,cast only after scaling done,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1748,check inverse transform,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1752,Constant feature,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1759,function interface,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1766,Test if partial_fit run over many batches of size 1 and 50,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1767,gives the same results as fit,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1772,Test mean at the end of the process,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1801,Test std after 1 step,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1812,"Test std until the end of partial fits, and",
scikit-learn/sklearn/preprocessing/tests/test_data.py,1814,Clean estimator,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1828,set the row number 3 to zero,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1831,set the row number 3 to zero without pruning (can happen in real life),
scikit-learn/sklearn/preprocessing/tests/test_data.py,1836,build the pruned variant using the regular constructor,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1839,check inputs that support the no-copy optim,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1858,check input for which copy=False won't prevent a copy,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1877,set the row number 3 to zero,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1880,set the row number 3 to zero without pruning (can happen in real life),
scikit-learn/sklearn/preprocessing/tests/test_data.py,1885,build the pruned variant using the regular constructor,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1888,check inputs that support the no-copy optim,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1906,check input for which copy=False won't prevent a copy,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1925,set the row number 3 to zero,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1928,set the row number 3 to zero without pruning (can happen in real life),
scikit-learn/sklearn/preprocessing/tests/test_data.py,1933,build the pruned variant using the regular constructor,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1936,check inputs that support the no-copy optim,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1955,check input for which copy=False won't prevent a copy,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1970,check that we normalize by a positive number even for negative data,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1973,set the row number 3 to zero,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1975,check for mixed data where the value with,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1976,largest magnitude is negative,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1991,Test normalize function,
scikit-learn/sklearn/preprocessing/tests/test_data.py,1992,Only tests functionality not used by the tests for Normalizer.,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2021,Test return_norm,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2091,Cannot use threshold < 0 for sparse,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2097,Test that KernelCenterer is equivalent to StandardScaler,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2098,in feature space,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2106,center fit time matrix,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2112,center predict time matrix,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2122,Cross-validate a regression on four coplanar points with the same,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2123,value. Use precomputed kernel to ensure Pipeline with KernelCenterer,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2124,is treated as a _pairwise operation.,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2131,did the pipeline set the _pairwise attribute?,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2134,"test cross-validation, score should be almost perfect",
scikit-learn/sklearn/preprocessing/tests/test_data.py,2135,NB: this test is pretty vacuous -- it's mainly to test integration,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2136,of Pipeline and KernelCenterer,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2181,Scalers that have a partial_fit method,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2188,"with a different shape, this may break the scaler unless the internal",
scikit-learn/sklearn/preprocessing/tests/test_data.py,2189,state is reset,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2217,Make sure we get the original input when applying transform and then,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2218,inverse transform,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2274,Test inverse transformation,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2283,Exceptions should be raised for negative arrays and zero arrays when,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2284,method is boxcox,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2313,Yeo-Johnson method should support any kind of input,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2323,Exceptions should be raised for arrays with different num_columns,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2324,than during fitting,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2338,An exception should be raised if PowerTransformer.method isn't valid,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2348,Test the lambda = 0 case,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2355,Make sure lambda = 1 corresponds to the identity for yeo-johnson,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2371,Test the optimization procedure:,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2372,- set a predefined value for lambda,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2373,- apply inverse_transform to a normal dist (we get X_inv),
scikit-learn/sklearn/preprocessing/tests/test_data.py,2374,- apply fit_transform to X_inv (we get X_inv_trans),
scikit-learn/sklearn/preprocessing/tests/test_data.py,2375,- check that X_inv_trans is roughly equal to X,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2395,"test from original paper ""A new family of power transformations to",
scikit-learn/sklearn/preprocessing/tests/test_data.py,2396,"improve normality or symmetry"" by Yeo and Johnson.",
scikit-learn/sklearn/preprocessing/tests/test_data.py,2406,Make sure lambda estimation is not influenced by NaN values,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2407,and that transform() supports NaN silently,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2414,concat nans at the end and check lambda stays the same,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2430,check that fit_transform() and fit().transform() return the same values,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2442,"Check that neither fit, transform, fit_transform nor inverse_transform",
scikit-learn/sklearn/preprocessing/tests/test_data.py,2443,modify X inplace when copy=True,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2449,sanity checks,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2470,"check that when copy=False fit doesn't change X inplace but transform,",
scikit-learn/sklearn/preprocessing/tests/test_data.py,2471,fit_transform and inverse_transform do.,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2477,sanity checks,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2483,fit didn't change X,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2503,non-regression test for:,
scikit-learn/sklearn/preprocessing/tests/test_data.py,2504,https://github.com/scikit-learn/scikit-learn/issues/16448,
scikit-learn/sklearn/preprocessing/tests/test_common.py,51,check that the preprocessing method let pass nan,
scikit-learn/sklearn/preprocessing/tests/test_common.py,60,sanity check,
scikit-learn/sklearn/preprocessing/tests/test_common.py,64,make sure this boundary case is tested,
scikit-learn/sklearn/preprocessing/tests/test_common.py,68,ensure no warnings are raised,
scikit-learn/sklearn/preprocessing/tests/test_common.py,70,"missing values should still be missing, and only them",
scikit-learn/sklearn/preprocessing/tests/test_common.py,73,check that the function leads to the same results as the class,
scikit-learn/sklearn/preprocessing/tests/test_common.py,81,check that the inverse transform keep NaN,
scikit-learn/sklearn/preprocessing/tests/test_common.py,84,FIXME: we can introduce equal_nan=True in recent version of numpy.,
scikit-learn/sklearn/preprocessing/tests/test_common.py,85,For the moment which just check that non-NaN values are almost equal.,
scikit-learn/sklearn/preprocessing/tests/test_common.py,89,train only on non-NaN,
scikit-learn/sklearn/preprocessing/tests/test_common.py,91,check transforming with NaN works even when training without NaN,
scikit-learn/sklearn/preprocessing/tests/test_common.py,96,check non-NaN is handled as before - the 1st column is all nan,
scikit-learn/sklearn/preprocessing/tests/test_common.py,115,check that the dense and sparse inputs lead to the same results,
scikit-learn/sklearn/preprocessing/tests/test_common.py,116,precompute the matrix to avoid catching side warnings,
scikit-learn/sklearn/preprocessing/tests/test_discretization.py,53,Bad shape,
scikit-learn/sklearn/preprocessing/tests/test_discretization.py,60,Incorrect number of features,
scikit-learn/sklearn/preprocessing/tests/test_discretization.py,67,Bad bin values,
scikit-learn/sklearn/preprocessing/tests/test_discretization.py,76,Float bin values,
scikit-learn/sklearn/preprocessing/tests/test_discretization.py,96,test the shape of bin_edges_,
scikit-learn/sklearn/preprocessing/tests/test_discretization.py,123,replace the feature with zeros,
scikit-learn/sklearn/preprocessing/tests/test_discretization.py,145,Test up to discretizing nano units,
scikit-learn/sklearn/preprocessing/tests/test_discretization.py,201,with 2 bins,
scikit-learn/sklearn/preprocessing/tests/test_discretization.py,206,with 3 bins,
scikit-learn/sklearn/preprocessing/tests/test_discretization.py,211,with 5 bins,
scikit-learn/sklearn/preprocessing/tests/test_function_transformer.py,22,(args|kwargs)_store will hold the positional and keyword arguments,
scikit-learn/sklearn/preprocessing/tests/test_function_transformer.py,23,passed to the function inside the FunctionTransformer.,
scikit-learn/sklearn/preprocessing/tests/test_function_transformer.py,32,The function should only have received X.,
scikit-learn/sklearn/preprocessing/tests/test_function_transformer.py,39,reset the argument stores.,
scikit-learn/sklearn/preprocessing/tests/test_function_transformer.py,49,The function should have received X,
scikit-learn/sklearn/preprocessing/tests/test_function_transformer.py,60,Test that the numpy.log example still works.,
scikit-learn/sklearn/preprocessing/tests/test_function_transformer.py,72,Test that rounding is correct,
scikit-learn/sklearn/preprocessing/tests/test_function_transformer.py,84,Test that rounding is correct,
scikit-learn/sklearn/preprocessing/tests/test_function_transformer.py,95,Test that rounding is correct,
scikit-learn/sklearn/preprocessing/tests/test_function_transformer.py,102,Test that inverse_transform works correctly,
scikit-learn/sklearn/preprocessing/tests/test_function_transformer.py,145,check that we don't check inverse when one of the func or inverse is not,
scikit-learn/sklearn/preprocessing/tests/test_function_transformer.py,146,provided.,
scikit-learn/sklearn/preprocessing/tests/test_label.py,40,one-class case defaults to negative label,
scikit-learn/sklearn/preprocessing/tests/test_label.py,41,For dense case:,
scikit-learn/sklearn/preprocessing/tests/test_label.py,50,For sparse case:,
scikit-learn/sklearn/preprocessing/tests/test_label.py,59,two-class case,
scikit-learn/sklearn/preprocessing/tests/test_label.py,72,multi-class case,
scikit-learn/sklearn/preprocessing/tests/test_label.py,107,two-class case with pos_label=0,
scikit-learn/sklearn/preprocessing/tests/test_label.py,116,multi-class case,
scikit-learn/sklearn/preprocessing/tests/test_label.py,130,Check that invalid arguments yield ValueError,
scikit-learn/sklearn/preprocessing/tests/test_label.py,152,Fail on y_type,
scikit-learn/sklearn/preprocessing/tests/test_label.py,158,Sequence of seq type should raise ValueError,
scikit-learn/sklearn/preprocessing/tests/test_label.py,163,Fail on the number of classes,
scikit-learn/sklearn/preprocessing/tests/test_label.py,170,Fail on the dimension of 'binary',
scikit-learn/sklearn/preprocessing/tests/test_label.py,177,Fail on multioutput data,
scikit-learn/sklearn/preprocessing/tests/test_label.py,195,"Test LabelEncoder's transform, fit_transform and",
scikit-learn/sklearn/preprocessing/tests/test_label.py,196,inverse_transform methods,
scikit-learn/sklearn/preprocessing/tests/test_label.py,232,Check that invalid arguments yield ValueError,
scikit-learn/sklearn/preprocessing/tests/test_label.py,239,Fail on unseen labels,
scikit-learn/sklearn/preprocessing/tests/test_label.py,248,"Fail on inverse_transform("""")",
scikit-learn/sklearn/preprocessing/tests/test_label.py,263,test empty transform,
scikit-learn/sklearn/preprocessing/tests/test_label.py,266,test empty inverse transform,
scikit-learn/sklearn/preprocessing/tests/test_label.py,272,test input as iterable of iterables,
scikit-learn/sklearn/preprocessing/tests/test_label.py,285,With fit_transform,
scikit-learn/sklearn/preprocessing/tests/test_label.py,290,verify CSR assumption that indices and indptr have same dtype,
scikit-learn/sklearn/preprocessing/tests/test_label.py,297,With fit,
scikit-learn/sklearn/preprocessing/tests/test_label.py,302,verify CSR assumption that indices and indptr have same dtype,
scikit-learn/sklearn/preprocessing/tests/test_label.py,316,test input as iterable of iterables,
scikit-learn/sklearn/preprocessing/tests/test_label.py,327,With fit_transform,
scikit-learn/sklearn/preprocessing/tests/test_label.py,334,With fit,
scikit-learn/sklearn/preprocessing/tests/test_label.py,372,fit_transform(),
scikit-learn/sklearn/preprocessing/tests/test_label.py,377,fit().transform(),
scikit-learn/sklearn/preprocessing/tests/test_label.py,382,ensure works with extra class,
scikit-learn/sklearn/preprocessing/tests/test_label.py,388,ensure fit is no-op as iterable is not consumed,
scikit-learn/sklearn/preprocessing/tests/test_label.py,393,ensure a ValueError is thrown if given duplicate classes,
scikit-learn/sklearn/preprocessing/tests/test_label.py,411,first call,
scikit-learn/sklearn/preprocessing/tests/test_label.py,414,second call change class,
scikit-learn/sklearn/preprocessing/tests/test_label.py,420,Ensure sequences of the same length are not interpreted as a 2-d array,
scikit-learn/sklearn/preprocessing/tests/test_label.py,425,fit_transform(),
scikit-learn/sklearn/preprocessing/tests/test_label.py,430,fit().transform(),
scikit-learn/sklearn/preprocessing/tests/test_label.py,447,fit_transform(),
scikit-learn/sklearn/preprocessing/tests/test_label.py,453,fit().transform(),
scikit-learn/sklearn/preprocessing/tests/test_label.py,475,Not binary,
scikit-learn/sklearn/preprocessing/tests/test_label.py,478,"The following binary cases are fine, however",
scikit-learn/sklearn/preprocessing/tests/test_label.py,483,Wrong shape,
scikit-learn/sklearn/preprocessing/tests/test_label.py,495,Modified class order,
scikit-learn/sklearn/preprocessing/tests/test_label.py,517,check label_binarize,
scikit-learn/sklearn/preprocessing/tests/test_label.py,524,check inverse,
scikit-learn/sklearn/preprocessing/tests/test_label.py,539,Check label binarizer,
scikit-learn/sklearn/preprocessing/tests/test_label.py,559,Binary case where sparse_output = True will not result in a ValueError,
scikit-learn/sklearn/preprocessing/tests/test_label.py,639,test for the check_unknown parameter of _encode(),
scikit-learn/sklearn/preprocessing/tests/test_label.py,643,"Default is True, raise error",
scikit-learn/sklearn/preprocessing/tests/test_label.py,648,dont raise error if False,
scikit-learn/sklearn/preprocessing/tests/test_label.py,651,parameter is ignored for object dtype,
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,1,-*- coding: utf-8 -*-,
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,18,check that sparse and dense will give the same results,
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,33,check outcome,
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,54,Test that one hot encoder raises error for unknown features,
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,55,present during transform.,
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,61,"Test the ignore option, ignores unknown features (giving all 0's)",
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,68,ensure transformed data was not modified in place,
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,71,Raise error if handle_unknown is neither ignore or error.,
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,90,Non Regression test for the issue #12470,
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,91,"Test the ignore option, when categories are numpy string dtype",
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,92,particularly when the known category strings are larger,
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,93,than the unknown category strings,
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,100,ensure transformed data was not modified in place,
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,179,set params on not yet fitted object,
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,183,set params on already fitted object,
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,234,with unknown categories,
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,235,drop is incompatible with handle_unknown=ignore,
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,245,"with an otherwise numerical output, still object if unknown",
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,255,incorrect shape raises,
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,271,check that resetting drop option without refitting does not throw an error,
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,321,order of categories should not depend on order of samples,
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,325,assert enc.categories == 'auto',
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,350,manually specified categories should have same dtype as,
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,351,the data when coerced from lists,
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,354,"when specifying categories manually, unknown categories should already",
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,355,raise when fitting,
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,375,unsorted passed categories still raise for numerical values,
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,384,multiple columns,
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,393,integer categories but from object dtype data,
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,423,Canonical case,
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,437,"with only one cat, the behaviour is equivalent to drop=None",
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,513,manually specified categories should have same dtype as,
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,514,the data when coerced from lists,
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,517,"when specifying categories manually, unknown categories should already",
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,518,raise when fitting,
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,531,incorrect shape raises,
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,568,check that dtypes are preserved when determining categories,
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,574,string dtype,
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,593,check dtype (similar to test_categorical_encoder_dtypes for dataframes),
scikit-learn/sklearn/metrics/_regression.py,10,Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/sklearn/metrics/_regression.py,11,Mathieu Blondel <mathieu@mblondel.org>,
scikit-learn/sklearn/metrics/_regression.py,12,Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/sklearn/metrics/_regression.py,13,Arnaud Joly <a.joly@ulg.ac.be>,
scikit-learn/sklearn/metrics/_regression.py,14,Jochen Wersdorfer <jochen@wersdoerfer.de>,
scikit-learn/sklearn/metrics/_regression.py,15,Lars Buitinck,
scikit-learn/sklearn/metrics/_regression.py,16,Joel Nothman <joel.nothman@gmail.com>,
scikit-learn/sklearn/metrics/_regression.py,17,Karan Desai <karandesai281196@gmail.com>,
scikit-learn/sklearn/metrics/_regression.py,18,Noel Dawe <noel@dawe.me>,
scikit-learn/sklearn/metrics/_regression.py,19,Manoj Kumar <manojkumarsivaraj334@gmail.com>,
scikit-learn/sklearn/metrics/_regression.py,20,Michael Eickenberg <michael.eickenberg@gmail.com>,
scikit-learn/sklearn/metrics/_regression.py,21,Konstantin Shmelkov <konstantin.shmelkov@polytechnique.edu>,
scikit-learn/sklearn/metrics/_regression.py,22,Christian Lorentzen <lorentzen.ch@googlemail.com>,
scikit-learn/sklearn/metrics/_regression.py,23,License: BSD 3 clause,
scikit-learn/sklearn/metrics/_regression.py,185,pass None as weights to np.average: uniform mean,
scikit-learn/sklearn/metrics/_regression.py,259,pass None as weights to np.average: uniform mean,
scikit-learn/sklearn/metrics/_regression.py,389,pass None as weights to np.average: uniform mean,
scikit-learn/sklearn/metrics/_regression.py,474,return scores individually,
scikit-learn/sklearn/metrics/_regression.py,477,passing to np.average() None as weights results is uniform mean,
scikit-learn/sklearn/metrics/_regression.py,601,"arbitrary set to zero to avoid -inf scores, having a constant",
scikit-learn/sklearn/metrics/_regression.py,602,y_true is not interesting for scoring a regression anyway,
scikit-learn/sklearn/metrics/_regression.py,606,return scores individually,
scikit-learn/sklearn/metrics/_regression.py,609,passing None as weights results is uniform mean,
scikit-learn/sklearn/metrics/_regression.py,613,avoid fail on constant y or one-element arrays,
scikit-learn/sklearn/metrics/_classification.py,10,Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/sklearn/metrics/_classification.py,11,Mathieu Blondel <mathieu@mblondel.org>,
scikit-learn/sklearn/metrics/_classification.py,12,Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/sklearn/metrics/_classification.py,13,Arnaud Joly <a.joly@ulg.ac.be>,
scikit-learn/sklearn/metrics/_classification.py,14,Jochen Wersdorfer <jochen@wersdoerfer.de>,
scikit-learn/sklearn/metrics/_classification.py,15,Lars Buitinck,
scikit-learn/sklearn/metrics/_classification.py,16,Joel Nothman <joel.nothman@gmail.com>,
scikit-learn/sklearn/metrics/_classification.py,17,Noel Dawe <noel@dawe.me>,
scikit-learn/sklearn/metrics/_classification.py,18,Jatin Shah <jatindshah@gmail.com>,
scikit-learn/sklearn/metrics/_classification.py,19,Saurabh Jha <saurabh.jhaa@gmail.com>,
scikit-learn/sklearn/metrics/_classification.py,20,Bernardo Stein <bernardovstein@gmail.com>,
scikit-learn/sklearn/metrics/_classification.py,21,Shangwu Yao <shangwuyao@gmail.com>,
scikit-learn/sklearn/metrics/_classification.py,22,License: BSD 3 clause,
scikit-learn/sklearn/metrics/_classification.py,92,We can't have more than one value on y_type => The set is no more needed,
scikit-learn/sklearn/metrics/_classification.py,95,"No metrics support ""multiclass-multioutput"" format",
scikit-learn/sklearn/metrics/_classification.py,184,Compute accuracy for each possible representation,
scikit-learn/sklearn/metrics/_classification.py,300,"convert yt, yp into index",
scikit-learn/sklearn/metrics/_classification.py,304,"intersect y_pred, y_true with labels, eliminate items not in labels",
scikit-learn/sklearn/metrics/_classification.py,308,also eliminate weights of eliminated items,
scikit-learn/sklearn/metrics/_classification.py,311,Choose the accumulator dtype to always have high precision,
scikit-learn/sklearn/metrics/_classification.py,460,labels are now from 0 to len(labels) - 1 -> use bincount,
scikit-learn/sklearn/metrics/_classification.py,472,Pathological case,
scikit-learn/sklearn/metrics/_classification.py,481,Retain only selected labels,
scikit-learn/sklearn/metrics/_classification.py,490,All labels are index integers for multilabel.,
scikit-learn/sklearn/metrics/_classification.py,491,Select labels:,
scikit-learn/sklearn/metrics/_classification.py,507,calculate weighted counts,
scikit-learn/sklearn/metrics/_classification.py,746,"numerator is 0, and warning should have already been issued",
scikit-learn/sklearn/metrics/_classification.py,1177,avoid infs/nans,
scikit-learn/sklearn/metrics/_classification.py,1183,"if ``zero_division=1``, set those with denominator == 0 equal to 1",
scikit-learn/sklearn/metrics/_classification.py,1186,the user will be removing warnings if zero_division is set to something,
scikit-learn/sklearn/metrics/_classification.py,1187,different than its default value. If we are computing only f-score,
scikit-learn/sklearn/metrics/_classification.py,1188,the warning will be raised only if precision and recall are ill-defined,
scikit-learn/sklearn/metrics/_classification.py,1192,build appropriate warning,
scikit-learn/sklearn/metrics/_classification.py,1193,"E.g. ""Precision and F-score are ill-defined and being set to 0.0 in",
scikit-learn/sklearn/metrics/_classification.py,1194,labels with no predicted samples. Use ``zero_division`` parameter to,
scikit-learn/sklearn/metrics/_classification.py,1195,"control this behavior.""",
scikit-learn/sklearn/metrics/_classification.py,1420,"Calculate tp_sum, pred_sum, true_sum",
scikit-learn/sklearn/metrics/_classification.py,1434,"Finally, we have all our sufficient statistics. Divide!",
scikit-learn/sklearn/metrics/_classification.py,1437,"Divide, and on zero-division, set scores and/or warn according to",
scikit-learn/sklearn/metrics/_classification.py,1438,zero_division:,
scikit-learn/sklearn/metrics/_classification.py,1444,"warn for f-score only if zero_division is warn, it is in warn_for",
scikit-learn/sklearn/metrics/_classification.py,1445,and BOTH prec and rec are ill-defined,
scikit-learn/sklearn/metrics/_classification.py,1452,"if tp == 0 F will be 1 only if all predictions are zero, all labels are",
scikit-learn/sklearn/metrics/_classification.py,1453,"zero, and zero_division=1. In all other case, 0",
scikit-learn/sklearn/metrics/_classification.py,1459,avoid division by 0,
scikit-learn/sklearn/metrics/_classification.py,1462,Average the results,
scikit-learn/sklearn/metrics/_classification.py,1467,precision is zero_division if there are no positive predictions,
scikit-learn/sklearn/metrics/_classification.py,1468,recall is zero_division if there are no positive labels,
scikit-learn/sklearn/metrics/_classification.py,1469,fscore is zero_division if all labels AND predictions are,
scikit-learn/sklearn/metrics/_classification.py,1470,negative,
scikit-learn/sklearn/metrics/_classification.py,1486,return no support,
scikit-learn/sklearn/metrics/_classification.py,1914,labelled micro average,
scikit-learn/sklearn/metrics/_classification.py,1935,compute per-class results without averaging,
scikit-learn/sklearn/metrics/_classification.py,1965,compute all applicable averages,
scikit-learn/sklearn/metrics/_classification.py,1972,compute averages with specified averaging method,
scikit-learn/sklearn/metrics/_classification.py,2185,Clipping,
scikit-learn/sklearn/metrics/_classification.py,2188,"If y_pred is of single dimension, assume y_true to be binary",
scikit-learn/sklearn/metrics/_classification.py,2189,and then check.,
scikit-learn/sklearn/metrics/_classification.py,2195,Check if dimensions are consistent.,
scikit-learn/sklearn/metrics/_classification.py,2211,Renormalize,
scikit-learn/sklearn/metrics/_classification.py,2319,Handles binary class case,
scikit-learn/sklearn/metrics/_classification.py,2320,this code assumes that positive and negative labels,
scikit-learn/sklearn/metrics/_classification.py,2321,are encoded as +1 and -1 respectively,
scikit-learn/sklearn/metrics/_classification.py,2334,The hinge_loss doesn't penalize good enough predictions.,
scikit-learn/sklearn/metrics/_classification.py,2418,"if pos_label=None, when y_true is in {-1, 1} or {0, 1},",
scikit-learn/sklearn/metrics/_classification.py,2419,"pos_label is set to 1 (consistent with precision_recall_curve/roc_curve),",
scikit-learn/sklearn/metrics/_classification.py,2420,otherwise pos_label is set to the greater label,
scikit-learn/sklearn/metrics/_classification.py,2421,"(different from precision_recall_curve/roc_curve,",
scikit-learn/sklearn/metrics/_classification.py,2422,the purpose is to keep backward compatibility).,
scikit-learn/sklearn/metrics/pairwise.py,1,-*- coding: utf-8 -*-,
scikit-learn/sklearn/metrics/pairwise.py,3,Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/sklearn/metrics/pairwise.py,4,Mathieu Blondel <mathieu@mblondel.org>,
scikit-learn/sklearn/metrics/pairwise.py,5,Robert Layton <robertlayton@gmail.com>,
scikit-learn/sklearn/metrics/pairwise.py,6,Andreas Mueller <amueller@ais.uni-bonn.de>,
scikit-learn/sklearn/metrics/pairwise.py,7,Philippe Gervais <philippe.gervais@inria.fr>,
scikit-learn/sklearn/metrics/pairwise.py,8,Lars Buitinck,
scikit-learn/sklearn/metrics/pairwise.py,9,Joel Nothman <joel.nothman@gmail.com>,
scikit-learn/sklearn/metrics/pairwise.py,10,License: BSD 3 clause,
scikit-learn/sklearn/metrics/pairwise.py,36,Utility Functions,
scikit-learn/sklearn/metrics/pairwise.py,194,Pairwise distances,
scikit-learn/sklearn/metrics/pairwise.py,264,"If norms are passed as float32, they are unused. If arrays are passed as",
scikit-learn/sklearn/metrics/pairwise.py,265,"float32, norms needs to be recomputed on upcast chunks.",
scikit-learn/sklearn/metrics/pairwise.py,266,TODO: use a float64 accumulator in row_norms to avoid the latter.,
scikit-learn/sklearn/metrics/pairwise.py,282,"shortcut in the common case euclidean_distances(X, X)",
scikit-learn/sklearn/metrics/pairwise.py,298,"To minimize precision issues with float32, we compute the distance",
scikit-learn/sklearn/metrics/pairwise.py,299,matrix on chunks of X and Y upcast to float64,
scikit-learn/sklearn/metrics/pairwise.py,302,"if dtype is already float64, no need to chunk and upcast",
scikit-learn/sklearn/metrics/pairwise.py,308,Ensure that distances between vectors and themselves are set to 0.0.,
scikit-learn/sklearn/metrics/pairwise.py,309,This may not be the case due to floating point rounding errors.,
scikit-learn/sklearn/metrics/pairwise.py,391,Get missing mask for X,
scikit-learn/sklearn/metrics/pairwise.py,394,Get missing mask for Y,
scikit-learn/sklearn/metrics/pairwise.py,397,set missing values to zero,
scikit-learn/sklearn/metrics/pairwise.py,403,Adjust distances for missing values,
scikit-learn/sklearn/metrics/pairwise.py,412,Ensure that distances between vectors and themselves are set to 0.0.,
scikit-learn/sklearn/metrics/pairwise.py,413,This may not be the case due to floating point rounding errors.,
scikit-learn/sklearn/metrics/pairwise.py,420,avoid divide by zero,
scikit-learn/sklearn/metrics/pairwise.py,450,"Allow 10% more memory than X, Y and the distance matrix take (at",
scikit-learn/sklearn/metrics/pairwise.py,451,least 10MiB),
scikit-learn/sklearn/metrics/pairwise.py,457,The increase amount of memory in 8-byte blocks is:,
scikit-learn/sklearn/metrics/pairwise.py,458,- x_density * batch_size * n_features (copy of chunk of X),
scikit-learn/sklearn/metrics/pairwise.py,459,- y_density * batch_size * n_features (copy of chunk of Y),
scikit-learn/sklearn/metrics/pairwise.py,460,- batch_size * batch_size (chunk of distance matrix),
scikit-learn/sklearn/metrics/pairwise.py,461,"Hence x² + (xd+yd)kx = M, where x=batch_size, k=n_features, M=maxmem",
scikit-learn/sklearn/metrics/pairwise.py,462,xd=x_density and yd=y_density,
scikit-learn/sklearn/metrics/pairwise.py,480,when X is Y the distance matrix is symmetric so we only need,
scikit-learn/sklearn/metrics/pairwise.py,481,to compute half of it.,
scikit-learn/sklearn/metrics/pairwise.py,779,this also sorts indices in-place,
scikit-learn/sklearn/metrics/pairwise.py,820,"1.0 - cosine_similarity(X, Y) without copy",
scikit-learn/sklearn/metrics/pairwise.py,826,Ensure that distances between vectors and themselves are set to 0.0.,
scikit-learn/sklearn/metrics/pairwise.py,827,This may not be the case due to floating point rounding errors.,
scikit-learn/sklearn/metrics/pairwise.py,832,Paired distances,
scikit-learn/sklearn/metrics/pairwise.py,958,Check the matrix first (it is usually done by the metric),
scikit-learn/sklearn/metrics/pairwise.py,968,Kernels,
scikit-learn/sklearn/metrics/pairwise.py,1061,compute tanh in-place,
scikit-learn/sklearn/metrics/pairwise.py,1094,exponentiate K in-place,
scikit-learn/sklearn/metrics/pairwise.py,1128,exponentiate K in-place,
scikit-learn/sklearn/metrics/pairwise.py,1165,to avoid recursive import,
scikit-learn/sklearn/metrics/pairwise.py,1290,Helper functions - distance,
scikit-learn/sklearn/metrics/pairwise.py,1292,"If updating this dictionary, update the doc in both distance_metrics()",
scikit-learn/sklearn/metrics/pairwise.py,1293,and also in pairwise_distances()!,
scikit-learn/sklearn/metrics/pairwise.py,1301,"HACK: precomputed is always allowed, never called",
scikit-learn/sklearn/metrics/pairwise.py,1350,enforce a threading backend to prevent data communication overhead,
scikit-learn/sklearn/metrics/pairwise.py,1358,zeroing diagonal for euclidean norm.,
scikit-learn/sklearn/metrics/pairwise.py,1359,TODO: do it also for other norms.,
scikit-learn/sklearn/metrics/pairwise.py,1371,Only calculate metric for upper triangle,
scikit-learn/sklearn/metrics/pairwise.py,1377,Make symmetric,
scikit-learn/sklearn/metrics/pairwise.py,1378,NB: out += out.T will produce incorrect results,
scikit-learn/sklearn/metrics/pairwise.py,1381,Calculate diagonal,
scikit-learn/sklearn/metrics/pairwise.py,1382,NB: nonzero diagonals are allowed for both metrics and kernels,
scikit-learn/sklearn/metrics/pairwise.py,1388,Calculate all cells,
scikit-learn/sklearn/metrics/pairwise.py,1571,We get as many rows as possible within our working_memory budget to,
scikit-learn/sklearn/metrics/pairwise.py,1572,store len(Y) distances in each row of output.,
scikit-learn/sklearn/metrics/pairwise.py,1573,,
scikit-learn/sklearn/metrics/pairwise.py,1574,Note:,
scikit-learn/sklearn/metrics/pairwise.py,1575,"- this will get at least 1 row, even if 1 row of distances will",
scikit-learn/sklearn/metrics/pairwise.py,1576,exceed working_memory.,
scikit-learn/sklearn/metrics/pairwise.py,1577,- this does not account for any temporary memory usage while,
scikit-learn/sklearn/metrics/pairwise.py,1578,calculating distances (e.g. difference of vectors in manhattan,
scikit-learn/sklearn/metrics/pairwise.py,1579,distance.,
scikit-learn/sklearn/metrics/pairwise.py,1585,precompute data-derived metric params,
scikit-learn/sklearn/metrics/pairwise.py,1591,enable optimised paths for X is Y,
scikit-learn/sklearn/metrics/pairwise.py,1599,"zeroing diagonal, taking care of aliases of ""euclidean"",",
scikit-learn/sklearn/metrics/pairwise.py,1600,"i.e. ""l2""",
scikit-learn/sklearn/metrics/pairwise.py,1743,precompute data-derived metric params,
scikit-learn/sklearn/metrics/pairwise.py,1755,"These distances require boolean arrays, when using scipy.spatial.distance",
scikit-learn/sklearn/metrics/pairwise.py,1768,Helper functions - distance,
scikit-learn/sklearn/metrics/pairwise.py,1770,"If updating this dictionary, update the doc in both distance_metrics()",
scikit-learn/sklearn/metrics/pairwise.py,1771,and also in pairwise_distances()!,
scikit-learn/sklearn/metrics/pairwise.py,1894,import GPKernel locally to prevent circular imports,
scikit-learn/sklearn/metrics/_base.py,5,Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/sklearn/metrics/_base.py,6,Mathieu Blondel <mathieu@mblondel.org>,
scikit-learn/sklearn/metrics/_base.py,7,Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/sklearn/metrics/_base.py,8,Arnaud Joly <a.joly@ulg.ac.be>,
scikit-learn/sklearn/metrics/_base.py,9,Jochen Wersdorfer <jochen@wersdoerfer.de>,
scikit-learn/sklearn/metrics/_base.py,10,Lars Buitinck,
scikit-learn/sklearn/metrics/_base.py,11,Joel Nothman <joel.nothman@gmail.com>,
scikit-learn/sklearn/metrics/_base.py,12,Noel Dawe <noel@dawe.me>,
scikit-learn/sklearn/metrics/_base.py,13,License: BSD 3 clause,
scikit-learn/sklearn/metrics/_base.py,103,swap average_weight <-> score_weight,
scikit-learn/sklearn/metrics/_base.py,122,Average the results,
scikit-learn/sklearn/metrics/_base.py,125,"Scores with 0 weights are forced to be 0, preventing the average",
scikit-learn/sklearn/metrics/_base.py,126,score from being affected by 0-weighted NaN elements.,
scikit-learn/sklearn/metrics/_base.py,185,"Compute scores treating a as positive class and b as negative class,",
scikit-learn/sklearn/metrics/_base.py,186,then b as positive class and a as negative class,
scikit-learn/sklearn/metrics/_ranking.py,10,Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/sklearn/metrics/_ranking.py,11,Mathieu Blondel <mathieu@mblondel.org>,
scikit-learn/sklearn/metrics/_ranking.py,12,Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/sklearn/metrics/_ranking.py,13,Arnaud Joly <a.joly@ulg.ac.be>,
scikit-learn/sklearn/metrics/_ranking.py,14,Jochen Wersdorfer <jochen@wersdoerfer.de>,
scikit-learn/sklearn/metrics/_ranking.py,15,Lars Buitinck,
scikit-learn/sklearn/metrics/_ranking.py,16,Joel Nothman <joel.nothman@gmail.com>,
scikit-learn/sklearn/metrics/_ranking.py,17,Noel Dawe <noel@dawe.me>,
scikit-learn/sklearn/metrics/_ranking.py,18,License: BSD 3 clause,
scikit-learn/sklearn/metrics/_ranking.py,97,Reductions such as .sum used internally in np.trapz do not return a,
scikit-learn/sklearn/metrics/_ranking.py,98,scalar by default for numpy.memmap instances contrary to,
scikit-learn/sklearn/metrics/_ranking.py,99,regular numpy.ndarray instances.,
scikit-learn/sklearn/metrics/_ranking.py,197,Return the step function integral,
scikit-learn/sklearn/metrics/_ranking.py,198,The following works because the last entry of precision is,
scikit-learn/sklearn/metrics/_ranking.py,199,"guaranteed to be 1, as returned by precision_recall_curve",
scikit-learn/sklearn/metrics/_ranking.py,231,Add a single point at max_fpr by linear interpolation,
scikit-learn/sklearn/metrics/_ranking.py,239,McClish correction: standardize result to be 0.5 if non-discriminant,
scikit-learn/sklearn/metrics/_ranking.py,240,and 1 if maximal,
scikit-learn/sklearn/metrics/_ranking.py,374,do not support partial ROC computation for multiclass,
scikit-learn/sklearn/metrics/_ranking.py,391,multilabel-indicator,
scikit-learn/sklearn/metrics/_ranking.py,439,validation of the input y_score,
scikit-learn/sklearn/metrics/_ranking.py,445,validation for multiclass parameter specifications,
scikit-learn/sklearn/metrics/_ranking.py,486,Hand & Till (2001) implementation (ovo),
scikit-learn/sklearn/metrics/_ranking.py,491,ovr is same as multi-label,
scikit-learn/sklearn/metrics/_ranking.py,532,Check to make sure y_true is valid,
scikit-learn/sklearn/metrics/_ranking.py,547,ensure binary classification if pos_label is not specified,
scikit-learn/sklearn/metrics/_ranking.py,548,"classes.dtype.kind in ('O', 'U', 'S') is required to avoid",
scikit-learn/sklearn/metrics/_ranking.py,549,"triggering a FutureWarning by calling np.array_equal(a, b)",
scikit-learn/sklearn/metrics/_ranking.py,550,when elements in the two arrays are not comparable.,
scikit-learn/sklearn/metrics/_ranking.py,568,make y_true a boolean vector,
scikit-learn/sklearn/metrics/_ranking.py,571,sort scores and corresponding truth values,
scikit-learn/sklearn/metrics/_ranking.py,580,y_score typically has many tied values. Here we extract,
scikit-learn/sklearn/metrics/_ranking.py,581,the indices associated with the distinct values. We also,
scikit-learn/sklearn/metrics/_ranking.py,582,concatenate a value for the end of the curve.,
scikit-learn/sklearn/metrics/_ranking.py,586,accumulate the true positives with decreasing threshold,
scikit-learn/sklearn/metrics/_ranking.py,589,express fps as a cumsum to ensure fps is increasing even in,
scikit-learn/sklearn/metrics/_ranking.py,590,the presence of floating point errors,
scikit-learn/sklearn/metrics/_ranking.py,679,stop when full recall attained,
scikit-learn/sklearn/metrics/_ranking.py,680,and reverse the outputs so recall is decreasing,
scikit-learn/sklearn/metrics/_ranking.py,773,Attempt to drop thresholds corresponding to points in between and,
scikit-learn/sklearn/metrics/_ranking.py,774,collinear with other points. These are always suboptimal and do not,
scikit-learn/sklearn/metrics/_ranking.py,775,appear on a plotted ROC curve (and thus do not affect the AUC).,
scikit-learn/sklearn/metrics/_ranking.py,776,"Here np.diff(_, 2) is used as a ""second derivative"" to tell if there",
scikit-learn/sklearn/metrics/_ranking.py,777,is a corner at the point. Both fps and tps must be tested to handle,
scikit-learn/sklearn/metrics/_ranking.py,778,thresholds with multiple data points (which are combined in,
scikit-learn/sklearn/metrics/_ranking.py,779,"_binary_clf_curve). This keeps all cases where the point should be kept,",
scikit-learn/sklearn/metrics/_ranking.py,780,"but does not drop more complicated cases like fps = [1, 3, 7],",
scikit-learn/sklearn/metrics/_ranking.py,781,"tps = [1, 2, 4]; there is no harm in keeping too many thresholds.",
scikit-learn/sklearn/metrics/_ranking.py,791,Add an extra threshold position,
scikit-learn/sklearn/metrics/_ranking.py,792,"to make sure that the curve starts at (0, 0)",
scikit-learn/sklearn/metrics/_ranking.py,865,Handle badly formatted array and the degenerate case with one label,
scikit-learn/sklearn/metrics/_ranking.py,881,"If all labels are relevant or unrelevant, the score is also",
scikit-learn/sklearn/metrics/_ranking.py,882,equal to 1. The label ranking has no meaning.,
scikit-learn/sklearn/metrics/_ranking.py,1018,Sort and bin the label scores,
scikit-learn/sklearn/metrics/_ranking.py,1028,"if the scores are ordered, it's possible to count the number of",
scikit-learn/sklearn/metrics/_ranking.py,1029,incorrectly ordered paires in linear time by cumulatively counting,
scikit-learn/sklearn/metrics/_ranking.py,1030,how many false labels of a given score have a score higher than the,
scikit-learn/sklearn/metrics/_ranking.py,1031,accumulated true labels with lower score.,
scikit-learn/sklearn/metrics/_ranking.py,1039,"When there is no positive or no negative labels, those values should",
scikit-learn/sklearn/metrics/_ranking.py,1040,"be consider as correct, i.e. the ranking doesn't matter.",
scikit-learn/sklearn/metrics/_ranking.py,1313,Here we use the order induced by y_true so we can ignore ties since,
scikit-learn/sklearn/metrics/_ranking.py,1314,the gain associated to tied indices is the same (permuting ties doesn't,
scikit-learn/sklearn/metrics/_ranking.py,1315,change the value of the re-ordered y_true),
scikit-learn/sklearn/metrics/_scorer.py,16,Authors: Andreas Mueller <amueller@ais.uni-bonn.de>,
scikit-learn/sklearn/metrics/_scorer.py,17,Lars Buitinck,
scikit-learn/sklearn/metrics/_scorer.py,18,Arnaud Joly <arnaud.v.joly@gmail.com>,
scikit-learn/sklearn/metrics/_scorer.py,19,License: Simplified BSD,
scikit-learn/sklearn/metrics/_scorer.py,106,Only one scorer,
scikit-learn/sklearn/metrics/_scorer.py,129,XXX After removing the deprecated scorers (v0.24) remove the,
scikit-learn/sklearn/metrics/_scorer.py,130,XXX deprecation_msg property again and remove __call__'s body again,
scikit-learn/sklearn/metrics/_scorer.py,250,not multiclass,
scikit-learn/sklearn/metrics/_scorer.py,308,For multi-output multi-class estimator,
scikit-learn/sklearn/metrics/_scorer.py,356,deprecated,
scikit-learn/sklearn/metrics/_scorer.py,405,Heuristic to ensure user has not passed a metric,
scikit-learn/sklearn/metrics/_scorer.py,615,Standard regression scores,
scikit-learn/sklearn/metrics/_scorer.py,639,Standard Classification Scores,
scikit-learn/sklearn/metrics/_scorer.py,643,Score functions that need decision values,
scikit-learn/sklearn/metrics/_scorer.py,659,Score function for probabilistic classification,
scikit-learn/sklearn/metrics/_scorer.py,674,Clustering scores,
scikit-learn/sklearn/metrics/_scorer.py,704,Cluster metrics that use supervised evaluation,
scikit-learn/sklearn/metrics/cluster/_unsupervised.py,3,Authors: Robert Layton <robertlayton@gmail.com>,
scikit-learn/sklearn/metrics/cluster/_unsupervised.py,4,Arnaud Fouchet <foucheta@gmail.com>,
scikit-learn/sklearn/metrics/cluster/_unsupervised.py,5,Thierry Guillemot <thierry.guillemot.work@gmail.com>,
scikit-learn/sklearn/metrics/cluster/_unsupervised.py,6,License: BSD 3 clause,
scikit-learn/sklearn/metrics/cluster/_unsupervised.py,132,accumulate distances from each sample to each cluster,
scikit-learn/sklearn/metrics/cluster/_unsupervised.py,139,intra_index selects intra-cluster distances within clust_dists,
scikit-learn/sklearn/metrics/cluster/_unsupervised.py,141,intra_clust_dists are averaged over cluster size outside this function,
scikit-learn/sklearn/metrics/cluster/_unsupervised.py,143,of the remaining distances we normalise and extract the minimum,
scikit-learn/sklearn/metrics/cluster/_unsupervised.py,213,Check for non-zero diagonal entries in precomputed distance matrix,
scikit-learn/sklearn/metrics/cluster/_unsupervised.py,244,"nan values are for clusters of size 1, and should be 0",
scikit-learn/sklearn/metrics/cluster/_supervised.py,7,Authors: Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/sklearn/metrics/cluster/_supervised.py,8,Wei LI <kuantkid@gmail.com>,
scikit-learn/sklearn/metrics/cluster/_supervised.py,9,Diego Molla <dmolla-aliod@gmail.com>,
scikit-learn/sklearn/metrics/cluster/_supervised.py,10,Arnaud Fouchet <foucheta@gmail.com>,
scikit-learn/sklearn/metrics/cluster/_supervised.py,11,Thierry Guillemot <thierry.guillemot.work@gmail.com>,
scikit-learn/sklearn/metrics/cluster/_supervised.py,12,Gregory Stupp <stuppie@gmail.com>,
scikit-learn/sklearn/metrics/cluster/_supervised.py,13,Joel Nothman <joel.nothman@gmail.com>,
scikit-learn/sklearn/metrics/cluster/_supervised.py,14,Arya McCarthy <arya@jhu.edu>,
scikit-learn/sklearn/metrics/cluster/_supervised.py,15,License: BSD 3 clause,
scikit-learn/sklearn/metrics/cluster/_supervised.py,30,the exact version is faster for k == 2: use it by default globally in,
scikit-learn/sklearn/metrics/cluster/_supervised.py,31,this module instead of the float approximate variant,
scikit-learn/sklearn/metrics/cluster/_supervised.py,53,input checks,
scikit-learn/sklearn/metrics/cluster/_supervised.py,119,"Using coo_matrix to accelerate simple histogram calculation,",
scikit-learn/sklearn/metrics/cluster/_supervised.py,120,i.e. bins are consecutive integers,
scikit-learn/sklearn/metrics/cluster/_supervised.py,121,"Currently, coo_matrix is faster than histogram2d for simple cases",
scikit-learn/sklearn/metrics/cluster/_supervised.py,132,don't use += as contingency is integer,
scikit-learn/sklearn/metrics/cluster/_supervised.py,137,clustering measures,
scikit-learn/sklearn/metrics/cluster/_supervised.py,225,Special limit cases: no clustering since the data is not split;,
scikit-learn/sklearn/metrics/cluster/_supervised.py,226,or trivial clustering where each document is assigned a unique cluster.,
scikit-learn/sklearn/metrics/cluster/_supervised.py,227,These are perfect matches hence return 1.0.,
scikit-learn/sklearn/metrics/cluster/_supervised.py,233,Compute the ARI using the contingency data,
scikit-learn/sklearn/metrics/cluster/_supervised.py,628,For an array,
scikit-learn/sklearn/metrics/cluster/_supervised.py,632,For a sparse matrix,
scikit-learn/sklearn/metrics/cluster/_supervised.py,643,"Don't need to calculate the full outer product, just for non-zeroes",
scikit-learn/sklearn/metrics/cluster/_supervised.py,744,Special limit cases: no clustering since the data is not split.,
scikit-learn/sklearn/metrics/cluster/_supervised.py,745,This is a perfect match hence return 1.0.,
scikit-learn/sklearn/metrics/cluster/_supervised.py,752,Calculate the MI for the two clusterings,
scikit-learn/sklearn/metrics/cluster/_supervised.py,755,Calculate the expected value for the mutual information,
scikit-learn/sklearn/metrics/cluster/_supervised.py,757,Calculate entropy for each labeling,
scikit-learn/sklearn/metrics/cluster/_supervised.py,761,"Avoid 0.0 / 0.0 when expectation equals maximum, i.e a perfect match.",
scikit-learn/sklearn/metrics/cluster/_supervised.py,762,"normalizer should always be >= emi, but because of floating-point",
scikit-learn/sklearn/metrics/cluster/_supervised.py,763,"representation, sometimes emi is slightly larger. Correct this",
scikit-learn/sklearn/metrics/cluster/_supervised.py,764,by preserving the sign.,
scikit-learn/sklearn/metrics/cluster/_supervised.py,852,Special limit cases: no clustering since the data is not split.,
scikit-learn/sklearn/metrics/cluster/_supervised.py,853,This is a perfect match hence return 1.0.,
scikit-learn/sklearn/metrics/cluster/_supervised.py,860,Calculate the MI for the two clusterings,
scikit-learn/sklearn/metrics/cluster/_supervised.py,863,Calculate the expected value for the mutual information,
scikit-learn/sklearn/metrics/cluster/_supervised.py,864,Calculate entropy for each labeling,
scikit-learn/sklearn/metrics/cluster/_supervised.py,867,Avoid 0.0 / 0.0 when either entropy is zero.,
scikit-learn/sklearn/metrics/cluster/_supervised.py,968,log(a / b) should be calculated as log(a) - log(b) for,
scikit-learn/sklearn/metrics/cluster/_supervised.py,969,possible loss of precision,
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,21,Dictionaries of metrics,
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,22,------------------------,
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,23,The goal of having those dictionaries is to have an easy way to call a,
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,24,particular metric and associate a name to each function:,
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,25,- SUPERVISED_METRICS: all supervised cluster metrics - (when given a,
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,26,ground truth value),
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,27,- UNSUPERVISED_METRICS: all unsupervised cluster metrics,
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,28,,
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,29,Those dictionaries will be used to test systematically some invariance,
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,30,"properties, e.g. invariance toward several input layout.",
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,31,,
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,51,Lists of metrics with common properties,
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,52,---------------------------------------,
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,53,Lists of metrics with common properties are used to test systematically some,
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,54,"functionalities and invariance, e.g. SYMMETRIC_METRICS lists all metrics",
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,55,that are symmetric with respect to their input argument y_true and y_pred.,
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,56,,
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,57,--------------------------------------------------------------------,
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,58,Symmetric with respect to their input arguments y_true and y_pred.,
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,59,Symmetric metrics only apply to supervised clusters.,
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,68,Metrics whose upper bound is 1,
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,86,0.22 AMI and NMI changes,
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,106,0.22 AMI and NMI changes,
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,126,0.22 AMI and NMI changes,
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,132,All clustering metrics do not change score due to permutations of labels,
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,133,that is when 0 and 1 exchanged.,
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,149,0.22 AMI and NMI changes,
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,154,For all clustering metrics Input parameters can be both,
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,155,"in the form of arrays lists, positive, negative or string",
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,190,only the supervised metrics support single sample,
scikit-learn/sklearn/metrics/cluster/tests/test_bicluster.py,49,"B contains 2 of the 3 biclusters in A, so score should be 2/3",
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,16,Tests the Silhouette Coefficient.,
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,26,"Given that the actual labels are used, we can assume that S would be",
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,27,positive.,
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,30,Test without calculating D,
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,40,Test with sampling,
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,58,Assert Silhouette Coefficient == 0 when there is 1 sample in a cluster,
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,59,(cluster 0). We also test the case where there are identical samples,
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,60,"as the only members of a cluster (cluster 2). To our knowledge, this case",
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,61,"is not discussed in reference material, and we choose for it a sample",
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,62,score of 1.,
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,66,Cluster 0: 1 sample -> score of 0 by Rousseeuw's convention,
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,67,"Cluster 1: intra-cluster = [.5, .5, 1]",
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,68,"inter-cluster = [1, 1, 1]",
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,69,"silhouette    = [.5, .5, 0]",
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,70,"Cluster 2: intra-cluster = [0, 0]",
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,71,"inter-cluster = [arbitrary, arbitrary]",
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,72,"silhouette    = [1., 1.]",
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,81,Explicitly check per-sample results against Rousseeuw (1987),
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,82,Data from Table 1,
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,101,Data from Figure 2,
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,108,Data from Figure 3,
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,118,we check to 2dp because that's what's in the paper,
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,130,Assert 1 < n_labels < n_samples,
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,134,n_labels = n_samples,
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,141,n_labels = 1,
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,169,Make sure silhouette_samples requires diagonal to be zero.,
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,170,Non-regression test for #12178,
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,172,Construct a zero-diagonal matrix,
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,177,small values on the diagonal are OK,
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,181,values bigger than eps * 100 are not,
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,206,Assert the value is 1. when all samples are equals,
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,210,Assert the value is 0. when all the mean cluster are equal,
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,214,General case (with non numpy arrays),
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,226,Assert the value is 0. when all samples are equals,
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,230,Assert the value is 0. when all the mean cluster are equal,
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,234,General case (with non numpy arrays),
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,240,Ensure divide by zero warning is not raised in general case,
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,249,General case - cluster have one sample,
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,88,homogeneous but not complete clustering,
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,98,complete but not homogeneous clustering,
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,108,neither complete nor homogeneous but not so bad either,
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,118,test for when beta passed to,
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,119,homogeneity_completeness_v_measure,
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,120,and v_measure_score,
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,143,regression tests for labels with gaps,
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,167,Compute score for random uniform cluster labelings,
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,180,Check that adjusted scores are almost zero on random labels,
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,193,Compute the Adjusted Mutual Information and test against known values,
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,196,Mutual information,
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,199,with provided sparse contingency,
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,203,with provided dense contingency,
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,207,Expected mutual information,
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,211,Adjusted mutual information,
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,216,Test with a very large array,
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,224,Test for regression where contingency cell exceeds 2**16,
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,225,"leading to overflow in np.outer, resulting in EMI > 1",
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,230,Test overflow in mutual_info_classif and fowlkes_mallows_score,
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,271,Check numerical stability when information is exactly zero,
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,287,"Check relation between v_measure, entropy and mutual information",
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,303,General case,
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,308,Perfect match but where the label names changed,
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,313,Worst case,
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,320,handcrafted example,
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,324,FMI = TP / sqrt((TP + FP) * (TP + FN)),
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,329,symmetric property,
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,333,permutation property,
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,337,symmetric and permutation(both together),
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,349,non-regression test for #16355,
scikit-learn/sklearn/metrics/tests/test_classification.py,54,,
scikit-learn/sklearn/metrics/tests/test_classification.py,55,Utilities for testing,
scikit-learn/sklearn/metrics/tests/test_classification.py,66,import some data to play with,
scikit-learn/sklearn/metrics/tests/test_classification.py,73,restrict to a binary classification task,
scikit-learn/sklearn/metrics/tests/test_classification.py,84,add noisy features to make the problem harder and avoid perfect results,
scikit-learn/sklearn/metrics/tests/test_classification.py,88,"run classifier, get class probabilities and label predictions",
scikit-learn/sklearn/metrics/tests/test_classification.py,93,only interested in probabilities of the positive case,
scikit-learn/sklearn/metrics/tests/test_classification.py,94,XXX: do we really want a special API for the binary case?,
scikit-learn/sklearn/metrics/tests/test_classification.py,102,,
scikit-learn/sklearn/metrics/tests/test_classification.py,103,Tests,
scikit-learn/sklearn/metrics/tests/test_classification.py,107,Test performance report with dictionary output,
scikit-learn/sklearn/metrics/tests/test_classification.py,111,print classification report with class names,
scikit-learn/sklearn/metrics/tests/test_classification.py,138,assert the 2 dicts are equal.,
scikit-learn/sklearn/metrics/tests/test_classification.py,173,Dense label indicator matrix format,
scikit-learn/sklearn/metrics/tests/test_classification.py,187,Test Precision Recall and F1 Score for binary classification task,
scikit-learn/sklearn/metrics/tests/test_classification.py,190,detailed measures for each class,
scikit-learn/sklearn/metrics/tests/test_classification.py,197,individual scoring function that can be used for grid search: in the,
scikit-learn/sklearn/metrics/tests/test_classification.py,198,binary class case the score is the value of the measure for the positive,
scikit-learn/sklearn/metrics/tests/test_classification.py,199,class (e.g. label == 1). This is deprecated for average != 'binary'.,
scikit-learn/sklearn/metrics/tests/test_classification.py,218,"Test precision, recall and F-scores behave with a single positive or",
scikit-learn/sklearn/metrics/tests/test_classification.py,219,negative class,
scikit-learn/sklearn/metrics/tests/test_classification.py,220,Such a case may occur with non-stratified cross-validation,
scikit-learn/sklearn/metrics/tests/test_classification.py,236,Test handling of explicit additional (not in input) labels to PRF,
scikit-learn/sklearn/metrics/tests/test_classification.py,245,No average: zeros in array,
scikit-learn/sklearn/metrics/tests/test_classification.py,250,Macro average is changed,
scikit-learn/sklearn/metrics/tests/test_classification.py,255,No effect otheriwse,
scikit-learn/sklearn/metrics/tests/test_classification.py,265,Error when introducing invalid label in multilabel case,
scikit-learn/sklearn/metrics/tests/test_classification.py,266,(although it would only affect performance if average='macro'/None),
scikit-learn/sklearn/metrics/tests/test_classification.py,275,tests non-regression on issue #10307,
scikit-learn/sklearn/metrics/tests/test_classification.py,286,Test a subset of labels may be requested for PRF,
scikit-learn/sklearn/metrics/tests/test_classification.py,304,ensure the above were meaningful tests:,
scikit-learn/sklearn/metrics/tests/test_classification.py,311,Test that average_precision_score function returns an error when trying,
scikit-learn/sklearn/metrics/tests/test_classification.py,312,to compute average_precision_score for multiclass task.,
scikit-learn/sklearn/metrics/tests/test_classification.py,316,y_true contains three different class values,
scikit-learn/sklearn/metrics/tests/test_classification.py,324,Duplicate values with precision-recall require a different,
scikit-learn/sklearn/metrics/tests/test_classification.py,325,"processing than when computing the AUC of a ROC, because the",
scikit-learn/sklearn/metrics/tests/test_classification.py,326,precision-recall curve is a decreasing curve,
scikit-learn/sklearn/metrics/tests/test_classification.py,327,The following situation corresponds to a perfect,
scikit-learn/sklearn/metrics/tests/test_classification.py,328,"test statistic, the average_precision_score should be 1",
scikit-learn/sklearn/metrics/tests/test_classification.py,335,"Here if we go from left to right in y_true, the 0 values are",
scikit-learn/sklearn/metrics/tests/test_classification.py,336,"are separated from the 1 values, so it appears that we've",
scikit-learn/sklearn/metrics/tests/test_classification.py,337,Correctly sorted our classifications. But in fact the first two,
scikit-learn/sklearn/metrics/tests/test_classification.py,338,values have the same score (0.5) and so the first two values,
scikit-learn/sklearn/metrics/tests/test_classification.py,339,"could be swapped around, creating an imperfect sorting. This",
scikit-learn/sklearn/metrics/tests/test_classification.py,340,"imperfection should come through in the end score, making it less",
scikit-learn/sklearn/metrics/tests/test_classification.py,341,than one.,
scikit-learn/sklearn/metrics/tests/test_classification.py,351,Bad beta,
scikit-learn/sklearn/metrics/tests/test_classification.py,355,Bad pos_label,
scikit-learn/sklearn/metrics/tests/test_classification.py,361,Bad average option,
scikit-learn/sklearn/metrics/tests/test_classification.py,368,Check warning that pos_label unused when set to non-default value,
scikit-learn/sklearn/metrics/tests/test_classification.py,369,but average != 'binary'; even if data is binary.,
scikit-learn/sklearn/metrics/tests/test_classification.py,379,Test confusion matrix - binary classification case,
scikit-learn/sklearn/metrics/tests/test_classification.py,401,Test multilabel confusion matrix - binary classification case,
scikit-learn/sklearn/metrics/tests/test_classification.py,415,Test multilabel confusion matrix - multi-class case,
scikit-learn/sklearn/metrics/tests/test_classification.py,419,compute confusion matrix with default labels introspection,
scikit-learn/sklearn/metrics/tests/test_classification.py,425,compute confusion matrix with explicit label ordering,
scikit-learn/sklearn/metrics/tests/test_classification.py,432,compute confusion matrix with super set of present labels,
scikit-learn/sklearn/metrics/tests/test_classification.py,447,Test multilabel confusion matrix - multilabel-indicator case,
scikit-learn/sklearn/metrics/tests/test_classification.py,457,cross test different types,
scikit-learn/sklearn/metrics/tests/test_classification.py,470,test support for samplewise,
scikit-learn/sklearn/metrics/tests/test_classification.py,476,test support for labels,
scikit-learn/sklearn/metrics/tests/test_classification.py,481,test support for labels with samplewise,
scikit-learn/sklearn/metrics/tests/test_classification.py,488,test support for sample_weight with sample_wise,
scikit-learn/sklearn/metrics/tests/test_classification.py,501,Bad sample_weight,
scikit-learn/sklearn/metrics/tests/test_classification.py,510,Bad labels,
scikit-learn/sklearn/metrics/tests/test_classification.py,518,Using samplewise outside multilabel,
scikit-learn/sklearn/metrics/tests/test_classification.py,522,Bad y_type,
scikit-learn/sklearn/metrics/tests/test_classification.py,558,additionally check that no warnings are raised due to a division by zero,
scikit-learn/sklearn/metrics/tests/test_classification.py,570,These label vectors reproduce the contingency matrix from Artstein and,
scikit-learn/sklearn/metrics/tests/test_classification.py,571,"Poesio (2008), Table 1: np.array([[20, 20], [10, 50]]).",
scikit-learn/sklearn/metrics/tests/test_classification.py,578,Add spurious labels and ignore them.,
scikit-learn/sklearn/metrics/tests/test_classification.py,585,"Multiclass example: Artstein and Poesio, Table 4.",
scikit-learn/sklearn/metrics/tests/test_classification.py,590,"Weighting example: none, linear, quadratic.",
scikit-learn/sklearn/metrics/tests/test_classification.py,616,Check that the multiclass matthews_corrcoef agrees with the definition,
scikit-learn/sklearn/metrics/tests/test_classification.py,617,"presented in Jurman, Riccadonna, Furlanello, (2012). A Comparison of MCC",
scikit-learn/sklearn/metrics/tests/test_classification.py,618,and CEN Error Measures in MultiClass Prediction,
scikit-learn/sklearn/metrics/tests/test_classification.py,650,corrcoef of same vectors must be 1,
scikit-learn/sklearn/metrics/tests/test_classification.py,653,"corrcoef, when the two vectors are opposites of each other, should be -1",
scikit-learn/sklearn/metrics/tests/test_classification.py,661,"For the zero vector case, the corrcoef cannot be calculated and should",
scikit-learn/sklearn/metrics/tests/test_classification.py,662,result in a RuntimeWarning,
scikit-learn/sklearn/metrics/tests/test_classification.py,665,But will output 0,
scikit-learn/sklearn/metrics/tests/test_classification.py,668,And also for any other vector with 0 variance,
scikit-learn/sklearn/metrics/tests/test_classification.py,671,But will output 0,
scikit-learn/sklearn/metrics/tests/test_classification.py,674,These two vectors have 0 correlation and hence mcc should be 0,
scikit-learn/sklearn/metrics/tests/test_classification.py,679,Check that sample weight is able to selectively exclude,
scikit-learn/sklearn/metrics/tests/test_classification.py,681,Now the first half of the vector elements are alone given a weight of 1,
scikit-learn/sklearn/metrics/tests/test_classification.py,682,and hence the mcc will not be a perfect 0 as in the previous case,
scikit-learn/sklearn/metrics/tests/test_classification.py,694,corrcoef of same vectors must be 1,
scikit-learn/sklearn/metrics/tests/test_classification.py,697,with multiclass > 2 it is not possible to achieve -1,
scikit-learn/sklearn/metrics/tests/test_classification.py,702,Maximizing false positives and negatives minimizes the MCC,
scikit-learn/sklearn/metrics/tests/test_classification.py,703,The minimum will be different for depending on the input,
scikit-learn/sklearn/metrics/tests/test_classification.py,709,Zero variance will result in an mcc of zero and a Runtime Warning,
scikit-learn/sklearn/metrics/tests/test_classification.py,716,These two vectors have 0 correlation and hence mcc should be 0,
scikit-learn/sklearn/metrics/tests/test_classification.py,721,We can test that binary assumptions hold using the multiclass computation,
scikit-learn/sklearn/metrics/tests/test_classification.py,722,by masking the weight of samples not in the first two classes,
scikit-learn/sklearn/metrics/tests/test_classification.py,724,Masking the last label should let us get an MCC of -1,
scikit-learn/sklearn/metrics/tests/test_classification.py,730,"For the zero vector case, the corrcoef cannot be calculated and should",
scikit-learn/sklearn/metrics/tests/test_classification.py,731,result in a RuntimeWarning,
scikit-learn/sklearn/metrics/tests/test_classification.py,739,But will output 0,
scikit-learn/sklearn/metrics/tests/test_classification.py,745,https://github.com/scikit-learn/scikit-learn/issues/9622,
scikit-learn/sklearn/metrics/tests/test_classification.py,760,binary,
scikit-learn/sklearn/metrics/tests/test_classification.py,767,binary,
scikit-learn/sklearn/metrics/tests/test_classification.py,769,multiclass,
scikit-learn/sklearn/metrics/tests/test_classification.py,779,Test Precision Recall and F1 Score for multiclass classification task,
scikit-learn/sklearn/metrics/tests/test_classification.py,782,compute scores with default labels introspection,
scikit-learn/sklearn/metrics/tests/test_classification.py,789,averaging tests,
scikit-learn/sklearn/metrics/tests/test_classification.py,826,same prediction but with and explicit label ordering,
scikit-learn/sklearn/metrics/tests/test_classification.py,838,test that labels need not be sorted in the multilabel case,
scikit-learn/sklearn/metrics/tests/test_classification.py,854,compute scores with default labels introspection,
scikit-learn/sklearn/metrics/tests/test_classification.py,871,Check that pathological cases do not bring NaNs,
scikit-learn/sklearn/metrics/tests/test_classification.py,891,Test confusion matrix - multi-class case with subset of labels,
scikit-learn/sklearn/metrics/tests/test_classification.py,894,compute confusion matrix with only first two labels considered,
scikit-learn/sklearn/metrics/tests/test_classification.py,899,compute confusion matrix with explicit label ordering for only subset,
scikit-learn/sklearn/metrics/tests/test_classification.py,900,of labels,
scikit-learn/sklearn/metrics/tests/test_classification.py,905,a label not in y_true should result in zeros for that row/column,
scikit-learn/sklearn/metrics/tests/test_classification.py,938,confusion_matrix returns int64 by default,
scikit-learn/sklearn/metrics/tests/test_classification.py,941,The dtype of confusion_matrix is always 64 bit,
scikit-learn/sklearn/metrics/tests/test_classification.py,951,np.iinfo(np.uint32).max should be accumulated correctly,
scikit-learn/sklearn/metrics/tests/test_classification.py,957,np.iinfo(np.int64).max should cause an overflow,
scikit-learn/sklearn/metrics/tests/test_classification.py,965,Test performance report,
scikit-learn/sklearn/metrics/tests/test_classification.py,969,print classification report with class names,
scikit-learn/sklearn/metrics/tests/test_classification.py,1009,print classification report with label detection,
scikit-learn/sklearn/metrics/tests/test_classification.py,1026,Test performance report with added digits in floating point values,
scikit-learn/sklearn/metrics/tests/test_classification.py,1030,print classification report with class names,
scikit-learn/sklearn/metrics/tests/test_classification.py,1188,Dense label indicator matrix format,
scikit-learn/sklearn/metrics/tests/test_classification.py,1202,Dense label indicator matrix format,
scikit-learn/sklearn/metrics/tests/test_classification.py,1217,sp_hamming only works with 1-D arrays,
scikit-learn/sklearn/metrics/tests/test_classification.py,1257,Dense label indicator matrix format,
scikit-learn/sklearn/metrics/tests/test_classification.py,1261,"size(y1 \inter y2) = [1, 2]",
scikit-learn/sklearn/metrics/tests/test_classification.py,1262,"size(y1 \union y2) = [2, 2]",
scikit-learn/sklearn/metrics/tests/test_classification.py,1274,average='macro',
scikit-learn/sklearn/metrics/tests/test_classification.py,1277,average='micro',
scikit-learn/sklearn/metrics/tests/test_classification.py,1280,average='samples',
scikit-learn/sklearn/metrics/tests/test_classification.py,1289,average=None,
scikit-learn/sklearn/metrics/tests/test_classification.py,1297,average='weighted',
scikit-learn/sklearn/metrics/tests/test_classification.py,1343,"other than average='samples'/'none-samples', test everything else here",
scikit-learn/sklearn/metrics/tests/test_classification.py,1361,"tp=0, fp=0, fn=1, tn=0",
scikit-learn/sklearn/metrics/tests/test_classification.py,1363,"tp=0, fp=0, fn=0, tn=1",
scikit-learn/sklearn/metrics/tests/test_classification.py,1371,"tp=1, fp=0, fn=0, tn=0 (pos_label=0)",
scikit-learn/sklearn/metrics/tests/test_classification.py,1387,Test precision_recall_f1_score on a crafted multilabel example,
scikit-learn/sklearn/metrics/tests/test_classification.py,1388,First crafted example,
scikit-learn/sklearn/metrics/tests/test_classification.py,1395,"tp = [0, 1, 1, 0]",
scikit-learn/sklearn/metrics/tests/test_classification.py,1396,"fn = [1, 0, 0, 1]",
scikit-learn/sklearn/metrics/tests/test_classification.py,1397,"fp = [1, 1, 0, 0]",
scikit-learn/sklearn/metrics/tests/test_classification.py,1398,Check per class,
scikit-learn/sklearn/metrics/tests/test_classification.py,1409,Check macro,
scikit-learn/sklearn/metrics/tests/test_classification.py,1419,Check micro,
scikit-learn/sklearn/metrics/tests/test_classification.py,1430,Check weighted,
scikit-learn/sklearn/metrics/tests/test_classification.py,1440,Check samples,
scikit-learn/sklearn/metrics/tests/test_classification.py,1441,"|h(x_i) inter y_i | = [0, 1, 1]",
scikit-learn/sklearn/metrics/tests/test_classification.py,1442,"|y_i| = [1, 1, 2]",
scikit-learn/sklearn/metrics/tests/test_classification.py,1443,"|h(x_i)| = [1, 1, 2]",
scikit-learn/sklearn/metrics/tests/test_classification.py,1456,Test precision_recall_f1_score on a crafted multilabel example 2,
scikit-learn/sklearn/metrics/tests/test_classification.py,1457,Second crafted example,
scikit-learn/sklearn/metrics/tests/test_classification.py,1461,tp = [ 0.  1.  0.  0.],
scikit-learn/sklearn/metrics/tests/test_classification.py,1462,fp = [ 1.  0.  0.  2.],
scikit-learn/sklearn/metrics/tests/test_classification.py,1463,fn = [ 1.  1.  1.  0.],
scikit-learn/sklearn/metrics/tests/test_classification.py,1508,Check samples,
scikit-learn/sklearn/metrics/tests/test_classification.py,1509,"|h(x_i) inter y_i | = [0, 0, 1]",
scikit-learn/sklearn/metrics/tests/test_classification.py,1510,"|y_i| = [1, 1, 2]",
scikit-learn/sklearn/metrics/tests/test_classification.py,1511,"|h(x_i)| = [1, 1, 2]",
scikit-learn/sklearn/metrics/tests/test_classification.py,1528,true_pos = [ 0.  1.  1.  0.],
scikit-learn/sklearn/metrics/tests/test_classification.py,1529,false_pos = [ 0.  0.  0.  1.],
scikit-learn/sklearn/metrics/tests/test_classification.py,1530,false_neg = [ 1.  1.  0.  0.],
scikit-learn/sklearn/metrics/tests/test_classification.py,1583,"|h(x_i) inter y_i | = [0, 0, 2]",
scikit-learn/sklearn/metrics/tests/test_classification.py,1584,"|y_i| = [1, 1, 2]",
scikit-learn/sklearn/metrics/tests/test_classification.py,1585,"|h(x_i)| = [0, 1, 2]",
scikit-learn/sklearn/metrics/tests/test_classification.py,1643,"tp = [0, 0, 0]",
scikit-learn/sklearn/metrics/tests/test_classification.py,1644,"fn = [0, 0, 0]",
scikit-learn/sklearn/metrics/tests/test_classification.py,1645,"fp = [0, 0, 0]",
scikit-learn/sklearn/metrics/tests/test_classification.py,1646,"support = [0, 0, 0]",
scikit-learn/sklearn/metrics/tests/test_classification.py,1647,"|y_hat_i inter y_i | = [0, 0, 0]",
scikit-learn/sklearn/metrics/tests/test_classification.py,1648,"|y_i| = [0, 0, 0]",
scikit-learn/sklearn/metrics/tests/test_classification.py,1649,"|y_hat_i| = [0, 0, 0]",
scikit-learn/sklearn/metrics/tests/test_classification.py,1679,"tp = [0, 0, 0]",
scikit-learn/sklearn/metrics/tests/test_classification.py,1680,"fn = [0, 0, 0]",
scikit-learn/sklearn/metrics/tests/test_classification.py,1681,"fp = [0, 0, 0]",
scikit-learn/sklearn/metrics/tests/test_classification.py,1682,"support = [0, 0, 0]",
scikit-learn/sklearn/metrics/tests/test_classification.py,1683,"|y_hat_i inter y_i | = [0, 0, 0]",
scikit-learn/sklearn/metrics/tests/test_classification.py,1684,"|y_i| = [0, 0, 0]",
scikit-learn/sklearn/metrics/tests/test_classification.py,1685,"|y_hat_i| = [0, 0, 0]",
scikit-learn/sklearn/metrics/tests/test_classification.py,1704,average of per-label scores,
scikit-learn/sklearn/metrics/tests/test_classification.py,1720,average of per-sample scores,
scikit-learn/sklearn/metrics/tests/test_classification.py,1735,single score: micro-average,
scikit-learn/sklearn/metrics/tests/test_classification.py,1750,single positive label,
scikit-learn/sklearn/metrics/tests/test_classification.py,1780,average of per-label scores,
scikit-learn/sklearn/metrics/tests/test_classification.py,1790,average of per-sample scores,
scikit-learn/sklearn/metrics/tests/test_classification.py,1799,single score: micro-average,
scikit-learn/sklearn/metrics/tests/test_classification.py,1808,single positive label,
scikit-learn/sklearn/metrics/tests/test_classification.py,1911,Error if user does not explicitly set non-binary average mode,
scikit-learn/sklearn/metrics/tests/test_classification.py,1934,"Check that _check_targets correctly merges target types, squeezes",
scikit-learn/sklearn/metrics/tests/test_classification.py,1935,output and fails if input lengths differ.,
scikit-learn/sklearn/metrics/tests/test_classification.py,1942,all of length 3,
scikit-learn/sklearn/metrics/tests/test_classification.py,1945,must not be considered binary,
scikit-learn/sklearn/metrics/tests/test_classification.py,1956,"expected type given input types, or None for error",
scikit-learn/sklearn/metrics/tests/test_classification.py,1957,(types will be tried in either order),
scikit-learn/sklearn/metrics/tests/test_classification.py,1967,Disallowed types,
scikit-learn/sklearn/metrics/tests/test_classification.py,2018,Make sure seq of seq is not supported,
scikit-learn/sklearn/metrics/tests/test_classification.py,2030,https://github.com/scikit-learn/scikit-learn/issues/8098,
scikit-learn/sklearn/metrics/tests/test_classification.py,2108,"Currently, invariance of string and integer labels cannot be tested",
scikit-learn/sklearn/metrics/tests/test_classification.py,2109,in common invariance tests because invariance tests for multiclass,
scikit-learn/sklearn/metrics/tests/test_classification.py,2110,decision functions is not implemented yet.,
scikit-learn/sklearn/metrics/tests/test_classification.py,2135,"binary case with symbolic labels (""no"" < ""yes"")",
scikit-learn/sklearn/metrics/tests/test_classification.py,2142,multiclass case; adapted from http://bit.ly/RJJHWA,
scikit-learn/sklearn/metrics/tests/test_classification.py,2148,check that we got all the shapes and axes right,
scikit-learn/sklearn/metrics/tests/test_classification.py,2149,by doubling the length of y_true and y_pred,
scikit-learn/sklearn/metrics/tests/test_classification.py,2155,check eps and handling of absolute zero and one probabilities,
scikit-learn/sklearn/metrics/tests/test_classification.py,2160,raise error if number of classes are not equal.,
scikit-learn/sklearn/metrics/tests/test_classification.py,2166,case when y_true is a string array object,
scikit-learn/sklearn/metrics/tests/test_classification.py,2172,test labels option,
scikit-learn/sklearn/metrics/tests/test_classification.py,2187,works when the labels argument is used,
scikit-learn/sklearn/metrics/tests/test_classification.py,2193,ensure labels work when len(np.unique(y_true)) != y_pred.shape[1],
scikit-learn/sklearn/metrics/tests/test_classification.py,2201,case when input is a pandas series and dataframe gh-5715,
scikit-learn/sklearn/metrics/tests/test_classification.py,2211,"y_pred dataframe, y_true series",
scikit-learn/sklearn/metrics/tests/test_classification.py,2218,Check brier_score_loss function,
scikit-learn/sklearn/metrics/tests/test_classification.py,2236,ensure to raise an error for multiclass y_true,
scikit-learn/sklearn/metrics/tests/test_classification.py,2244,calculate correctly when there's only one class in y_true,
scikit-learn/sklearn/metrics/tests/test_classification.py,2269,Warnings are tested in test_balanced_accuracy_score_unseen,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,52,Test the pairwise_distance helper function.,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,55,Euclidean distance should be equivalent to calling the function.,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,61,"Euclidean distance, with Y != X.",
scikit-learn/sklearn/metrics/tests/test_pairwise.py,66,Check to ensure NaNs work with pairwise_distances.,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,74,Test with tuples as X and Y,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,80,Test haversine distance,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,81,The data should be valid latitude and longitude,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,89,"Test haversine distance, with Y != X",
scikit-learn/sklearn/metrics/tests/test_pairwise.py,97,"""cityblock"" uses scikit-learn metric, cityblock (function) is",
scikit-learn/sklearn/metrics/tests/test_pairwise.py,98,scipy.spatial.,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,105,The manhattan metric should be equivalent to cityblock.,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,112,Test cosine as a string metric versus cosine callable,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,113,"The string ""cosine"" uses sklearn.metric,",
scikit-learn/sklearn/metrics/tests/test_pairwise.py,114,while the function cosine is scipy.spatial,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,121,"Test with sparse X and Y,",
scikit-learn/sklearn/metrics/tests/test_pairwise.py,122,"currently only supported for Euclidean, L1 and cosine.",
scikit-learn/sklearn/metrics/tests/test_pairwise.py,137,"Test with scipy.spatial.distance metric, with a kwd",
scikit-learn/sklearn/metrics/tests/test_pairwise.py,143,same with Y = None,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,149,Test that scipy distance metrics throw an error if sparse matrix given,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,155,Test that a value error is raised if the metric is unknown,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,162,test that we convert to boolean arrays for boolean distances,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,168,ignore conversion to boolean in pairwise_distances,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,175,non-boolean arrays are converted to boolean for boolean,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,176,distance metrics with a data conversion warning,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,181,Check that the warning is raised if X is boolean by Y is not boolean:,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,185,Check that no warning is raised if X is already boolean and Y is None:,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,192,No warnings issued if metric is not a boolean distance function,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,202,Test correct shape,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,205,with two args,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,208,even if shape[1] agrees (although thus second arg is spurious),
scikit-learn/sklearn/metrics/tests/test_pairwise.py,212,Test not copied (if appropriate dtype),
scikit-learn/sklearn/metrics/tests/test_pairwise.py,216,with two args,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,221,Test always returns float dtype,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,225,Test converts list to array-like,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,231,Test non-negative values,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,240,Callable version of pairwise.rbf_kernel.,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,262,Not all metrics support sparse input,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,263,ValueError may be triggered by bad callable,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,279,"paired_distances should allow callable metric where metric(x, x) != 0",
scikit-learn/sklearn/metrics/tests/test_pairwise.py,280,Knowing that the callable is a strict metric would allow the diagonal to,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,281,be left uncalculated and set to 0.,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,285,Test with all metrics that should be in PAIRWISE_KERNEL_FUNCTIONS.,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,291,Test the pairwise_kernels helper function.,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,297,Test with Y=None,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,301,Test with Y=Y,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,305,Test with tuples as X and Y,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,311,Test with sparse X and Y,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,315,these don't support sparse matrices yet,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,325,Test the pairwise_kernels helper function,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,326,"with a callable function, with given keywords.",
scikit-learn/sklearn/metrics/tests/test_pairwise.py,337,"callable function, X=Y",
scikit-learn/sklearn/metrics/tests/test_pairwise.py,358,Test the pairwise_distance helper function.,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,360,Euclidean distance should be equivalent to calling the function.,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,362,"Euclidean distance, with Y != X.",
scikit-learn/sklearn/metrics/tests/test_pairwise.py,371,Check the pairwise_distances implementation,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,372,gives the same value,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,379,Test the pairwise_distance helper function,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,380,with the callable implementation,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,382,Euclidean distance should be equivalent to calling the function.,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,384,"Euclidean distance, with Y != X.",
scikit-learn/sklearn/metrics/tests/test_pairwise.py,391,Test that a value error is raised when the lengths of X and Y should not,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,392,differ,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,399,Check pairwise minimum distances computation for any metric,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,410,euclidean metric,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,416,sparse matrix case,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,420,We don't want np.matrix here,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,424,euclidean metric squared,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,430,Non-euclidean scikit-learn metric,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,436,sparse matrix case,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,441,Non-euclidean Scipy distance (callable),
scikit-learn/sklearn/metrics/tests/test_pairwise.py,447,Non-euclidean Scipy distance (string),
scikit-learn/sklearn/metrics/tests/test_pairwise.py,453,Compare with naive implementation,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,475,Reduced Euclidean distance,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,482,atol is for diagonal where S is explicitly zeroed on the diagonal,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,487,check that the reduce func is allowed to return None,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,577,Test the pairwise_distance helper function.,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,579,Euclidean distance should be equivalent to calling the function.,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,583,Test small amounts of memory,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,587,X as list,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,590,"Euclidean distance, with Y != X.",
scikit-learn/sklearn/metrics/tests/test_pairwise.py,596,absurdly large working_memory,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,599,"""cityblock"" uses scikit-learn metric, cityblock (function) is",
scikit-learn/sklearn/metrics/tests/test_pairwise.py,600,scipy.spatial.,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,603,Test that a value error is raised if the metric is unknown,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,607,Test precomputed returns all at once,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,623,Check the pairwise Euclidean distances computation on known result,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,634,"check that we still get the right answers with {X,Y}_norm_squared",
scikit-learn/sklearn/metrics/tests/test_pairwise.py,635,"and that we get a wrong answer with wrong {X,Y}_norm_squared",
scikit-learn/sklearn/metrics/tests/test_pairwise.py,640,norms will only be used if their dtype is float64,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,655,"check we get the wrong answer with wrong {X,Y}_norm_squared",
scikit-learn/sklearn/metrics/tests/test_pairwise.py,669,check that euclidean distances gives same result as scipy cdist,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,670,when X and Y != X are provided,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,683,the default rtol=1e-7 is too close to the float32 precision,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,684,and fails due too rounding errors.,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,693,check that euclidean distances gives same result as scipy pdist,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,694,when only X is provided,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,704,the default rtol=1e-7 is too close to the float32 precision,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,705,and fails due too rounding errors.,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,717,check batches handling when Y != X (#13910),
scikit-learn/sklearn/metrics/tests/test_pairwise.py,731,the default rtol=1e-7 is too close to the float32 precision,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,732,and fails due too rounding errors.,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,740,check batches handling when X is Y (#13910),
scikit-learn/sklearn/metrics/tests/test_pairwise.py,751,the default rtol=1e-7 is too close to the float32 precision,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,752,and fails due too rounding errors.,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,764,check that euclidean distances is correct with float32 input thanks to,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,765,upcasting. On float64 there are still precision issues.,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,777,with no nan values,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,856,Check for symmetry,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,862,Check with explicit formula and squared=True,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,868,Check with explicit formula and squared=False,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,874,Check when Y = X is explicitly passed,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,881,Check copy = True against copy = False,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,889,First feature is the only feature that is non-nan and in both,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,890,samples. The result of `nan_euclidean_distances` with squared=True,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,891,should be non-negative. The non-squared version should all be close to 0.,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,905,Check the pairwise Cosine distances computation,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,911,"check that all elements are in [0, 2]",
scikit-learn/sklearn/metrics/tests/test_pairwise.py,914,check that diagonal elements are equal to 0,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,919,"check that all elements are in [0, 2]",
scikit-learn/sklearn/metrics/tests/test_pairwise.py,922,check that diagonal elements are equal to 0 and non diagonal to 2,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,925,check large random matrix,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,928,check that diagonal elements are equal to 0,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,935,Check haversine distance with distances computation,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,950,Test haversine distance does not accept X where n_feature != 2,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,957,Paired distances,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,960,Check the paired Euclidean distances computation,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,968,Check the paired manhattan distances computation,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,990,check diagonal is ones for data with itself,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,993,check off-diagonal is < 1 but > 0:,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,996,check that float32 is preserved,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1002,"check integer type gets converted,",
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1003,check that zeros are handled,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1009,check that kernel of similar things is greater than dissimilar ones,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1016,test negative input,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1024,different n_features in X and Y,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1028,sparse matrices,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1040,Valid kernels should be symmetric,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1064,the diagonal elements of a linear kernel are their squared norm,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1072,the diagonal elements of a rbf kernel are 1,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1080,the diagonal elements of a laplacian kernel are 1,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1083,off-diagonal elements are < 1 but > 0:,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1098,should be sparse,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1102,"should be dense, and equal to K1",
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1107,show the kernel output equal to the sparse.todense(),
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1113,Test the cosine_similarity.,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1123,Test that the cosine is kernel is equal to a linear kernel when data,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1124,has been previously normalized by L2-norm.,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1134,Ensure that pairwise array check works for dense matrices.,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1135,"Check that if XB is None, XB is returned as reference to XA",
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1143,"Ensure that if XA and XB are given correctly, they return as equal.",
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1144,"Check that if XB is not None, it is returned equal.",
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1145,Note that the second dimension of XB is the same as XA.,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1159,Ensure an error is raised if the dimensions are different.,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1171,Ensure an error is raised on 1D input arrays.,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1172,"The modified tests are not 1D. In the old test, the array was internally",
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1173,converted to 2D anyways,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1185,Ensures that checks return valid sparse matrices.,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1192,compare their difference because testing csr matrices for,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1193,equality with '==' does not work as expected.,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1207,Turns a numpy matrix (any n-dimensional array) into tuples.,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1210,Tuplify each sub-array in the input.,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1213,"Single dimension input, just return tuple of contents.",
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1218,Ensures that checks return valid tuples.,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1230,Ensures that type float32 is preserved.,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1237,both float32,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1242,mismatched A,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1248,mismatched B,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1262,check that pairwise_distances give the same result in sequential and,
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1263,"parallel, when metric has data-derived parameters.",
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1264,to have more than 1 chunk,
scikit-learn/sklearn/metrics/tests/test_regression.py,41,"Tweedie deviance needs positive y_pred, except for p=0,",
scikit-learn/sklearn/metrics/tests/test_regression.py,42,p>=2 needs positive y_true,
scikit-learn/sklearn/metrics/tests/test_regression.py,43,results evaluated by sympy,
scikit-learn/sklearn/metrics/tests/test_regression.py,60,non-regression test for,
scikit-learn/sklearn/metrics/tests/test_regression.py,61,https://github.com/scikit-learn/scikit-learn/pull/16323,
scikit-learn/sklearn/metrics/tests/test_regression.py,84,mean_absolute_error and mean_squared_error are equal because,
scikit-learn/sklearn/metrics/tests/test_regression.py,85,it is a binary problem.,
scikit-learn/sklearn/metrics/tests/test_regression.py,120,Tweedie deviance error,
scikit-learn/sklearn/metrics/tests/test_regression.py,159,All of length 3,
scikit-learn/sklearn/metrics/tests/test_regression.py,209,mean_absolute_error and mean_squared_error are equal because,
scikit-learn/sklearn/metrics/tests/test_regression.py,210,it is a binary problem.,
scikit-learn/sklearn/metrics/tests/test_regression.py,228,Checking for the condition in which both numerator and denominator is,
scikit-learn/sklearn/metrics/tests/test_regression.py,229,zero.,
scikit-learn/sklearn/metrics/tests/test_regression.py,240,Handling msle separately as it does not accept negative inputs.,
scikit-learn/sklearn/metrics/tests/test_regression.py,266,Handling msle separately as it does not accept negative inputs.,
scikit-learn/sklearn/metrics/tests/test_regression.py,281,Trigger the warning,
scikit-learn/sklearn/metrics/tests/test_regression.py,296,"Ws we get closer to the limit, with 1e-12 difference the absolute",
scikit-learn/sklearn/metrics/tests/test_regression.py,297,tolerance to pass the below check increases. There are likely,
scikit-learn/sklearn/metrics/tests/test_regression.py,298,numerical precision issues on the edges of different definition,
scikit-learn/sklearn/metrics/tests/test_regression.py,299,regions.,
scikit-learn/sklearn/metrics/tests/test_common.py,63,Note toward developers about metric testing,
scikit-learn/sklearn/metrics/tests/test_common.py,64,-------------------------------------------,
scikit-learn/sklearn/metrics/tests/test_common.py,65,It is often possible to write one general test for several metrics:,
scikit-learn/sklearn/metrics/tests/test_common.py,66,,
scikit-learn/sklearn/metrics/tests/test_common.py,67,"- invariance properties, e.g. invariance to sample order",
scikit-learn/sklearn/metrics/tests/test_common.py,68,"- common behavior for an argument, e.g. the ""normalize"" with value True",
scikit-learn/sklearn/metrics/tests/test_common.py,69,will return the mean of the metrics and with value False will return,
scikit-learn/sklearn/metrics/tests/test_common.py,70,the sum of the metrics.,
scikit-learn/sklearn/metrics/tests/test_common.py,71,,
scikit-learn/sklearn/metrics/tests/test_common.py,72,"In order to improve the overall metric testing, it is a good idea to write",
scikit-learn/sklearn/metrics/tests/test_common.py,73,first a specific test for the given metric and then add a general test for,
scikit-learn/sklearn/metrics/tests/test_common.py,74,all metrics that have the same behavior.,
scikit-learn/sklearn/metrics/tests/test_common.py,75,,
scikit-learn/sklearn/metrics/tests/test_common.py,76,Two types of datastructures are used in order to implement this system:,
scikit-learn/sklearn/metrics/tests/test_common.py,77,dictionaries of metrics and lists of metrics wit common properties.,
scikit-learn/sklearn/metrics/tests/test_common.py,78,,
scikit-learn/sklearn/metrics/tests/test_common.py,79,Dictionaries of metrics,
scikit-learn/sklearn/metrics/tests/test_common.py,80,------------------------,
scikit-learn/sklearn/metrics/tests/test_common.py,81,The goal of having those dictionaries is to have an easy way to call a,
scikit-learn/sklearn/metrics/tests/test_common.py,82,particular metric and associate a name to each function:,
scikit-learn/sklearn/metrics/tests/test_common.py,83,,
scikit-learn/sklearn/metrics/tests/test_common.py,84,- REGRESSION_METRICS: all regression metrics.,
scikit-learn/sklearn/metrics/tests/test_common.py,85,- CLASSIFICATION_METRICS: all classification metrics,
scikit-learn/sklearn/metrics/tests/test_common.py,86,which compare a ground truth and the estimated targets as returned by a,
scikit-learn/sklearn/metrics/tests/test_common.py,87,classifier.,
scikit-learn/sklearn/metrics/tests/test_common.py,88,- THRESHOLDED_METRICS: all classification metrics which,
scikit-learn/sklearn/metrics/tests/test_common.py,89,"compare a ground truth and a score, e.g. estimated probabilities or",
scikit-learn/sklearn/metrics/tests/test_common.py,90,decision function (format might vary),
scikit-learn/sklearn/metrics/tests/test_common.py,91,,
scikit-learn/sklearn/metrics/tests/test_common.py,92,Those dictionaries will be used to test systematically some invariance,
scikit-learn/sklearn/metrics/tests/test_common.py,93,"properties, e.g. invariance toward several input layout.",
scikit-learn/sklearn/metrics/tests/test_common.py,94,,
scikit-learn/sklearn/metrics/tests/test_common.py,117,`confusion_matrix` returns absolute values and hence behaves unnormalized,
scikit-learn/sklearn/metrics/tests/test_common.py,118,. Naming it with an unnormalized_ prefix is necessary for this module to,
scikit-learn/sklearn/metrics/tests/test_common.py,119,skip sample_weight scaling checks which will fail for unnormalized,
scikit-learn/sklearn/metrics/tests/test_common.py,120,metrics.,
scikit-learn/sklearn/metrics/tests/test_common.py,135,These are needed to test averaging,
scikit-learn/sklearn/metrics/tests/test_common.py,218,"default: average=""macro""",
scikit-learn/sklearn/metrics/tests/test_common.py,231,"default: average=""macro""",
scikit-learn/sklearn/metrics/tests/test_common.py,250,Lists of metrics with common properties,
scikit-learn/sklearn/metrics/tests/test_common.py,251,---------------------------------------,
scikit-learn/sklearn/metrics/tests/test_common.py,252,Lists of metrics with common properties are used to test systematically some,
scikit-learn/sklearn/metrics/tests/test_common.py,253,"functionalities and invariance, e.g. SYMMETRIC_METRICS lists all metrics that",
scikit-learn/sklearn/metrics/tests/test_common.py,254,are symmetric with respect to their input argument y_true and y_pred.,
scikit-learn/sklearn/metrics/tests/test_common.py,255,,
scikit-learn/sklearn/metrics/tests/test_common.py,256,"When you add a new metric or functionality, check if a general test",
scikit-learn/sklearn/metrics/tests/test_common.py,257,is already written.,
scikit-learn/sklearn/metrics/tests/test_common.py,259,Those metrics don't support binary inputs,
scikit-learn/sklearn/metrics/tests/test_common.py,275,Those metrics don't support multiclass inputs,
scikit-learn/sklearn/metrics/tests/test_common.py,292,"with default average='binary', multiclass is prohibited",
scikit-learn/sklearn/metrics/tests/test_common.py,299,curves,
scikit-learn/sklearn/metrics/tests/test_common.py,304,"Metric undefined with ""binary"" or ""multiclass"" input",
scikit-learn/sklearn/metrics/tests/test_common.py,308,"Metrics with an ""average"" argument",
scikit-learn/sklearn/metrics/tests/test_common.py,314,"Threshold-based metrics with an ""average"" argument",
scikit-learn/sklearn/metrics/tests/test_common.py,319,"Metrics with a ""pos_label"" argument",
scikit-learn/sklearn/metrics/tests/test_common.py,334,pos_label support deprecated; to be removed in 0.18:,
scikit-learn/sklearn/metrics/tests/test_common.py,345,"Metrics with a ""labels"" argument",
scikit-learn/sklearn/metrics/tests/test_common.py,346,TODO: Handle multi_class metrics that has a labels argument as well as a,
scikit-learn/sklearn/metrics/tests/test_common.py,347,decision function argument. e.g hinge_loss,
scikit-learn/sklearn/metrics/tests/test_common.py,375,"Metrics with a ""normalize"" option",
scikit-learn/sklearn/metrics/tests/test_common.py,381,"Threshold-based metrics with ""multilabel-indicator"" format support",
scikit-learn/sklearn/metrics/tests/test_common.py,400,"Classification metrics with  ""multilabel-indicator"" format",
scikit-learn/sklearn/metrics/tests/test_common.py,425,"Regression metrics with ""multioutput-continuous"" format support",
scikit-learn/sklearn/metrics/tests/test_common.py,431,Symmetric with respect to their input arguments y_true and y_pred,
scikit-learn/sklearn/metrics/tests/test_common.py,432,"metric(y_true, y_pred) == metric(y_pred, y_true).",
scikit-learn/sklearn/metrics/tests/test_common.py,444,P = R = F = accuracy in multiclass case,
scikit-learn/sklearn/metrics/tests/test_common.py,454,Asymmetric with respect to their input arguments y_true and y_pred,
scikit-learn/sklearn/metrics/tests/test_common.py,455,"metric(y_true, y_pred) != metric(y_pred, y_true).",
scikit-learn/sklearn/metrics/tests/test_common.py,479,No Sample weight support,
scikit-learn/sklearn/metrics/tests/test_common.py,504,We shouldn't forget any metrics,
scikit-learn/sklearn/metrics/tests/test_common.py,517,Test the symmetry of score and loss functions,
scikit-learn/sklearn/metrics/tests/test_common.py,544,Test the symmetry of score and loss functions,
scikit-learn/sklearn/metrics/tests/test_common.py,554,use context manager to supply custom error message,
scikit-learn/sklearn/metrics/tests/test_common.py,583,Generate some data,
scikit-learn/sklearn/metrics/tests/test_common.py,654,Mix format support,
scikit-learn/sklearn/metrics/tests/test_common.py,679,These mix representations aren't allowed,
scikit-learn/sklearn/metrics/tests/test_common.py,693,"NB: We do not test for y1_row, y2_row as these may be",
scikit-learn/sklearn/metrics/tests/test_common.py,694,interpreted as multilabel or multioutput data.,
scikit-learn/sklearn/metrics/tests/test_common.py,705,Ensure that classification metrics with string labels are invariant,
scikit-learn/sklearn/metrics/tests/test_common.py,720,"Ugly, but handle case with a pos_label and label",
scikit-learn/sklearn/metrics/tests/test_common.py,753,Ensure that thresholded metrics with string labels are invariant,
scikit-learn/sklearn/metrics/tests/test_common.py,765,"Ugly, but handle case with a pos_label and label",
scikit-learn/sklearn/metrics/tests/test_common.py,781,TODO those metrics doesn't support string label yet,
scikit-learn/sklearn/metrics/tests/test_common.py,805,Classification metrics all raise a mixed input exception,
scikit-learn/sklearn/metrics/tests/test_common.py,814,Non-regression test: scores should work with a single sample.,
scikit-learn/sklearn/metrics/tests/test_common.py,815,This is important for leave-one-out cross validation.,
scikit-learn/sklearn/metrics/tests/test_common.py,816,"Score functions tested are those that formerly called np.squeeze,",
scikit-learn/sklearn/metrics/tests/test_common.py,817,which turns an array of size 1 into a 0-d array (!).,
scikit-learn/sklearn/metrics/tests/test_common.py,820,assert that no exception is thrown,
scikit-learn/sklearn/metrics/tests/test_common.py,840,Those metrics are not always defined with one sample,
scikit-learn/sklearn/metrics/tests/test_common.py,841,or in multiclass classification,
scikit-learn/sklearn/metrics/tests/test_common.py,865,test invariance to dimension shuffling,
scikit-learn/sklearn/metrics/tests/test_common.py,883,Generate some data,
scikit-learn/sklearn/metrics/tests/test_common.py,894,To make sure at least one empty label is present,
scikit-learn/sklearn/metrics/tests/test_common.py,910,XXX cruel hack to work with partial functions,
scikit-learn/sklearn/metrics/tests/test_common.py,917,Check representation invariance,
scikit-learn/sklearn/metrics/tests/test_common.py,938,make sure the multilabel-sequence format raises ValueError,
scikit-learn/sklearn/metrics/tests/test_common.py,954,Test in the binary case,
scikit-learn/sklearn/metrics/tests/test_common.py,971,Test in the multiclass case,
scikit-learn/sklearn/metrics/tests/test_common.py,987,Test in the multilabel case,
scikit-learn/sklearn/metrics/tests/test_common.py,991,"for both random_state 0 and 1, y_true and y_pred has at least one",
scikit-learn/sklearn/metrics/tests/test_common.py,992,unlabelled entry,
scikit-learn/sklearn/metrics/tests/test_common.py,1004,To make sure at least one empty label is present,
scikit-learn/sklearn/metrics/tests/test_common.py,1023,No averaging,
scikit-learn/sklearn/metrics/tests/test_common.py,1029,Micro measure,
scikit-learn/sklearn/metrics/tests/test_common.py,1034,Macro measure,
scikit-learn/sklearn/metrics/tests/test_common.py,1038,Weighted measure,
scikit-learn/sklearn/metrics/tests/test_common.py,1049,Sample measure,
scikit-learn/sklearn/metrics/tests/test_common.py,1129,Test _average_binary_score for weight.sum() == 0,
scikit-learn/sklearn/metrics/tests/test_common.py,1154,check that unit weights gives the same score as no weight,
scikit-learn/sklearn/metrics/tests/test_common.py,1163,check that the weighted and unweighted scores are unequal,
scikit-learn/sklearn/metrics/tests/test_common.py,1166,use context manager to supply custom error message,
scikit-learn/sklearn/metrics/tests/test_common.py,1173,check that sample_weight can be a list,
scikit-learn/sklearn/metrics/tests/test_common.py,1182,check that integer weights is the same as repeated samples,
scikit-learn/sklearn/metrics/tests/test_common.py,1190,check that ignoring a fraction of the samples is equivalent to setting,
scikit-learn/sklearn/metrics/tests/test_common.py,1191,the corresponding weights to zero,
scikit-learn/sklearn/metrics/tests/test_common.py,1208,check that the score is invariant under scaling of the weights by a,
scikit-learn/sklearn/metrics/tests/test_common.py,1209,common factor,
scikit-learn/sklearn/metrics/tests/test_common.py,1217,Check that if number of samples in y_true and sample_weight are not,
scikit-learn/sklearn/metrics/tests/test_common.py,1218,"equal, meaningful error is raised.",
scikit-learn/sklearn/metrics/tests/test_common.py,1236,regression,
scikit-learn/sklearn/metrics/tests/test_common.py,1249,binary,
scikit-learn/sklearn/metrics/tests/test_common.py,1268,multiclass,
scikit-learn/sklearn/metrics/tests/test_common.py,1276,softmax,
scikit-learn/sklearn/metrics/tests/test_common.py,1289,multilabel indicator,
scikit-learn/sklearn/metrics/tests/test_common.py,1310,test labels argument when not using averaging,
scikit-learn/sklearn/metrics/tests/test_common.py,1311,in multi-class and multi-label cases,
scikit-learn/sklearn/metrics/tests/test_common.py,1361,Makes sure all samples have at least one label. This works around errors,
scikit-learn/sklearn/metrics/tests/test_common.py,1362,"when running metrics where average=""sample""",
scikit-learn/sklearn/metrics/tests/test_score_objects.py,64,All supervised cluster scorers (They behave like classification metric),
scikit-learn/sklearn/metrics/tests/test_score_objects.py,89,Make estimators that make sense to test various scoring methods,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,91,some of the regressions scorers require strictly positive input.,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,111,Create some memory mapped data,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,125,GC closes the mmap file descriptors,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,167,Test that all scorers have a working repr,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,173,Test all branches of single metric usecases,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,200,Test the allow_none parameter for check_scoring alone,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,208,This wraps the _check_multimetric_scoring to take in,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,209,single metric scoring parameter so we can run the tests,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,210,"that we will run for check_scoring, for check_multimetric_scoring",
scikit-learn/sklearn/metrics/tests/test_score_objects.py,211,too for single-metric usecases,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,214,"For all single metric use cases, it should register as not multimetric",
scikit-learn/sklearn/metrics/tests/test_score_objects.py,227,To make sure the check_scoring is correctly applied to the constituent,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,228,scorers,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,232,For multiple metric use cases,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,233,Make sure it works for the valid cases,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,262,Make sure it raises errors when scoring parameter is not valid.,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,263,More weird corner cases are tested at test_validation.py,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,265,Tuple of callables,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,273,test that check_scoring works on GridSearchCV and pipeline.,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,274,slightly redundant non-regression test.,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,284,check that cross_val_score definitely calls the scorer,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,285,and doesn't make any assumptions about the estimator apart from having a,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,286,fit.,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,293,Sanity check on the make_scorer factory function.,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,300,Test classification scorers.,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,329,test fbeta score that takes an argument,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,335,test that custom scorer can be pickled,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,340,smoke test the repr:,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,345,Test regression scorers.,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,357,Test scorers that take thresholds.,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,372,same for an estimator without decision_function,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,379,test with a regressor (no decision_function),
scikit-learn/sklearn/metrics/tests/test_score_objects.py,386,Test that an exception is raised on more than two classes,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,393,test error is raised with a single class present in model,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,394,(predict_proba shape is not suitable for binary auc),
scikit-learn/sklearn/metrics/tests/test_score_objects.py,402,for proba scorers,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,408,Test that the scorer work with multilabel-indicator format,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,409,for multilabel and multi-output multi-class classifier,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,414,Multi-output multi-class predict_proba,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,422,Multi-output multi-class decision_function,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,423,TODO Is there any yet?,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,435,Multilabel predict_proba,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,442,Multilabel decision function,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,451,Test clustering scorers against gold standard labeling.,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,464,"Test that when a list of scores is returned, we raise proper errors.",
scikit-learn/sklearn/metrics/tests/test_score_objects.py,478,Test that scorers support sample_weight or raise sensible errors,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,480,"Unlike the metrics invariance test, in the scorer case it's harder",
scikit-learn/sklearn/metrics/tests/test_score_objects.py,481,"to ensure that, on the classifier output, weighted and unweighted",
scikit-learn/sklearn/metrics/tests/test_score_objects.py,482,scores really should be unequal.,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,492,get sensible estimators for each metric,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,525,Non-regression test for #6147: some score functions would,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,526,return singleton memmap when computed on memmap data instead of scalar,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,527,float values.,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,535,UndefinedMetricWarning for P / R scores,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,598,compare dict keys,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,616,no decision function,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,639,no decision function,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,652,scoring dictionary returned is the same as calling each scorer separately,
scikit-learn/sklearn/metrics/tests/test_score_objects.py,714,Perceptron has no predict_proba,
scikit-learn/sklearn/metrics/tests/test_ranking.py,35,,
scikit-learn/sklearn/metrics/tests/test_ranking.py,36,Utilities for testing,
scikit-learn/sklearn/metrics/tests/test_ranking.py,46,import some data to play with,
scikit-learn/sklearn/metrics/tests/test_ranking.py,53,restrict to a binary classification task,
scikit-learn/sklearn/metrics/tests/test_ranking.py,64,add noisy features to make the problem harder and avoid perfect results,
scikit-learn/sklearn/metrics/tests/test_ranking.py,68,"run classifier, get class probabilities and label predictions",
scikit-learn/sklearn/metrics/tests/test_ranking.py,73,only interested in probabilities of the positive case,
scikit-learn/sklearn/metrics/tests/test_ranking.py,74,XXX: do we really want a special API for the binary case?,
scikit-learn/sklearn/metrics/tests/test_ranking.py,82,,
scikit-learn/sklearn/metrics/tests/test_ranking.py,83,Tests,
scikit-learn/sklearn/metrics/tests/test_ranking.py,90,Count the number of times positive samples are correctly ranked above,
scikit-learn/sklearn/metrics/tests/test_ranking.py,91,negative samples.,
scikit-learn/sklearn/metrics/tests/test_ranking.py,118,Compute precision up to document i,
scikit-learn/sklearn/metrics/tests/test_ranking.py,119,"i.e, percentage of relevant documents up to document i.",
scikit-learn/sklearn/metrics/tests/test_ranking.py,169,Formula (5) from McClish 1989,
scikit-learn/sklearn/metrics/tests/test_ranking.py,179,Test Area under Receiver Operating Characteristic (ROC) curve,
scikit-learn/sklearn/metrics/tests/test_ranking.py,193,Make sure that roc_curve returns a curve start at 0 and ending and,
scikit-learn/sklearn/metrics/tests/test_ranking.py,194,1 even in corner cases,
scikit-learn/sklearn/metrics/tests/test_ranking.py,206,Test whether the returned threshold matches up with tpr,
scikit-learn/sklearn/metrics/tests/test_ranking.py,207,make small toy dataset,
scikit-learn/sklearn/metrics/tests/test_ranking.py,211,use the given thresholds to determine the tpr,
scikit-learn/sklearn/metrics/tests/test_ranking.py,218,compare tpr and tpr_correct to see if the thresholds' order was correct,
scikit-learn/sklearn/metrics/tests/test_ranking.py,225,roc_curve not applicable for multi-class problems,
scikit-learn/sklearn/metrics/tests/test_ranking.py,233,roc_curve for confidence scores,
scikit-learn/sklearn/metrics/tests/test_ranking.py,244,roc_curve for hard decisions,
scikit-learn/sklearn/metrics/tests/test_ranking.py,247,always predict one,
scikit-learn/sklearn/metrics/tests/test_ranking.py,255,always predict zero,
scikit-learn/sklearn/metrics/tests/test_ranking.py,263,hard decisions,
scikit-learn/sklearn/metrics/tests/test_ranking.py,274,assert there are warnings,
scikit-learn/sklearn/metrics/tests/test_ranking.py,277,"all true labels, all fpr should be nan",
scikit-learn/sklearn/metrics/tests/test_ranking.py,282,assert there are warnings,
scikit-learn/sklearn/metrics/tests/test_ranking.py,286,"all negative labels, all tpr should be nan",
scikit-learn/sklearn/metrics/tests/test_ranking.py,293,Binary classification,
scikit-learn/sklearn/metrics/tests/test_ranking.py,336,assert UndefinedMetricWarning because of no positive sample in y_true,
scikit-learn/sklearn/metrics/tests/test_ranking.py,346,assert UndefinedMetricWarning because of no negative sample in y_true,
scikit-learn/sklearn/metrics/tests/test_ranking.py,354,Multi-label classification task,
scikit-learn/sklearn/metrics/tests/test_ranking.py,389,Test that drop_intermediate drops the correct thresholds,
scikit-learn/sklearn/metrics/tests/test_ranking.py,395,Test dropping thresholds with repeating scores,
scikit-learn/sklearn/metrics/tests/test_ranking.py,406,Ensure that fpr and tpr returned by roc_curve are increasing.,
scikit-learn/sklearn/metrics/tests/test_ranking.py,407,Construct an edge case with float y_score and sample_weight,
scikit-learn/sklearn/metrics/tests/test_ranking.py,408,when some adjacent values of fpr and tpr are actually the same.,
scikit-learn/sklearn/metrics/tests/test_ranking.py,418,Test Area Under Curve (AUC) computation,
scikit-learn/sklearn/metrics/tests/test_ranking.py,437,Incompatible shapes,
scikit-learn/sklearn/metrics/tests/test_ranking.py,441,Too few x values,
scikit-learn/sklearn/metrics/tests/test_ranking.py,445,x is not in order,
scikit-learn/sklearn/metrics/tests/test_ranking.py,462,Tests the one-vs-one multiclass ROC AUC algorithm,
scikit-learn/sklearn/metrics/tests/test_ranking.py,463,"on a small example, representative of an expected use case.",
scikit-learn/sklearn/metrics/tests/test_ranking.py,467,Used to compute the expected output.,
scikit-learn/sklearn/metrics/tests/test_ranking.py,468,Consider labels 0 and 1:,
scikit-learn/sklearn/metrics/tests/test_ranking.py,469,"positive label is 0, negative label is 1",
scikit-learn/sklearn/metrics/tests/test_ranking.py,471,"positive label is 1, negative label is 0",
scikit-learn/sklearn/metrics/tests/test_ranking.py,475,Consider labels 0 and 2:,
scikit-learn/sklearn/metrics/tests/test_ranking.py,480,Consider labels 1 and 2:,
scikit-learn/sklearn/metrics/tests/test_ranking.py,485,"Unweighted, one-vs-one multiclass ROC AUC algorithm",
scikit-learn/sklearn/metrics/tests/test_ranking.py,492,"Weighted, one-vs-one multiclass ROC AUC algorithm",
scikit-learn/sklearn/metrics/tests/test_ranking.py,493,Each term is weighted by the prevalence for the positive label.,
scikit-learn/sklearn/metrics/tests/test_ranking.py,510,Tests the one-vs-one multiclass ROC AUC algorithm for binary y_true,
scikit-learn/sklearn/metrics/tests/test_ranking.py,511,,
scikit-learn/sklearn/metrics/tests/test_ranking.py,512,"on a small example, representative of an expected use case.",
scikit-learn/sklearn/metrics/tests/test_ranking.py,516,Used to compute the expected output.,
scikit-learn/sklearn/metrics/tests/test_ranking.py,517,Consider labels 0 and 1:,
scikit-learn/sklearn/metrics/tests/test_ranking.py,518,"positive label is 0, negative label is 1",
scikit-learn/sklearn/metrics/tests/test_ranking.py,520,"positive label is 1, negative label is 0",
scikit-learn/sklearn/metrics/tests/test_ranking.py,528,"Weighted, one-vs-one multiclass ROC AUC algorithm",
scikit-learn/sklearn/metrics/tests/test_ranking.py,541,"Tests the unweighted, one-vs-rest multiclass ROC AUC algorithm",
scikit-learn/sklearn/metrics/tests/test_ranking.py,542,"on a small example, representative of an expected use case.",
scikit-learn/sklearn/metrics/tests/test_ranking.py,545,Compute the expected result by individually computing the 'one-vs-rest',
scikit-learn/sklearn/metrics/tests/test_ranking.py,546,"ROC AUC scores for classes 0, 1, and 2.",
scikit-learn/sklearn/metrics/tests/test_ranking.py,556,"Tests the weighted, one-vs-rest multiclass ROC AUC algorithm",
scikit-learn/sklearn/metrics/tests/test_ranking.py,557,"on the same input (Provost & Domingos, 2000)",
scikit-learn/sklearn/metrics/tests/test_ranking.py,622,Test that roc_auc_score function returns an error when trying,
scikit-learn/sklearn/metrics/tests/test_ranking.py,623,to compute multiclass AUC for parameters where an output,
scikit-learn/sklearn/metrics/tests/test_ranking.py,624,is not defined.,
scikit-learn/sklearn/metrics/tests/test_ranking.py,634,Test that roc_auc_score function returns an error when trying,
scikit-learn/sklearn/metrics/tests/test_ranking.py,635,to compute AUC for non-binary class values.,
scikit-learn/sklearn/metrics/tests/test_ranking.py,638,y_true contains only one class value,
scikit-learn/sklearn/metrics/tests/test_ranking.py,653,y_true contains only one class value,
scikit-learn/sklearn/metrics/tests/test_ranking.py,683,Check that using string class labels raises an informative,
scikit-learn/sklearn/metrics/tests/test_ranking.py,684,error for any supported string dtype:,
scikit-learn/sklearn/metrics/tests/test_ranking.py,695,The error message is slightly different for bytes-encoded,
scikit-learn/sklearn/metrics/tests/test_ranking.py,696,"class labels, but otherwise the behavior is the same:",
scikit-learn/sklearn/metrics/tests/test_ranking.py,704,Check that it is possible to use floating point class labels,
scikit-learn/sklearn/metrics/tests/test_ranking.py,705,that are interpreted similarly to integer class labels:,
scikit-learn/sklearn/metrics/tests/test_ranking.py,717,"Use {-1, 1} for labels; make sure original labels aren't modified",
scikit-learn/sklearn/metrics/tests/test_ranking.py,734,Test Precision-Recall and aread under PR curve,
scikit-learn/sklearn/metrics/tests/test_ranking.py,740,`_average_precision` is not very precise in case of 0.5 ties: be tolerant,
scikit-learn/sklearn/metrics/tests/test_ranking.py,745,Smoke test in the case of proba having only one value,
scikit-learn/sklearn/metrics/tests/test_ranking.py,753,Contains non-binary labels,
scikit-learn/sklearn/metrics/tests/test_ranking.py,760,Binary classification,
scikit-learn/sklearn/metrics/tests/test_ranking.py,775,Here we are doing a terrible prediction: we are always getting,
scikit-learn/sklearn/metrics/tests/test_ranking.py,776,"it wrong, hence the average_precision_score is the accuracy at",
scikit-learn/sklearn/metrics/tests/test_ranking.py,777,chance: 50%,
scikit-learn/sklearn/metrics/tests/test_ranking.py,818,Multi-label classification task,
scikit-learn/sklearn/metrics/tests/test_ranking.py,864,if one class is never present weighted should not be NaN,
scikit-learn/sklearn/metrics/tests/test_ranking.py,872,Check the average_precision_score of a constant predictor is,
scikit-learn/sklearn/metrics/tests/test_ranking.py,873,the TPR,
scikit-learn/sklearn/metrics/tests/test_ranking.py,875,Generate a dataset with 25% of positives,
scikit-learn/sklearn/metrics/tests/test_ranking.py,878,And a constant score,
scikit-learn/sklearn/metrics/tests/test_ranking.py,880,The precision is then the fraction of positive whatever the recall,
scikit-learn/sklearn/metrics/tests/test_ranking.py,881,"is, as there is only one threshold:",
scikit-learn/sklearn/metrics/tests/test_ranking.py,886,Raise an error when pos_label is not in binary y_true,
scikit-learn/sklearn/metrics/tests/test_ranking.py,892,Raise an error for multilabel-indicator y_true with,
scikit-learn/sklearn/metrics/tests/test_ranking.py,893,pos_label other than 1,
scikit-learn/sklearn/metrics/tests/test_ranking.py,904,Test that average_precision_score and roc_auc_score are invariant by,
scikit-learn/sklearn/metrics/tests/test_ranking.py,905,the scaling or shifting of probabilities,
scikit-learn/sklearn/metrics/tests/test_ranking.py,906,This test was expanded (added scaled_down) in response to github,
scikit-learn/sklearn/metrics/tests/test_ranking.py,907,"issue #3864 (and others), where overly aggressive rounding was causing",
scikit-learn/sklearn/metrics/tests/test_ranking.py,908,problems for users with very small y_score values,
scikit-learn/sklearn/metrics/tests/test_ranking.py,929,Check on several small example that it works,
scikit-learn/sklearn/metrics/tests/test_ranking.py,963,Tie handling,
scikit-learn/sklearn/metrics/tests/test_ranking.py,991,No relevant labels,
scikit-learn/sklearn/metrics/tests/test_ranking.py,996,Only relevant labels,
scikit-learn/sklearn/metrics/tests/test_ranking.py,1001,Degenerate case: only one label,
scikit-learn/sklearn/metrics/tests/test_ranking.py,1007,Raise value error if not appropriate format,
scikit-learn/sklearn/metrics/tests/test_ranking.py,1017,Check that y_true.shape != y_score.shape raise the proper exception,
scikit-learn/sklearn/metrics/tests/test_ranking.py,1033,Check tie handling in score,
scikit-learn/sklearn/metrics/tests/test_ranking.py,1034,Basic check with only ties and increasing label space,
scikit-learn/sklearn/metrics/tests/test_ranking.py,1038,Check for growing number of consecutive relevant,
scikit-learn/sklearn/metrics/tests/test_ranking.py,1040,Check for a bunch of positions,
scikit-learn/sklearn/metrics/tests/test_ranking.py,1049,Check that Label ranking average precision works for various,
scikit-learn/sklearn/metrics/tests/test_ranking.py,1050,Basic check with increasing label space size and decreasing score,
scikit-learn/sklearn/metrics/tests/test_ranking.py,1054,First and last,
scikit-learn/sklearn/metrics/tests/test_ranking.py,1061,Check for growing number of consecutive relevant label,
scikit-learn/sklearn/metrics/tests/test_ranking.py,1063,Check for a bunch of position,
scikit-learn/sklearn/metrics/tests/test_ranking.py,1080,The best rank correspond to 1. Rank higher than 1 are worse.,
scikit-learn/sklearn/metrics/tests/test_ranking.py,1081,The best inverse ranking correspond to n_labels.,
scikit-learn/sklearn/metrics/tests/test_ranking.py,1086,Rank need to be corrected to take into account ties,
scikit-learn/sklearn/metrics/tests/test_ranking.py,1087,ex: rank 1 ex aequo means that both label are rank 2.,
scikit-learn/sklearn/metrics/tests/test_ranking.py,1098,Let's count the number of relevant label with better rank,
scikit-learn/sklearn/metrics/tests/test_ranking.py,1099,(smaller rank).,
scikit-learn/sklearn/metrics/tests/test_ranking.py,1102,Weight by the rank of the actual label,
scikit-learn/sklearn/metrics/tests/test_ranking.py,1118,Score with ties,
scikit-learn/sklearn/metrics/tests/test_ranking.py,1129,Uniform score,
scikit-learn/sklearn/metrics/tests/test_ranking.py,1165,"Degenerate sample labeling (e.g., zero labels for a sample) is a valid",
scikit-learn/sklearn/metrics/tests/test_ranking.py,1166,special case for lrap (the sample is considered to achieve perfect,
scikit-learn/sklearn/metrics/tests/test_ranking.py,1167,"precision), but this case is not tested in test_common.",
scikit-learn/sklearn/metrics/tests/test_ranking.py,1168,"For these test samples, the APs are 0.5, 0.75, and 1.0 (default for zero",
scikit-learn/sklearn/metrics/tests/test_ranking.py,1169,labels).,
scikit-learn/sklearn/metrics/tests/test_ranking.py,1184,Toy case,
scikit-learn/sklearn/metrics/tests/test_ranking.py,1217,Non trival case,
scikit-learn/sklearn/metrics/tests/test_ranking.py,1264,Undefined metrics -  the ranking doesn't matter,
scikit-learn/sklearn/metrics/tests/test_ranking.py,1278,Non trival case,
scikit-learn/sklearn/metrics/tests/test_ranking.py,1293,Sparse csr matrices,
scikit-learn/sklearn/metrics/tests/test_ranking.py,1301,Check that y_true.shape != y_score.shape raise the proper exception,
scikit-learn/sklearn/metrics/tests/test_ranking.py,1317,Tie handling,
scikit-learn/sklearn/metrics/tests/test_ranking.py,1450,Check `roc_auc_score` for max_fpr != `None`,
scikit-learn/sklearn/metrics/_plot/confusion_matrix.py,91,print text with appropriate color depending on background,
scikit-learn/sklearn/metrics/_plot/tests/test_plot_confusion_matrix.py,19,TODO: Remove when https://github.com/numpy/numpy/issues/14397 is resolved,
scikit-learn/sklearn/metrics/_plot/tests/test_plot_confusion_matrix.py,194,make sure text color is appropriate depending on background,
scikit-learn/sklearn/metrics/_plot/tests/test_plot_confusion_matrix.py,200,diagonal text is black,
scikit-learn/sklearn/metrics/_plot/tests/test_plot_confusion_matrix.py,204,off-diagonal text is white,
scikit-learn/sklearn/metrics/_plot/tests/test_plot_confusion_matrix.py,209,diagonal text is white,
scikit-learn/sklearn/metrics/_plot/tests/test_plot_confusion_matrix.py,213,off-diagonal text is black,
scikit-learn/sklearn/metrics/_plot/tests/test_plot_confusion_matrix.py,217,Regression test for #15920,
scikit-learn/sklearn/metrics/_plot/tests/test_plot_confusion_matrix.py,252,Make sure plot text is formatted with 'values_format'.,
scikit-learn/sklearn/metrics/_plot/tests/test_plot_confusion_matrix.py,271,"Values should be shown as whole numbers 'd',",
scikit-learn/sklearn/metrics/_plot/tests/test_plot_confusion_matrix.py,272,except the first number which should be shown as 1e+07 (longer length),
scikit-learn/sklearn/metrics/_plot/tests/test_plot_confusion_matrix.py,273,and the last number will be showns as 1.2e+07 (longer length),
scikit-learn/sklearn/metrics/_plot/tests/test_plot_confusion_matrix.py,279,"Values should now formatted as '.2g', since there's a float in",
scikit-learn/sklearn/metrics/_plot/tests/test_plot_confusion_matrix.py,280,"Values are have two dec places max, (e.g 100 becomes 1e+02)",
scikit-learn/sklearn/metrics/_plot/tests/test_plot_roc_curve.py,17,TODO: Remove when https://github.com/numpy/numpy/issues/14397 is resolved,
scikit-learn/sklearn/metrics/_plot/tests/test_plot_roc_curve.py,110,cannot fail thanks to pyplot fixture,
scikit-learn/sklearn/metrics/_plot/tests/test_plot_roc_curve.py,111,noqal,
scikit-learn/sklearn/metrics/_plot/tests/test_plot_roc_curve.py,139,non-regression test checking that the `name` used when calling,
scikit-learn/sklearn/metrics/_plot/tests/test_plot_roc_curve.py,140,`plot_roc_curve` is used as well when calling `disp.plot()`,
scikit-learn/sklearn/metrics/_plot/tests/test_plot_precision_recall.py,19,TODO: Remove when https://github.com/numpy/numpy/issues/14397 is resolved,
scikit-learn/sklearn/metrics/_plot/tests/test_plot_precision_recall.py,31,Unfitted classifer,
scikit-learn/sklearn/metrics/_plot/tests/test_plot_precision_recall.py,39,Fitted multiclass classifier with binary data,
scikit-learn/sklearn/metrics/_plot/tests/test_plot_precision_recall.py,107,cannot fail thanks to pyplot fixture,
scikit-learn/sklearn/metrics/_plot/tests/test_plot_precision_recall.py,108,noqa,
scikit-learn/sklearn/metrics/_plot/tests/test_plot_precision_recall.py,119,draw again with another label,
scikit-learn/sklearn/metrics/_plot/tests/test_plot_precision_recall.py,139,regression test #15738,
scikit-learn/sklearn/metrics/_plot/tests/test_plot_precision_recall.py,159,non-regression test checking that the `name` used when calling,
scikit-learn/sklearn/metrics/_plot/tests/test_plot_precision_recall.py,160,`plot_roc_curve` is used as well when calling `disp.plot()`,
scikit-learn/sklearn/manifold/_mds.py,5,author: Nelle Varoquaux <nelle.varoquaux@gmail.com>,
scikit-learn/sklearn/manifold/_mds.py,6,License: BSD,
scikit-learn/sklearn/manifold/_mds.py,76,Randomly choose initial configuration,
scikit-learn/sklearn/manifold/_mds.py,80,overrides the parameter p,
scikit-learn/sklearn/manifold/_mds.py,90,Compute distance and monotonic regression,
scikit-learn/sklearn/manifold/_mds.py,97,dissimilarities with 0 are considered as missing values,
scikit-learn/sklearn/manifold/_mds.py,100,Compute the disparities using a monotonic regression,
scikit-learn/sklearn/manifold/_mds.py,108,Compute stress,
scikit-learn/sklearn/manifold/_mds.py,111,Update X using the Guttman transform,
scikit-learn/sklearn/manifold/_locally_linear.py,3,Author: Fabian Pedregosa -- <fabian.pedregosa@inria.fr>,
scikit-learn/sklearn/manifold/_locally_linear.py,4,Jake Vanderplas  -- <vanderplas@astro.washington.edu>,
scikit-learn/sklearn/manifold/_locally_linear.py,5,License: BSD 3 clause (C) INRIA 2011,
scikit-learn/sklearn/manifold/_locally_linear.py,51,this might raise a LinalgError if G is singular and has trace,
scikit-learn/sklearn/manifold/_locally_linear.py,52,zero,
scikit-learn/sklearn/manifold/_locally_linear.py,54,broadcasting,
scikit-learn/sklearn/manifold/_locally_linear.py,159,"initialize with [-1,1] as in ARPACK",
scikit-learn/sklearn/manifold/_locally_linear.py,316,we'll compute M = (I-W)'(I-W),
scikit-learn/sklearn/manifold/_locally_linear.py,317,"depending on the solver, we'll do this differently",
scikit-learn/sklearn/manifold/_locally_linear.py,323,W = W - I = W - I,
scikit-learn/sklearn/manifold/_locally_linear.py,348,build Hessian estimator,
scikit-learn/sklearn/manifold/_locally_linear.py,386,find the eigenvectors and eigenvalues of each local covariance,
scikit-learn/sklearn/manifold/_locally_linear.py,387,"matrix. We want V[i] to be a [n_neighbors x n_neighbors] matrix,",
scikit-learn/sklearn/manifold/_locally_linear.py,388,where the columns are eigenvectors,
scikit-learn/sklearn/manifold/_locally_linear.py,393,choose the most efficient way to find the eigenvectors,
scikit-learn/sklearn/manifold/_locally_linear.py,410,find regularized weights: this is like normal LLE.,
scikit-learn/sklearn/manifold/_locally_linear.py,411,"because we've already computed the SVD of each covariance matrix,",
scikit-learn/sklearn/manifold/_locally_linear.py,412,it's faster to use this rather than np.linalg.solve,
scikit-learn/sklearn/manifold/_locally_linear.py,424,calculate eta: the median of the ratio of small to large eigenvalues,
scikit-learn/sklearn/manifold/_locally_linear.py,425,"across the points.  This is used to determine s_i, below",
scikit-learn/sklearn/manifold/_locally_linear.py,429,"find s_i, the size of the ""almost null space"" for each point:",
scikit-learn/sklearn/manifold/_locally_linear.py,430,this is the size of the largest set of eigenvalues,
scikit-learn/sklearn/manifold/_locally_linear.py,431,such that Sum[v; v in set]/Sum[v; v not in set] < eta,
scikit-learn/sklearn/manifold/_locally_linear.py,437,number of zero eigenvalues,
scikit-learn/sklearn/manifold/_locally_linear.py,439,Now calculate M.,
scikit-learn/sklearn/manifold/_locally_linear.py,440,This is the [N x N] matrix whose null space is the desired embedding,
scikit-learn/sklearn/manifold/_locally_linear.py,445,select bottom s_i eigenvectors and calculate alpha,
scikit-learn/sklearn/manifold/_locally_linear.py,449,compute Householder matrix which satisfies,
scikit-learn/sklearn/manifold/_locally_linear.py,450,Hi*Vi.T*ones(n_neighbors) = alpha_i*ones(s),
scikit-learn/sklearn/manifold/_locally_linear.py,451,using prescription from paper,
scikit-learn/sklearn/manifold/_locally_linear.py,460,Householder matrix is,
scikit-learn/sklearn/manifold/_locally_linear.py,461,">> Hi = np.identity(s_i) - 2*np.outer(h,h)",
scikit-learn/sklearn/manifold/_locally_linear.py,462,Then the weight matrix is,
scikit-learn/sklearn/manifold/_locally_linear.py,463,">> Wi = np.dot(Vi,Hi) + (1-alpha_i) * w_reg[i,:,None]",
scikit-learn/sklearn/manifold/_locally_linear.py,464,We do this much more efficiently:,
scikit-learn/sklearn/manifold/_locally_linear.py,468,Update M as follows:,
scikit-learn/sklearn/manifold/_locally_linear.py,469,">> W_hat = np.zeros( (N,s_i) )",
scikit-learn/sklearn/manifold/_locally_linear.py,470,">> W_hat[neighbors[i],:] = Wi",
scikit-learn/sklearn/manifold/_locally_linear.py,471,>> W_hat[i] -= 1,
scikit-learn/sklearn/manifold/_locally_linear.py,472,">> M += np.dot(W_hat,W_hat.T)",
scikit-learn/sklearn/manifold/_locally_linear.py,473,We can do this much more efficiently:,
scikit-learn/sklearn/manifold/_locally_linear.py,497,compute n_components largest eigenvalues of Xi * Xi^T,
scikit-learn/sklearn/manifold/_isomap.py,3,Author: Jake Vanderplas  -- <vanderplas@astro.washington.edu>,
scikit-learn/sklearn/manifold/_isomap.py,4,License: BSD 3 clause (C) 2011,
scikit-learn/sklearn/manifold/_isomap.py,170,mypy error: Decorated property not supported,
scikit-learn/sklearn/manifold/_isomap.py,171,type: ignore,
scikit-learn/sklearn/manifold/_isomap.py,266,Create the graph of shortest distances from X to,
scikit-learn/sklearn/manifold/_isomap.py,267,training data via the nearest neighbors of X.,
scikit-learn/sklearn/manifold/_isomap.py,268,"This can be done as a single array operation, but it potentially",
scikit-learn/sklearn/manifold/_isomap.py,269,"takes a lot of memory.  To avoid that, use a loop:",
scikit-learn/sklearn/manifold/_spectral_embedding.py,3,Author: Gael Varoquaux <gael.varoquaux@normalesup.org>,
scikit-learn/sklearn/manifold/_spectral_embedding.py,4,Wei LI <kuantkid@gmail.com>,
scikit-learn/sklearn/manifold/_spectral_embedding.py,5,License: BSD 3 clause,
scikit-learn/sklearn/manifold/_spectral_embedding.py,47,speed up row-wise access to boolean connection mask,
scikit-learn/sklearn/manifold/_spectral_embedding.py,83,"sparse graph, find all the connected components",
scikit-learn/sklearn/manifold/_spectral_embedding.py,87,"dense graph, find all connected components start from node 0",
scikit-learn/sklearn/manifold/_spectral_embedding.py,112,We need all entries in the diagonal to values,
scikit-learn/sklearn/manifold/_spectral_embedding.py,121,If the matrix has a small number of diagonals (as in the,
scikit-learn/sklearn/manifold/_spectral_embedding.py,122,"case of structured matrices coming from images), the",
scikit-learn/sklearn/manifold/_spectral_embedding.py,123,dia format might be best suited for matvec products:,
scikit-learn/sklearn/manifold/_spectral_embedding.py,126,3 or less outer diagonals on each side,
scikit-learn/sklearn/manifold/_spectral_embedding.py,129,csr has the fastest matvec and is thus best suited to,
scikit-learn/sklearn/manifold/_spectral_embedding.py,130,arpack,
scikit-learn/sklearn/manifold/_spectral_embedding.py,229,Whether to drop the first eigenvector,
scikit-learn/sklearn/manifold/_spectral_embedding.py,241,lobpcg used with eigen_solver='amg' has bugs for low number of nodes,
scikit-learn/sklearn/manifold/_spectral_embedding.py,242,for details see the source code in scipy:,
scikit-learn/sklearn/manifold/_spectral_embedding.py,243,https://github.com/scipy/scipy/blob/v0.11.0/scipy/sparse/linalg/eigen,
scikit-learn/sklearn/manifold/_spectral_embedding.py,244,/lobpcg/lobpcg.py#L237,
scikit-learn/sklearn/manifold/_spectral_embedding.py,245,or matlab:,
scikit-learn/sklearn/manifold/_spectral_embedding.py,246,https://www.mathworks.com/matlabcentral/fileexchange/48-lobpcg-m,
scikit-learn/sklearn/manifold/_spectral_embedding.py,249,Here we'll use shift-invert mode for fast eigenvalues,
scikit-learn/sklearn/manifold/_spectral_embedding.py,250,(see https://docs.scipy.org/doc/scipy/reference/tutorial/arpack.html,
scikit-learn/sklearn/manifold/_spectral_embedding.py,251,for a short explanation of what this means),
scikit-learn/sklearn/manifold/_spectral_embedding.py,252,"Because the normalized Laplacian has eigenvalues between 0 and 2,",
scikit-learn/sklearn/manifold/_spectral_embedding.py,253,I - L has eigenvalues between -1 and 1.  ARPACK is most efficient,
scikit-learn/sklearn/manifold/_spectral_embedding.py,254,when finding eigenvalues of largest magnitude (keyword which='LM'),
scikit-learn/sklearn/manifold/_spectral_embedding.py,255,and when these eigenvalues are very large compared to the rest.,
scikit-learn/sklearn/manifold/_spectral_embedding.py,256,"For very large, very sparse graphs, I - L can have many, many",
scikit-learn/sklearn/manifold/_spectral_embedding.py,257,eigenvalues very near 1.0.  This leads to slow convergence.  So,
scikit-learn/sklearn/manifold/_spectral_embedding.py,258,"instead, we'll use ARPACK's shift-invert mode, asking for the",
scikit-learn/sklearn/manifold/_spectral_embedding.py,259,eigenvalues near 1.0.  This effectively spreads-out the spectrum,
scikit-learn/sklearn/manifold/_spectral_embedding.py,260,near 1.0 and leads to much faster convergence: potentially an,
scikit-learn/sklearn/manifold/_spectral_embedding.py,261,orders-of-magnitude speedup over simply using keyword which='LA',
scikit-learn/sklearn/manifold/_spectral_embedding.py,262,in standard mode.,
scikit-learn/sklearn/manifold/_spectral_embedding.py,264,We are computing the opposite of the laplacian inplace so as,
scikit-learn/sklearn/manifold/_spectral_embedding.py,265,to spare a memory allocation of a possibly very large array,
scikit-learn/sklearn/manifold/_spectral_embedding.py,275,"When submatrices are exactly singular, an LU decomposition",
scikit-learn/sklearn/manifold/_spectral_embedding.py,276,in arpack fails. We fallback to lobpcg,
scikit-learn/sklearn/manifold/_spectral_embedding.py,278,Revert the laplacian to its opposite to have lobpcg work,
scikit-learn/sklearn/manifold/_spectral_embedding.py,282,Use AMG to get a preconditioner and speed up the eigenvalue,
scikit-learn/sklearn/manifold/_spectral_embedding.py,283,problem.,
scikit-learn/sklearn/manifold/_spectral_embedding.py,286,lobpcg needs double precision floats,
scikit-learn/sklearn/manifold/_spectral_embedding.py,291,"The Laplacian matrix is always singular, having at least one zero",
scikit-learn/sklearn/manifold/_spectral_embedding.py,292,"eigenvalue, corresponding to the trivial eigenvector, which is a",
scikit-learn/sklearn/manifold/_spectral_embedding.py,293,constant. Using a singular matrix for preconditioning may result in,
scikit-learn/sklearn/manifold/_spectral_embedding.py,294,random failures in LOBPCG and is not supported by the existing,
scikit-learn/sklearn/manifold/_spectral_embedding.py,295,theory:,
scikit-learn/sklearn/manifold/_spectral_embedding.py,296,see https://doi.org/10.1007/s10208-015-9297-1,
scikit-learn/sklearn/manifold/_spectral_embedding.py,297,Shift the Laplacian so its diagononal is not all ones. The shift,
scikit-learn/sklearn/manifold/_spectral_embedding.py,298,"does change the eigenpairs however, so we'll feed the shifted",
scikit-learn/sklearn/manifold/_spectral_embedding.py,299,matrix to the solver and afterward set it back to the original.,
scikit-learn/sklearn/manifold/_spectral_embedding.py,317,lobpcg needs double precision floats,
scikit-learn/sklearn/manifold/_spectral_embedding.py,321,see note above under arpack why lobpcg has problems with small,
scikit-learn/sklearn/manifold/_spectral_embedding.py,322,number of nodes,
scikit-learn/sklearn/manifold/_spectral_embedding.py,323,"lobpcg will fallback to eigh, so we short circuit it",
scikit-learn/sklearn/manifold/_spectral_embedding.py,332,"We increase the number of eigenvectors requested, as lobpcg",
scikit-learn/sklearn/manifold/_spectral_embedding.py,333,doesn't behave well in low dimension,
scikit-learn/sklearn/manifold/_spectral_embedding.py,502,currently only symmetric affinity_matrix supported,
scikit-learn/sklearn/manifold/_t_sne.py,1,Author: Alexander Fabisch  -- <afabisch@informatik.uni-bremen.de>,
scikit-learn/sklearn/manifold/_t_sne.py,2,Author: Christopher Moody <chrisemoody@gmail.com>,
scikit-learn/sklearn/manifold/_t_sne.py,3,Author: Nick Travers <nickt@squareup.com>,
scikit-learn/sklearn/manifold/_t_sne.py,4,License: BSD 3 clause (C) 2014,
scikit-learn/sklearn/manifold/_t_sne.py,6,This is the exact and Barnes-Hut t-SNE implementation. There are other,
scikit-learn/sklearn/manifold/_t_sne.py,7,modifications of the algorithm:,
scikit-learn/sklearn/manifold/_t_sne.py,8,* Fast Optimization for t-SNE:,
scikit-learn/sklearn/manifold/_t_sne.py,9,https://cseweb.ucsd.edu/~lvdmaaten/workshops/nips2010/papers/vandermaaten.pdf,
scikit-learn/sklearn/manifold/_t_sne.py,25,mypy error: Module 'sklearn.manifold' has no attribute '_barnes_hut_tsne',
scikit-learn/sklearn/manifold/_t_sne.py,26,type: ignore,
scikit-learn/sklearn/manifold/_t_sne.py,53,Compute conditional probabilities such that they approximately match,
scikit-learn/sklearn/manifold/_t_sne.py,54,the desired perplexity,
scikit-learn/sklearn/manifold/_t_sne.py,90,Compute conditional probabilities such that they approximately match,
scikit-learn/sklearn/manifold/_t_sne.py,91,the desired perplexity,
scikit-learn/sklearn/manifold/_t_sne.py,101,Symmetrize the joint probability distribution using sparse operations,
scikit-learn/sklearn/manifold/_t_sne.py,107,Normalize the joint probability distribution,
scikit-learn/sklearn/manifold/_t_sne.py,160,Q is a heavy-tailed distribution: Student's t-distribution,
scikit-learn/sklearn/manifold/_t_sne.py,167,"Optimization trick below: np.dot(x, y) is faster than",
scikit-learn/sklearn/manifold/_t_sne.py,168,np.sum(x * y) because it calls BLAS,
scikit-learn/sklearn/manifold/_t_sne.py,170,Objective: C (Kullback-Leibler divergence of P and Q),
scikit-learn/sklearn/manifold/_t_sne.py,177,Gradient: dC/dY,
scikit-learn/sklearn/manifold/_t_sne.py,178,pdist always returns double precision distances. Thus we need to take,
scikit-learn/sklearn/manifold/_t_sne.py,355,only compute the error when needed,
scikit-learn/sklearn/manifold/_t_sne.py,448,we set the diagonal to np.inf to exclude the points themselves from,
scikit-learn/sklearn/manifold/_t_sne.py,449,their own neighborhood,
scikit-learn/sklearn/manifold/_t_sne.py,452,`ind_X[i]` is the index of sorted distances between i and other samples,
scikit-learn/sklearn/manifold/_t_sne.py,456,"We build an inverted index of neighbors in the input space: For sample i,",
scikit-learn/sklearn/manifold/_t_sne.py,457,we define `inverted_index[i]` as the inverted index of sorted distances:,
scikit-learn/sklearn/manifold/_t_sne.py,458,"inverted_index[i][ind_X[i]] = np.arange(1, n_sample + 1)",
scikit-learn/sklearn/manifold/_t_sne.py,629,Control the number of exploration iterations with early_exaggeration on,
scikit-learn/sklearn/manifold/_t_sne.py,632,Control the number of iterations between progress checks,
scikit-learn/sklearn/manifold/_t_sne.py,703,"Retrieve the distance matrix, either using the precomputed one or",
scikit-learn/sklearn/manifold/_t_sne.py,704,computing it.,
scikit-learn/sklearn/manifold/_t_sne.py,722,compute the joint probability distribution for the input space,
scikit-learn/sklearn/manifold/_t_sne.py,730,Compute the number of nearest neighbors to find.,
scikit-learn/sklearn/manifold/_t_sne.py,731,LvdM uses 3 * perplexity as the number of neighbors.,
scikit-learn/sklearn/manifold/_t_sne.py,732,In the event that we have very small # of points,
scikit-learn/sklearn/manifold/_t_sne.py,733,set the neighbors to n - 1.,
scikit-learn/sklearn/manifold/_t_sne.py,740,Find the nearest neighbors for every point,
scikit-learn/sklearn/manifold/_t_sne.py,759,Free the memory used by the ball_tree,
scikit-learn/sklearn/manifold/_t_sne.py,763,knn return the euclidean distance but we need it squared,
scikit-learn/sklearn/manifold/_t_sne.py,764,to be consistent with the 'exact' method. Note that the,
scikit-learn/sklearn/manifold/_t_sne.py,765,the method was derived using the euclidean method as in the,
scikit-learn/sklearn/manifold/_t_sne.py,766,input space. Not sure of the implication of using a different,
scikit-learn/sklearn/manifold/_t_sne.py,767,metric.,
scikit-learn/sklearn/manifold/_t_sne.py,770,compute the joint probability distribution for the input space,
scikit-learn/sklearn/manifold/_t_sne.py,781,The embedding is initialized with iid samples from Gaussians with,
scikit-learn/sklearn/manifold/_t_sne.py,782,standard deviation 1e-4.,
scikit-learn/sklearn/manifold/_t_sne.py,789,Degrees of freedom of the Student's t-distribution. The suggestion,
scikit-learn/sklearn/manifold/_t_sne.py,790,degrees_of_freedom = n_components - 1 comes from,
scikit-learn/sklearn/manifold/_t_sne.py,791,"""Learning a Parametric Embedding by Preserving Local Structure""",
scikit-learn/sklearn/manifold/_t_sne.py,792,"Laurens van der Maaten, 2009.",
scikit-learn/sklearn/manifold/_t_sne.py,803,t-SNE minimizes the Kullback-Leiber divergence of the Gaussians P,
scikit-learn/sklearn/manifold/_t_sne.py,804,and the Student's t-distributions Q. The optimization algorithm that,
scikit-learn/sklearn/manifold/_t_sne.py,805,we use is batch gradient descent with two stages:,
scikit-learn/sklearn/manifold/_t_sne.py,806,* initial optimization with early exaggeration and momentum at 0.5,
scikit-learn/sklearn/manifold/_t_sne.py,807,* final optimization with momentum at 0.8,
scikit-learn/sklearn/manifold/_t_sne.py,825,Repeat verbose argument for _kl_divergence_bh,
scikit-learn/sklearn/manifold/_t_sne.py,827,Get the number of threads for gradient computation here to,
scikit-learn/sklearn/manifold/_t_sne.py,828,avoid recomputing it at each iteration.,
scikit-learn/sklearn/manifold/_t_sne.py,833,Learning schedule (part 1): do 250 iteration with lower momentum but,
scikit-learn/sklearn/manifold/_t_sne.py,834,higher learning rate controlled via the early exaggeration parameter,
scikit-learn/sklearn/manifold/_t_sne.py,842,Learning schedule (part 2): disable early exaggeration and finish,
scikit-learn/sklearn/manifold/_t_sne.py,843,optimization with a higher momentum at 0.8,
scikit-learn/sklearn/manifold/_t_sne.py,854,Save the final number of iterations,
scikit-learn/sklearn/manifold/tests/test_mds.py,9,"test metric smacof using the data of ""Modern Multidimensional Scaling"",",
scikit-learn/sklearn/manifold/tests/test_mds.py,10,"Borg & Groenen, p 154",
scikit-learn/sklearn/manifold/tests/test_mds.py,28,Not symmetric similarity matrix:,
scikit-learn/sklearn/manifold/tests/test_mds.py,37,Not squared similarity matrix:,
scikit-learn/sklearn/manifold/tests/test_mds.py,45,init not None and not correct format:,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,24,mypy error: Module 'sklearn.manifold' has no attribute '_barnes_hut_tsne',
scikit-learn/sklearn/manifold/tests/test_t_sne.py,25,type: ignore,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,45,Test stopping conditions of gradient descent.,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,57,Gradient norm,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,73,Maximum number of iterations without improvement,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,89,Maximum number of iterations,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,107,Test if the binary search finds Gaussians with desired perplexity.,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,120,Binary perplexity search approximation.,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,121,Should be approximately equal to the slow method when we use,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,122,all points as neighbors.,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,130,Test that when we use all the neighbors the results are identical,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,144,Test that the highest P_ij are the same when fewer neighbors are used,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,147,check the top 10 * k entries out of k * k entries,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,162,Binary perplexity search should be stable.,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,163,The binary_search_perplexity had a bug wherein the P array,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,164,"was uninitialized, leading to sporadically failing tests.",
scikit-learn/sklearn/manifold/tests/test_t_sne.py,181,Convert the sparse matrix to a dense one for testing,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,192,Test gradient of Kullback-Leibler divergence.,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,219,Test trustworthiness score.,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,222,Affine transformation,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,226,Randomly shuffled,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,232,Completely different,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,241,Nearest neighbors should be preserved approximately.,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,268,X can be a sparse matrix.,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,281,Nearest neighbors should be preserved approximately.,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,295,Test trustworthiness with a metric different from 'euclidean' and,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,296,'precomputed',
scikit-learn/sklearn/manifold/tests/test_t_sne.py,305,Early exaggeration factor must be >= 1.,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,312,Number of gradient descent iterations must be at least 200.,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,340,Perplexity should be less than 50,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,370,Computed distance matrices must be positive.,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,381,"'init' must be 'pca', 'random', or numpy array.",
scikit-learn/sklearn/manifold/tests/test_t_sne.py,389,Initialize TSNE with ndarray and test fit,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,396,Initialize TSNE with ndarray and metric 'precomputed',
scikit-learn/sklearn/manifold/tests/test_t_sne.py,397,Make sure no FutureWarning is thrown from _fit,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,403,'metric' must be valid.,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,414,'nethod' must be 'barnes_hut' or 'exact',
scikit-learn/sklearn/manifold/tests/test_t_sne.py,421,check the angle parameter range,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,430,Precomputed distance matrices must be square matrices.,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,439,barnes_hut method should only be used with n_components <= 3,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,446,check that the ``early_exaggeration`` parameter has an effect,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,465,check that the ``n_iter`` parameter has an effect,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,481,Test the tree with only a single set of children.,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,482,,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,483,These tests & answers have been checked against the reference,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,484,implementation by LvdM.,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,496,Four points tests the tree with multiple levels of children.,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,497,,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,498,These tests & answers have been checked against the reference,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,499,implementation by LvdM.,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,518,Test the kwargs option skip_num_points.,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,519,,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,520,Skip num points should make it such that the Barnes_hut gradient,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,521,is not calculated for indices below skip_num_point.,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,522,Aside from skip_num_points=2 and the first two gradient rows,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,523,"being set to zero, these data points are the same as in",
scikit-learn/sklearn/manifold/tests/test_t_sne.py,524,test_answer_gradient_four_points(),
scikit-learn/sklearn/manifold/tests/test_t_sne.py,565,Verbose options write to stdout.,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,587,t-SNE should allow metrics that cannot be squared (issue #3526).,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,595,t-SNE should allow reduction to one component (issue #4154).,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,606,Ensure 64bit arrays are handled correctly.,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,616,"tsne cython code is only single precision, so the output will",
scikit-learn/sklearn/manifold/tests/test_t_sne.py,617,"always be single precision, irrespectively of the input dtype",
scikit-learn/sklearn/manifold/tests/test_t_sne.py,623,Ensure kl_divergence_ is computed at last iteration,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,624,"even though n_iter % n_iter_check != 0, i.e. 1003 % 50 != 0",
scikit-learn/sklearn/manifold/tests/test_t_sne.py,636,When Barnes-Hut's angle=0 this corresponds to the exact method.,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,669,Use a dummy negative n_iter_without_progress and check output on stdout,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,687,The output needs to contain the value of n_iter_without_progress,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,693,Make sure that the parameter min_grad_norm is used correctly,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,711,extract the gradient norm from the verbose output,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,714,When the computation is Finished just an old gradient norm value,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,715,is repeated that we do not need to store,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,725,Compute how often the gradient norm is smaller than min_grad_norm,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,730,"The gradient norm can be smaller than min_grad_norm at most once,",
scikit-learn/sklearn/manifold/tests/test_t_sne.py,731,because in the moment it becomes smaller the optimization stops,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,736,Ensures that the accessible kl_divergence matches the computed value,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,752,The output needs to contain the accessible kl_divergence as the error at,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,753,the last iteration,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,787,"If the test fails a first time, re-run with init=Y to see if",
scikit-learn/sklearn/manifold/tests/test_t_sne.py,788,this was caused by a bad initialization. Note that this will,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,789,also run an early_exaggeration step.,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,797,Ensure that the resulting embedding leads to approximately,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,798,uniformly spaced points: the distance to the closest neighbors,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,799,should be non-zero and approximately constant.,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,812,check that the ``barnes_hut`` method match the exact one when,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,813,``angle = 0`` and ``perplexity > n_samples / 3``,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,823,Kill the early_exaggeration,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,833,check that the bh gradient with different num_threads gives the same,
scikit-learn/sklearn/manifold/tests/test_t_sne.py,834,results,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,23,"non centered, sparse centers to check the",
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,53,Connect all elements within the group at least once via an,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,54,arbitrary path that spans the group.,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,58,Add some more random connections within the group,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,65,Build a symmetric affinity matrix,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,76,We should retrieve the same component mask by starting by both ends,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,77,of the group,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,84,Test spectral embedding with two components,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,88,first component,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,91,second component,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,95,Test of internal _graph_connected_component before connection,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,103,connection,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,115,Some numpy versions are touchy with types,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,118,thresholding on the first components using 0.,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,126,Test spectral embedding with precomputed kernel,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,141,Test precomputed graph filtering when containing too many neighbors,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,160,Test spectral embedding with callable affinity,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,179,TODO: Remove when pyamg does replaces sp.rand call with np.random.rand,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,180,https://github.com/scikit-learn/scikit-learn/issues/15913,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,184,Test spectral embedding with amg solver,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,197,same with special case in which amg is not actually used,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,198,regression test for #10715,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,199,affinity between nodes,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,213,TODO: Remove filterwarnings when pyamg does replaces sp.rand call with,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,214,np.random.rand:,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,215,https://github.com/scikit-learn/scikit-learn/issues/15913,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,219,Non-regression test for amg solver failure (issue #13393 on github),
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,231,Check that the learned embedding is stable w.r.t. random solver init:,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,243,Test using pipeline to do spectral clustering,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,262,Test that SpectralClustering fails with an unknown eigensolver,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,271,Test that SpectralClustering fails with an unknown affinity type,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,279,Test that graph connectivity test works as expected,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,299,Test that Spectral Embedding is deterministic,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,309,Test that spectral_embedding is also processing unnormalized laplacian,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,310,correctly,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,320,Verify using manual computation with dense eigh,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,331,Test that the first eigenvector of spectral_embedding,
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,332,is constant and that the second is not (for a connected graph),
scikit-learn/sklearn/manifold/tests/test_isomap.py,19,Isomap should preserve distances when all neighbors are used,
scikit-learn/sklearn/manifold/tests/test_isomap.py,24,"grid of equidistant points in 2D, n_components = n_dim",
scikit-learn/sklearn/manifold/tests/test_isomap.py,27,distances from each point to all others,
scikit-learn/sklearn/manifold/tests/test_isomap.py,45,"Same setup as in test_isomap_simple_grid, with an added dimension",
scikit-learn/sklearn/manifold/tests/test_isomap.py,50,"grid of equidistant points in 2D, n_components = n_dim",
scikit-learn/sklearn/manifold/tests/test_isomap.py,53,add noise in a third dimension,
scikit-learn/sklearn/manifold/tests/test_isomap.py,58,compute input kernel,
scikit-learn/sklearn/manifold/tests/test_isomap.py,72,compute output kernel,
scikit-learn/sklearn/manifold/tests/test_isomap.py,79,make sure error agrees,
scikit-learn/sklearn/manifold/tests/test_isomap.py,90,Create S-curve dataset,
scikit-learn/sklearn/manifold/tests/test_isomap.py,93,Compute isomap embedding,
scikit-learn/sklearn/manifold/tests/test_isomap.py,97,Re-embed a noisy version of the points,
scikit-learn/sklearn/manifold/tests/test_isomap.py,102,Make sure the rms error on re-embedding is comparable to noise_scale,
scikit-learn/sklearn/manifold/tests/test_isomap.py,107,check that Isomap works fine as a transformer in a Pipeline,
scikit-learn/sklearn/manifold/tests/test_isomap.py,108,only checks that no error is raised.,
scikit-learn/sklearn/manifold/tests/test_isomap.py,109,TODO check that it actually does something useful,
scikit-learn/sklearn/manifold/tests/test_isomap.py,119,Test chaining NearestNeighborsTransformer and Isomap with,
scikit-learn/sklearn/manifold/tests/test_isomap.py,120,neighbors_algorithm='precomputed',
scikit-learn/sklearn/manifold/tests/test_isomap.py,127,compare the chained version and the compact version,
scikit-learn/sklearn/manifold/tests/test_isomap.py,145,"Test that the metric parameters work correctly, and default to euclidean",
scikit-learn/sklearn/manifold/tests/test_isomap.py,149,"metric, p, is_euclidean",
scikit-learn/sklearn/manifold/tests/test_isomap.py,170,regression test for bug reported in #6062,
scikit-learn/sklearn/manifold/tests/test_isomap.py,182,Should not error,
scikit-learn/sklearn/manifold/tests/test_locally_linear.py,16,----------------------------------------------------------------------,
scikit-learn/sklearn/manifold/tests/test_locally_linear.py,17,Test utility routines,
scikit-learn/sklearn/manifold/tests/test_locally_linear.py,29,check that columns sum to one,
scikit-learn/sklearn/manifold/tests/test_locally_linear.py,35,----------------------------------------------------------------------,
scikit-learn/sklearn/manifold/tests/test_locally_linear.py,36,Test LLE by computing the reconstruction error on some manifolds.,
scikit-learn/sklearn/manifold/tests/test_locally_linear.py,39,"note: ARPACK is numerically unstable, so this test will fail for",
scikit-learn/sklearn/manifold/tests/test_locally_linear.py,40,some random seeds.  We choose 2 because the tests pass.,
scikit-learn/sklearn/manifold/tests/test_locally_linear.py,43,"grid of equidistant points in 2D, n_components = n_dim",
scikit-learn/sklearn/manifold/tests/test_locally_linear.py,67,re-embed a noisy version of X using the transform method,
scikit-learn/sklearn/manifold/tests/test_locally_linear.py,75,similar test on a slightly more complex manifold,
scikit-learn/sklearn/manifold/tests/test_locally_linear.py,103,Test the error raised when parameter passed to lle is invalid,
scikit-learn/sklearn/manifold/tests/test_locally_linear.py,117,check that LocallyLinearEmbedding works fine as a Pipeline,
scikit-learn/sklearn/manifold/tests/test_locally_linear.py,118,only checks that no error is raised.,
scikit-learn/sklearn/manifold/tests/test_locally_linear.py,119,TODO check that it actually does something useful,
scikit-learn/sklearn/manifold/tests/test_locally_linear.py,129,Test the error raised when the weight matrix is singular,
scikit-learn/sklearn/manifold/tests/test_locally_linear.py,139,regression test for #6033,
scikit-learn/sklearn/manifold/tests/test_locally_linear.py,146,this previously raised a TypeError,
scikit-learn/sklearn/model_selection/_search.py,6,"Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,",
scikit-learn/sklearn/model_selection/_search.py,7,Gael Varoquaux <gael.varoquaux@normalesup.org>,
scikit-learn/sklearn/model_selection/_search.py,8,Andreas Mueller <amueller@ais.uni-bonn.de>,
scikit-learn/sklearn/model_selection/_search.py,9,Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/sklearn/model_selection/_search.py,10,Raghav RV <rvraghav93@gmail.com>,
scikit-learn/sklearn/model_selection/_search.py,11,License: BSD 3 clause,
scikit-learn/sklearn/model_selection/_search.py,96,wrap dictionary in a singleton list to support either dict,
scikit-learn/sklearn/model_selection/_search.py,97,or list of dicts,
scikit-learn/sklearn/model_selection/_search.py,100,check if all entries are dictionaries of lists,
scikit-learn/sklearn/model_selection/_search.py,123,"Always sort the keys of a dictionary, for reproducibility",
scikit-learn/sklearn/model_selection/_search.py,135,Product function that can handle iterables (np.product can't).,
scikit-learn/sklearn/model_selection/_search.py,153,This is used to make discrete sampling without replacement memory,
scikit-learn/sklearn/model_selection/_search.py,154,efficient.,
scikit-learn/sklearn/model_selection/_search.py,156,XXX: could memoize information used here,
scikit-learn/sklearn/model_selection/_search.py,164,Reverse so most frequent cycling parameter comes first,
scikit-learn/sklearn/model_selection/_search.py,170,Try the next grid,
scikit-learn/sklearn/model_selection/_search.py,243,wrap dictionary in a singleton list to support either dict,
scikit-learn/sklearn/model_selection/_search.py,244,or list of dicts,
scikit-learn/sklearn/model_selection/_search.py,262,check if all distributions are given as lists,
scikit-learn/sklearn/model_selection/_search.py,263,in this case we want to sample without replacement,
scikit-learn/sklearn/model_selection/_search.py,270,look up sampled parameter settings in parameter grid,
scikit-learn/sklearn/model_selection/_search.py,289,"Always sort the keys of a dictionary, for reproducibility",
scikit-learn/sklearn/model_selection/_search.py,304,FIXME Remove fit_grid_point in 0.25,
scikit-learn/sklearn/model_selection/_search.py,365,NOTE we are not using the return value as the scorer by itself should be,
scikit-learn/sklearn/model_selection/_search.py,366,validated before. We use check_scoring only to reject multimetric scorer,
scikit-learn/sklearn/model_selection/_search.py,424,allows cross-validation to see 'precomputed' metrics,
scikit-learn/sklearn/model_selection/_search.py,572,For consistency with other estimators we raise a AttributeError so,
scikit-learn/sklearn/model_selection/_search.py,573,that hasattr() fails if the search estimator isn't fitted.,
scikit-learn/sklearn/model_selection/_search.py,655,This will work for both dict / list (tuple),
scikit-learn/sklearn/model_selection/_search.py,733,"For multi-metric evaluation, store the best_index_, best_params_ and",
scikit-learn/sklearn/model_selection/_search.py,734,best_score_ iff refit is one of the scorer names,
scikit-learn/sklearn/model_selection/_search.py,735,"In single metric evaluation, refit_metric is ""score""",
scikit-learn/sklearn/model_selection/_search.py,737,"If callable, refit is expected to return the index of the best",
scikit-learn/sklearn/model_selection/_search.py,738,parameter set.,
scikit-learn/sklearn/model_selection/_search.py,754,we clone again after setting params in case some,
scikit-learn/sklearn/model_selection/_search.py,755,of the params are estimators as well.,
scikit-learn/sklearn/model_selection/_search.py,766,Store the only scorer not as a dict for single metric evaluation,
scikit-learn/sklearn/model_selection/_search.py,777,"if one choose to see train score, ""out"" will contain train score info",
scikit-learn/sklearn/model_selection/_search.py,785,test_score_dicts and train_score dicts are lists of dictionaries and,
scikit-learn/sklearn/model_selection/_search.py,786,we make them into dict of lists,
scikit-learn/sklearn/model_selection/_search.py,795,"When iterated first by splits, then by parameters",
scikit-learn/sklearn/model_selection/_search.py,796,We want `array` to have `n_candidates` rows and `n_splits` cols.,
scikit-learn/sklearn/model_selection/_search.py,801,Uses closure to alter the results,
scikit-learn/sklearn/model_selection/_search.py,807,Weighted std is not directly available in numpy,
scikit-learn/sklearn/model_selection/_search.py,819,Use one MaskedArray and mask all the places where the param is not,
scikit-learn/sklearn/model_selection/_search.py,820,applicable for that candidate. Use defaultdict as each candidate may,
scikit-learn/sklearn/model_selection/_search.py,821,not contain all the params,
scikit-learn/sklearn/model_selection/_search.py,828,An all masked empty array gets created for the key,
scikit-learn/sklearn/model_selection/_search.py,829,"`""param_%s"" % name` at the first occurrence of `name`.",
scikit-learn/sklearn/model_selection/_search.py,830,Setting the value at an index also unmasks that index,
scikit-learn/sklearn/model_selection/_search.py,834,Store a list of param dicts at the key 'params',
scikit-learn/sklearn/model_selection/_search.py,837,NOTE test_sample counts (weights) remain the same for all candidates,
scikit-learn/sklearn/model_selection/_search.py,851,Computed the (weighted) mean and std for test scores alone,
scikit-learn/sklearn/model_selection/_validation.py,6,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/sklearn/model_selection/_validation.py,7,Gael Varoquaux <gael.varoquaux@normalesup.org>,
scikit-learn/sklearn/model_selection/_validation.py,8,Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/sklearn/model_selection/_validation.py,9,Raghav RV <rvraghav93@gmail.com>,
scikit-learn/sklearn/model_selection/_validation.py,10,License: BSD 3 clause,
scikit-learn/sklearn/model_selection/_validation.py,227,We clone the estimator to make sure that all the folds are,
scikit-learn/sklearn/model_selection/_validation.py,228,"independent, and that it is pickle-able.",
scikit-learn/sklearn/model_selection/_validation.py,384,To ensure multimetric format is not supported,
scikit-learn/sklearn/model_selection/_validation.py,493,Adjust length of sample weights,
scikit-learn/sklearn/model_selection/_validation.py,499,clone after setting parameters in case any parameters,
scikit-learn/sklearn/model_selection/_validation.py,500,are estimators (like pipeline steps),
scikit-learn/sklearn/model_selection/_validation.py,501,because pipeline doesn't clone steps in fit,
scikit-learn/sklearn/model_selection/_validation.py,520,Note fit time as time until error,
scikit-learn/sklearn/model_selection/_validation.py,588,will cache method calls if needed. scorer() returns a dict,
scikit-learn/sklearn/model_selection/_validation.py,601,e.g. unwrap memmapped scalars,
scikit-learn/sklearn/model_selection/_validation.py,606,scalar,
scikit-learn/sklearn/model_selection/_validation.py,609,e.g. unwrap memmapped scalars,
scikit-learn/sklearn/model_selection/_validation.py,737,"If classification methods produce multiple columns of output,",
scikit-learn/sklearn/model_selection/_validation.py,738,we need to manually encode classes to ensure consistent column ordering.,
scikit-learn/sklearn/model_selection/_validation.py,752,We clone the estimator to make sure that all the folds are,
scikit-learn/sklearn/model_selection/_validation.py,753,"independent, and that it is pickle-able.",
scikit-learn/sklearn/model_selection/_validation.py,760,Concatenate the predictions,
scikit-learn/sklearn/model_selection/_validation.py,774,`predictions` is a list of method outputs from each fold.,
scikit-learn/sklearn/model_selection/_validation.py,775,"If each of those is also a list, then treat this as a",
scikit-learn/sklearn/model_selection/_validation.py,776,multioutput-multiclass task. We need to separately concatenate,
scikit-learn/sklearn/model_selection/_validation.py,777,the method outputs for each label into an `n_labels` long list.,
scikit-learn/sklearn/model_selection/_validation.py,834,Adjust length of sample weights,
scikit-learn/sklearn/model_selection/_validation.py,858,A 2D y array should be a binary label indicator matrix,
scikit-learn/sklearn/model_selection/_validation.py,891,This handles the case when the shape of predictions,
scikit-learn/sklearn/model_selection/_validation.py,892,does not match the number of classes used to train,
scikit-learn/sklearn/model_selection/_validation.py,893,it with. This case is found when sklearn.svm.SVC is,
scikit-learn/sklearn/model_selection/_validation.py,894,set to `decision_function_shape='ovo'`.,
scikit-learn/sklearn/model_selection/_validation.py,902,"In this special case, `predictions` contains a 1D array.",
scikit-learn/sklearn/model_selection/_validation.py,1050,We clone the estimator to make sure that all the folds are,
scikit-learn/sklearn/model_selection/_validation.py,1051,"independent, and that it is pickle-able.",
scikit-learn/sklearn/model_selection/_validation.py,1224,Store it as list as we will be iterating over the list multiple times,
scikit-learn/sklearn/model_selection/_validation.py,1230,"Because the lengths of folds can be significantly different, it is",
scikit-learn/sklearn/model_selection/_validation.py,1231,not guaranteed that we use all of the available training data when we,
scikit-learn/sklearn/model_selection/_validation.py,1232,use the first 'n_max_training_samples' samples.,
scikit-learn/sklearn/model_selection/_validation.py,1476,NOTE do not change order of iteration to allow one time cv splitters,
scikit-learn/sklearn/model_selection/_split.py,6,"Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,",
scikit-learn/sklearn/model_selection/_split.py,7,"Gael Varoquaux <gael.varoquaux@normalesup.org>,",
scikit-learn/sklearn/model_selection/_split.py,8,Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/sklearn/model_selection/_split.py,9,Raghav RV <rvraghav93@gmail.com>,
scikit-learn/sklearn/model_selection/_split.py,10,License: BSD 3 clause,
scikit-learn/sklearn/model_selection/_split.py,85,Since subclasses must implement either _iter_test_masks or,
scikit-learn/sklearn/model_selection/_split.py,86,"_iter_test_indices, neither can be abstract.",
scikit-learn/sklearn/model_selection/_split.py,290,None is the default,
scikit-learn/sklearn/model_selection/_split.py,291,TODO 0.24: raise a ValueError instead of a warning,
scikit-learn/sklearn/model_selection/_split.py,516,Weight groups by their number of occurrences,
scikit-learn/sklearn/model_selection/_split.py,519,Distribute the most frequent groups first,
scikit-learn/sklearn/model_selection/_split.py,523,Total weight of each fold,
scikit-learn/sklearn/model_selection/_split.py,526,Mapping from group index to fold index,
scikit-learn/sklearn/model_selection/_split.py,529,Distribute samples by adding the largest weight to the lightest fold,
scikit-learn/sklearn/model_selection/_split.py,653,y_inv encodes y according to lexicographic order. We invert y_idx to,
scikit-learn/sklearn/model_selection/_split.py,654,map the classes so that they are encoded by order of appearance:,
scikit-learn/sklearn/model_selection/_split.py,655,"0 represents the first label appearing in y, 1 the second, etc.",
scikit-learn/sklearn/model_selection/_split.py,671,"Determine the optimal number of samples from each class in each fold,",
scikit-learn/sklearn/model_selection/_split.py,672,using round robin over the sorted y. (This can be done direct from,
scikit-learn/sklearn/model_selection/_split.py,673,"counts, but that code is unreadable.)",
scikit-learn/sklearn/model_selection/_split.py,679,To maintain the data order dependencies as best as possible within,
scikit-learn/sklearn/model_selection/_split.py,680,"the stratification constraint, we assign samples from each class in",
scikit-learn/sklearn/model_selection/_split.py,681,blocks (and then mess that up when shuffle=True).,
scikit-learn/sklearn/model_selection/_split.py,684,since the kth column of allocation stores the number of samples,
scikit-learn/sklearn/model_selection/_split.py,685,"of class k in each test set, this generates blocks of fold",
scikit-learn/sklearn/model_selection/_split.py,686,indices corresponding to the allocation for class k.,
scikit-learn/sklearn/model_selection/_split.py,884,We make a copy of groups to avoid side-effects during iteration,
scikit-learn/sklearn/model_selection/_split.py,1441,random partition,
scikit-learn/sklearn/model_selection/_split.py,1529,these are the indices of classes in the partition,
scikit-learn/sklearn/model_selection/_split.py,1530,invert them into data indices,
scikit-learn/sklearn/model_selection/_split.py,1647,"for multi-label y, map each distinct row to a string repr",
scikit-learn/sklearn/model_selection/_split.py,1648,using join because str(row) uses an ellipsis if len(row) > 1000,
scikit-learn/sklearn/model_selection/_split.py,1670,Find the sorted list of instances for each class:,
scikit-learn/sklearn/model_selection/_split.py,1671,"(np.unique above performs a sort, so code is O(n logn) already)",
scikit-learn/sklearn/model_selection/_split.py,1678,"if there are ties in the class-counts, we want",
scikit-learn/sklearn/model_selection/_split.py,1679,to make sure to break them anew in each iteration,
scikit-learn/sklearn/model_selection/_split.py,2013,New style cv objects are passed without any modification,
scikit-learn/sklearn/model_selection/_split.py,2145,Tell nose that train_test_split is not a test.,
scikit-learn/sklearn/model_selection/_split.py,2146,(Needed for external libraries that may use nose.),
scikit-learn/sklearn/model_selection/_split.py,2147,Use setattr to avoid mypy errors when monkeypatching.,
scikit-learn/sklearn/model_selection/_split.py,2152,XXX This is copied from BaseEstimator's get_params,
scikit-learn/sklearn/model_selection/_split.py,2155,"Ignore varargs, kw and default values and pop self",
scikit-learn/sklearn/model_selection/_split.py,2157,Consider the constructor parameters excluding 'self',
scikit-learn/sklearn/model_selection/_split.py,2166,We need deprecation warnings to always be on in order to,
scikit-learn/sklearn/model_selection/_split.py,2167,catch deprecated param values.,
scikit-learn/sklearn/model_selection/_split.py,2168,This is set in utils/__init__.py but it gets overwritten,
scikit-learn/sklearn/model_selection/_split.py,2169,when running under python3 somehow.,
scikit-learn/sklearn/model_selection/_split.py,2177,"if the parameter is deprecated, don't show it",
scikit-learn/sklearn/model_selection/tests/test_validation.py,103,"training score becomes worse (2 -> 1), test error better (0 -> 1)",
scikit-learn/sklearn/model_selection/tests/test_validation.py,227,"XXX: use 2D array, since 1D X is being detected as a single sample in",
scikit-learn/sklearn/model_selection/tests/test_validation.py,228,check_consistent_length,
scikit-learn/sklearn/model_selection/tests/test_validation.py,232,"The number of samples per class needs to be > n_splits,",
scikit-learn/sklearn/model_selection/tests/test_validation.py,233,for StratifiedKFold(n_splits=3),
scikit-learn/sklearn/model_selection/tests/test_validation.py,243,Smoke test,
scikit-learn/sklearn/model_selection/tests/test_validation.py,247,test with multioutput y,
scikit-learn/sklearn/model_selection/tests/test_validation.py,255,test with multioutput y,
scikit-learn/sklearn/model_selection/tests/test_validation.py,259,test with X and y as list,
scikit-learn/sklearn/model_selection/tests/test_validation.py,269,test with 3d X and,
scikit-learn/sklearn/model_selection/tests/test_validation.py,280,regression test for #12154: cv='warn' with n_jobs>1 trigger a copy of,
scikit-learn/sklearn/model_selection/tests/test_validation.py,281,the parameters leading to a failure in check_cv due to cv is 'warn',
scikit-learn/sklearn/model_selection/tests/test_validation.py,282,instead of cv == 'warn'.,
scikit-learn/sklearn/model_selection/tests/test_validation.py,293,Test the errors,
scikit-learn/sklearn/model_selection/tests/test_validation.py,296,List/tuple of callables should raise a message advising users to use,
scikit-learn/sklearn/model_selection/tests/test_validation.py,297,dict of names to callables mapping,
scikit-learn/sklearn/model_selection/tests/test_validation.py,306,So should empty lists/tuples,
scikit-learn/sklearn/model_selection/tests/test_validation.py,310,So should duplicated entries,
scikit-learn/sklearn/model_selection/tests/test_validation.py,315,Nested Lists should raise a generic error message,
scikit-learn/sklearn/model_selection/tests/test_validation.py,323,Empty dict should raise invalid scoring error,
scikit-learn/sklearn/model_selection/tests/test_validation.py,327,And so should any other invalid entry,
scikit-learn/sklearn/model_selection/tests/test_validation.py,333,Multiclass Scorers that return multiple values are not supported yet,
scikit-learn/sklearn/model_selection/tests/test_validation.py,347,Multiclass Scorers that return multiple values are not supported yet,
scikit-learn/sklearn/model_selection/tests/test_validation.py,360,Compute train and test mse/r2 scores,
scikit-learn/sklearn/model_selection/tests/test_validation.py,363,Regression,
scikit-learn/sklearn/model_selection/tests/test_validation.py,367,Classification,
scikit-learn/sklearn/model_selection/tests/test_validation.py,372,It's okay to evaluate regression metrics on classification too,
scikit-learn/sklearn/model_selection/tests/test_validation.py,404,Test single metric evaluation when scoring is string or singleton list,
scikit-learn/sklearn/model_selection/tests/test_validation.py,406,Single metric passed as a string,
scikit-learn/sklearn/model_selection/tests/test_validation.py,422,Single metric passed as a list,
scikit-learn/sklearn/model_selection/tests/test_validation.py,424,It must be True by default - deprecated,
scikit-learn/sklearn/model_selection/tests/test_validation.py,436,Test return_estimator option,
scikit-learn/sklearn/model_selection/tests/test_validation.py,446,Test multimetric evaluation when scoring is a list / dict,
scikit-learn/sklearn/model_selection/tests/test_validation.py,461,return_train_score must be True by default - deprecated,
scikit-learn/sklearn/model_selection/tests/test_validation.py,480,Make sure all the arrays are of np.ndarray type,
scikit-learn/sklearn/model_selection/tests/test_validation.py,487,Ensure all the times are within sane limits,
scikit-learn/sklearn/model_selection/tests/test_validation.py,495,Check if ValueError (when groups is None) propagates to cross_val_score,
scikit-learn/sklearn/model_selection/tests/test_validation.py,496,and cross_val_predict,
scikit-learn/sklearn/model_selection/tests/test_validation.py,497,And also check if groups is correctly passed to the cv object,
scikit-learn/sklearn/model_selection/tests/test_validation.py,515,check cross_val_score doesn't destroy pandas dataframe,
scikit-learn/sklearn/model_selection/tests/test_validation.py,523,"X dataframe, y series",
scikit-learn/sklearn/model_selection/tests/test_validation.py,524,3 fold cross val is used so we need atleast 3 samples per class,
scikit-learn/sklearn/model_selection/tests/test_validation.py,533,test that cross_val_score works with boolean masks,
scikit-learn/sklearn/model_selection/tests/test_validation.py,552,test for svm with precomputed kernel,
scikit-learn/sklearn/model_selection/tests/test_validation.py,562,test with callable,
scikit-learn/sklearn/model_selection/tests/test_validation.py,567,Error raised for non-square X,
scikit-learn/sklearn/model_selection/tests/test_validation.py,571,test error is raised when the precomputed kernel is not array-like,
scikit-learn/sklearn/model_selection/tests/test_validation.py,572,or sparse,
scikit-learn/sklearn/model_selection/tests/test_validation.py,591,Function to test that the values are passed correctly to the,
scikit-learn/sklearn/model_selection/tests/test_validation.py,592,classifier arguments for non-array type,
scikit-learn/sklearn/model_selection/tests/test_validation.py,621,Test that score function is called only 3 times (for cv=3),
scikit-learn/sklearn/model_selection/tests/test_validation.py,636,Default score (should be the accuracy score),
scikit-learn/sklearn/model_selection/tests/test_validation.py,640,Correct classification score (aka. zero / one score) - should be the,
scikit-learn/sklearn/model_selection/tests/test_validation.py,641,same as the default estimator score,
scikit-learn/sklearn/model_selection/tests/test_validation.py,646,F1 score (class are balanced so f1_score should be equal to zero/one,
scikit-learn/sklearn/model_selection/tests/test_validation.py,647,score,
scikit-learn/sklearn/model_selection/tests/test_validation.py,658,Default score of the Ridge regression estimator,
scikit-learn/sklearn/model_selection/tests/test_validation.py,662,R2 score (aka. determination coefficient) - should be the,
scikit-learn/sklearn/model_selection/tests/test_validation.py,663,same as the default estimator score,
scikit-learn/sklearn/model_selection/tests/test_validation.py,667,"Mean squared error; this is a loss function, so ""scores"" are negative",
scikit-learn/sklearn/model_selection/tests/test_validation.py,673,Explained variance,
scikit-learn/sklearn/model_selection/tests/test_validation.py,698,check that we obtain the same results with a sparse representation,
scikit-learn/sklearn/model_selection/tests/test_validation.py,708,test with custom scoring object,
scikit-learn/sklearn/model_selection/tests/test_validation.py,719,set random y,
scikit-learn/sklearn/model_selection/tests/test_validation.py,730,Check that permutation_test_score allows input data with NaNs,
scikit-learn/sklearn/model_selection/tests/test_validation.py,742,Check that cross_val_score allows input data with NaNs,
scikit-learn/sklearn/model_selection/tests/test_validation.py,776,Naive loop (should be same as cross_val_predict):,
scikit-learn/sklearn/model_selection/tests/test_validation.py,832,This specifically tests imbalanced splits for binary,
scikit-learn/sklearn/model_selection/tests/test_validation.py,833,classification with decision_function. This is only,
scikit-learn/sklearn/model_selection/tests/test_validation.py,834,applicable to classifiers that can be fit on a single,
scikit-learn/sklearn/model_selection/tests/test_validation.py,835,class.,
scikit-learn/sklearn/model_selection/tests/test_validation.py,901,3 fold cv is used --> atleast 3 samples per class,
scikit-learn/sklearn/model_selection/tests/test_validation.py,902,Smoke test,
scikit-learn/sklearn/model_selection/tests/test_validation.py,906,test with multioutput y,
scikit-learn/sklearn/model_selection/tests/test_validation.py,913,test with multioutput y,
scikit-learn/sklearn/model_selection/tests/test_validation.py,917,test with X and y as list,
scikit-learn/sklearn/model_selection/tests/test_validation.py,925,test with X and y as list and non empty method,
scikit-learn/sklearn/model_selection/tests/test_validation.py,933,test with 3d X and,
scikit-learn/sklearn/model_selection/tests/test_validation.py,942,python3.7 deprecation warnings in pandas via matplotlib :-/,
scikit-learn/sklearn/model_selection/tests/test_validation.py,944,check cross_val_score doesn't destroy pandas dataframe,
scikit-learn/sklearn/model_selection/tests/test_validation.py,952,"X dataframe, y series",
scikit-learn/sklearn/model_selection/tests/test_validation.py,964,Change the first sample to a new class,
scikit-learn/sklearn/model_selection/tests/test_validation.py,970,sanity check for further assertions,
scikit-learn/sklearn/model_selection/tests/test_validation.py,979,ensure that cross_val_predict works when y is None,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1025,Cannot use assert_array_almost_equal for fit and score times because,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1026,the values are hardware-dependant,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1030,Test a custom cv splitter that can iterate only once,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1080,The mockup does not have partial_fit(),
scikit-learn/sklearn/model_selection/tests/test_validation.py,1185,Following test case was designed this way to verify the code,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1186,changes made in pull request: #7506.,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1192,Splits on these groups fail without shuffle as the first iteration,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1193,of the learning curve doesn't contain label 4 in the training set.,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1257,"The OneTimeSplitter is a non-re-entrant cv splitter. Unless, the",
scikit-learn/sklearn/model_selection/tests/test_validation.py,1258,"`split` is called for each parameter, the following should produce",
scikit-learn/sklearn/model_selection/tests/test_validation.py,1259,identical results for param setting 1 and param setting 2 as both have,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1260,the same C value.,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1268,"For scores2, compare the 1st and 2nd parameter's scores",
scikit-learn/sklearn/model_selection/tests/test_validation.py,1269,"(Since the C value for 1st two param setting is 0.1, they must be",
scikit-learn/sklearn/model_selection/tests/test_validation.py,1270,consistent unless the train test folds differ between the param settings),
scikit-learn/sklearn/model_selection/tests/test_validation.py,1278,OneTimeSplitter is basically unshuffled KFold(n_splits=5). Sanity check.,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1292,Check if the additional duplicate indices are caught,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1297,check that cross_val_predict gives same result for sparse and dense input,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1315,Generate expected outputs,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1325,Check actual outputs for several representations of y,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1335,Generate expected outputs,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1350,Check actual outputs for several representations of y,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1363,Create empty arrays of the correct size to hold outputs,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1379,Generate expected outputs,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1389,Decision function with <=2 classes,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1395,Check actual outputs for several representations of y,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1404,This test includes the decision_function with two classes.,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1405,This is a special case: it has only one column of output.,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1427,Regression test for issue #9639. Tests that cross_val_predict does not,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1428,check estimator methods (e.g. predict_proba) before fitting,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1449,"OVR does multilabel predictions, but only arrays of",
scikit-learn/sklearn/model_selection/tests/test_validation.py,1450,binary indicator columns. The output of predict_proba,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1451,"is a 2D array with shape (n_samples, n_classes).",
scikit-learn/sklearn/model_selection/tests/test_validation.py,1464,None of the current multioutput-multiclass estimators have,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1465,decision function methods. Create a mock decision function,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1466,to test the cross_val_predict function's handling of this case.,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1476,The RandomForest allows multiple classes in each label.,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1477,Output of predict_proba is a list of outputs of predict_proba,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1478,for each individual label.,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1483,Put three classes in the first column,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1487,"Suppress ""RuntimeWarning: divide by zero encountered in log""",
scikit-learn/sklearn/model_selection/tests/test_validation.py,1493,Test a multiclass problem where one class will be missing from,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1494,one of the CV training sets.,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1501,Suppress warning about too few examples of a class,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1507,The RandomForest allows anything for the contents of the labels.,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1508,Output of predict_proba is a list of outputs of predict_proba,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1509,for each individual label.,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1510,"In this test, the first label has a class with a single example.",
scikit-learn/sklearn/model_selection/tests/test_validation.py,1511,We'll have one CV fold where the training data don't include it.,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1518,"Suppress ""RuntimeWarning: divide by zero encountered in log""",
scikit-learn/sklearn/model_selection/tests/test_validation.py,1531,To avoid 2 dimensional indexing,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1558,Test with n_splits=3,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1562,Runs a naive loop (should be same as cross_val_predict):,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1567,Test with n_splits=4,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1574,Testing unordered labels,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1585,Ensure a scalar score of memmap type is accepted,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1596,non-scalar should still fail,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1600,Best effort to release the mmap file handles before deleting the,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1601,backing file under Windows,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1613,check permutation_test_score doesn't destroy pandas dataframe,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1621,"X dataframe, y series",
scikit-learn/sklearn/model_selection/tests/test_validation.py,1632,Create a failing classifier to deliberately fail,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1634,dummy X data,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1639,passing error score to trigger the warning message,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1641,check if the warning message type is as expected,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1644,"since we're using FailingClassfier, our error will be the following",
scikit-learn/sklearn/model_selection/tests/test_validation.py,1646,the warning message we're expecting to see,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1654,note: handles more than '\n',
scikit-learn/sklearn/model_selection/tests/test_validation.py,1657,check traceback is included,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1662,"check if exception was raised, with default error_score='raise'",
scikit-learn/sklearn/model_selection/tests/test_validation.py,1667,check that functions upstream pass error_score param to _fit_and_score,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1686,FailingClassifier coverage,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1693,Test return_parameters option,
scikit-learn/sklearn/model_selection/tests/test_validation.py,1721,test print without train score,
scikit-learn/sklearn/model_selection/tests/test_search.py,70,noqa,
scikit-learn/sklearn/model_selection/tests/test_search.py,76,"Neither of the following two estimators inherit from BaseEstimator,",
scikit-learn/sklearn/model_selection/tests/test_search.py,77,to test hyperparameter search on user-defined classifiers.,
scikit-learn/sklearn/model_selection/tests/test_search.py,146,Test basic properties of ParameterGrid.,
scikit-learn/sklearn/model_selection/tests/test_search.py,159,loop to assert we can iterate over the grid multiple times,
scikit-learn/sklearn/model_selection/tests/test_search.py,161,"tuple + chain transforms {""a"": 1, ""b"": 2} to (""a"", 1, ""b"", 2)",
scikit-learn/sklearn/model_selection/tests/test_search.py,168,Special case: empty grid (useful to get default estimator settings),
scikit-learn/sklearn/model_selection/tests/test_search.py,182,Test that the best estimator contains the right value for foo_param,
scikit-learn/sklearn/model_selection/tests/test_search.py,185,make sure it selects the smallest parameter in case of ties,
scikit-learn/sklearn/model_selection/tests/test_search.py,195,Smoke test the score etc:,
scikit-learn/sklearn/model_selection/tests/test_search.py,201,Test exception handling on scoring,
scikit-learn/sklearn/model_selection/tests/test_search.py,207,check that parameters that are estimators are cloned before fitting,
scikit-learn/sklearn/model_selection/tests/test_search.py,219,check that we didn't modify the parameter grid that was passed,
scikit-learn/sklearn/model_selection/tests/test_search.py,233,The CheckingClassifier generates an assertion error if,
scikit-learn/sklearn/model_selection/tests/test_search.py,234,a parameter is missing or has length != len(X).,
scikit-learn/sklearn/model_selection/tests/test_search.py,247,Test grid-search on classifier that has no score function.,
scikit-learn/sklearn/model_selection/tests/test_search.py,257,smoketest grid search,
scikit-learn/sklearn/model_selection/tests/test_search.py,260,check that best params are equal,
scikit-learn/sklearn/model_selection/tests/test_search.py,262,check that we can call score and that it gives the correct result,
scikit-learn/sklearn/model_selection/tests/test_search.py,265,giving no scoring function raises an error,
scikit-learn/sklearn/model_selection/tests/test_search.py,284,Check warning only occurs in situation where behavior changed:,
scikit-learn/sklearn/model_selection/tests/test_search.py,285,estimator requires score method to compete with scoring parameter,
scikit-learn/sklearn/model_selection/tests/test_search.py,291,ensure the test is sane,
scikit-learn/sklearn/model_selection/tests/test_search.py,301,Check if ValueError (when groups is None) propagates to GridSearchCV,
scikit-learn/sklearn/model_selection/tests/test_search.py,302,And also check if groups is correctly passed to the cv object,
scikit-learn/sklearn/model_selection/tests/test_search.py,323,Should not raise an error,
scikit-learn/sklearn/model_selection/tests/test_search.py,328,Test that classes_ property matches best_estimator_.classes_,
scikit-learn/sklearn/model_selection/tests/test_search.py,338,Test that regressors do not have a classes_ attribute,
scikit-learn/sklearn/model_selection/tests/test_search.py,343,Test that the grid searcher has no classes_ attribute before it's fit,
scikit-learn/sklearn/model_selection/tests/test_search.py,347,Test that the grid searcher has no classes_ attribute without a refit,
scikit-learn/sklearn/model_selection/tests/test_search.py,355,"Test search over a ""grid"" with only one point.",
scikit-learn/sklearn/model_selection/tests/test_search.py,367,Test that GSCV can be used for model selection alone without refitting,
scikit-learn/sklearn/model_selection/tests/test_search.py,378,Make sure the functions predict/transform etc raise meaningful,
scikit-learn/sklearn/model_selection/tests/test_search.py,379,error messages,
scikit-learn/sklearn/model_selection/tests/test_search.py,387,Test that an invalid refit param raises appropriate error messages,
scikit-learn/sklearn/model_selection/tests/test_search.py,399,Test that grid search will capture errors on data with different length,
scikit-learn/sklearn/model_selection/tests/test_search.py,422,Test that the best estimator contains the right value for foo_param,
scikit-learn/sklearn/model_selection/tests/test_search.py,464,Test that grid search works with both dense and sparse matrices,
scikit-learn/sklearn/model_selection/tests/test_search.py,502,Smoke test the score,
scikit-learn/sklearn/model_selection/tests/test_search.py,503,"np.testing.assert_allclose(f1_score(cv.predict(X_[:180]), y[:180]),",
scikit-learn/sklearn/model_selection/tests/test_search.py,504,"cv.score(X_[:180], y[:180]))",
scikit-learn/sklearn/model_selection/tests/test_search.py,506,test loss where greater is worse,
scikit-learn/sklearn/model_selection/tests/test_search.py,520,Test that grid search works when the input features are given in the,
scikit-learn/sklearn/model_selection/tests/test_search.py,521,form of a precomputed kernel matrix,
scikit-learn/sklearn/model_selection/tests/test_search.py,524,compute the training kernel matrix corresponding to the linear kernel,
scikit-learn/sklearn/model_selection/tests/test_search.py,534,compute the test kernel matrix,
scikit-learn/sklearn/model_selection/tests/test_search.py,542,test error is raised when the precomputed kernel is not array-like,
scikit-learn/sklearn/model_selection/tests/test_search.py,543,or sparse,
scikit-learn/sklearn/model_selection/tests/test_search.py,548,Test that grid search returns an error with a non-square precomputed,
scikit-learn/sklearn/model_selection/tests/test_search.py,549,training kernel matrix,
scikit-learn/sklearn/model_selection/tests/test_search.py,573,Regression test for bug in refitting,
scikit-learn/sklearn/model_selection/tests/test_search.py,574,Simulates re-fitting a broken estimator; this used to break with,
scikit-learn/sklearn/model_selection/tests/test_search.py,575,sparse SVMs.,
scikit-learn/sklearn/model_selection/tests/test_search.py,595,Fit a dummy clf with `refit=True` to get a list of keys in,
scikit-learn/sklearn/model_selection/tests/test_search.py,596,clf.cv_results_.,
scikit-learn/sklearn/model_selection/tests/test_search.py,602,Ensure that `best_index_ != 0` for this dummy clf,
scikit-learn/sklearn/model_selection/tests/test_search.py,605,Assert every key matches those in `cv_results`,
scikit-learn/sklearn/model_selection/tests/test_search.py,618,Ensure `best_score_` is disabled when using `refit=callable`,
scikit-learn/sklearn/model_selection/tests/test_search.py,686,Ensure `best_score_` is disabled when using `refit=callable`,
scikit-learn/sklearn/model_selection/tests/test_search.py,691,Pass X as list in GridSearchCV,
scikit-learn/sklearn/model_selection/tests/test_search.py,703,Pass X as list in GridSearchCV,
scikit-learn/sklearn/model_selection/tests/test_search.py,715,Pass y as list in GridSearchCV,
scikit-learn/sklearn/model_selection/tests/test_search.py,728,check cross_val_score doesn't destroy pandas dataframe,
scikit-learn/sklearn/model_selection/tests/test_search.py,740,"X dataframe, y series",
scikit-learn/sklearn/model_selection/tests/test_search.py,758,test grid-search with unsupervised estimator,
scikit-learn/sklearn/model_selection/tests/test_search.py,762,Multi-metric evaluation unsupervised,
scikit-learn/sklearn/model_selection/tests/test_search.py,768,Both ARI and FMS can find the right number :),
scikit-learn/sklearn/model_selection/tests/test_search.py,771,Single metric evaluation unsupervised,
scikit-learn/sklearn/model_selection/tests/test_search.py,777,"Now without a score, and without y",
scikit-learn/sklearn/model_selection/tests/test_search.py,784,test grid-search with an estimator without predict.,
scikit-learn/sklearn/model_selection/tests/test_search.py,785,slight duplication of a test from KDE,
scikit-learn/sklearn/model_selection/tests/test_search.py,799,test basic properties of param sampler,
scikit-learn/sklearn/model_selection/tests/test_search.py,810,test that repeated calls yield identical parameters,
scikit-learn/sklearn/model_selection/tests/test_search.py,824,Check if the search `cv_results`'s array are of correct types,
scikit-learn/sklearn/model_selection/tests/test_search.py,841,Test the search.cv_results_ contains all the required results,
scikit-learn/sklearn/model_selection/tests/test_search.py,848,0.24,
scikit-learn/sklearn/model_selection/tests/test_search.py,876,Check if score and timing are reasonable,
scikit-learn/sklearn/model_selection/tests/test_search.py,883,Check cv_results structure,
scikit-learn/sklearn/model_selection/tests/test_search.py,886,Check masking,
scikit-learn/sklearn/model_selection/tests/test_search.py,901,0.24,
scikit-learn/sklearn/model_selection/tests/test_search.py,931,Check results structure,
scikit-learn/sklearn/model_selection/tests/test_search.py,954,Test the IID parameter,
scikit-learn/sklearn/model_selection/tests/test_search.py,955,noise-free simple 2d-data,
scikit-learn/sklearn/model_selection/tests/test_search.py,958,split dataset into two folds that are not iid,
scikit-learn/sklearn/model_selection/tests/test_search.py,959,"first one contains data of all 4 blobs, second only from two.",
scikit-learn/sklearn/model_selection/tests/test_search.py,963,this leads to perfect classification on one fold and a score of 1/3 on,
scikit-learn/sklearn/model_selection/tests/test_search.py,964,the other,
scikit-learn/sklearn/model_selection/tests/test_search.py,965,"create ""cv"" for splits",
scikit-learn/sklearn/model_selection/tests/test_search.py,988,scores are the same as above,
scikit-learn/sklearn/model_selection/tests/test_search.py,991,Unweighted mean/std is used,
scikit-learn/sklearn/model_selection/tests/test_search.py,995,"For the train scores, we do not take a weighted mean irrespective of",
scikit-learn/sklearn/model_selection/tests/test_search.py,996,i.i.d. or not,
scikit-learn/sklearn/model_selection/tests/test_search.py,1001,0.24,
scikit-learn/sklearn/model_selection/tests/test_search.py,1003,Test the IID parameter,
scikit-learn/sklearn/model_selection/tests/test_search.py,1004,noise-free simple 2d-data,
scikit-learn/sklearn/model_selection/tests/test_search.py,1007,split dataset into two folds that are not iid,
scikit-learn/sklearn/model_selection/tests/test_search.py,1008,"first one contains data of all 4 blobs, second only from two.",
scikit-learn/sklearn/model_selection/tests/test_search.py,1012,this leads to perfect classification on one fold and a score of 1/3 on,
scikit-learn/sklearn/model_selection/tests/test_search.py,1013,the other,
scikit-learn/sklearn/model_selection/tests/test_search.py,1014,"create ""cv"" for splits",
scikit-learn/sklearn/model_selection/tests/test_search.py,1016,once with iid=True (default),
scikit-learn/sklearn/model_selection/tests/test_search.py,1039,Test the first candidate,
scikit-learn/sklearn/model_selection/tests/test_search.py,1044,"for first split, 1/4 of dataset is in test, for second 3/4.",
scikit-learn/sklearn/model_selection/tests/test_search.py,1045,take weighted average and weighted std,
scikit-learn/sklearn/model_selection/tests/test_search.py,1056,"For the train scores, we do not take a weighted mean irrespective of",
scikit-learn/sklearn/model_selection/tests/test_search.py,1057,i.i.d. or not,
scikit-learn/sklearn/model_selection/tests/test_search.py,1061,once with iid=False,
scikit-learn/sklearn/model_selection/tests/test_search.py,1087,scores are the same as above,
scikit-learn/sklearn/model_selection/tests/test_search.py,1089,Unweighted mean/std is used,
scikit-learn/sklearn/model_selection/tests/test_search.py,1093,"For the train scores, we do not take a weighted mean irrespective of",
scikit-learn/sklearn/model_selection/tests/test_search.py,1094,i.i.d. or not,
scikit-learn/sklearn/model_selection/tests/test_search.py,1099,0.24,
scikit-learn/sklearn/model_selection/tests/test_search.py,1122,0.24,
scikit-learn/sklearn/model_selection/tests/test_search.py,1129,"Scipy 0.12's stats dists do not accept seed, hence we use param grid",
scikit-learn/sklearn/model_selection/tests/test_search.py,1136,"If True, for multi-metric pass refit='accuracy'",
scikit-learn/sklearn/model_selection/tests/test_search.py,1157,0.24,
scikit-learn/sklearn/model_selection/tests/test_search.py,1174,"Check if score and timing are reasonable, also checks if the keys",
scikit-learn/sklearn/model_selection/tests/test_search.py,1175,are present,
scikit-learn/sklearn/model_selection/tests/test_search.py,1180,"Compare the keys, other than time keys, among multi-metric and",
scikit-learn/sklearn/model_selection/tests/test_search.py,1181,single metric grid search results. np.testing.assert_equal performs a,
scikit-learn/sklearn/model_selection/tests/test_search.py,1182,deep nested comparison of the two cv_results dicts,
scikit-learn/sklearn/model_selection/tests/test_search.py,1196,search cannot predict/score without refit,
scikit-learn/sklearn/model_selection/tests/test_search.py,1210,The two C values are close enough to give similar models,
scikit-learn/sklearn/model_selection/tests/test_search.py,1211,which would result in a tie of their mean cv-scores,
scikit-learn/sklearn/model_selection/tests/test_search.py,1223,Check tie breaking strategy -,
scikit-learn/sklearn/model_selection/tests/test_search.py,1224,Check that there is a tie in the mean scores between,
scikit-learn/sklearn/model_selection/tests/test_search.py,1225,candidates 1 and 2 alone,
scikit-learn/sklearn/model_selection/tests/test_search.py,1234,'min' rank should be assigned to the tied candidates,
scikit-learn/sklearn/model_selection/tests/test_search.py,1264,NOTE The precision of time.time in windows is not high,
scikit-learn/sklearn/model_selection/tests/test_search.py,1265,enough for the fit/score times to be non-zero for trivial X and y,
scikit-learn/sklearn/model_selection/tests/test_search.py,1280,test that correct scores are used,
scikit-learn/sklearn/model_selection/tests/test_search.py,1289,Test scorer names,
scikit-learn/sklearn/model_selection/tests/test_search.py,1314,FIXME remove test_fit_grid_point as the function will be removed on 0.25,
scikit-learn/sklearn/model_selection/tests/test_search.py,1332,Test the return values of fit_grid_point,
scikit-learn/sklearn/model_selection/tests/test_search.py,1337,Should raise an error upon multimetric scorer,
scikit-learn/sklearn/model_selection/tests/test_search.py,1344,FIXME remove test_fit_grid_point_deprecated as,
scikit-learn/sklearn/model_selection/tests/test_search.py,1345,fit_grid_point will be removed on 0.25,
scikit-learn/sklearn/model_selection/tests/test_search.py,1360,Test that a fit search can be pickled,
scikit-learn/sklearn/model_selection/tests/test_search.py,1377,Test search with multi-output estimator,
scikit-learn/sklearn/model_selection/tests/test_search.py,1388,Test with grid search cv,
scikit-learn/sklearn/model_selection/tests/test_search.py,1403,Test with a randomized search,
scikit-learn/sklearn/model_selection/tests/test_search.py,1422,Test predict_proba when disabled on estimator.,
scikit-learn/sklearn/model_selection/tests/test_search.py,1431,Test GridSearchCV with SimpleImputer,
scikit-learn/sklearn/model_selection/tests/test_search.py,1462,GridSearchCV with on_error != 'raise',
scikit-learn/sklearn/model_selection/tests/test_search.py,1463,Ensures that a warning is raised and score reset where appropriate.,
scikit-learn/sklearn/model_selection/tests/test_search.py,1469,refit=False because we only want to check that errors caused by fits,
scikit-learn/sklearn/model_selection/tests/test_search.py,1470,to individual folds will be caught and warnings raised instead. If,
scikit-learn/sklearn/model_selection/tests/test_search.py,1471,"refit was done, then an exception would be raised on refit and not",
scikit-learn/sklearn/model_selection/tests/test_search.py,1472,"caught by grid_search (expected behavior), and this would cause an",
scikit-learn/sklearn/model_selection/tests/test_search.py,1473,error in this test.,
scikit-learn/sklearn/model_selection/tests/test_search.py,1479,Ensure that grid scores were set to zero as required for those fits,
scikit-learn/sklearn/model_selection/tests/test_search.py,1480,that are expected to fail.,
scikit-learn/sklearn/model_selection/tests/test_search.py,1501,Check that succeeded estimators have lower ranks,
scikit-learn/sklearn/model_selection/tests/test_search.py,1503,Check that failed estimator has the highest rank,
scikit-learn/sklearn/model_selection/tests/test_search.py,1509,GridSearchCV with on_error == 'raise' raises the error,
scikit-learn/sklearn/model_selection/tests/test_search.py,1515,refit=False because we want to test the behaviour of the grid search part,
scikit-learn/sklearn/model_selection/tests/test_search.py,1519,FailingClassifier issues a ValueError so this is what we look for.,
scikit-learn/sklearn/model_selection/tests/test_search.py,1524,raise warning if n_iter is bigger than total parameter space,
scikit-learn/sklearn/model_selection/tests/test_search.py,1537,degenerates to GridSearchCV if n_iter the same as grid_size,
scikit-learn/sklearn/model_selection/tests/test_search.py,1544,test sampling without replacement in a large grid,
scikit-learn/sklearn/model_selection/tests/test_search.py,1553,doesn't go into infinite loops,
scikit-learn/sklearn/model_selection/tests/test_search.py,1561,Make sure the predict_proba works when loss is specified,
scikit-learn/sklearn/model_selection/tests/test_search.py,1562,as one of the parameters in the param_grid.,
scikit-learn/sklearn/model_selection/tests/test_search.py,1571,"When the estimator is not fitted, `predict_proba` is not available as the",
scikit-learn/sklearn/model_selection/tests/test_search.py,1572,loss is 'hinge'.,
scikit-learn/sklearn/model_selection/tests/test_search.py,1578,Make sure `predict_proba` is not available when setting loss=['hinge'],
scikit-learn/sklearn/model_selection/tests/test_search.py,1579,in param_grid,
scikit-learn/sklearn/model_selection/tests/test_search.py,1600,Check if a one time iterable is accepted as a cv parameter.,
scikit-learn/sklearn/model_selection/tests/test_search.py,1617,Give generator as a cv parameter,
scikit-learn/sklearn/model_selection/tests/test_search.py,1640,Check if generators are supported as cv and,
scikit-learn/sklearn/model_selection/tests/test_search.py,1641,that the splits are consistent,
scikit-learn/sklearn/model_selection/tests/test_search.py,1645,OneTimeSplitter is a non-re-entrant cv where split can be called only,
scikit-learn/sklearn/model_selection/tests/test_search.py,1646,once if ``cv.split`` is called once per param setting in GridSearchCV.fit,
scikit-learn/sklearn/model_selection/tests/test_search.py,1647,the 2nd and 3rd parameter will not be evaluated as no train/test indices,
scikit-learn/sklearn/model_selection/tests/test_search.py,1648,will be generated for the 2nd and subsequent cv.split calls.,
scikit-learn/sklearn/model_selection/tests/test_search.py,1649,This is a check to make sure cv.split is not called once per param,
scikit-learn/sklearn/model_selection/tests/test_search.py,1650,setting.,
scikit-learn/sklearn/model_selection/tests/test_search.py,1656,Check consistency of folds across the parameters,
scikit-learn/sklearn/model_selection/tests/test_search.py,1663,As the first two param settings (C=0.1) and the next two param,
scikit-learn/sklearn/model_selection/tests/test_search.py,1664,"settings (C=0.2) are same, the test and train scores must also be",
scikit-learn/sklearn/model_selection/tests/test_search.py,1665,same as long as the same train/test indices are generated for all,
scikit-learn/sklearn/model_selection/tests/test_search.py,1666,"the cv splits, for both param setting",
scikit-learn/sklearn/model_selection/tests/test_search.py,1695,XXX: results['params'] is a list :|,
scikit-learn/sklearn/model_selection/tests/test_search.py,1720,Using regressor to make sure each score differs,
scikit-learn/sklearn/model_selection/tests/test_search.py,1730,"TODO: remove in v0.24, the deprecation goes away then.",
scikit-learn/sklearn/model_selection/tests/test_search.py,1750,this should not raise any exceptions,
scikit-learn/sklearn/model_selection/tests/test_search.py,1759,this should raise a NotImplementedError,
scikit-learn/sklearn/model_selection/tests/test_search.py,1765,FIXME: remove in 0.24,
scikit-learn/sklearn/model_selection/tests/test_search.py,1776,"Use global X, y",
scikit-learn/sklearn/model_selection/tests/test_search.py,1778,create cv,
scikit-learn/sklearn/model_selection/tests/test_search.py,1781,"pop all of it, this should cause the expected ValueError",
scikit-learn/sklearn/model_selection/tests/test_search.py,1783,cv is empty now,
scikit-learn/sklearn/model_selection/tests/test_search.py,1789,assert that this raises an error,
scikit-learn/sklearn/model_selection/tests/test_search.py,1798,"Use global X, y",
scikit-learn/sklearn/model_selection/tests/test_search.py,1804,create bad cv,
scikit-learn/sklearn/model_selection/tests/test_search.py,1811,assert that this raises an error,
scikit-learn/sklearn/model_selection/tests/test_search.py,1820,make sure grid search and random search delegate n_features_in to the,
scikit-learn/sklearn/model_selection/tests/test_search.py,1821,best estimator,
scikit-learn/sklearn/model_selection/tests/test_search.py,1866,defaults to euclidean metric (minkowski p = 2),
scikit-learn/sklearn/model_selection/tests/test_search.py,1872,precompute euclidean metric to validate _pairwise is working,
scikit-learn/sklearn/model_selection/tests/test_search.py,1889,unofficially sanctioned tolerance for scalar values in fit_params,
scikit-learn/sklearn/model_selection/tests/test_search.py,1890,non-regression test for:,
scikit-learn/sklearn/model_selection/tests/test_search.py,1891,https://github.com/scikit-learn/scikit-learn/issues/15805,
scikit-learn/sklearn/model_selection/tests/test_search.py,1914,"check support for scalar values in fit_params, for instance in LightGBM",
scikit-learn/sklearn/model_selection/tests/test_search.py,1915,that do not exactly respect the scikit-learn API contract but that we do,
scikit-learn/sklearn/model_selection/tests/test_search.py,1916,not want to break without an explicit deprecation cycle and API,
scikit-learn/sklearn/model_selection/tests/test_search.py,1917,recommendations for implementing early stopping with a user provided,
scikit-learn/sklearn/model_selection/tests/test_search.py,1918,validation set. non-regression test for:,
scikit-learn/sklearn/model_selection/tests/test_search.py,1919,https://github.com/scikit-learn/scikit-learn/issues/15805,
scikit-learn/sklearn/model_selection/tests/test_search.py,1932,The tuple of arrays should be preserved as tuple.,
scikit-learn/sklearn/model_selection/tests/test_search.py,1945,NOTE: `fit_params` should be data dependent (e.g. `sample_weight`) which,
scikit-learn/sklearn/model_selection/tests/test_search.py,1946,is not the case for the following parameters. But this abuse is common in,
scikit-learn/sklearn/model_selection/tests/test_search.py,1947,popular third-party libraries and we should tolerate this behavior for,
scikit-learn/sklearn/model_selection/tests/test_search.py,1948,now and be careful not to break support for those without following,
scikit-learn/sklearn/model_selection/tests/test_search.py,1949,proper deprecation cycle.,
scikit-learn/sklearn/model_selection/tests/test_split.py,71,(the default value),
scikit-learn/sklearn/model_selection/tests/test_split.py,84,n_splits = np of unique folds = 2,
scikit-learn/sklearn/model_selection/tests/test_split.py,104,Test if get_n_splits works correctly,
scikit-learn/sklearn/model_selection/tests/test_split.py,107,Test if the cross-validator works as expected even if,
scikit-learn/sklearn/model_selection/tests/test_split.py,108,the data is 1d,
scikit-learn/sklearn/model_selection/tests/test_split.py,111,"Test that train, test indices returned are integers",
scikit-learn/sklearn/model_selection/tests/test_split.py,116,Test if the repr works without any errors,
scikit-learn/sklearn/model_selection/tests/test_split.py,119,ValueError for get_n_splits methods,
scikit-learn/sklearn/model_selection/tests/test_split.py,128,smoke test for 2d y and multi-label,
scikit-learn/sklearn/model_selection/tests/test_split.py,155,Use python sets to get more informative assertion failure messages,
scikit-learn/sklearn/model_selection/tests/test_split.py,158,Train and test split should not overlap,
scikit-learn/sklearn/model_selection/tests/test_split.py,162,Check that the union of train an test split cover all the indices,
scikit-learn/sklearn/model_selection/tests/test_split.py,168,Check that a all the samples appear at least once in a test fold,
scikit-learn/sklearn/model_selection/tests/test_split.py,178,Check that the accumulated test samples cover the whole dataset,
scikit-learn/sklearn/model_selection/tests/test_split.py,187,Check that errors are raised if there is not enough samples,
scikit-learn/sklearn/model_selection/tests/test_split.py,190,Check that a warning is raised if the least populated class has too few,
scikit-learn/sklearn/model_selection/tests/test_split.py,191,members.,
scikit-learn/sklearn/model_selection/tests/test_split.py,198,Check that despite the warning the folds are still computed even,
scikit-learn/sklearn/model_selection/tests/test_split.py,199,though all the classes are not necessarily represented at on each,
scikit-learn/sklearn/model_selection/tests/test_split.py,200,side of the split at each split,
scikit-learn/sklearn/model_selection/tests/test_split.py,205,Check that errors are raised if all n_groups for individual,
scikit-learn/sklearn/model_selection/tests/test_split.py,206,classes are less than n_splits.,
scikit-learn/sklearn/model_selection/tests/test_split.py,211,Error when number of folds is <= 1,
scikit-learn/sklearn/model_selection/tests/test_split.py,221,When n_splits is not integer:,
scikit-learn/sklearn/model_selection/tests/test_split.py,227,When shuffle is not  a bool:,
scikit-learn/sklearn/model_selection/tests/test_split.py,232,Check all indices are returned in the test folds,
scikit-learn/sklearn/model_selection/tests/test_split.py,237,Check all indices are returned in the test folds even when equal-sized,
scikit-learn/sklearn/model_selection/tests/test_split.py,238,folds are not possible,
scikit-learn/sklearn/model_selection/tests/test_split.py,243,Check if get_n_splits returns the number of folds,
scikit-learn/sklearn/model_selection/tests/test_split.py,248,Manually check that KFold preserves the data ordering on toy datasets,
scikit-learn/sklearn/model_selection/tests/test_split.py,271,Manually check that StratifiedKFold preserves the data ordering as much,
scikit-learn/sklearn/model_selection/tests/test_split.py,272,as possible on toy datasets in order to avoid hiding sample dependencies,
scikit-learn/sklearn/model_selection/tests/test_split.py,273,when possible,
scikit-learn/sklearn/model_selection/tests/test_split.py,294,Check if get_n_splits returns the number of folds,
scikit-learn/sklearn/model_selection/tests/test_split.py,297,Make sure string labels are also supported,
scikit-learn/sklearn/model_selection/tests/test_split.py,305,Check equivalence to KFold,
scikit-learn/sklearn/model_selection/tests/test_split.py,316,Check that stratified kfold preserves class ratios in individual splits,
scikit-learn/sklearn/model_selection/tests/test_split.py,317,Repeat with shuffling turned off and on,
scikit-learn/sklearn/model_selection/tests/test_split.py,338,Check that stratified kfold gives the same indices regardless of labels,
scikit-learn/sklearn/model_selection/tests/test_split.py,360,Check that KFold returns folds with balanced sizes,
scikit-learn/sklearn/model_selection/tests/test_split.py,370,Check that KFold returns folds with balanced sizes (only when,
scikit-learn/sklearn/model_selection/tests/test_split.py,371,stratification is possible),
scikit-learn/sklearn/model_selection/tests/test_split.py,372,Repeat with shuffling turned off and on,
scikit-learn/sklearn/model_selection/tests/test_split.py,387,Check the indices are shuffled properly,
scikit-learn/sklearn/model_selection/tests/test_split.py,398,Assert that there is no complete overlap,
scikit-learn/sklearn/model_selection/tests/test_split.py,401,Set all test indices in successive iterations of kf2 to 1,
scikit-learn/sklearn/model_selection/tests/test_split.py,404,Check that all indices are returned in the different test folds,
scikit-learn/sklearn/model_selection/tests/test_split.py,409,Divisible by 3,
scikit-learn/sklearn/model_selection/tests/test_split.py,411,Not divisible by 3,
scikit-learn/sklearn/model_selection/tests/test_split.py,414,"Check that when the shuffle is True, multiple split calls produce the",
scikit-learn/sklearn/model_selection/tests/test_split.py,415,same split when random_state is int,
scikit-learn/sklearn/model_selection/tests/test_split.py,423,"Check that when the shuffle is True, multiple split calls often",
scikit-learn/sklearn/model_selection/tests/test_split.py,424,(not always) produce different splits when random_state is,
scikit-learn/sklearn/model_selection/tests/test_split.py,425,RandomState instance or None,
scikit-learn/sklearn/model_selection/tests/test_split.py,432,Test if the two splits are different cv,
scikit-learn/sklearn/model_selection/tests/test_split.py,435,"cv.split(...) returns an array of tuples, each tuple",
scikit-learn/sklearn/model_selection/tests/test_split.py,436,consisting of an array with train indices and test indices,
scikit-learn/sklearn/model_selection/tests/test_split.py,437,Ensure that the splits for data are not same,
scikit-learn/sklearn/model_selection/tests/test_split.py,438,when random state is not set,
scikit-learn/sklearn/model_selection/tests/test_split.py,444,"Check that shuffling is happening when requested, and for proper",
scikit-learn/sklearn/model_selection/tests/test_split.py,445,sample coverage,
scikit-learn/sklearn/model_selection/tests/test_split.py,455,Ensure that we shuffle each class's samples with different,
scikit-learn/sklearn/model_selection/tests/test_split.py,456,random_state in StratifiedKFold,
scikit-learn/sklearn/model_selection/tests/test_split.py,457,See https://github.com/scikit-learn/scikit-learn/pull/13124,
scikit-learn/sklearn/model_selection/tests/test_split.py,467,see #2372,
scikit-learn/sklearn/model_selection/tests/test_split.py,468,The digits samples are dependent: they are apparently grouped by authors,
scikit-learn/sklearn/model_selection/tests/test_split.py,469,although we don't have any information on the groups segment locations,
scikit-learn/sklearn/model_selection/tests/test_split.py,470,for this data. We can highlight this fact by computing k-fold cross-,
scikit-learn/sklearn/model_selection/tests/test_split.py,471,validation with and without shuffling: we observe that the shuffling case,
scikit-learn/sklearn/model_selection/tests/test_split.py,472,wrongly makes the IID assumption and is therefore too optimistic: it,
scikit-learn/sklearn/model_selection/tests/test_split.py,473,estimates a much higher accuracy (around 0.93) than that the non,
scikit-learn/sklearn/model_selection/tests/test_split.py,474,shuffling variant (around 0.81).,
scikit-learn/sklearn/model_selection/tests/test_split.py,486,Shuffling the data artificially breaks the dependency and hides the,
scikit-learn/sklearn/model_selection/tests/test_split.py,487,overfitting of the model with regards to the writing style of the authors,
scikit-learn/sklearn/model_selection/tests/test_split.py,488,by yielding a seriously overestimated score:,
scikit-learn/sklearn/model_selection/tests/test_split.py,498,"Similarly, StratifiedKFold should try to shuffle the data as little",
scikit-learn/sklearn/model_selection/tests/test_split.py,499,as possible (while respecting the balanced class constraints),
scikit-learn/sklearn/model_selection/tests/test_split.py,500,and thus be able to detect the dependency by not overestimating,
scikit-learn/sklearn/model_selection/tests/test_split.py,501,the CV score either. As the digits dataset is approximately balanced,
scikit-learn/sklearn/model_selection/tests/test_split.py,502,the estimated mean score is close to the score measured with,
scikit-learn/sklearn/model_selection/tests/test_split.py,503,non-shuffled KFold,
scikit-learn/sklearn/model_selection/tests/test_split.py,533,"Check that the default value has the expected behavior, i.e. 0.1 if both",
scikit-learn/sklearn/model_selection/tests/test_split.py,534,unspecified or complement train_size unless both are specified.,
scikit-learn/sklearn/model_selection/tests/test_split.py,550,"Check that the default value has the expected behavior, i.e. 0.2 if both",
scikit-learn/sklearn/model_selection/tests/test_split.py,551,unspecified or complement train_size unless both are specified.,
scikit-learn/sklearn/model_selection/tests/test_split.py,567,Check that error is raised if there is a class with only one sample,
scikit-learn/sklearn/model_selection/tests/test_split.py,571,Check that error is raised if the test set size is smaller than n_classes,
scikit-learn/sklearn/model_selection/tests/test_split.py,573,Check that error is raised if the train set size is smaller than,
scikit-learn/sklearn/model_selection/tests/test_split.py,574,n_classes,
scikit-learn/sklearn/model_selection/tests/test_split.py,581,Train size or test size too small,
scikit-learn/sklearn/model_selection/tests/test_split.py,613,To make it indexable for y[train],
scikit-learn/sklearn/model_selection/tests/test_split.py,614,this is how test-size is computed internally,
scikit-learn/sklearn/model_selection/tests/test_split.py,615,in _validate_shuffle_split,
scikit-learn/sklearn/model_selection/tests/test_split.py,620,Checks if folds keep classes proportions,
scikit-learn/sklearn/model_selection/tests/test_split.py,635,"Test the StratifiedShuffleSplit, indices are drawn with a",
scikit-learn/sklearn/model_selection/tests/test_split.py,636,equal chance,
scikit-learn/sklearn/model_selection/tests/test_split.py,641,Here we test that the distribution of the counts,
scikit-learn/sklearn/model_selection/tests/test_split.py,642,per index is close enough to a binomial,
scikit-learn/sklearn/model_selection/tests/test_split.py,685,See https://github.com/scikit-learn/scikit-learn/issues/6121 for,
scikit-learn/sklearn/model_selection/tests/test_split.py,686,the original bug report,
scikit-learn/sklearn/model_selection/tests/test_split.py,695,no overlap,
scikit-learn/sklearn/model_selection/tests/test_split.py,698,complete partition,
scikit-learn/sklearn/model_selection/tests/test_split.py,703,fix for issue 9037,
scikit-learn/sklearn/model_selection/tests/test_split.py,712,no overlap,
scikit-learn/sklearn/model_selection/tests/test_split.py,715,complete partition,
scikit-learn/sklearn/model_selection/tests/test_split.py,718,correct stratification of entire rows,
scikit-learn/sklearn/model_selection/tests/test_split.py,719,"(by design, here y[:, 0] uniquely determines the entire row of y)",
scikit-learn/sklearn/model_selection/tests/test_split.py,726,"fix in PR #9922: for multilabel data with > 1000 labels, str(row)",
scikit-learn/sklearn/model_selection/tests/test_split.py,727,truncates with an ellipsis for elements in positions 4 through,
scikit-learn/sklearn/model_selection/tests/test_split.py,728,"len(row) - 4, so labels were not being correctly split using the powerset",
scikit-learn/sklearn/model_selection/tests/test_split.py,729,method for transforming a multilabel problem to a multiclass one; this,
scikit-learn/sklearn/model_selection/tests/test_split.py,730,test checks that this problem is fixed.,
scikit-learn/sklearn/model_selection/tests/test_split.py,741,correct stratification of entire rows,
scikit-learn/sklearn/model_selection/tests/test_split.py,742,"(by design, here y[:, 4] uniquely determines the entire row of y)",
scikit-learn/sklearn/model_selection/tests/test_split.py,749,Check that PredefinedSplit can reproduce a split generated by Kfold.,
scikit-learn/sklearn/model_selection/tests/test_split.py,758,n_splits is simply the no of unique folds,
scikit-learn/sklearn/model_selection/tests/test_split.py,772,Make sure the repr works,
scikit-learn/sklearn/model_selection/tests/test_split.py,775,Test that the length is correct,
scikit-learn/sklearn/model_selection/tests/test_split.py,782,First test: no train group is in the test set and vice versa,
scikit-learn/sklearn/model_selection/tests/test_split.py,788,Second test: train and test add up to all the data,
scikit-learn/sklearn/model_selection/tests/test_split.py,791,Third test: train and test are disjoint,
scikit-learn/sklearn/model_selection/tests/test_split.py,794,Fourth test:,
scikit-learn/sklearn/model_selection/tests/test_split.py,795,"unique train and test groups are correct, +- 1 for rounding error",
scikit-learn/sklearn/model_selection/tests/test_split.py,807,Make sure the repr works,
scikit-learn/sklearn/model_selection/tests/test_split.py,822,Test that the length is correct,
scikit-learn/sklearn/model_selection/tests/test_split.py,827,Split using the original list / array / list of string groups_i,
scikit-learn/sklearn/model_selection/tests/test_split.py,829,First test: no train group is in the test set and vice versa,
scikit-learn/sklearn/model_selection/tests/test_split.py,834,Second test: train and test add up to all the data,
scikit-learn/sklearn/model_selection/tests/test_split.py,837,Third test:,
scikit-learn/sklearn/model_selection/tests/test_split.py,838,The number of groups in test must be equal to p_groups_out,
scikit-learn/sklearn/model_selection/tests/test_split.py,841,check get_n_splits() with dummy parameters,
scikit-learn/sklearn/model_selection/tests/test_split.py,847,raise ValueError if a `groups` parameter is illegal,
scikit-learn/sklearn/model_selection/tests/test_split.py,861,Check that LeaveOneGroupOut and LeavePGroupsOut work normally if,
scikit-learn/sklearn/model_selection/tests/test_split.py,862,the groups variable is changed before calling split,
scikit-learn/sklearn/model_selection/tests/test_split.py,876,n_splits = no of 2 (p) group combinations of the unique groups = 3C2 = 3,
scikit-learn/sklearn/model_selection/tests/test_split.py,880,"n_splits = no of unique groups (C(uniq_lbls, 1) = n_unique_groups)",
scikit-learn/sklearn/model_selection/tests/test_split.py,912,n_repeats is not integer or <= 0,
scikit-learn/sklearn/model_selection/tests/test_split.py,937,split should produce same and deterministic splits on,
scikit-learn/sklearn/model_selection/tests/test_split.py,938,each call,
scikit-learn/sklearn/model_selection/tests/test_split.py,985,split should produce same and deterministic splits on,
scikit-learn/sklearn/model_selection/tests/test_split.py,986,each call,
scikit-learn/sklearn/model_selection/tests/test_split.py,1067,"Check that the default value has the expected behavior, i.e. complement",
scikit-learn/sklearn/model_selection/tests/test_split.py,1068,train_size unless both are specified.,
scikit-learn/sklearn/model_selection/tests/test_split.py,1080,simple test,
scikit-learn/sklearn/model_selection/tests/test_split.py,1084,test correspondence of X and y,
scikit-learn/sklearn/model_selection/tests/test_split.py,1088,don't convert lists to anything else by default,
scikit-learn/sklearn/model_selection/tests/test_split.py,1094,allow nd-arrays,
scikit-learn/sklearn/model_selection/tests/test_split.py,1103,test stratification option,
scikit-learn/sklearn/model_selection/tests/test_split.py,1112,check the 1:1 ratio of ones and twos in the data is preserved,
scikit-learn/sklearn/model_selection/tests/test_split.py,1115,test unshuffled split,
scikit-learn/sklearn/model_selection/tests/test_split.py,1125,check train_test_split doesn't destroy pandas dataframe,
scikit-learn/sklearn/model_selection/tests/test_split.py,1133,X dataframe,
scikit-learn/sklearn/model_selection/tests/test_split.py,1141,check that train_test_split converts scipy sparse matrices,
scikit-learn/sklearn/model_selection/tests/test_split.py,1142,"to csr, as stated in the documentation",
scikit-learn/sklearn/model_selection/tests/test_split.py,1153,X mock dataframe,
scikit-learn/sklearn/model_selection/tests/test_split.py,1162,"Check that when y is a list / list of string labels, it works.",
scikit-learn/sklearn/model_selection/tests/test_split.py,1196,Check that iterating twice on the ShuffleSplit gives the same,
scikit-learn/sklearn/model_selection/tests/test_split.py,1197,sequence of train-test when the random_state is given,
scikit-learn/sklearn/model_selection/tests/test_split.py,1204,"Check that when y is a list / list of string labels, it works.",
scikit-learn/sklearn/model_selection/tests/test_split.py,1218,Check that train_test_split allows input data with NaNs,
scikit-learn/sklearn/model_selection/tests/test_split.py,1228,Use numpy.testing.assert_equal which recursively compares,
scikit-learn/sklearn/model_selection/tests/test_split.py,1229,lists of lists,
scikit-learn/sklearn/model_selection/tests/test_split.py,1241,also works with 2d multiclass,
scikit-learn/sklearn/model_selection/tests/test_split.py,1267,"Since the wrapped iterable is enlisted and stored,",
scikit-learn/sklearn/model_selection/tests/test_split.py,1268,split can be called any number of times to produce,
scikit-learn/sklearn/model_selection/tests/test_split.py,1269,consistent results.,
scikit-learn/sklearn/model_selection/tests/test_split.py,1272,"If the splits are randomized, successive calls to split yields different",
scikit-learn/sklearn/model_selection/tests/test_split.py,1273,results,
scikit-learn/sklearn/model_selection/tests/test_split.py,1276,numpy's assert_array_equal properly compares nested lists,
scikit-learn/sklearn/model_selection/tests/test_split.py,1294,Parameters of the test,
scikit-learn/sklearn/model_selection/tests/test_split.py,1301,Construct the test data,
scikit-learn/sklearn/model_selection/tests/test_split.py,1302,5 percent error allowed,
scikit-learn/sklearn/model_selection/tests/test_split.py,1308,Get the test fold indices from the test set indices of each fold,
scikit-learn/sklearn/model_selection/tests/test_split.py,1314,Check that folds have approximately the same size,
scikit-learn/sklearn/model_selection/tests/test_split.py,1320,Check that each group appears only in 1 fold,
scikit-learn/sklearn/model_selection/tests/test_split.py,1324,Check that no group is on both sides of the split,
scikit-learn/sklearn/model_selection/tests/test_split.py,1329,Construct the test data,
scikit-learn/sklearn/model_selection/tests/test_split.py,1341,5 percent error allowed,
scikit-learn/sklearn/model_selection/tests/test_split.py,1346,Get the test fold indices from the test set indices of each fold,
scikit-learn/sklearn/model_selection/tests/test_split.py,1351,Check that folds have approximately the same size,
scikit-learn/sklearn/model_selection/tests/test_split.py,1357,Check that each group appears only in 1 fold,
scikit-learn/sklearn/model_selection/tests/test_split.py,1363,Check that no group is on both sides of the split,
scikit-learn/sklearn/model_selection/tests/test_split.py,1368,groups can also be a list,
scikit-learn/sklearn/model_selection/tests/test_split.py,1375,Should fail if there are more folds than groups,
scikit-learn/sklearn/model_selection/tests/test_split.py,1385,Should fail if there are more folds than samples,
scikit-learn/sklearn/model_selection/tests/test_split.py,1392,Manually check that Time Series CV preserves the data,
scikit-learn/sklearn/model_selection/tests/test_split.py,1393,ordering on toy datasets,
scikit-learn/sklearn/model_selection/tests/test_split.py,1413,Check get_n_splits returns the correct number of splits,
scikit-learn/sklearn/model_selection/tests/test_split.py,1434,Test for the case where the size of a fold is greater than max_train_size,
scikit-learn/sklearn/model_selection/tests/test_split.py,1438,Test for the case where the size of each fold is less than max_train_size,
scikit-learn/sklearn/model_selection/tests/test_split.py,1444,Test if nested cross validation works with different combinations of cv,
scikit-learn/sklearn/model_selection/tests/test_split.py,1478,1 sample,
scikit-learn/sklearn/model_selection/tests/test_split.py,1487,1 sample,
scikit-learn/sklearn/model_selection/tests/test_split.py,1494,"3 samples, ask for more than 2 thirds",
scikit-learn/sklearn/model_selection/tests/test_split.py,1503,LeaveOneGroup out expect at least 2 groups so no need to check,
scikit-learn/sklearn/model_selection/tests/test_split.py,1505,1 sample,
scikit-learn/sklearn/model_selection/tests/test_split.py,1513,No need to check LeavePGroupsOut,
scikit-learn/sklearn/model_selection/tests/test_split.py,1515,2 samples,
scikit-learn/sklearn/model_selection/tests/test_split.py,1524,passing a non-default random_state when shuffle=False makes no sense,
scikit-learn/sklearn/model_selection/tests/test_split.py,1525,TODO 0.24: raise a ValueError instead of a warning,
scikit-learn/sklearn/experimental/enable_iterative_imputer.py,18,use settattr to avoid mypy errors when monkeypatching,
scikit-learn/sklearn/experimental/enable_hist_gradient_boosting.py,29,use settattr to avoid mypy errors when monkeypatching,
scikit-learn/sklearn/experimental/tests/test_enable_iterative_imputer.py,9,Make sure different import strategies work or fail as expected.,
scikit-learn/sklearn/experimental/tests/test_enable_iterative_imputer.py,11,"Since Python caches the imported modules, we need to run a child process",
scikit-learn/sklearn/experimental/tests/test_enable_iterative_imputer.py,12,"for every test case. Else, the tests would not be independent",
scikit-learn/sklearn/experimental/tests/test_enable_iterative_imputer.py,13,(manually removing the imports from the cache (sys.modules) is not,
scikit-learn/sklearn/experimental/tests/test_enable_iterative_imputer.py,14,recommended and can lead to many complications).,
scikit-learn/sklearn/experimental/tests/test_enable_hist_gradient_boosting.py,9,Make sure different import strategies work or fail as expected.,
scikit-learn/sklearn/experimental/tests/test_enable_hist_gradient_boosting.py,11,"Since Python caches the imported modules, we need to run a child process",
scikit-learn/sklearn/experimental/tests/test_enable_hist_gradient_boosting.py,12,"for every test case. Else, the tests would not be independent",
scikit-learn/sklearn/experimental/tests/test_enable_hist_gradient_boosting.py,13,(manually removing the imports from the cache (sys.modules) is not,
scikit-learn/sklearn/experimental/tests/test_enable_hist_gradient_boosting.py,14,recommended and can lead to many complications).,
scikit-learn/sklearn/svm/_bounds.py,2,Author: Paolo Losi,
scikit-learn/sklearn/svm/_bounds.py,3,License: BSD 3 clause,
scikit-learn/sklearn/svm/_bounds.py,59,maximum absolute value over classes and features,
scikit-learn/sklearn/svm/_bounds.py,71,loss == 'log':,
scikit-learn/sklearn/svm/_classes.py,411,SVR only accepts l2 penalty,
scikit-learn/sklearn/svm/_classes.py,1005,mypy error: Decorated property not supported,
scikit-learn/sklearn/svm/_classes.py,1006,type: ignore,
scikit-learn/sklearn/svm/_classes.py,1013,mypy error: Decorated property not supported,
scikit-learn/sklearn/svm/_classes.py,1014,type: ignore,
scikit-learn/sklearn/svm/_classes.py,1346,mypy error: Decorated property not supported,
scikit-learn/sklearn/svm/_classes.py,1347,type: ignore,
scikit-learn/sklearn/svm/_classes.py,1354,mypy error: Decorated property not supported,
scikit-learn/sklearn/svm/_classes.py,1355,type: ignore,
scikit-learn/sklearn/svm/__init__.py,5,See http://scikit-learn.sourceforge.net/modules/svm.html for complete,
scikit-learn/sklearn/svm/__init__.py,6,documentation.,
scikit-learn/sklearn/svm/__init__.py,8,Author: Fabian Pedregosa <fabian.pedregosa@inria.fr> with help from,
scikit-learn/sklearn/svm/__init__.py,9,the scikit-learn community. LibSVM and LibLinear are copyright,
scikit-learn/sklearn/svm/__init__.py,10,of their respective owners.,
scikit-learn/sklearn/svm/__init__.py,11,License: BSD 3 clause (C) INRIA 2010,
scikit-learn/sklearn/svm/setup.py,13,Section LibSVM,
scikit-learn/sklearn/svm/setup.py,15,we compile both libsvm and libsvm_sparse,
scikit-learn/sklearn/svm/setup.py,21,Force C++ linking in case gcc is picked up instead,
scikit-learn/sklearn/svm/setup.py,22,of g++ under windows with some versions of MinGW,
scikit-learn/sklearn/svm/setup.py,24,Use C++11 to use the random number generator fix,
scikit-learn/sklearn/svm/setup.py,44,liblinear module,
scikit-learn/sklearn/svm/setup.py,49,precompile liblinear to use C++11 flag,
scikit-learn/sklearn/svm/setup.py,56,Force C++ linking in case gcc is picked up instead,
scikit-learn/sklearn/svm/setup.py,57,of g++ under windows with some versions of MinGW,
scikit-learn/sklearn/svm/setup.py,59,Use C++11 to use the random number generator fix,
scikit-learn/sklearn/svm/setup.py,76,"extra_compile_args=['-O0 -fno-inline'],",
scikit-learn/sklearn/svm/setup.py,79,end liblinear module,
scikit-learn/sklearn/svm/setup.py,81,this should go *after* libsvm-skl,
scikit-learn/sklearn/svm/_base.py,6,mypy error: error: Module 'sklearn.svm' has no attribute '_libsvm',
scikit-learn/sklearn/svm/_base.py,7,(and same for other imports),
scikit-learn/sklearn/svm/_base.py,8,type: ignore,
scikit-learn/sklearn/svm/_base.py,9,type: ignore,
scikit-learn/sklearn/svm/_base.py,10,type: ignore,
scikit-learn/sklearn/svm/_base.py,34,get 1vs1 weights for all n*(n-1) classifiers.,
scikit-learn/sklearn/svm/_base.py,35,this is somewhat messy.,
scikit-learn/sklearn/svm/_base.py,36,shape of dual_coef_ is nSV * (n_classes -1),
scikit-learn/sklearn/svm/_base.py,37,see docs for details,
scikit-learn/sklearn/svm/_base.py,40,XXX we could do preallocation of coef but,
scikit-learn/sklearn/svm/_base.py,41,would have to take care in the sparse case,
scikit-learn/sklearn/svm/_base.py,45,SVs for class1:,
scikit-learn/sklearn/svm/_base.py,48,SVs for class1:,
scikit-learn/sklearn/svm/_base.py,51,dual coef for class1 SVs:,
scikit-learn/sklearn/svm/_base.py,53,dual coef for class2 SVs:,
scikit-learn/sklearn/svm/_base.py,55,build weight for class1 vs class2,
scikit-learn/sklearn/svm/_base.py,70,The order of these must match the integer values in LibSVM.,
scikit-learn/sklearn/svm/_base.py,71,XXX These are actually the same in the dense case. Need to factor,
scikit-learn/sklearn/svm/_base.py,72,this out.,
scikit-learn/sklearn/svm/_base.py,107,Used by cross_val_score.,
scikit-learn/sklearn/svm/_base.py,171,input validation,
scikit-learn/sklearn/svm/_base.py,193,unused but needs to be a float for cython code that ignores,
scikit-learn/sklearn/svm/_base.py,194,it anyway,
scikit-learn/sklearn/svm/_base.py,198,var = E[X^2] - E[X]^2 if sparse,
scikit-learn/sklearn/svm/_base.py,218,see comment on the other call to np.iinfo in this file,
scikit-learn/sklearn/svm/_base.py,222,"In binary case, we need to flip the sign of coef, intercept and",
scikit-learn/sklearn/svm/_base.py,223,decision function. Use self._intercept_ and self._dual_coef_,
scikit-learn/sklearn/svm/_base.py,224,internally.,
scikit-learn/sklearn/svm/_base.py,238,XXX this is ugly.,
scikit-learn/sklearn/svm/_base.py,239,Regression models should not have a class_weight_ attribute.,
scikit-learn/sklearn/svm/_base.py,254,you must store a reference to X to compute the kernel in predict,
scikit-learn/sklearn/svm/_base.py,255,TODO: add keyword copy to copy on demand,
scikit-learn/sklearn/svm/_base.py,264,we don't pass **self.get_params() to allow subclasses to,
scikit-learn/sklearn/svm/_base.py,265,add other parameters to __init__,
scikit-learn/sklearn/svm/_base.py,304,regression,
scikit-learn/sklearn/svm/_base.py,360,Precondition: X is a csr_matrix of dtype np.float64.,
scikit-learn/sklearn/svm/_base.py,367,C is not useful here,
scikit-learn/sklearn/svm/_base.py,385,"in the case of precomputed kernel given as a function, we",
scikit-learn/sklearn/svm/_base.py,386,have to compute explicitly the kernel matrix,
scikit-learn/sklearn/svm/_base.py,406,NOTE: _validate_for_predict contains check for is_fitted,
scikit-learn/sklearn/svm/_base.py,407,hence must be placed before any other attributes are used.,
scikit-learn/sklearn/svm/_base.py,416,"In binary case, we need to flip the sign of coef, intercept and",
scikit-learn/sklearn/svm/_base.py,417,decision function.,
scikit-learn/sklearn/svm/_base.py,497,"coef_ being a read-only property, it's better to mark the value as",
scikit-learn/sklearn/svm/_base.py,498,immutable to avoid hiding potential bugs for the unsuspecting user.,
scikit-learn/sklearn/svm/_base.py,500,sparse matrix do not have global flags,
scikit-learn/sklearn/svm/_base.py,503,regular dense array,
scikit-learn/sklearn/svm/_base.py,521,SVR and OneClass,
scikit-learn/sklearn/svm/_base.py,522,"_n_support has size 2, we make it size 1",
scikit-learn/sklearn/svm/_base.py,617,Hacky way of getting predict_proba to raise an AttributeError when,
scikit-learn/sklearn/svm/_base.py,618,probability=False using properties. Do not use this in new code; when,
scikit-learn/sklearn/svm/_base.py,619,"probabilities are not available depending on a setting, introduce two",
scikit-learn/sklearn/svm/_base.py,620,estimators.,
scikit-learn/sklearn/svm/_base.py,743,binary classifier,
scikit-learn/sklearn/svm/_base.py,746,1vs1 classifier,
scikit-learn/sklearn/svm/_base.py,777,"nested dicts containing level 1: available loss functions,",
scikit-learn/sklearn/svm/_base.py,778,"level2: available penalties for the given loss function,",
scikit-learn/sklearn/svm/_base.py,779,level3: whether the dual solver is available for the specified,
scikit-learn/sklearn/svm/_base.py,780,combination of loss function and penalty,
scikit-learn/sklearn/svm/_base.py,938,LinearSVC breaks when intercept_scaling is <= 0,
scikit-learn/sklearn/svm/_base.py,952,Liblinear doesn't support 64bit sparse matrix indices yet,
scikit-learn/sklearn/svm/_base.py,956,"LibLinear wants targets as doubles, even for classification",
scikit-learn/sklearn/svm/_base.py,968,Regarding rnd.randint(..) in the above signature:,
scikit-learn/sklearn/svm/_base.py,969,seed for srand in range [0..INT_MAX); due to limitations in Numpy,
scikit-learn/sklearn/svm/_base.py,970,"on 32-bit platforms, we can't get to the UINT_MAX limit that",
scikit-learn/sklearn/svm/_base.py,971,srand supports,
scikit-learn/sklearn/svm/tests/test_svm.py,31,mypy error: Module 'sklearn.svm' has no attribute '_libsvm',
scikit-learn/sklearn/svm/tests/test_svm.py,32,type: ignore,
scikit-learn/sklearn/svm/tests/test_svm.py,34,toy sample,
scikit-learn/sklearn/svm/tests/test_svm.py,40,also load the iris dataset,
scikit-learn/sklearn/svm/tests/test_svm.py,49,Test parameters on classes that make use of libsvm.,
scikit-learn/sklearn/svm/tests/test_svm.py,59,Check consistency on dataset iris.,
scikit-learn/sklearn/svm/tests/test_svm.py,61,shuffle the dataset so that labels are not ordered,
scikit-learn/sklearn/svm/tests/test_svm.py,69,check also the low-level API,
scikit-learn/sklearn/svm/tests/test_svm.py,85,"If random_seed >= 0, the libsvm rng is seeded (by calling `srand`), hence",
scikit-learn/sklearn/svm/tests/test_svm.py,86,we should get deterministic results (assuming that there is no other,
scikit-learn/sklearn/svm/tests/test_svm.py,87,thread calling this wrapper calling `srand` concurrently).,
scikit-learn/sklearn/svm/tests/test_svm.py,96,SVC with a precomputed kernel.,
scikit-learn/sklearn/svm/tests/test_svm.py,97,We test it with a toy dataset and with iris.,
scikit-learn/sklearn/svm/tests/test_svm.py,99,Gram matrix for train data (square matrix),
scikit-learn/sklearn/svm/tests/test_svm.py,100,(we use just a linear kernel),
scikit-learn/sklearn/svm/tests/test_svm.py,103,Gram matrix for test data (rectangular matrix),
scikit-learn/sklearn/svm/tests/test_svm.py,115,"Gram matrix for test data but compute KT[i,j]",
scikit-learn/sklearn/svm/tests/test_svm.py,116,for support vectors j only.,
scikit-learn/sklearn/svm/tests/test_svm.py,125,"same as before, but using a callable function instead of the kernel",
scikit-learn/sklearn/svm/tests/test_svm.py,126,matrix. kernel is just a linear kernel,
scikit-learn/sklearn/svm/tests/test_svm.py,138,test a precomputed kernel with the iris dataset,
scikit-learn/sklearn/svm/tests/test_svm.py,139,and check parameters against a linear SVC,
scikit-learn/sklearn/svm/tests/test_svm.py,151,"Gram matrix for test data but compute KT[i,j]",
scikit-learn/sklearn/svm/tests/test_svm.py,152,for support vectors j only.,
scikit-learn/sklearn/svm/tests/test_svm.py,167,Test Support Vector Regression,
scikit-learn/sklearn/svm/tests/test_svm.py,178,"non-regression test; previously, BaseLibSVM would check that",
scikit-learn/sklearn/svm/tests/test_svm.py,179,"len(np.unique(y)) < 2, which must only be done for SVC",
scikit-learn/sklearn/svm/tests/test_svm.py,185,check that SVR(kernel='linear') and LinearSVC() give,
scikit-learn/sklearn/svm/tests/test_svm.py,186,comparable results,
scikit-learn/sklearn/svm/tests/test_svm.py,200,check correct result when sample_weight is 1,
scikit-learn/sklearn/svm/tests/test_svm.py,201,check that SVR(kernel='linear') and LinearSVC() give,
scikit-learn/sklearn/svm/tests/test_svm.py,202,comparable results,
scikit-learn/sklearn/svm/tests/test_svm.py,218,"check that fit(X)  = fit([X1, X2, X3],sample_weight = [n1, n2, n3]) where",
scikit-learn/sklearn/svm/tests/test_svm.py,219,"X = X1 repeated n1 times, X2 repeated n2 times and so forth",
scikit-learn/sklearn/svm/tests/test_svm.py,240,Bad kernel,
scikit-learn/sklearn/svm/tests/test_svm.py,248,Test OneClassSVM,
scikit-learn/sklearn/svm/tests/test_svm.py,264,Test OneClassSVM decision function,
scikit-learn/sklearn/svm/tests/test_svm.py,268,Generate train data,
scikit-learn/sklearn/svm/tests/test_svm.py,272,Generate some regular novel observations,
scikit-learn/sklearn/svm/tests/test_svm.py,275,Generate some abnormal novel observations,
scikit-learn/sklearn/svm/tests/test_svm.py,278,fit the model,
scikit-learn/sklearn/svm/tests/test_svm.py,282,predict things,
scikit-learn/sklearn/svm/tests/test_svm.py,301,Make sure some tweaking of parameters works.,
scikit-learn/sklearn/svm/tests/test_svm.py,302,We change clf.dual_coef_ at run time and expect .predict() to change,
scikit-learn/sklearn/svm/tests/test_svm.py,303,accordingly. Notice that this is not trivial since it involves a lot,
scikit-learn/sklearn/svm/tests/test_svm.py,304,of C/Python copying in the libsvm bindings.,
scikit-learn/sklearn/svm/tests/test_svm.py,305,The success of this test ensures that the mapping between libsvm and,
scikit-learn/sklearn/svm/tests/test_svm.py,306,the python classifier is complete.,
scikit-learn/sklearn/svm/tests/test_svm.py,316,Predict probabilities using SVC,
scikit-learn/sklearn/svm/tests/test_svm.py,317,"This uses cross validation, so we use a slightly bigger testing set.",
scikit-learn/sklearn/svm/tests/test_svm.py,334,Test decision_function,
scikit-learn/sklearn/svm/tests/test_svm.py,335,"Sanity check, test that decision_function implemented in python",
scikit-learn/sklearn/svm/tests/test_svm.py,336,returns the same as the one in libsvm,
scikit-learn/sklearn/svm/tests/test_svm.py,337,multi class:,
scikit-learn/sklearn/svm/tests/test_svm.py,345,binary:,
scikit-learn/sklearn/svm/tests/test_svm.py,356,kernel binary:,
scikit-learn/sklearn/svm/tests/test_svm.py,367,check that decision_function_shape='ovr' or 'ovo' gives,
scikit-learn/sklearn/svm/tests/test_svm.py,368,correct shape and is consistent with predict,
scikit-learn/sklearn/svm/tests/test_svm.py,376,with five classes:,
scikit-learn/sklearn/svm/tests/test_svm.py,386,check shape of ovo_decition_function=True,
scikit-learn/sklearn/svm/tests/test_svm.py,397,Test SVR's decision_function,
scikit-learn/sklearn/svm/tests/test_svm.py,398,"Sanity check, test that predict implemented in python",
scikit-learn/sklearn/svm/tests/test_svm.py,399,returns the same as the one in libsvm,
scikit-learn/sklearn/svm/tests/test_svm.py,404,linear kernel,
scikit-learn/sklearn/svm/tests/test_svm.py,410,rbf kernel,
scikit-learn/sklearn/svm/tests/test_svm.py,419,Test class weights,
scikit-learn/sklearn/svm/tests/test_svm.py,421,we give a small weights to class 1,
scikit-learn/sklearn/svm/tests/test_svm.py,423,so all predicted values belong to class 2,
scikit-learn/sklearn/svm/tests/test_svm.py,439,fit a linear SVM and check that giving more weight to opposed samples,
scikit-learn/sklearn/svm/tests/test_svm.py,440,in the space will flip the decision toward these samples.,
scikit-learn/sklearn/svm/tests/test_svm.py,444,"check that with unit weights, a sample is supposed to be predicted on",
scikit-learn/sklearn/svm/tests/test_svm.py,445,the boundary,
scikit-learn/sklearn/svm/tests/test_svm.py,451,give more weights to opposed samples,
scikit-learn/sklearn/svm/tests/test_svm.py,468,similar test to test_svm_classifier_sided_sample_weight but for,
scikit-learn/sklearn/svm/tests/test_svm.py,469,SVM regressors,
scikit-learn/sklearn/svm/tests/test_svm.py,473,"check that with unit weights, a sample is supposed to be predicted on",
scikit-learn/sklearn/svm/tests/test_svm.py,474,the boundary,
scikit-learn/sklearn/svm/tests/test_svm.py,480,give more weights to opposed samples,
scikit-learn/sklearn/svm/tests/test_svm.py,493,test that rescaling all samples is the same as changing C,
scikit-learn/sklearn/svm/tests/test_svm.py,580,model generates equal coefficients,
scikit-learn/sklearn/svm/tests/test_svm.py,589,Test class weights for imbalanced data,
scikit-learn/sklearn/svm/tests/test_svm.py,591,We take as dataset the two-dimensional projection of iris so,
scikit-learn/sklearn/svm/tests/test_svm.py,592,that it is not separable and remove half of predictors from,
scikit-learn/sklearn/svm/tests/test_svm.py,593,class 1.,
scikit-learn/sklearn/svm/tests/test_svm.py,594,We add one to the targets as a non-regression test:,
scikit-learn/sklearn/svm/tests/test_svm.py,595,"class_weight=""balanced""",
scikit-learn/sklearn/svm/tests/test_svm.py,596,used to work only when the labels where a range [0..K).,
scikit-learn/sklearn/svm/tests/test_svm.py,607,check that score is better when class='balanced' is set.,
scikit-learn/sklearn/svm/tests/test_svm.py,617,Test that it gives proper exception on deficient input,
scikit-learn/sklearn/svm/tests/test_svm.py,618,impossible value of C,
scikit-learn/sklearn/svm/tests/test_svm.py,622,impossible value of nu,
scikit-learn/sklearn/svm/tests/test_svm.py,627,wrong dimensions for labels,
scikit-learn/sklearn/svm/tests/test_svm.py,631,Test with arrays that are non-contiguous.,
scikit-learn/sklearn/svm/tests/test_svm.py,642,error for precomputed kernelsx,
scikit-learn/sklearn/svm/tests/test_svm.py,647,predict with sparse input when trained with dense,
scikit-learn/sklearn/svm/tests/test_svm.py,680,Test that a unicode kernel name does not cause a TypeError,
scikit-learn/sklearn/svm/tests/test_svm.py,698,Regression test for #14893,
scikit-learn/sklearn/svm/tests/test_svm.py,711,Test possible parameter combinations in LinearSVC,
scikit-learn/sklearn/svm/tests/test_svm.py,712,Generate list of possible parameter combinations,
scikit-learn/sklearn/svm/tests/test_svm.py,732,Incorrect loss value - test if explicit error message is raised,
scikit-learn/sklearn/svm/tests/test_svm.py,738,Check if Upper case notation raises error at _fit_liblinear,
scikit-learn/sklearn/svm/tests/test_svm.py,739,which is called by fit,
scikit-learn/sklearn/svm/tests/test_svm.py,753,Test basic routines using LinearSVC,
scikit-learn/sklearn/svm/tests/test_svm.py,756,by default should have intercept,
scikit-learn/sklearn/svm/tests/test_svm.py,762,the same with l1 penalty,
scikit-learn/sklearn/svm/tests/test_svm.py,767,l2 penalty with dual formulation,
scikit-learn/sklearn/svm/tests/test_svm.py,771,"l2 penalty, l1 loss",
scikit-learn/sklearn/svm/tests/test_svm.py,776,test also decision function,
scikit-learn/sklearn/svm/tests/test_svm.py,783,Test LinearSVC with crammer_singer multi-class svm,
scikit-learn/sklearn/svm/tests/test_svm.py,788,similar prediction for ovr and crammer-singer:,
scikit-learn/sklearn/svm/tests/test_svm.py,792,classifiers shouldn't be the same,
scikit-learn/sklearn/svm/tests/test_svm.py,795,test decision function,
scikit-learn/sklearn/svm/tests/test_svm.py,803,check correct result when sample_weight is 1,
scikit-learn/sklearn/svm/tests/test_svm.py,810,check if same as sample_weight=None,
scikit-learn/sklearn/svm/tests/test_svm.py,814,"check that fit(X)  = fit([X1, X2, X3],sample_weight = [n1, n2, n3]) where",
scikit-learn/sklearn/svm/tests/test_svm.py,815,"X = X1 repeated n1 times, X2 repeated n2 times and so forth",
scikit-learn/sklearn/svm/tests/test_svm.py,834,Test Crammer-Singer formulation in the binary case,
scikit-learn/sklearn/svm/tests/test_svm.py,845,Test that LinearSVC gives plausible predictions on the iris dataset,
scikit-learn/sklearn/svm/tests/test_svm.py,846,"Also, test symbolic class names (classes_).",
scikit-learn/sklearn/svm/tests/test_svm.py,858,Test that dense liblinear honours intercept_scaling param,
scikit-learn/sklearn/svm/tests/test_svm.py,869,"when intercept_scaling is low the intercept value is highly ""penalized""",
scikit-learn/sklearn/svm/tests/test_svm.py,870,by regularization,
scikit-learn/sklearn/svm/tests/test_svm.py,875,"when intercept_scaling is sufficiently high, the intercept value",
scikit-learn/sklearn/svm/tests/test_svm.py,876,is not affected by regularization,
scikit-learn/sklearn/svm/tests/test_svm.py,882,"when intercept_scaling is sufficiently high, the intercept value",
scikit-learn/sklearn/svm/tests/test_svm.py,883,doesn't depend on intercept_scaling value,
scikit-learn/sklearn/svm/tests/test_svm.py,891,multi-class case,
scikit-learn/sklearn/svm/tests/test_svm.py,899,binary-class case,
scikit-learn/sklearn/svm/tests/test_svm.py,915,Check that primal coef modification are not silently ignored,
scikit-learn/sklearn/svm/tests/test_svm.py,931,stdout: redirect,
scikit-learn/sklearn/svm/tests/test_svm.py,933,save original stdout,
scikit-learn/sklearn/svm/tests/test_svm.py,934,replace it,
scikit-learn/sklearn/svm/tests/test_svm.py,936,actual call,
scikit-learn/sklearn/svm/tests/test_svm.py,940,stdout: restore,
scikit-learn/sklearn/svm/tests/test_svm.py,941,restore original stdout,
scikit-learn/sklearn/svm/tests/test_svm.py,945,"create SVM with callable linear kernel, check that results are the same",
scikit-learn/sklearn/svm/tests/test_svm.py,946,as with built-in linear kernel,
scikit-learn/sklearn/svm/tests/test_svm.py,950,clone for checking clonability with lambda functions..,
scikit-learn/sklearn/svm/tests/test_svm.py,985,input validation not required when SVM not fitted,
scikit-learn/sklearn/svm/tests/test_svm.py,996,ignore convergence warnings from max_iter=1,
scikit-learn/sklearn/svm/tests/test_svm.py,1007,Test that warnings are raised if model does not converge,
scikit-learn/sklearn/svm/tests/test_svm.py,1019,"Test that SVR(kernel=""linear"") has coef_ with the right sign.",
scikit-learn/sklearn/svm/tests/test_svm.py,1020,Non-regression test for #2933.,
scikit-learn/sklearn/svm/tests/test_svm.py,1033,Test that the right error message is thrown when intercept_scaling <= 0,
scikit-learn/sklearn/svm/tests/test_svm.py,1044,Test that intercept_scaling is ignored when fit_intercept is False,
scikit-learn/sklearn/svm/tests/test_svm.py,1052,"Method must be (un)available before or after fit, switched by",
scikit-learn/sklearn/svm/tests/test_svm.py,1053,`probability` param,
scikit-learn/sklearn/svm/tests/test_svm.py,1065,Switching to `probability=True` after fitting should make,
scikit-learn/sklearn/svm/tests/test_svm.py,1066,"predict_proba available, but calling it must not work:",
scikit-learn/sklearn/svm/tests/test_svm.py,1083,One point from each quadrant represents one class,
scikit-learn/sklearn/svm/tests/test_svm.py,1087,First point is closer to the decision boundaries than the second point,
scikit-learn/sklearn/svm/tests/test_svm.py,1090,For all the quadrants (classes),
scikit-learn/sklearn/svm/tests/test_svm.py,1092,Q1,
scikit-learn/sklearn/svm/tests/test_svm.py,1093,Q2,
scikit-learn/sklearn/svm/tests/test_svm.py,1094,Q3,
scikit-learn/sklearn/svm/tests/test_svm.py,1095,Q4,
scikit-learn/sklearn/svm/tests/test_svm.py,1105,Test if the prediction is the same as y,
scikit-learn/sklearn/svm/tests/test_svm.py,1110,Assert that the predicted class has the maximum value,
scikit-learn/sklearn/svm/tests/test_svm.py,1113,Get decision value at test points for the predicted class,
scikit-learn/sklearn/svm/tests/test_svm.py,1116,Assert pred_class_deci_val > 0 here,
scikit-learn/sklearn/svm/tests/test_svm.py,1119,Test if the first point has lower decision value on every quadrant,
scikit-learn/sklearn/svm/tests/test_svm.py,1120,compared to the second point,
scikit-learn/sklearn/svm/tests/test_svm.py,1173,"X_var ~= 1 shouldn't raise warning, for when",
scikit-learn/sklearn/svm/tests/test_svm.py,1174,gamma is not explicitly set.,
scikit-learn/sklearn/svm/tests/test_svm.py,1219,Make n_support is correct for oneclass and SVR (used to be,
scikit-learn/sklearn/svm/tests/test_svm.py,1220,non-initialized),
scikit-learn/sklearn/svm/tests/test_svm.py,1221,this is a non regression test for issue #14774,
scikit-learn/sklearn/svm/tests/test_svm.py,1237,TODO: Remove in 0.25 when probA_ and probB_ are deprecated,
scikit-learn/sklearn/svm/tests/test_svm.py,1256,count encoding,
scikit-learn/sklearn/svm/tests/test_svm.py,1280,classifier,
scikit-learn/sklearn/svm/tests/test_svm.py,1287,regressor,
scikit-learn/sklearn/svm/tests/test_bounds.py,38,loss='l2' should raise ValueError,
scikit-learn/sklearn/svm/tests/test_sparse.py,17,test sample 1,
scikit-learn/sklearn/svm/tests/test_sparse.py,24,test sample 2,
scikit-learn/sklearn/svm/tests/test_sparse.py,34,permute,
scikit-learn/sklearn/svm/tests/test_sparse.py,39,sparsify,
scikit-learn/sklearn/svm/tests/test_sparse.py,79,many class dataset:,
scikit-learn/sklearn/svm/tests/test_sparse.py,97,test that the result with sorted and unsorted indices in csr is the same,
scikit-learn/sklearn/svm/tests/test_sparse.py,98,"we use a subset of digits as iris, blobs or make_classification didn't",
scikit-learn/sklearn/svm/tests/test_sparse.py,99,show the problem,
scikit-learn/sklearn/svm/tests/test_sparse.py,110,make sure dense and sparse SVM give the same result,
scikit-learn/sklearn/svm/tests/test_sparse.py,113,reverse each row's indices,
scikit-learn/sklearn/svm/tests/test_sparse.py,133,make sure unsorted indices give same result,
scikit-learn/sklearn/svm/tests/test_sparse.py,148,Test the sparse SVC with the iris dataset,
scikit-learn/sklearn/svm/tests/test_sparse.py,164,Test decision_function,
scikit-learn/sklearn/svm/tests/test_sparse.py,166,"Sanity check, test that decision_function implemented in python",
scikit-learn/sklearn/svm/tests/test_sparse.py,167,returns the same as the one in libsvm,
scikit-learn/sklearn/svm/tests/test_sparse.py,169,multi class:,
scikit-learn/sklearn/svm/tests/test_sparse.py,177,binary:,
scikit-learn/sklearn/svm/tests/test_sparse.py,190,Test that it gives proper exception on deficient input,
scikit-learn/sklearn/svm/tests/test_sparse.py,191,impossible value of C,
scikit-learn/sklearn/svm/tests/test_sparse.py,195,impossible value of nu,
scikit-learn/sklearn/svm/tests/test_sparse.py,200,wrong dimensions for labels,
scikit-learn/sklearn/svm/tests/test_sparse.py,210,Similar to test_SVC,
scikit-learn/sklearn/svm/tests/test_sparse.py,229,Test the sparse LinearSVC with the iris dataset,
scikit-learn/sklearn/svm/tests/test_sparse.py,241,check decision_function,
scikit-learn/sklearn/svm/tests/test_sparse.py,245,sparsify the coefficients on both models and check that they still,
scikit-learn/sklearn/svm/tests/test_sparse.py,246,produce the same results,
scikit-learn/sklearn/svm/tests/test_sparse.py,254,Test class weights,
scikit-learn/sklearn/svm/tests/test_sparse.py,269,Test weights on individual samples,
scikit-learn/sklearn/svm/tests/test_sparse.py,280,Test that sparse liblinear honours intercept_scaling param,
scikit-learn/sklearn/svm/tests/test_sparse.py,288,Check that sparse OneClassSVM gives the same result as dense OneClassSVM,
scikit-learn/sklearn/svm/tests/test_sparse.py,289,many class dataset:,
scikit-learn/sklearn/svm/tests/test_sparse.py,302,Test on a subset from the 20newsgroups dataset.,
scikit-learn/sklearn/svm/tests/test_sparse.py,303,This catches some bugs if input is not correctly converted into,
scikit-learn/sklearn/svm/tests/test_sparse.py,304,sparse format or weights are not correctly initialized.,
scikit-learn/sklearn/svm/tests/test_sparse.py,331,"Test that the ""dense_fit"" is called even though we use sparse input",
scikit-learn/sklearn/svm/tests/test_sparse.py,332,meaning that everything works fine.,
scikit-learn/sklearn/svm/tests/test_sparse.py,345,b.decision_function(X_sp)  # XXX : should be supported,
scikit-learn/sklearn/mixture/_bayesian_mixture.py,2,Author: Wei Xue <xuewei4d@gmail.com>,
scikit-learn/sklearn/mixture/_bayesian_mixture.py,3,Thierry Guillemot <thierry.guillemot.work@gmail.com>,
scikit-learn/sklearn/mixture/_bayesian_mixture.py,4,License: BSD 3 clause,
scikit-learn/sklearn/mixture/_bayesian_mixture.py,57,To simplify the computation we have removed the np.log(np.pi) term,
scikit-learn/sklearn/mixture/_bayesian_mixture.py,448,spherical case,
scikit-learn/sklearn/mixture/_bayesian_mixture.py,480,For dirichlet process weight_concentration will be a tuple,
scikit-learn/sklearn/mixture/_bayesian_mixture.py,481,containing the two parameters of the beta distribution,
scikit-learn/sklearn/mixture/_bayesian_mixture.py,487,case Variationnal Gaussian mixture with dirichlet distribution,
scikit-learn/sklearn/mixture/_bayesian_mixture.py,544,"Warning : in some Bishop book, there is a typo on the formula 10.63",
scikit-learn/sklearn/mixture/_bayesian_mixture.py,545,`degrees_of_freedom_k = degrees_of_freedom_0 + Nk` is,
scikit-learn/sklearn/mixture/_bayesian_mixture.py,546,the correct formula,
scikit-learn/sklearn/mixture/_bayesian_mixture.py,559,"Contrary to the original bishop book, we normalize the covariances",
scikit-learn/sklearn/mixture/_bayesian_mixture.py,578,"Warning : in some Bishop book, there is a typo on the formula 10.63",
scikit-learn/sklearn/mixture/_bayesian_mixture.py,579,`degrees_of_freedom_k = degrees_of_freedom_0 + Nk`,
scikit-learn/sklearn/mixture/_bayesian_mixture.py,580,is the correct formula,
scikit-learn/sklearn/mixture/_bayesian_mixture.py,590,"Contrary to the original bishop book, we normalize the covariances",
scikit-learn/sklearn/mixture/_bayesian_mixture.py,608,"Warning : in some Bishop book, there is a typo on the formula 10.63",
scikit-learn/sklearn/mixture/_bayesian_mixture.py,609,`degrees_of_freedom_k = degrees_of_freedom_0 + Nk`,
scikit-learn/sklearn/mixture/_bayesian_mixture.py,610,is the correct formula,
scikit-learn/sklearn/mixture/_bayesian_mixture.py,619,"Contrary to the original bishop book, we normalize the covariances",
scikit-learn/sklearn/mixture/_bayesian_mixture.py,637,"Warning : in some Bishop book, there is a typo on the formula 10.63",
scikit-learn/sklearn/mixture/_bayesian_mixture.py,638,`degrees_of_freedom_k = degrees_of_freedom_0 + Nk`,
scikit-learn/sklearn/mixture/_bayesian_mixture.py,639,is the correct formula,
scikit-learn/sklearn/mixture/_bayesian_mixture.py,648,"Contrary to the original bishop book, we normalize the covariances",
scikit-learn/sklearn/mixture/_bayesian_mixture.py,679,case Variationnal Gaussian mixture with dirichlet distribution,
scikit-learn/sklearn/mixture/_bayesian_mixture.py,685,We remove `n_features * np.log(self.degrees_of_freedom_)` because,
scikit-learn/sklearn/mixture/_bayesian_mixture.py,686,the precision matrix is normalized,
scikit-learn/sklearn/mixture/_bayesian_mixture.py,720,"Contrary to the original formula, we have done some simplification",
scikit-learn/sklearn/mixture/_bayesian_mixture.py,721,and removed all the constant terms.,
scikit-learn/sklearn/mixture/_bayesian_mixture.py,724,We removed `.5 * n_features * np.log(self.degrees_of_freedom_)`,
scikit-learn/sklearn/mixture/_bayesian_mixture.py,725,because the precision matrix is normalized.,
scikit-learn/sklearn/mixture/_bayesian_mixture.py,758,Weights computation,
scikit-learn/sklearn/mixture/_bayesian_mixture.py,771,Precisions matrices computation,
scikit-learn/sklearn/mixture/_gaussian_mixture.py,3,Author: Wei Xue <xuewei4d@gmail.com>,
scikit-learn/sklearn/mixture/_gaussian_mixture.py,4,Modified by Thierry Guillemot <thierry.guillemot.work@gmail.com>,
scikit-learn/sklearn/mixture/_gaussian_mixture.py,5,License: BSD 3 clause,
scikit-learn/sklearn/mixture/_gaussian_mixture.py,16,,
scikit-learn/sklearn/mixture/_gaussian_mixture.py,17,Gaussian mixture shape checkers used by the GaussianMixture class,
scikit-learn/sklearn/mixture/_gaussian_mixture.py,38,check range,
scikit-learn/sklearn/mixture/_gaussian_mixture.py,45,check normalization,
scikit-learn/sklearn/mixture/_gaussian_mixture.py,138,,
scikit-learn/sklearn/mixture/_gaussian_mixture.py,139,Gaussian mixture parameters estimators (used by the M-Step),
scikit-learn/sklearn/mixture/_gaussian_mixture.py,337,,
scikit-learn/sklearn/mixture/_gaussian_mixture.py,338,Gaussian mixture probability estimators,
scikit-learn/sklearn/mixture/_gaussian_mixture.py,403,det(precision_chol) is half of det(precision),
scikit-learn/sklearn/mixture/_gaussian_mixture.py,698,Attributes computation,
scikit-learn/sklearn/mixture/_base.py,3,Author: Wei Xue <xuewei4d@gmail.com>,
scikit-learn/sklearn/mixture/_base.py,4,Modified by Thierry Guillemot <thierry.guillemot.work@gmail.com>,
scikit-learn/sklearn/mixture/_base.py,5,License: BSD 3 clause,
scikit-learn/sklearn/mixture/_base.py,119,Check all the parameters values of the derived class,
scikit-learn/sklearn/mixture/_base.py,224,"if we enable warm_start, we will have a unique initialisation",
scikit-learn/sklearn/mixture/_base.py,275,Always do a final e-step to guarantee that the labels returned by,
scikit-learn/sklearn/mixture/_base.py,276,fit_predict(X) are always consistent with fit(X).predict(X),
scikit-learn/sklearn/mixture/_base.py,277,for any value of max_iter and tol (and any random_state).,
scikit-learn/sklearn/mixture/_base.py,506,ignore underflow,
scikit-learn/sklearn/mixture/tests/test_mixture.py,1,Author: Guillaume Lemaitre <g.lemaitre58@gmail.com>,
scikit-learn/sklearn/mixture/tests/test_mixture.py,2,License: BSD 3 clause,
scikit-learn/sklearn/mixture/tests/test_mixture.py,17,check that n_iter is the number of iteration performed.,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,1,Author: Wei Xue <xuewei4d@gmail.com>,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,2,Thierry Guillemot <thierry.guillemot.work@gmail.com>,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,3,License: BSD 3 clause,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,102,test bad parameters,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,113,"covariance_type should be in [spherical, diag, tied, full]",
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,158,test good parameters,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,208,Check bad shape,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,217,Check bad range,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,227,Check bad normalization,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,237,Check good weights matrix,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,253,Check means bad shape,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,260,Check good means matrix,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,273,Define the bad precisions for each covariance_type,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,280,Define not positive-definite precisions,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,303,Check precisions with bad shapes,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,310,Check not positive precisions,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,317,Check the correct init of precisions_init,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,324,compare the precision matrix compute from the,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,325,EmpiricalCovariance.covariance fitted on X*sqrt(resp),
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,326,"with _sufficient_sk_full, n_components=1",
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,330,"special case 1, assuming data is ""centered""",
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,342,check the precision computation,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,348,"special case 2, assuming resp are all ones",
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,358,check the precision computation,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,366,use equation Nk * Sk / N = S_tied,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,387,check the precision computation,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,395,test against 'full' case,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,414,check the precision computation,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,420,computing spherical covariance equals to the variance of one-dimension,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,421,"data after flattening, n_components=1",
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,436,check the precision computation,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,458,We compute the cholesky decomposition of the covariance matrix,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,475,test against with _naive_lmvnpdf_diag,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,487,full covariances,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,493,diag covariances,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,498,tied,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,508,spherical,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,518,"skip tests on weighted_log_probabilities, log_weights",
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,522,test whether responsibilities are normalized,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,558,Check a warning message arrive if we don't do fit,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,573,strict non-convergence,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,574,loose non-convergence,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,575,strict convergence,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,576,loose convergence,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,591,check if fit_predict(X) is equivalent to fit(X).predict(X),
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,600,"Check that fit_predict is equivalent to fit.predict, when n_init > 1",
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,609,recover the ground truth,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,622,needs more data to pass the test with rtol=1e-7,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,652,"the accuracy depends on the number of data and randomness, rng",
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,696,Test that multiple inits does not much worse than a single one,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,711,Test that the right number of parameters is estimated,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,724,Test all of the covariance_types return the same BIC score for,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,725,"1-dimensional, 1 component fits.",
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,740,Test the aic and bic criteria,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,744,standard gaussian entropy,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,789,Assert the warm_start give the same result for the same number of iter,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,806,Assert that by using warm_start we can converge to a good solution,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,818,depending on the data there is large variability in the number of,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,819,refit necessary to converge due to the complete randomness of the,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,820,data,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,830,We check that convergence is detected when warm_start=True,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,854,Check the error message if we don't call fit,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,863,Check score value,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,871,Check if the score increase,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,885,Check the error message if we don't call fit,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,898,We check that each step of the EM without regularization improve,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,899,monotonically the training set likelihood,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,913,Do one training iteration at a time so we can make sure that the,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,914,training log likelihood increases after each iteration.,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,927,We train the GaussianMixture on degenerate data by defining two clusters,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,928,of a 0 covariance.,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,984,To sample we need that GaussianMixture is fitted,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,992,Just to make sure the class samples correctly,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,1016,"Check shapes of sampled data, see",
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,1017,https://github.com/scikit-learn/scikit-learn/issues/7701,
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,1027,We check that by increasing the n_init number we have a better solution,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,1,Author: Wei Xue <xuewei4d@gmail.com>,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,2,Thierry Guillemot <thierry.guillemot.work@gmail.com>,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,3,License: BSD 3 clause,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,96,Check raise message for a bad value of weight_concentration_prior,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,107,Check correct init for a given value of weight_concentration_prior,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,115,Check correct init for the default value of weight_concentration_prior,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,126,Check raise message for a bad value of mean_precision_prior,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,137,Check correct init for a given value of mean_precision_prior,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,144,Check correct init for the default value of mean_precision_prior,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,148,Check raise message for a bad shape of mean_prior,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,157,Check correct init for a given value of mean_prior,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,164,Check correct init for the default value of bemean_priorta,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,175,Check raise message for a bad value of degrees_of_freedom_prior,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,186,Check correct init for a given value of degrees_of_freedom_prior,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,194,Check correct init for the default value of degrees_of_freedom_prior,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,202,Check correct init for a given value of covariance_prior,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,217,Check raise message for a bad spherical value of covariance_prior,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,228,Check correct init for the default value of covariance_prior,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,247,Check raise message,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,261,Case Dirichlet distribution for the weight concentration prior type,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,271,Case Dirichlet process for the weight concentration prior type,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,287,We check that each step of the each step of variational inference without,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,288,regularization improve monotonically the training set of the bound,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,301,Do one training iteration at a time so we can make sure that the,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,302,training log likelihood increases after each iteration.,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,314,We can compare the 'full' precision with the other cov_type if we apply,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,315,1 iter of the M-step (done during _initialize_parameters).,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,322,Computation of the full_covariance,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,333,"Check tied_covariance = mean(full_covariances, 0)",
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,344,Check diag_covariance = diag(full_covariances),
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,358,"Check spherical_covariance = np.mean(diag_covariances, 0)",
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,373,We check that the dot product of the covariance and the precision,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,374,matrices is identity.,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,379,Computation of the full_covariance,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,406,We check here that adding a constant in the data change correctly the,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,407,parameters of the mixture,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,431,strict non-convergence,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,432,loose non-convergence,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,433,strict convergence,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,434,loose convergence,
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,455,"Check that fit_predict is equivalent to fit.predict, when n_init > 1",
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,464,this is the same test as test_gaussian_mixture_predict_predict_proba(),
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,477,Check a warning message arrive if we don't do fit,
scikit-learn/sklearn/semi_supervised/_label_propagation.py,1,coding=utf8,
scikit-learn/sklearn/semi_supervised/_label_propagation.py,55,Authors: Clay Woolam <clay@woolam.org>,
scikit-learn/sklearn/semi_supervised/_label_propagation.py,56,Utkarsh Upadhyay <mail@musicallyut.in>,
scikit-learn/sklearn/semi_supervised/_label_propagation.py,57,License: BSD,
scikit-learn/sklearn/semi_supervised/_label_propagation.py,114,kernel parameters,
scikit-learn/sklearn/semi_supervised/_label_propagation.py,119,clamping factor,
scikit-learn/sklearn/semi_supervised/_label_propagation.py,230,actual graph construction (implementations should override this),
scikit-learn/sklearn/semi_supervised/_label_propagation.py,233,label construction,
scikit-learn/sklearn/semi_supervised/_label_propagation.py,234,construct a categorical distribution for classification only,
scikit-learn/sklearn/semi_supervised/_label_propagation.py,249,initialize distributions,
scikit-learn/sklearn/semi_supervised/_label_propagation.py,256,LabelPropagation,
scikit-learn/sklearn/semi_supervised/_label_propagation.py,259,LabelSpreading,
scikit-learn/sklearn/semi_supervised/_label_propagation.py,284,clamp,
scikit-learn/sklearn/semi_supervised/_label_propagation.py,298,set the transduction item,
scikit-learn/sklearn/semi_supervised/_label_propagation.py,497,this one has different base parameters,
scikit-learn/sklearn/semi_supervised/_label_propagation.py,504,compute affinity matrix (or gram matrix),
scikit-learn/sklearn/semi_supervised/_label_propagation.py,515,set diag to 0.0,
scikit-learn/sklearn/semi_supervised/tests/test_label_propagation.py,46,unstable test; changes in k-NN ordering break it,
scikit-learn/sklearn/semi_supervised/tests/test_label_propagation.py,77,adopting notation from Zhou et al (2004):,
scikit-learn/sklearn/semi_supervised/tests/test_label_propagation.py,103,adopting notation from Zhu et al 2002,
scikit-learn/sklearn/semi_supervised/tests/test_label_propagation.py,130,This is a non-regression test for #5774,
scikit-learn/sklearn/semi_supervised/tests/test_label_propagation.py,136,this should converge quickly:,
scikit-learn/sklearn/semi_supervised/tests/test_label_propagation.py,142,This is a non-regression test for #5774,
scikit-learn/sklearn/semi_supervised/tests/test_label_propagation.py,161,check that we don't divide by zero in case of null normalizer,
scikit-learn/sklearn/semi_supervised/tests/test_label_propagation.py,162,non-regression test for,
scikit-learn/sklearn/semi_supervised/tests/test_label_propagation.py,163,https://github.com/scikit-learn/scikit-learn/pull/15946,
scikit-learn/sklearn/semi_supervised/tests/test_label_propagation.py,173,This is a non-regression test for #15866,
scikit-learn/sklearn/semi_supervised/tests/test_label_propagation.py,175,Custom sparse kernel (top-K RBF),
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,2,Fabian Pedregosa <fabian.pedregosa@inria.fr>,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,3,Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,4,Gael Varoquaux <gael.varoquaux@inria.fr>,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,5,,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,6,License: BSD 3 clause,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,28,mypy error: Module 'sklearn.linear_model' has no attribute '_cd_fast',
scikit-learn/sklearn/linear_model/_coordinate_descent.py,29,type: ignore,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,64,"As of scipy 1.1.0, new argument copy=False by default.",
scikit-learn/sklearn/linear_model/_coordinate_descent.py,65,This is what we want.,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,76,,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,77,Paths functions,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,136,X can be touched inplace thanks to the above line,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,142,Workaround to find alpha_max for sparse matrices.,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,143,since we should not destroy the sparsity of such matrices.,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,434,We expect X and y to be already Fortran ordered when bypassing,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,435,checks,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,442,Xy should be a 1d contiguous array or a 2D C ordered array,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,457,MultiTaskElasticNet does not support sparse matrices,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,460,As sparse matrices are not actually centered we need this,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,461,to be passed to the CD solver.,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,467,X should be normalized and fit already if function is called,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,468,from ElasticNet.fit,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,474,No need to normalize of fit_intercept: it has been done,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,475,above,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,480,make sure alphas are properly ordered,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,517,We expect precompute to be already Fortran ordered when bypassing,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,518,checks,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,550,,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,551,ElasticNet model,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,749,Remember if X is copied,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,751,We expect X and y to be float64 or float32 Fortran ordered arrays,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,752,when bypassing checks,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,775,simplify things by rescaling sw to sum up to n_samples,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,776,"=> np.average(x, weights=sw) = np.mean(sw * x)",
scikit-learn/sklearn/linear_model/_coordinate_descent.py,778,Objective function is:,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,779,"1/2 * np.average(squared error, weights=sw) + alpha * penalty",
scikit-learn/sklearn/linear_model/_coordinate_descent.py,780,but coordinate descent minimizes:,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,781,1/2 * sum(squared error) + alpha * penalty,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,782,enet_path therefore sets alpha = n_samples * alpha,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,783,"With sw, enet_path should set alpha = sum(sw) * alpha",
scikit-learn/sklearn/linear_model/_coordinate_descent.py,784,"Therefore, we rescale alpha = sum(sw) / n_samples * alpha",
scikit-learn/sklearn/linear_model/_coordinate_descent.py,785,"Note: As we rescaled sample_weights to sum up to n_samples,",
scikit-learn/sklearn/linear_model/_coordinate_descent.py,786,we don't need this,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,787,alpha *= np.sum(sample_weight) / n_samples,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,789,"Ensure copying happens only once, don't do it again if done above.",
scikit-learn/sklearn/linear_model/_coordinate_descent.py,790,"X and y will be rescaled if sample_weight is not None, order='F'",
scikit-learn/sklearn/linear_model/_coordinate_descent.py,791,ensures that the returned X and y are still F-contiguous.,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,797,coordinate descent needs F-ordered arrays and _pre_fit might have,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,798,called _rescale_data,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,854,workaround since _set_intercept will cast self.coef_ into X.dtype,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,857,return self for chaining fit and predict calls,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,885,,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,886,Lasso model,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1018,,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1019,Functions for CV with paths functions,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1074,No Gram variant of multi-task exists right now.,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1075,Fall back to default enet_multitask,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1093,"Do the ordering and type casting here, as if it is done in the path,",
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1094,X is copied and a reference is kept here,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1100,Doing this so that it becomes coherent with multioutput.,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1191,This makes sure that there is no duplication in memory.,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1192,Dealing right with copy_X is important in the following:,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1193,Multiple functions touch X and subsamples of X and can induce a,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1194,lot of duplication of memory,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1198,Keep a reference to X,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1200,Let us not impose fortran ordering so far: it is,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1201,not useful for the cross-validation loop and will be done,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1202,by the model fitting itself,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1208,X is a sparse matrix and has been copied,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1211,X has been copied,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1224,All LinearModelCV parameters except 'cv' are acceptable,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1228,"For the first path, we need to set l1_ratio",
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1244,Making sure alphas is properly ordered.,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1246,We want n_alphas to be the number of alphas used for each l1_ratio.,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1251,"We are not computing in parallel, we can modify X",
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1252,inplace in the folds,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1256,init cross-validation generator,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1259,Compute path for all folds and compute MSE to get the best alpha,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1263,"We do a double for loop folded in one, in order to be able to",
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1264,iterate in parallel on l1_ratio and folds,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1291,Remove duplicate alphas in case alphas is provided.,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1295,Refit the model with the parameters selected,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1690,,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1691,Multi Task ElasticNet and Lasso models (with joint feature selection),
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1866,coef contiguous in memory,
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1879,return self for chaining fit and predict calls,
scikit-learn/sklearn/linear_model/_ridge.py,5,Author: Mathieu Blondel <mathieu@mblondel.org>,
scikit-learn/sklearn/linear_model/_ridge.py,6,Reuben Fletcher-Costin <reuben.fletchercostin@gmail.com>,
scikit-learn/sklearn/linear_model/_ridge.py,7,Fabian Pedregosa <fabian@fseoane.net>,
scikit-learn/sklearn/linear_model/_ridge.py,8,Michael Eickenberg <michael.eickenberg@nsup.org>,
scikit-learn/sklearn/linear_model/_ridge.py,9,License: BSD 3 clause,
scikit-learn/sklearn/linear_model/_ridge.py,80,kernel ridge,
scikit-learn/sklearn/linear_model/_ridge.py,81,w = X.T * inv(X X^t + alpha*Id) y,
scikit-learn/sklearn/linear_model/_ridge.py,84,FIXME atol,
scikit-learn/sklearn/linear_model/_ridge.py,88,old scipy,
scikit-learn/sklearn/linear_model/_ridge.py,92,linear ridge,
scikit-learn/sklearn/linear_model/_ridge.py,93,w = inv(X^t X + alpha*Id) * X.T y,
scikit-learn/sklearn/linear_model/_ridge.py,97,FIXME atol,
scikit-learn/sklearn/linear_model/_ridge.py,102,old scipy,
scikit-learn/sklearn/linear_model/_ridge.py,121,"According to the lsqr documentation, alpha = damp^2.",
scikit-learn/sklearn/linear_model/_ridge.py,135,w = inv(X^t X + alpha*Id) * X.T y,
scikit-learn/sklearn/linear_model/_ridge.py,159,dual_coef = inv(X X^t + alpha*Id) y,
scikit-learn/sklearn/linear_model/_ridge.py,172,"Unlike other solvers, we need to support sample_weight directly",
scikit-learn/sklearn/linear_model/_ridge.py,173,because K might be a pre-computed kernel.,
scikit-learn/sklearn/linear_model/_ridge.py,179,"Only one penalty, we can solve multi-target problems in one time.",
scikit-learn/sklearn/linear_model/_ridge.py,183,Note: we must use overwrite_a=False in order to be able to,
scikit-learn/sklearn/linear_model/_ridge.py,184,use the fall-back solution below in case a LinAlgError,
scikit-learn/sklearn/linear_model/_ridge.py,185,is raised,
scikit-learn/sklearn/linear_model/_ridge.py,193,K is expensive to compute and store in memory so change it back in,
scikit-learn/sklearn/linear_model/_ridge.py,194,case it was user-given.,
scikit-learn/sklearn/linear_model/_ridge.py,202,One penalty per target. We need to solve each target separately.,
scikit-learn/sklearn/linear_model/_ridge.py,221,same default value as scipy.linalg.pinv,
scikit-learn/sklearn/linear_model/_ridge.py,387,only sag supports fitting intercept directly,
scikit-learn/sklearn/linear_model/_ridge.py,431,"SAG supports sample_weight directly. For other solvers,",
scikit-learn/sklearn/linear_model/_ridge.py,432,we implement sample_weight via a simple rescaling.,
scikit-learn/sklearn/linear_model/_ridge.py,435,There should be either 1 or n_targets penalties,
scikit-learn/sklearn/linear_model/_ridge.py,465,use SVD solver if matrix is singular,
scikit-learn/sklearn/linear_model/_ridge.py,471,use SVD solver if matrix is singular,
scikit-learn/sklearn/linear_model/_ridge.py,475,precompute max_squared_sum for all targets,
scikit-learn/sklearn/linear_model/_ridge.py,506,"When y was passed as a 1d-array, we flatten the coefficients.",
scikit-learn/sklearn/linear_model/_ridge.py,535,all other solvers work at both float precision levels,
scikit-learn/sklearn/linear_model/_ridge.py,568,when X is sparse we only remove offset from y,
scikit-learn/sklearn/linear_model/_ridge.py,579,add the offset which was subtracted by _preprocess_data,
scikit-learn/sklearn/linear_model/_ridge.py,584,required to fit intercept with sparse_cg solver,
scikit-learn/sklearn/linear_model/_ridge.py,587,for dense matrices or when intercept is set to 0,
scikit-learn/sklearn/linear_model/_ridge.py,932,we don't (yet) support multi-label classification in Ridge,
scikit-learn/sklearn/linear_model/_ridge.py,938,modify the sample weights with the corresponding class weight,
scikit-learn/sklearn/linear_model/_ridge.py,959,"if X has more rows than columns, use decomposition of X^T.X,",
scikit-learn/sklearn/linear_model/_ridge.py,960,otherwise X.X^T,
scikit-learn/sklearn/linear_model/_ridge.py,1132,"compute diagonal of the matrix: dot(Q, dot(diag(v_prime), Q^T))",
scikit-learn/sklearn/linear_model/_ridge.py,1137,"compute dot(diag(D), B)",
scikit-learn/sklearn/linear_model/_ridge.py,1139,handle case where B is > 1-d,
scikit-learn/sklearn/linear_model/_ridge.py,1176,in this case centering has been done in preprocessing,
scikit-learn/sklearn/linear_model/_ridge.py,1177,or we are not fitting an intercept.,
scikit-learn/sklearn/linear_model/_ridge.py,1180,X is sparse,
scikit-learn/sklearn/linear_model/_ridge.py,1222,in this case centering has been done in preprocessing,
scikit-learn/sklearn/linear_model/_ridge.py,1223,or we are not fitting an intercept.,
scikit-learn/sklearn/linear_model/_ridge.py,1226,this function only gets called for sparse X,
scikit-learn/sklearn/linear_model/_ridge.py,1278,if X is dense it has already been centered in preprocessing,
scikit-learn/sklearn/linear_model/_ridge.py,1281,"to emulate centering X with sample weights,",
scikit-learn/sklearn/linear_model/_ridge.py,1282,"ie removing the weighted average, we add a column",
scikit-learn/sklearn/linear_model/_ridge.py,1283,containing the square roots of the sample weights.,
scikit-learn/sklearn/linear_model/_ridge.py,1284,"by centering, it is orthogonal to the other columns",
scikit-learn/sklearn/linear_model/_ridge.py,1297,the vector containing the square roots of the sample weights (1,
scikit-learn/sklearn/linear_model/_ridge.py,1298,when no sample weights) is the eigenvector of XX^T which,
scikit-learn/sklearn/linear_model/_ridge.py,1299,corresponds to the intercept; we cancel the regularization on,
scikit-learn/sklearn/linear_model/_ridge.py,1300,this dimension. the corresponding eigenvalue is,
scikit-learn/sklearn/linear_model/_ridge.py,1301,sum(sample_weight).,
scikit-learn/sklearn/linear_model/_ridge.py,1304,cancel regularization for the intercept,
scikit-learn/sklearn/linear_model/_ridge.py,1308,handle case where y is 2-d,
scikit-learn/sklearn/linear_model/_ridge.py,1322,"to emulate centering X with sample weights,",
scikit-learn/sklearn/linear_model/_ridge.py,1323,"ie removing the weighted average, we add a column",
scikit-learn/sklearn/linear_model/_ridge.py,1324,containing the square roots of the sample weights.,
scikit-learn/sklearn/linear_model/_ridge.py,1325,"by centering, it is orthogonal to the other columns",
scikit-learn/sklearn/linear_model/_ridge.py,1326,when all samples have the same weight we add a column of 1,
scikit-learn/sklearn/linear_model/_ridge.py,1333,remove eigenvalues and vectors in the null space of X^T.X,
scikit-learn/sklearn/linear_model/_ridge.py,1351,handle case where y is 2-d,
scikit-learn/sklearn/linear_model/_ridge.py,1363,"the vector [0, 0, ..., 0, 1]",
scikit-learn/sklearn/linear_model/_ridge.py,1364,is the eigenvector of X^TX which,
scikit-learn/sklearn/linear_model/_ridge.py,1365,corresponds to the intercept; we cancel the regularization on,
scikit-learn/sklearn/linear_model/_ridge.py,1366,this dimension. the corresponding eigenvalue is,
scikit-learn/sklearn/linear_model/_ridge.py,1367,"sum(sample_weight), e.g. n when uniform sample weights.",
scikit-learn/sklearn/linear_model/_ridge.py,1374,add a column to X containing the square roots of sample weights,
scikit-learn/sklearn/linear_model/_ridge.py,1379,"return (1 - hat_diag), (y - y_hat)",
scikit-learn/sklearn/linear_model/_ridge.py,1381,handle case where y is 2-d,
scikit-learn/sklearn/linear_model/_ridge.py,1399,X already centered,
scikit-learn/sklearn/linear_model/_ridge.py,1402,"to emulate fit_intercept=True situation, add a column",
scikit-learn/sklearn/linear_model/_ridge.py,1403,containing the square roots of the sample weights,
scikit-learn/sklearn/linear_model/_ridge.py,1404,"by centering, the other columns are orthogonal to that one",
scikit-learn/sklearn/linear_model/_ridge.py,1421,detect intercept column,
scikit-learn/sklearn/linear_model/_ridge.py,1424,cancel the regularization for the intercept,
scikit-learn/sklearn/linear_model/_ridge.py,1429,handle case where y is 2-d,
scikit-learn/sklearn/linear_model/_ridge.py,1897,modify the sample weights with the corresponding class weight,
scikit-learn/sklearn/linear_model/_logistic.py,5,Author: Gael Varoquaux <gael.varoquaux@normalesup.org>,
scikit-learn/sklearn/linear_model/_logistic.py,6,Fabian Pedregosa <f@bianp.net>,
scikit-learn/sklearn/linear_model/_logistic.py,7,Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>,
scikit-learn/sklearn/linear_model/_logistic.py,8,Manoj Kumar <manojkumarsivaraj334@gmail.com>,
scikit-learn/sklearn/linear_model/_logistic.py,9,Lars Buitinck,
scikit-learn/sklearn/linear_model/_logistic.py,10,Simon Wu <s8wu@uwaterloo.ca>,
scikit-learn/sklearn/linear_model/_logistic.py,11,Arthur Mensch <arthur.mensch@m4x.org,
scikit-learn/sklearn/linear_model/_logistic.py,44,.. some helper functions for logistic_regression_path ..,
scikit-learn/sklearn/linear_model/_logistic.py,120,Logistic loss is the negative of the log of the logistic function.,
scikit-learn/sklearn/linear_model/_logistic.py,128,Case where we fit the intercept.,
scikit-learn/sklearn/linear_model/_logistic.py,165,Logistic loss is the negative of the log of the logistic function.,
scikit-learn/sklearn/linear_model/_logistic.py,214,Case where we fit the intercept.,
scikit-learn/sklearn/linear_model/_logistic.py,218,The mat-vec product of the Hessian,
scikit-learn/sklearn/linear_model/_logistic.py,224,Precompute as much as possible,
scikit-learn/sklearn/linear_model/_logistic.py,228,Calculate the double derivative with respect to intercept,
scikit-learn/sklearn/linear_model/_logistic.py,229,In the case of sparse matrices this returns a matrix object.,
scikit-learn/sklearn/linear_model/_logistic.py,237,For the fit intercept case.,
scikit-learn/sklearn/linear_model/_logistic.py,398,`loss` is unused. Refactoring to avoid computing it does not,
scikit-learn/sklearn/linear_model/_logistic.py,399,significantly speed up the computation and decreases readability,
scikit-learn/sklearn/linear_model/_logistic.py,403,Hessian-vector product derived by applying the R-operator on the gradient,
scikit-learn/sklearn/linear_model/_logistic.py,404,of the multinomial loss function.,
scikit-learn/sklearn/linear_model/_logistic.py,412,r_yhat holds the result of applying the R-operator on the multinomial,
scikit-learn/sklearn/linear_model/_logistic.py,413,estimator.,
scikit-learn/sklearn/linear_model/_logistic.py,635,Preprocessing.,
scikit-learn/sklearn/linear_model/_logistic.py,650,np.unique(y) gives labels in sorted order.,
scikit-learn/sklearn/linear_model/_logistic.py,653,"If sample weights exist, convert them to array (support for lists)",
scikit-learn/sklearn/linear_model/_logistic.py,654,and check length,
scikit-learn/sklearn/linear_model/_logistic.py,655,Otherwise set them to 1 for all examples,
scikit-learn/sklearn/linear_model/_logistic.py,659,"If class_weights is a dict (provided by the user), the weights",
scikit-learn/sklearn/linear_model/_logistic.py,660,"are assigned to the original labels. If it is ""balanced"", then",
scikit-learn/sklearn/linear_model/_logistic.py,661,the class_weights are assigned after masking the labels with a OvR.,
scikit-learn/sklearn/linear_model/_logistic.py,667,"For doing a ovr, we need to mask the labels first. for the",
scikit-learn/sklearn/linear_model/_logistic.py,668,multinomial case this is not necessary.,
scikit-learn/sklearn/linear_model/_logistic.py,675,for compute_class_weight,
scikit-learn/sklearn/linear_model/_logistic.py,689,"SAG multinomial solver needs LabelEncoder, not LabelBinarizer",
scikit-learn/sklearn/linear_model/_logistic.py,697,it must work both giving the bias term and not,
scikit-learn/sklearn/linear_model/_logistic.py,705,"For binary problems coef.shape[0] should be 1, otherwise it",
scikit-learn/sklearn/linear_model/_logistic.py,706,should be classes.size.,
scikit-learn/sklearn/linear_model/_logistic.py,726,scipy.optimize.minimize and newton-cg accepts only,
scikit-learn/sklearn/linear_model/_logistic.py,727,ravelled parameters.,
scikit-learn/sklearn/linear_model/_logistic.py,783,"alpha is for L2-norm, beta is for L1-norm",
scikit-learn/sklearn/linear_model/_logistic.py,790,Elastic-Net penalty,
scikit-learn/sklearn/linear_model/_logistic.py,818,helper function for LogisticCV,
scikit-learn/sklearn/linear_model/_logistic.py,973,The score method of Logistic Regression has a classes_ attribute.,
scikit-learn/sklearn/linear_model/_logistic.py,1316,default values,
scikit-learn/sklearn/linear_model/_logistic.py,1321,Note that check for l1_ratio is done right above,
scikit-learn/sklearn/linear_model/_logistic.py,1389,Hack so that we iterate only once for the multinomial case.,
scikit-learn/sklearn/linear_model/_logistic.py,1398,The SAG solver releases the GIL so it's more efficient to use,
scikit-learn/sklearn/linear_model/_logistic.py,1399,threads for this solver.,
scikit-learn/sklearn/linear_model/_logistic.py,1469,"Workaround for multi_class=""multinomial"" and binary outcomes",
scikit-learn/sklearn/linear_model/_logistic.py,1470,which requires softmax prediction with only a 1D decision.,
scikit-learn/sklearn/linear_model/_logistic.py,1820,Encode for string labels,
scikit-learn/sklearn/linear_model/_logistic.py,1827,The original class labels,
scikit-learn/sklearn/linear_model/_logistic.py,1839,init cross-validation generator,
scikit-learn/sklearn/linear_model/_logistic.py,1843,Use the label encoded classes,
scikit-learn/sklearn/linear_model/_logistic.py,1852,OvR in case of binary problems is as good as fitting,
scikit-learn/sklearn/linear_model/_logistic.py,1853,the higher label,
scikit-learn/sklearn/linear_model/_logistic.py,1858,"We need this hack to iterate only once over labels, in the case of",
scikit-learn/sklearn/linear_model/_logistic.py,1859,"multi_class = multinomial, without changing the value of the labels.",
scikit-learn/sklearn/linear_model/_logistic.py,1866,compute the class weights for the entire dataset y,
scikit-learn/sklearn/linear_model/_logistic.py,1875,The SAG solver releases the GIL so it's more efficient to use,
scikit-learn/sklearn/linear_model/_logistic.py,1876,threads for this solver.,
scikit-learn/sklearn/linear_model/_logistic.py,1900,_log_reg_scoring_path will output different shapes depending on the,
scikit-learn/sklearn/linear_model/_logistic.py,1901,"multi_class param, so we need to reshape the outputs accordingly.",
scikit-learn/sklearn/linear_model/_logistic.py,1902,"Cs is of shape (n_classes . n_folds . n_l1_ratios, n_Cs) and all the",
scikit-learn/sklearn/linear_model/_logistic.py,1903,"rows are equal, so we just take the first one.",
scikit-learn/sklearn/linear_model/_logistic.py,1904,"After reshaping,",
scikit-learn/sklearn/linear_model/_logistic.py,1905,"- scores is of shape (n_classes, n_folds, n_Cs . n_l1_ratios)",
scikit-learn/sklearn/linear_model/_logistic.py,1906,- coefs_paths is of shape,
scikit-learn/sklearn/linear_model/_logistic.py,1907,"(n_classes, n_folds, n_Cs . n_l1_ratios, n_features)",
scikit-learn/sklearn/linear_model/_logistic.py,1908,- n_iter is of shape,
scikit-learn/sklearn/linear_model/_logistic.py,1909,"(n_classes, n_folds, n_Cs . n_l1_ratios) or",
scikit-learn/sklearn/linear_model/_logistic.py,1910,"(1, n_folds, n_Cs . n_l1_ratios)",
scikit-learn/sklearn/linear_model/_logistic.py,1918,"equiv to coefs_paths = np.moveaxis(coefs_paths, (0, 1, 2, 3),",
scikit-learn/sklearn/linear_model/_logistic.py,1919,"(1, 2, 0, 3))",
scikit-learn/sklearn/linear_model/_logistic.py,1926,repeat same scores across all classes,
scikit-learn/sklearn/linear_model/_logistic.py,1953,"For multinomial, all scores are the same across classes",
scikit-learn/sklearn/linear_model/_logistic.py,1955,coefs_paths will keep its original shape because,
scikit-learn/sklearn/linear_model/_logistic.py,1956,logistic_regression_path expects it this way,
scikit-learn/sklearn/linear_model/_logistic.py,1959,best_index is between 0 and (n_Cs . n_l1_ratios - 1),
scikit-learn/sklearn/linear_model/_logistic.py,1960,"for example, with n_cs=2 and n_l1_ratios=3",
scikit-learn/sklearn/linear_model/_logistic.py,1961,the layout of scores is,
scikit-learn/sklearn/linear_model/_logistic.py,1962,"[c1, c2, c1, c2, c1, c2]",
scikit-learn/sklearn/linear_model/_logistic.py,1963,"l1_1 ,  l1_2 ,  l1_3",
scikit-learn/sklearn/linear_model/_logistic.py,1980,Note that y is label encoded and hence pos_class must be,
scikit-learn/sklearn/linear_model/_logistic.py,1981,the encoded label / None (for 'multinomial'),
scikit-learn/sklearn/linear_model/_logistic.py,1997,Take the best scores across every fold and the average of,
scikit-learn/sklearn/linear_model/_logistic.py,1998,all coefficients corresponding to the best scores.,
scikit-learn/sklearn/linear_model/_logistic.py,2030,"if elasticnet was used, add the l1_ratios dimension to some",
scikit-learn/sklearn/linear_model/_logistic.py,2031,attributes,
scikit-learn/sklearn/linear_model/_logistic.py,2033,with n_cs=2 and n_l1_ratios=3,
scikit-learn/sklearn/linear_model/_logistic.py,2034,the layout of scores is,
scikit-learn/sklearn/linear_model/_logistic.py,2035,"[c1, c2, c1, c2, c1, c2]",
scikit-learn/sklearn/linear_model/_logistic.py,2036,"l1_1 ,  l1_2 ,  l1_3",
scikit-learn/sklearn/linear_model/_logistic.py,2037,To get a 2d array with the following layout,
scikit-learn/sklearn/linear_model/_logistic.py,2038,"l1_1, l1_2, l1_3",
scikit-learn/sklearn/linear_model/_logistic.py,2039,"c1 [[ .  ,  .  ,  .  ],",
scikit-learn/sklearn/linear_model/_logistic.py,2040,"c2  [ .  ,  .  ,  .  ]]",
scikit-learn/sklearn/linear_model/_logistic.py,2041,We need to first reshape and then transpose.,
scikit-learn/sklearn/linear_model/_logistic.py,2042,The same goes for the other arrays,
scikit-learn/sklearn/linear_model/_ransac.py,1,coding: utf-8,
scikit-learn/sklearn/linear_model/_ransac.py,3,Author: Johannes Schönberger,
scikit-learn/sklearn/linear_model/_ransac.py,4,,
scikit-learn/sklearn/linear_model/_ransac.py,5,License: BSD 3 clause,
scikit-learn/sklearn/linear_model/_ransac.py,259,assume linear model by default,
scikit-learn/sklearn/linear_model/_ransac.py,279,MAD (median absolute deviation),
scikit-learn/sklearn/linear_model/_ransac.py,309,Not all estimator accept a random_state,
scikit-learn/sklearn/linear_model/_ransac.py,335,number of data samples,
scikit-learn/sklearn/linear_model/_ransac.py,348,choose random sample set,
scikit-learn/sklearn/linear_model/_ransac.py,354,check if random sample set is valid,
scikit-learn/sklearn/linear_model/_ransac.py,360,fit model for current random sample set,
scikit-learn/sklearn/linear_model/_ransac.py,367,check if estimated model is valid,
scikit-learn/sklearn/linear_model/_ransac.py,373,residuals of all data for current random sample model,
scikit-learn/sklearn/linear_model/_ransac.py,377,classify data into inliers and outliers,
scikit-learn/sklearn/linear_model/_ransac.py,381,less inliers -> skip current random sample,
scikit-learn/sklearn/linear_model/_ransac.py,386,extract inlier data set,
scikit-learn/sklearn/linear_model/_ransac.py,391,score of inlier data set,
scikit-learn/sklearn/linear_model/_ransac.py,395,same number of inliers but worse score -> skip current random,
scikit-learn/sklearn/linear_model/_ransac.py,396,sample,
scikit-learn/sklearn/linear_model/_ransac.py,401,save current random sample as best sample,
scikit-learn/sklearn/linear_model/_ransac.py,414,break if sufficient number of inliers or score is reached,
scikit-learn/sklearn/linear_model/_ransac.py,419,if none of the iterations met the required criteria,
scikit-learn/sklearn/linear_model/_ransac.py,444,estimate final model using all inliers,
scikit-learn/sklearn/linear_model/_sag.py,3,Authors: Tom Dupre la Tour <tom.dupre-la-tour@m4x.org>,
scikit-learn/sklearn/linear_model/_sag.py,4,,
scikit-learn/sklearn/linear_model/_sag.py,5,License: BSD 3 clause,
scikit-learn/sklearn/linear_model/_sag.py,69,inverse Lipschitz constant for squared loss,
scikit-learn/sklearn/linear_model/_sag.py,75,SAGA theoretical step size is 1/3L or 1 / (2 * (L + mu n)),
scikit-learn/sklearn/linear_model/_sag.py,76,See Defazio et al. 2014,
scikit-learn/sklearn/linear_model/_sag.py,80,SAG theoretical step size is 1/16L but it is recommended to use 1 / L,
scikit-learn/sklearn/linear_model/_sag.py,81,"see http://www.birs.ca//workshops//2014/14w5003/files/schmidt.pdf,",
scikit-learn/sklearn/linear_model/_sag.py,82,slide 65,
scikit-learn/sklearn/linear_model/_sag.py,235,Ridge default max_iter is None,
scikit-learn/sklearn/linear_model/_sag.py,245,"As in SGD, the alpha is scaled by n_samples.",
scikit-learn/sklearn/linear_model/_sag.py,249,"if loss == 'multinomial', y should be label encoded.",
scikit-learn/sklearn/linear_model/_sag.py,252,initialization,
scikit-learn/sklearn/linear_model/_sag.py,258,assume fit_intercept is False,
scikit-learn/sklearn/linear_model/_sag.py,262,coef_init contains possibly the intercept_init at the end.,
scikit-learn/sklearn/linear_model/_sag.py,263,"Note that Ridge centers the data before fitting, so fit_intercept=False.",
scikit-learn/sklearn/linear_model/_least_angle.py,5,Author: Fabian Pedregosa <fabian.pedregosa@inria.fr>,
scikit-learn/sklearn/linear_model/_least_angle.py,6,Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/sklearn/linear_model/_least_angle.py,7,Gael Varoquaux,
scikit-learn/sklearn/linear_model/_least_angle.py,8,,
scikit-learn/sklearn/linear_model/_least_angle.py,9,License: BSD 3 clause,
scikit-learn/sklearn/linear_model/_least_angle.py,22,mypy error: Module 'sklearn.utils' has no attribute 'arrayfuncs',
scikit-learn/sklearn/linear_model/_least_angle.py,23,type: ignore,
scikit-learn/sklearn/linear_model/_least_angle.py,411,force copy. setting the array to be fortran-ordered,
scikit-learn/sklearn/linear_model/_least_angle.py,412,speeds up the calculation of the (partial) Gram matrix,
scikit-learn/sklearn/linear_model/_least_angle.py,413,and allows to easily swap columns,
scikit-learn/sklearn/linear_model/_least_angle.py,438,better ideas?,
scikit-learn/sklearn/linear_model/_least_angle.py,442,holds the sign of covariance,
scikit-learn/sklearn/linear_model/_least_angle.py,446,will hold the cholesky factorization. Only lower part is,
scikit-learn/sklearn/linear_model/_least_angle.py,447,referenced.,
scikit-learn/sklearn/linear_model/_least_angle.py,463,to avoid division by 0 warning,
scikit-learn/sklearn/linear_model/_least_angle.py,493,early stopping,
scikit-learn/sklearn/linear_model/_least_angle.py,495,interpolation factor 0 <= ss < 1,
scikit-learn/sklearn/linear_model/_least_angle.py,497,"In the first iteration, all alphas are zero, the formula",
scikit-learn/sklearn/linear_model/_least_angle.py,498,below would make ss a NaN,
scikit-learn/sklearn/linear_model/_least_angle.py,511,,
scikit-learn/sklearn/linear_model/_least_angle.py,512,Append x_j to the Cholesky factorization of (Xa * Xa'),
scikit-learn/sklearn/linear_model/_least_angle.py,513,,
scikit-learn/sklearn/linear_model/_least_angle.py,514,( L   0 ),
scikit-learn/sklearn/linear_model/_least_angle.py,515,"L  ->  (       )  , where L * w = Xa' x_j",
scikit-learn/sklearn/linear_model/_least_angle.py,516,( w   z )    and z = ||x_j||,
scikit-learn/sklearn/linear_model/_least_angle.py,517,,
scikit-learn/sklearn/linear_model/_least_angle.py,518,,
scikit-learn/sklearn/linear_model/_least_angle.py,529,remove Cov[0],
scikit-learn/sklearn/linear_model/_least_angle.py,537,swap does only work inplace if matrix is fortran,
scikit-learn/sklearn/linear_model/_least_angle.py,538,contiguous ...,
scikit-learn/sklearn/linear_model/_least_angle.py,544,Update the cholesky decomposition for the Gram matrix,
scikit-learn/sklearn/linear_model/_least_angle.py,557,The system is becoming too ill-conditioned.,
scikit-learn/sklearn/linear_model/_least_angle.py,558,We have degenerate vectors in our active set.,
scikit-learn/sklearn/linear_model/_least_angle.py,559,We'll 'drop for good' the last regressor added.,
scikit-learn/sklearn/linear_model/_least_angle.py,561,Note: this case is very rare. It is no longer triggered by,
scikit-learn/sklearn/linear_model/_least_angle.py,562,the test suite. The `equality_tolerance` margin added in 0.16,
scikit-learn/sklearn/linear_model/_least_angle.py,563,to get early stopping to work consistently on all versions of,
scikit-learn/sklearn/linear_model/_least_angle.py,564,Python including 32 bit Python under Windows seems to make it,
scikit-learn/sklearn/linear_model/_least_angle.py,565,very difficult to trigger the 'drop for good' strategy.,
scikit-learn/sklearn/linear_model/_least_angle.py,575,XXX: need to figure a 'drop for good' way,
scikit-learn/sklearn/linear_model/_least_angle.py,589,alpha is increasing. This is because the updates of Cov are,
scikit-learn/sklearn/linear_model/_least_angle.py,590,bringing in too much numerical error that is greater than,
scikit-learn/sklearn/linear_model/_least_angle.py,591,than the remaining correlation with the,
scikit-learn/sklearn/linear_model/_least_angle.py,592,regressors. Time to bail out,
scikit-learn/sklearn/linear_model/_least_angle.py,602,least squares solution,
scikit-learn/sklearn/linear_model/_least_angle.py,608,This happens because sign_active[:n_active] = 0,
scikit-learn/sklearn/linear_model/_least_angle.py,612,is this really needed ?,
scikit-learn/sklearn/linear_model/_least_angle.py,616,L is too ill-conditioned,
scikit-learn/sklearn/linear_model/_least_angle.py,630,equiangular direction of variables in the active set,
scikit-learn/sklearn/linear_model/_least_angle.py,632,correlation between each unactive variables and,
scikit-learn/sklearn/linear_model/_least_angle.py,633,eqiangular vector,
scikit-learn/sklearn/linear_model/_least_angle.py,636,"if huge number of features, this takes 50% of time, I",
scikit-learn/sklearn/linear_model/_least_angle.py,637,think could be avoided if we just update it using an,
scikit-learn/sklearn/linear_model/_least_angle.py,638,orthogonal (QR) decomposition of X,
scikit-learn/sklearn/linear_model/_least_angle.py,649,TODO: better names for these variables: z,
scikit-learn/sklearn/linear_model/_least_angle.py,654,some coefficients have changed sign,
scikit-learn/sklearn/linear_model/_least_angle.py,657,"update the sign, important for LAR",
scikit-learn/sklearn/linear_model/_least_angle.py,669,resize the coefs and alphas array,
scikit-learn/sklearn/linear_model/_least_angle.py,678,mimic the effect of incrementing n_iter on the array references,
scikit-learn/sklearn/linear_model/_least_angle.py,685,update correlations,
scikit-learn/sklearn/linear_model/_least_angle.py,688,See if any coefficient has changed sign,
scikit-learn/sklearn/linear_model/_least_angle.py,691,handle the case when idx is not length of 1,
scikit-learn/sklearn/linear_model/_least_angle.py,696,handle the case when idx is not length of 1,
scikit-learn/sklearn/linear_model/_least_angle.py,700,propagate dropped variable,
scikit-learn/sklearn/linear_model/_least_angle.py,704,yeah this is stupid,
scikit-learn/sklearn/linear_model/_least_angle.py,707,TODO: this could be updated,
scikit-learn/sklearn/linear_model/_least_angle.py,720,Cov_n = Cov_j + x_j * X + increment(betas) TODO:,
scikit-learn/sklearn/linear_model/_least_angle.py,721,will this still work with multiple drops ?,
scikit-learn/sklearn/linear_model/_least_angle.py,723,recompute covariance. Probably could be done better,
scikit-learn/sklearn/linear_model/_least_angle.py,724,wrong as Xy is not swapped with the rest of variables,
scikit-learn/sklearn/linear_model/_least_angle.py,726,TODO: this could be updated,
scikit-learn/sklearn/linear_model/_least_angle.py,731,just to maintain size,
scikit-learn/sklearn/linear_model/_least_angle.py,737,resize coefs in case of early stop,
scikit-learn/sklearn/linear_model/_least_angle.py,752,,
scikit-learn/sklearn/linear_model/_least_angle.py,753,Estimator classes,
scikit-learn/sklearn/linear_model/_least_angle.py,952,n_nonzero_coefs parametrization takes priority,
scikit-learn/sklearn/linear_model/_least_angle.py,1099,,
scikit-learn/sklearn/linear_model/_least_angle.py,1100,Cross-validated estimator classes,
scikit-learn/sklearn/linear_model/_least_angle.py,1375,init cross-validation generator,
scikit-learn/sklearn/linear_model/_least_angle.py,1378,"As we use cross-validation, the Gram matrix is not precomputed here",
scikit-learn/sklearn/linear_model/_least_angle.py,1394,Unique also sorts,
scikit-learn/sklearn/linear_model/_least_angle.py,1396,Take at most max_n_alphas values,
scikit-learn/sklearn/linear_model/_least_angle.py,1419,Select the alpha that minimizes left-out error,
scikit-learn/sklearn/linear_model/_least_angle.py,1423,Store our parameters,
scikit-learn/sklearn/linear_model/_least_angle.py,1428,Now compute the full model,
scikit-learn/sklearn/linear_model/_least_angle.py,1429,it will call a lasso internally when self if LassoLarsCV,
scikit-learn/sklearn/linear_model/_least_angle.py,1430,as self.method == 'lasso',
scikit-learn/sklearn/linear_model/_least_angle.py,1594,XXX : we don't use super().__init__,
scikit-learn/sklearn/linear_model/_least_angle.py,1595,to avoid setting n_nonzero_coefs,
scikit-learn/sklearn/linear_model/_least_angle.py,1768,AIC,
scikit-learn/sklearn/linear_model/_least_angle.py,1770,BIC,
scikit-learn/sklearn/linear_model/_least_angle.py,1774,residuals,
scikit-learn/sklearn/linear_model/_least_angle.py,1778,Degrees of freedom,
scikit-learn/sklearn/linear_model/_least_angle.py,1783,get the number of degrees of freedom equal to:,
scikit-learn/sklearn/linear_model/_least_angle.py,1784,"Xc = X[:, mask]",
scikit-learn/sklearn/linear_model/_least_angle.py,1785,"Trace(Xc * inv(Xc.T, Xc) * Xc.T) ie the number of non-zero coefs",
scikit-learn/sklearn/linear_model/_least_angle.py,1791,"Eqns. 2.15--16 in (Zou et al, 2007)",
scikit-learn/sklearn/linear_model/_perceptron.py,1,Author: Mathieu Blondel,
scikit-learn/sklearn/linear_model/_perceptron.py,2,License: BSD 3 clause,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,1,Authors: Peter Prettenhofer <peter.prettenhofer@gmail.com> (main author),
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,2,Mathieu Blondel (partial_fit support),
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,3,,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,4,License: BSD 3 clause,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,44,Default value of ``epsilon`` parameter.,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,55,to pass check_is_fitted,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,98,current tests expect init to do parameter validation,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,99,but we are not allowed to set attributes,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,149,raises ValueError if not registered,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,186,allocate coef_ for multi-class,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,197,allocate intercept_ for multi-class,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,208,allocate coef_ for binary problem,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,222,allocate intercept_ for binary problem,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,232,initialize average parameters,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,259,"use the full set for training, with an empty validation set",
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,290,mypy error: Decorated property not supported,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,291,type: ignore,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,297,mypy error: Decorated property not supported,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,298,type: ignore,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,306,mypy error: Decorated property not supported,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,307,type: ignore,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,313,mypy error: Decorated property not supported,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,314,type: ignore,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,406,"if average is not true, average_coef, and average_intercept will be",
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,407,unused,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,425,numpy mtrand expects a C long which is a signed 32 bit integer under,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,426,Windows,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,497,Allocate datastructures from input arguments,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,513,delegate to concrete training procedure,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,541,"labels can be encoded as float, int, or string literals",
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,542,np.unique sorts in asc order; largest class id is positive class,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,560,Clear iteration count for multiple call to fit.,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,587,need to be 2d,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,598,"intercept is a float, need to convert it to an array of length 1",
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,608,Precompute the validation split using the multiclass labels,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,609,to ensure proper balancing of the classes.,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,612,Use joblib to fit OvA in parallel.,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,613,Pick the random seed for each job outside of fit_binary to avoid,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,614,sharing the estimator random state between threads which could lead,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,615,to non-deterministic behavior,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,627,take the maximum of n_iter_ over every binary fit,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,1040,"the above might assign zero to all classes, which doesn't",
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,1041,normalize neatly; work around this to produce uniform,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,1042,probabilities,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,1049,normalize,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,1127,Allocate datastructures from input arguments,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,1188,Clear iteration count for multiple call to fit.,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,1285,numpy mtrand expects a C long which is a signed 32 bit integer under,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,1286,Windows,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,1299,Not used,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,1300,Not used,
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,1332,made enough updates for averaging to be taken into account,
scikit-learn/sklearn/linear_model/_bayes.py,5,"Authors: V. Michel, F. Pedregosa, A. Gramfort",
scikit-learn/sklearn/linear_model/_bayes.py,6,License: BSD 3 clause,
scikit-learn/sklearn/linear_model/_bayes.py,19,,
scikit-learn/sklearn/linear_model/_bayes.py,20,BayesianRidge regression,
scikit-learn/sklearn/linear_model/_bayes.py,203,Sample weight can be implemented via a simple rescaling.,
scikit-learn/sklearn/linear_model/_bayes.py,210,Initialization of the values of the parameters,
scikit-learn/sklearn/linear_model/_bayes.py,212,Add `eps` in the denominator to omit division by zero if `np.var(y)`,
scikit-learn/sklearn/linear_model/_bayes.py,213,is zero,
scikit-learn/sklearn/linear_model/_bayes.py,234,Convergence loop of the bayesian ridge regression,
scikit-learn/sklearn/linear_model/_bayes.py,237,update posterior mean coef_ based on alpha_ and lambda_ and,
scikit-learn/sklearn/linear_model/_bayes.py,238,compute corresponding rmse,
scikit-learn/sklearn/linear_model/_bayes.py,243,compute the log marginal likelihood,
scikit-learn/sklearn/linear_model/_bayes.py,250,"Update alpha and lambda according to (MacKay, 1992)",
scikit-learn/sklearn/linear_model/_bayes.py,258,Check for convergence,
scikit-learn/sklearn/linear_model/_bayes.py,267,"return regularization parameters and corresponding posterior mean,",
scikit-learn/sklearn/linear_model/_bayes.py,268,log marginal likelihood and posterior covariance,
scikit-learn/sklearn/linear_model/_bayes.py,275,compute the log marginal likelihood,
scikit-learn/sklearn/linear_model/_bayes.py,283,posterior covariance is given by 1/alpha_ * scaled_sigma_,
scikit-learn/sklearn/linear_model/_bayes.py,356,compute the log of the determinant of the posterior covariance.,
scikit-learn/sklearn/linear_model/_bayes.py,357,posterior covariance is given by,
scikit-learn/sklearn/linear_model/_bayes.py,358,"sigma = (lambda_ * np.eye(n_features) + alpha_ * np.dot(X.T, X))^-1",
scikit-learn/sklearn/linear_model/_bayes.py,379,,
scikit-learn/sklearn/linear_model/_bayes.py,380,ARD (Automatic Relevance Determination) regression,
scikit-learn/sklearn/linear_model/_bayes.py,537,Launch the convergence loop,
scikit-learn/sklearn/linear_model/_bayes.py,546,Initialization of the values of the parameters,
scikit-learn/sklearn/linear_model/_bayes.py,548,Add `eps` in the denominator to omit division by zero if `np.var(y)`,
scikit-learn/sklearn/linear_model/_bayes.py,549,is zero,
scikit-learn/sklearn/linear_model/_bayes.py,556,Compute sigma and mu (using Woodbury matrix identity),
scikit-learn/sklearn/linear_model/_bayes.py,574,Iterative procedure of ARDRegression,
scikit-learn/sklearn/linear_model/_bayes.py,579,Update alpha and lambda,
scikit-learn/sklearn/linear_model/_bayes.py,588,Prune the weights with a precision over a threshold,
scikit-learn/sklearn/linear_model/_bayes.py,592,Compute the objective function,
scikit-learn/sklearn/linear_model/_bayes.py,601,Check for convergence,
scikit-learn/sklearn/linear_model/_bayes.py,608,update sigma and mu using updated parameters from the last iteration,
scikit-learn/sklearn/linear_model/_passive_aggressive.py,1,"Authors: Rob Zinkov, Mathieu Blondel",
scikit-learn/sklearn/linear_model/_passive_aggressive.py,2,License: BSD 3 clause,
scikit-learn/sklearn/linear_model/_theil_sen.py,1,-*- coding: utf-8 -*-,
scikit-learn/sklearn/linear_model/_theil_sen.py,6,Author: Florian Wilhelm <florian.wilhelm@gmail.com>,
scikit-learn/sklearn/linear_model/_theil_sen.py,7,,
scikit-learn/sklearn/linear_model/_theil_sen.py,8,License: BSD 3 clause,
scikit-learn/sklearn/linear_model/_theil_sen.py,58,x_old equals one of our samples,
scikit-learn/sklearn/linear_model/_theil_sen.py,65,to avoid division by zero,
scikit-learn/sklearn/linear_model/_theil_sen.py,113,We are computing the tol on the squared norm,
scikit-learn/sklearn/linear_model/_theil_sen.py,182,gelss need to pad y_subpopulation to be of the max dim of X_subpopulation,
scikit-learn/sklearn/linear_model/_theil_sen.py,326,if n_samples < n_features,
scikit-learn/sklearn/linear_model/_theil_sen.py,373,Determine indices of subpopulation,
scikit-learn/sklearn/linear_model/_huber.py,1,Authors: Manoj Kumar mks542@nyu.edu,
scikit-learn/sklearn/linear_model/_huber.py,2,License: BSD 3 clause,
scikit-learn/sklearn/linear_model/_huber.py,59,Calculate the values where |y - X'w -c / sigma| > epsilon,
scikit-learn/sklearn/linear_model/_huber.py,60,The values above this threshold are outliers.,
scikit-learn/sklearn/linear_model/_huber.py,67,Calculate the linear loss due to the outliers.,
scikit-learn/sklearn/linear_model/_huber.py,68,This is equal to (2 * M * |y - X'w -c / sigma| - M**2) * sigma,
scikit-learn/sklearn/linear_model/_huber.py,73,n_sq_outliers includes the weight give to the outliers while,
scikit-learn/sklearn/linear_model/_huber.py,74,num_outliers is just the number of outliers.,
scikit-learn/sklearn/linear_model/_huber.py,80,Calculate the quadratic loss due to the non-outliers.-,
scikit-learn/sklearn/linear_model/_huber.py,81,This is equal to |(y - X'w - c)**2 / sigma**2| * sigma,
scikit-learn/sklearn/linear_model/_huber.py,92,Gradient due to the squared loss.,
scikit-learn/sklearn/linear_model/_huber.py,97,Gradient due to the linear loss.,
scikit-learn/sklearn/linear_model/_huber.py,106,Gradient due to the penalty.,
scikit-learn/sklearn/linear_model/_huber.py,109,Gradient due to sigma.,
scikit-learn/sklearn/linear_model/_huber.py,114,Gradient due to the intercept.,
scikit-learn/sklearn/linear_model/_huber.py,273,Make sure to initialize the scale parameter to a strictly,
scikit-learn/sklearn/linear_model/_huber.py,274,positive value:,
scikit-learn/sklearn/linear_model/_huber.py,277,Sigma or the scale factor should be non-negative.,
scikit-learn/sklearn/linear_model/_huber.py,278,Setting it to be zero might cause undefined bounds hence we set it,
scikit-learn/sklearn/linear_model/_huber.py,279,to a value close to zero.,
scikit-learn/sklearn/linear_model/__init__.py,5,See http://scikit-learn.sourceforge.net/modules/sgd.html and,
scikit-learn/sklearn/linear_model/__init__.py,6,http://scikit-learn.sourceforge.net/modules/linear_model.html for,
scikit-learn/sklearn/linear_model/__init__.py,7,complete documentation.,
scikit-learn/sklearn/linear_model/setup.py,26,generate sag_fast from template,
scikit-learn/sklearn/linear_model/setup.py,34,add other directories,
scikit-learn/sklearn/linear_model/_base.py,5,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/sklearn/linear_model/_base.py,6,Fabian Pedregosa <fabian.pedregosa@inria.fr>,
scikit-learn/sklearn/linear_model/_base.py,7,Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/sklearn/linear_model/_base.py,8,Vincent Michel <vincent.michel@inria.fr>,
scikit-learn/sklearn/linear_model/_base.py,9,Peter Prettenhofer <peter.prettenhofer@gmail.com>,
scikit-learn/sklearn/linear_model/_base.py,10,Mathieu Blondel <mathieu@mblondel.org>,
scikit-learn/sklearn/linear_model/_base.py,11,Lars Buitinck,
scikit-learn/sklearn/linear_model/_base.py,12,Maryan Morel <maryan.morel@polytechnique.edu>,
scikit-learn/sklearn/linear_model/_base.py,13,Giorgio Patrini <giorgio.patrini@anu.edu.au>,
scikit-learn/sklearn/linear_model/_base.py,14,License: BSD 3 clause,
scikit-learn/sklearn/linear_model/_base.py,40,TODO: bayesian_ridge_regression and bayesian_regression_ard,
scikit-learn/sklearn/linear_model/_base.py,41,should be squashed into its respective objects.,
scikit-learn/sklearn/linear_model/_base.py,44,For sparse data intercept updates are scaled by this decay factor to avoid,
scikit-learn/sklearn/linear_model/_base.py,45,intercept oscillation.,
scikit-learn/sklearn/linear_model/_base.py,79,seed should never be 0 in SequentialDataset64,
scikit-learn/sklearn/linear_model/_base.py,145,TODO: f_normalize could be used here as well but the function,
scikit-learn/sklearn/linear_model/_base.py,146,inplace_csr_row_normalize_l2 must be changed such that it,
scikit-learn/sklearn/linear_model/_base.py,147,can return also the norms computed internally,
scikit-learn/sklearn/linear_model/_base.py,149,transform variance to norm in-place,
scikit-learn/sklearn/linear_model/_base.py,179,TODO: _rescale_data should be factored into _preprocess_data.,
scikit-learn/sklearn/linear_model/_base.py,180,"Currently, the fact that sag implements its own way to deal with",
scikit-learn/sklearn/linear_model/_base.py,181,sample_weight makes the refactoring tricky.,
scikit-learn/sklearn/linear_model/_base.py,249,"XXX Should this derive from LinearModel? It should be a mixin, not an ABC.",
scikit-learn/sklearn/linear_model/_base.py,250,Maybe the n_features checking can be moved to LinearModel.,
scikit-learn/sklearn/linear_model/_base.py,322,"OvR normalization, like LibLinear's predict_probability",
scikit-learn/sklearn/linear_model/_base.py,514,Sample weight can be implemented via a simple rescaling.,
scikit-learn/sklearn/linear_model/_base.py,535,"sparse_lstsq cannot handle y with shape (M, K)",
scikit-learn/sklearn/linear_model/_base.py,565,copy is not needed here as X is not modified inplace when X is sparse,
scikit-learn/sklearn/linear_model/_base.py,571,copy was done in fit if necessary,
scikit-learn/sklearn/linear_model/_base.py,584,recompute Gram,
scikit-learn/sklearn/linear_model/_base.py,588,precompute if n_samples > n_features,
scikit-learn/sklearn/linear_model/_base.py,593,make sure that the 'precompute' array is contiguous.,
scikit-learn/sklearn/linear_model/_base.py,599,cannot use Xy if precompute is not Gram,
scikit-learn/sklearn/linear_model/_base.py,604,"Xy is 1d, make sure it is contiguous.",
scikit-learn/sklearn/linear_model/_base.py,608,Make sure that Xy is always F contiguous even if X or y are not,
scikit-learn/sklearn/linear_model/_base.py,609,contiguous: the goal is to make it fast to extract the data for a,
scikit-learn/sklearn/linear_model/_base.py,610,specific target.,
scikit-learn/sklearn/linear_model/_omp.py,4,Author: Vlad Niculae,
scikit-learn/sklearn/linear_model/_omp.py,5,,
scikit-learn/sklearn/linear_model/_omp.py,6,License: BSD 3 clause,
scikit-learn/sklearn/linear_model/_omp.py,72,"even if we are allowed to overwrite, still copy it if bad order",
scikit-learn/sklearn/linear_model/_omp.py,83,keeping track of swapping,
scikit-learn/sklearn/linear_model/_omp.py,95,atom already selected or inner product too small,
scikit-learn/sklearn/linear_model/_omp.py,100,Updates the Cholesky decomposition of X' X,
scikit-learn/sklearn/linear_model/_omp.py,109,selected atoms are dependent,
scikit-learn/sklearn/linear_model/_omp.py,121,solves LL'x = X'y as a composition of two triangular systems,
scikit-learn/sklearn/linear_model/_omp.py,201,keeping track of swapping,
scikit-learn/sklearn/linear_model/_omp.py,219,"selected same atom twice, or inner product too small",
scikit-learn/sklearn/linear_model/_omp.py,231,selected atoms are dependent,
scikit-learn/sklearn/linear_model/_omp.py,243,solves LL'x = X'y as a composition of two triangular systems,
scikit-learn/sklearn/linear_model/_omp.py,351,subsequent targets will be affected,
scikit-learn/sklearn/linear_model/_omp.py,354,default for n_nonzero_coefs is 0.1 * n_features,
scikit-learn/sklearn/linear_model/_omp.py,355,but at least one.,
scikit-learn/sklearn/linear_model/_omp.py,488,or subsequent target will be affected,
scikit-learn/sklearn/linear_model/_omp.py,495,Make the copy once instead of many times in _gram_omp itself.,
scikit-learn/sklearn/linear_model/_omp.py,655,default for n_nonzero_coefs is 0.1 * n_features,
scikit-learn/sklearn/linear_model/_omp.py,656,but at least one.,
scikit-learn/sklearn/linear_model/tests/test_sag.py,1,Authors: Danny Sullivan <dbsullivan23@gmail.com>,
scikit-learn/sklearn/linear_model/tests/test_sag.py,2,Tom Dupre la Tour <tom.dupre-la-tour@m4x.org>,
scikit-learn/sklearn/linear_model/tests/test_sag.py,3,,
scikit-learn/sklearn/linear_model/tests/test_sag.py,4,License: BSD 3 clause,
scikit-learn/sklearn/linear_model/tests/test_sag.py,32,this is used for sag classification,
scikit-learn/sklearn/linear_model/tests/test_sag.py,35,approximately equal and saves the computation of the log,
scikit-learn/sklearn/linear_model/tests/test_sag.py,47,this is used for sag regression,
scikit-learn/sklearn/linear_model/tests/test_sag.py,56,function for measuring the log loss,
scikit-learn/sklearn/linear_model/tests/test_sag.py,81,sparse data has a fixed decay of .01,
scikit-learn/sklearn/linear_model/tests/test_sag.py,88,idx = k,
scikit-learn/sklearn/linear_model/tests/test_sag.py,142,sparse data has a fixed decay of .01,
scikit-learn/sklearn/linear_model/tests/test_sag.py,149,idx = k,
scikit-learn/sklearn/linear_model/tests/test_sag.py,245,SAGA variance w.r.t. stream order is higher,
scikit-learn/sklearn/linear_model/tests/test_sag.py,405,TODO: uncomment when sparse Ridge with intercept will be fixed (#4710),
scikit-learn/sklearn/linear_model/tests/test_sag.py,406,"assert_array_almost_equal(clf2.coef_.ravel(),",
scikit-learn/sklearn/linear_model/tests/test_sag.py,407,"spweights2.ravel(),",
scikit-learn/sklearn/linear_model/tests/test_sag.py,408,decimal=3),
scikit-learn/sklearn/linear_model/tests/test_sag.py,409,"assert_almost_equal(clf2.intercept_, spintercept2, decimal=1)'''",
scikit-learn/sklearn/linear_model/tests/test_sag.py,416,sum the squares of the second sample because that's the largest,
scikit-learn/sklearn/linear_model/tests/test_sag.py,466,simple linear function without noise,
scikit-learn/sklearn/linear_model/tests/test_sag.py,479,simple linear function with noise,
scikit-learn/sklearn/linear_model/tests/test_sag.py,762,test if the multinomial loss and gradient computations are consistent,
scikit-learn/sklearn/linear_model/tests/test_sag.py,773,compute loss and gradient like in multinomial SAG,
scikit-learn/sklearn/linear_model/tests/test_sag.py,778,compute loss and gradient like in multinomial LogisticRegression,
scikit-learn/sklearn/linear_model/tests/test_sag.py,787,comparison,
scikit-learn/sklearn/linear_model/tests/test_sag.py,793,"n_samples, n_features, n_classes = 4, 2, 3",
scikit-learn/sklearn/linear_model/tests/test_sag.py,820,ground truth,
scikit-learn/sklearn/linear_model/tests/test_sag.py,830,"Following #13316, the error handling behavior changed in cython sag. This",
scikit-learn/sklearn/linear_model/tests/test_sag.py,831,is simply a non-regression test to make sure numerical errors are,
scikit-learn/sklearn/linear_model/tests/test_sag.py,832,properly raised.,
scikit-learn/sklearn/linear_model/tests/test_sag.py,834,Train a classifier on a simple problem,
scikit-learn/sklearn/linear_model/tests/test_sag.py,840,Trigger a numerical error by:,
scikit-learn/sklearn/linear_model/tests/test_sag.py,841,- corrupting the fitted coefficients of the classifier,
scikit-learn/sklearn/linear_model/tests/test_sag.py,842,- fit it again starting from its current state thanks to warm_start,
scikit-learn/sklearn/linear_model/tests/test_passive_aggressive.py,106,Classifier can be retrained on different labels and features.,
scikit-learn/sklearn/linear_model/tests/test_passive_aggressive.py,137,Test class weights.,
scikit-learn/sklearn/linear_model/tests/test_passive_aggressive.py,147,we give a small weights to class 1,
scikit-learn/sklearn/linear_model/tests/test_passive_aggressive.py,153,now the hyperplane should rotate clock-wise and,
scikit-learn/sklearn/linear_model/tests/test_passive_aggressive.py,154,the prediction on this point should shift,
scikit-learn/sklearn/linear_model/tests/test_passive_aggressive.py,159,partial_fit with class_weight='balanced' not supported,
scikit-learn/sklearn/linear_model/tests/test_passive_aggressive.py,171,"Already balanced, so ""balanced"" weights should have no effect",
scikit-learn/sklearn/linear_model/tests/test_passive_aggressive.py,180,should be similar up to some epsilon due to learning rate schedule,
scikit-learn/sklearn/linear_model/tests/test_passive_aggressive.py,186,ValueError due to wrong class_weight label.,
scikit-learn/sklearn/linear_model/tests/test_passive_aggressive.py,196,ValueError due to wrong class_weight argument type.,
scikit-learn/sklearn/linear_model/tests/test_passive_aggressive.py,270,TODO: remove in 0.25,
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,15,Check that the sparse_coef property works,
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,24,Check that the normalize option in enet works,
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,37,Check that the sparse lasso can handle zero data without crashing,
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,49,Test ElasticNet for various values of alpha and l1_ratio with list X,
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,53,just a straight line,
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,54,test sample,
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,56,this should be the same as unregularized least squares,
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,58,catch warning about alpha=0.,
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,59,this is discouraged but should work.,
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,82,Test ElasticNet for various values of alpha and l1_ratio with sparse X,
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,84,training samples,
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,87,"X[1, 0] = 0",
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,89,just a straight line (the identity function),
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,91,test samples,
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,97,this should be the same as lasso,
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,124,build an ill-posed linear regression problem with many noisy features and,
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,125,comparatively few samples,
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,127,generate a ground truth model,
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,129,only the top features are impacting the model,
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,135,50% of zeros in input signal,
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,137,generate training ground truth labels,
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,163,check the convergence is the same as the dense version,
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,175,check that the coefs are sparse,
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,204,check the convergence is the same as the dense version,
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,210,check that the coefs are sparse,
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,219,XXX: There is a bug when precompute is not None!,
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,238,new params,
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,243,compare with dense data,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,67,Simple sanity check on a 2 classes dataset,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,68,Make sure it predicts the correct result on simple datasets.,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,82,Test for appropriate exception on errors,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,123,Cs[2] has the highest score (0.8) from MockScorer,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,126,scorer called 8 times (cv*len(Cs)),
scikit-learn/sklearn/linear_model/tests/test_logistic.py,129,reset mock_scorer,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,165,Test logistic regression with the iris dataset,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,170,Test that both multinomial and OvR solvers handle,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,171,multiclass data correctly and give good accuracy,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,172,score (>0.95) for the training data.,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,218,only 'liblinear' solver,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,223,all solvers except 'liblinear' and 'saga',
scikit-learn/sklearn/linear_model/tests/test_logistic.py,235,only saga supports elasticnet. We only test for liblinear because the,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,236,error is raised before for the other solvers (solver %s supports only l2,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,237,penalties),
scikit-learn/sklearn/linear_model/tests/test_logistic.py,244,liblinear does not support penalty='none',
scikit-learn/sklearn/linear_model/tests/test_logistic.py,252,Test multinomial LR on a binary problem.,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,273,Test multinomial LR gives expected probabilities based on the,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,274,"decision function, for a binary problem.",
scikit-learn/sklearn/linear_model/tests/test_logistic.py,290,Test sparsify and densify members.,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,313,Test that an exception is raised on inconsistent input,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,321,Wrong dimensions for training data,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,325,Wrong dimensions for test data,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,331,Test that we can write to coef_ and intercept_,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,340,Test proper NaN handling.,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,341,Regression test for Issue #252: fit used to go into an infinite loop.,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,349,Test that the path algorithm is consistent,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,356,can't test with fit_intercept=True since LIBLINEAR,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,357,penalizes the intercept,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,371,test for fit_intercept=True,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,392,Check that the convergence message points to both a model agnostic,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,393,advice (scaling the data) and to the logistic regression specific,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,394,documentation that includes hints on the solver configuration.,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,408,random_state is relevant for liblinear solver only if dual=True,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,420,same result for same random state,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,422,different results for different random states,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,438,First check that our derivation of the grad is correct,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,445,Second check that our intercept implementation is good,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,471,First check that _logistic_grad_hess is consistent,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,472,with _logistic_loss_and_grad,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,477,Now check our hessian along the second direction of the grad,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,482,Computation of the Hessian is particularly fragile to numerical,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,483,errors when doing simple finite differences. Here we compute the,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,484,grad along a path in the direction of the vector and then use a,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,485,least-square regression to estimate the slope,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,498,Second check that our intercept implementation is good,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,508,test for LogisticRegressionCV object,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,537,no need to test for micro averaging because it,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,538,"is the same as accuracy for f1, precision,",
scikit-learn/sklearn/linear_model/tests/test_logistic.py,539,and recall (see https://github.com/,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,540,scikit-learn/scikit-learn/pull/,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,541,11578#discussion_r203250062),
scikit-learn/sklearn/linear_model/tests/test_logistic.py,546,test that LogisticRegressionCV uses the right score to compute its,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,547,cross-validation scores when using a multinomial scoring,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,548,see https://github.com/scikit-learn/scikit-learn/issues/8720,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,553,we use lbfgs to support multinomial,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,555,we store the params to set them further in _log_reg_scoring_path,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,568,Test with string labels for LogisticRegression(CV),
scikit-learn/sklearn/linear_model/tests/test_logistic.py,574,"For numerical labels, let y values be taken from set (-1, 0, 1)",
scikit-learn/sklearn/linear_model/tests/test_logistic.py,576,Test for string labels,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,593,The predictions should be in original labels,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,597,Make sure class weights can be given with string labels,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,624,Fit intercept case.,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,630,Do not fit intercept. This can be considered equivalent to adding,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,631,"a feature vector of ones, i.e column of one vectors.",
scikit-learn/sklearn/linear_model/tests/test_logistic.py,636,"In the fit_intercept=False case, the feature vector of ones is",
scikit-learn/sklearn/linear_model/tests/test_logistic.py,637,penalized. This should be taken care of.,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,640,Check gradient.,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,653,Test that OvR and multinomial are correct using the iris dataset.,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,657,The cv indices from stratified kfold (where stratification is done based,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,658,"on the fine-grained iris classes, i.e, before the classes 0 and 1 are",
scikit-learn/sklearn/linear_model/tests/test_logistic.py,659,conflated) is used for both clf and clf1,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,664,Train clf on the original dataset where classes 0 and 1 are separated,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,668,Conflate classes 0 and 1 and train clf1 on this modified dataset,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,674,Ensure that what OvR learns for class2 is same regardless of whether,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,675,classes 0 and 1 are separated or not,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,680,Test the shape of various attributes.,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,689,Test that for the iris data multinomial gives a better accuracy than OvR,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,701,Test attributes of LogisticRegressionCV,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,820,Test that passing sample_weight as ones is the same as,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,821,not passing them at all (default None),
scikit-learn/sklearn/linear_model/tests/test_logistic.py,830,"Test that sample weights work the same with the lbfgs,",
scikit-learn/sklearn/linear_model/tests/test_logistic.py,831,"newton-cg, and 'sag' solvers",
scikit-learn/sklearn/linear_model/tests/test_logistic.py,837,ignore convergence warning due to small dataset,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,849,"Test that passing class_weight as [1,2] is the same as",
scikit-learn/sklearn/linear_model/tests/test_logistic.py,850,"passing class weight = [1,1] but adjusting sample weights",
scikit-learn/sklearn/linear_model/tests/test_logistic.py,851,to be 2 for all instances of class 2,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,860,Test the above for l1 penalty and l2 penalty with dual=True.,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,861,since the patched liblinear code is different.,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,884,helper for returning a dictionary instead of an array,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,892,Multinomial case: remove 90% of class 0,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,907,Binary case: remove 90% of class 0 and 100% of class 2,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,924,Tests for the multinomial option in logistic regression,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,926,Some basic attributes of Logistic Regression,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,935,'lbfgs' is used as a referenced,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,956,Compare solutions between lbfgs and the other solvers,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,961,Test that the path give almost the same results. However since in this,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,962,case we take the average of the coefs after fitting across all the,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,963,"folds, it need not be exactly the same.",
scikit-learn/sklearn/linear_model/tests/test_logistic.py,984,extract first column of hessian matrix,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,989,Estimate hessian using least squares as done in,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,990,test_logistic_grad_hess,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1004,Test negative prediction when decision_function values are zero.,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1005,Liblinear predicts the positive class when decision_function values,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1006,are zero. This is a test to verify that we do not do the same.,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1007,See Issue: https://github.com/scikit-learn/scikit-learn/issues/3600,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1008,and the PR https://github.com/scikit-learn/scikit-learn/pull/3623,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1014,Dummy data such that the decision function becomes zero.,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1020,Test LogRegCV with solver='liblinear' works for sparse matrices,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1028,Test LogRegCV with solver='liblinear' works for sparse matrices,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1036,Test that the right error message is thrown when intercept_scaling <= 0,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1048,Test that intercept_scaling is ignored when fit_intercept is False,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1056,"Because liblinear penalizes the intercept and saga does not, we do not",
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1057,fit the intercept to make it possible to compare the coefficients of,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1058,the two models at convergence.,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1077,Noise and constant features should be regularized to zero by the l1,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1078,penalty,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1084,"Because liblinear penalizes the intercept and saga does not, we do not",
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1085,fit the intercept to make it possible to compare the coefficients of,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1086,the two models at convergence.,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1107,Noise and constant features should be regularized to zero by the l1,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1108,penalty,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1112,Check that solving on the sparse and dense data yield the same results,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1123,"Test that when refit=True, logistic regression cv with the saga solver",
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1124,converges to the same solution as logistic regression with a fixed,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1125,regularization parameter.,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1126,Internally the LogisticRegressionCV model uses a warm start to refit on,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1127,the full data model with the optimal C found by CV. As the penalized,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1128,"logistic regression loss is convex, we should still recover exactly",
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1129,the same solution as long as the stopping criterion is strict enough (and,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1130,that there are no exactly duplicated features when penalty='l1').,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1151,Predicted probabilities using the true-entropy loss should give a,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1152,smaller loss than those using the ovr method.,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1161,Predicted probabilities using the soft-max function should give a,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1162,smaller loss than those using the logistic function.,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1169,Test that the maximum number of iteration is reached,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1190,Test that self.n_iter_ has the correct format.,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1199,OvR case,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1216,multinomial case,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1241,A 1-iteration second fit on same data should give almost same result,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1242,"with warm starting, and quite different result without warm starting.",
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1243,Warm starting does not work with liblinear solver.,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1284,alpha=1e-3 is time consuming,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1304,Convergence for alpha=1e-3 is very slow,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1312,Test that np.float32 input data is not cast to np.float64 when possible,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1313,and that the output is approximately the same no matter the input format.,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1332,Check 32-bit type consistency,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1337,Check 32-bit type consistency with sparsity,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1342,Check 64-bit type consistency,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1347,Check 64-bit type consistency with sparsity,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1352,solver_tol bounds the norm of the loss gradient,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1353,"dw ~= inv(H)*grad ==> |dw| ~= |inv(H)| * solver_tol, where H - hessian",
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1354,,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1355,See https://github.com/scikit-learn/scikit-learn/pull/13645,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1356,,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1357,"with  Z = np.hstack((np.ones((3,1)), np.array(X)))",
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1358,"In [8]: np.linalg.norm(np.diag([0,2,2]) + np.linalg.inv((Z.T @ Z)/4))",
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1359,Out[8]: 1.7193336918135917,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1361,factor of 2 to get the ball diameter,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1364,FIXME,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1367,Check accuracy consistency,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1371,FIXME: SAGA on sparse data fits the intercept inaccurately with the,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1372,default tol and max_iter parameters.,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1380,"Test to see that the logistic regression converges on warm start,",
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1381,with multi_class='multinomial'. Non-regressive test for #10836,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1401,make sure elasticnet penalty gives different coefficients from l1 and l2,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1402,with saga solver (l1_ratio different from 0 or 1),
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1415,make sure coeffs differ by at least .1,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1426,Make sure elasticnet is equivalent to l1 when l1_ratio=1 and to l2 when,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1427,l1_ratio=0.,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1442,Make sure that elasticnet with grid search on l1_ratio gives same or,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1443,better results than just l1 or just l2.,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1469,Check that training with a penalty matching the objective leads,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1470,to a lower objective.,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1471,Here we train a logistic regression with l2 (a) and elasticnet (b),
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1472,"penalties, and compute the elasticnet objective. That of a should be",
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1473,greater than that of b (both objectives are convex).,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1499,make sure LogisticRegressionCV gives same best params (l1 and C) as,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1500,GridSearchCV when penalty is elasticnet,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1503,"This is actually binary classification, ovr multiclass is treated in",
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1504,test_LogisticRegressionCV_GridSearchCV_elastic_net_ovr,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1531,make sure LogisticRegressionCV gives same best params (l1 and C) as,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1532,GridSearchCV when penalty is elasticnet and multiclass is ovr. We can't,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1533,compare best_params like in the previous test because,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1534,LogisticRegressionCV with multi_class='ovr' will have one C and one,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1535,"l1_param for each class, while LogisticRegression will share the",
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1536,parameters over the *n_classes* classifiers.,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1557,Check that predictions are 80% the same,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1565,Test LogisticRegressionCV attribute shapes when refit is False,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1589,Make sure the shapes of scores_ and coefs_paths_ attributes are correct,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1590,when using elasticnet (added one dimension for l1_ratios),
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1652,Compare elasticnet penalty in LogisticRegression() and SGD(loss='log'),
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1672,Make sure that the returned coefs by logistic_regression_path when,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1673,multi_class='multinomial' don't override each other (used to be a,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1674,bug).,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1699,check multi_class='auto' => multi_class='ovr' iff binary y or liblinear,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1731,Make sure multi_class='ovr' is distinct from ='multinomial',
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1742,- Make sure warning is raised if penalty='none' and C is set to a,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1743,non-default value.,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1744,- Make sure setting penalty='none' is equivalent to setting C=np.inf with,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1745,l2 penalty.,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1776,check that we support sample_weight with liblinear in all possible cases:,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1777,"l1-primal, l2-primal, l2-dual",
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1803,Non regression test for issue #14955.,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1804,when penalty is elastic net the scores_ attribute has shape,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1805,"(n_classes, n_Cs, n_l1_ratios)",
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1806,We here make sure that the second dimension indeed corresponds to Cs and,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1807,the third dimension corresponds to l1_ratios.,
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1820,average over folds,
scikit-learn/sklearn/linear_model/tests/test_huber.py,1,Authors: Manoj Kumar mks542@nyu.edu,
scikit-learn/sklearn/linear_model/tests/test_huber.py,2,License: BSD 3 clause,
scikit-learn/sklearn/linear_model/tests/test_huber.py,19,Generate data with outliers by replacing 10% of the samples with noise.,
scikit-learn/sklearn/linear_model/tests/test_huber.py,24,Replace 10% of the sample with noise.,
scikit-learn/sklearn/linear_model/tests/test_huber.py,32,Test that Ridge matches LinearRegression for large epsilon,
scikit-learn/sklearn/linear_model/tests/test_huber.py,50,Test that the gradient calculated by _huber_loss_and_gradient is correct,
scikit-learn/sklearn/linear_model/tests/test_huber.py,61,Check using optimize.check_grad that the gradients are equal.,
scikit-learn/sklearn/linear_model/tests/test_huber.py,63,Check for both fit_intercept and otherwise.,
scikit-learn/sklearn/linear_model/tests/test_huber.py,73,"Test sample_weights implementation in HuberRegressor""""""",
scikit-learn/sklearn/linear_model/tests/test_huber.py,81,Rescale coefs before comparing with assert_array_almost_equal to make,
scikit-learn/sklearn/linear_model/tests/test_huber.py,82,sure that the number of decimal places used is somewhat insensitive to,
scikit-learn/sklearn/linear_model/tests/test_huber.py,83,the amplitude of the coefficients and therefore to the scale of the,
scikit-learn/sklearn/linear_model/tests/test_huber.py,84,data and the regularization parameter,
scikit-learn/sklearn/linear_model/tests/test_huber.py,108,Test sparse implementation with sample weights.,
scikit-learn/sklearn/linear_model/tests/test_huber.py,129,Test that outliers filtering is scaling independent.,
scikit-learn/sklearn/linear_model/tests/test_huber.py,146,Test they should converge to same coefficients for same parameters,
scikit-learn/sklearn/linear_model/tests/test_huber.py,150,Fit once to find out the scale parameter. Scale down X and y by scale,
scikit-learn/sklearn/linear_model/tests/test_huber.py,151,so that the scale parameter is optimized to 1.0,
scikit-learn/sklearn/linear_model/tests/test_huber.py,176,"SciPy performs the tol check after doing the coef updates, so",
scikit-learn/sklearn/linear_model/tests/test_huber.py,177,these would be almost same but not equal.,
scikit-learn/sklearn/linear_model/tests/test_huber.py,184,"Test that huber returns a better r2 score than non-outliers""""""",
scikit-learn/sklearn/linear_model/tests/test_huber.py,193,The Ridge regressor should be influenced by the outliers and hence,
scikit-learn/sklearn/linear_model/tests/test_huber.py,194,give a worse score on the non-outliers as compared to the huber,
scikit-learn/sklearn/linear_model/tests/test_huber.py,195,regressor.,
scikit-learn/sklearn/linear_model/tests/test_huber.py,202,The huber model should also fit poorly on the outliers.,
scikit-learn/sklearn/linear_model/tests/test_huber.py,207,Test that it does not crash with bool data,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,21,TODO: use another dataset that has multiple drops,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,30,Principle of Lars is to keep covariances tied and decreasing,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,32,also test verbose output,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,53,no more than max_pred variables can go into the active set,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,60,"The same, with precomputed Gram matrix",
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,74,no more than max_pred variables can go into the active set,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,97,Test that lars_path with no X and Gram raises exception,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,104,Test that lars_path with precomputed Gram and Xy gives the right answer,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,116,numpy deprecation,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,118,Test that Lars gives least square solution at the end,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,119,of the path,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,120,use un-normalized dataset,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,123,Avoid FutureWarning about default value change when numpy >= 1.14,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,130,numpy deprecation,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,132,Test that Lars Lasso gives least square solution at the end,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,133,of the path,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,140,Check that lars_path is robust to collinearity in input,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,151,just make sure it's bounded,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,164,Test that the ``return_path=False`` option returns the correct output,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,175,Test that the ``return_path=False`` option with Gram remains correct,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,186,Test that the ``return_path=False`` option with Gram and Xy remains,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,187,correct,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,204,Check for different values of precompute,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,216,Test when input is a singular matrix,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,224,consistency test that checks that LARS Lasso is handling rank,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,225,deficient input data (with n_features < rank) in the same way,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,226,as coordinate descent Lasso,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,236,"To be able to use the coefs to compute the objective function,",
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,237,we need to turn off normalization,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,251,Test that LassoLars and Lasso using coordinate descent give the,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,252,same results.,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,265,"similar test, with the classifiers",
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,273,"same test, with normalized data",
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,288,Test that LassoLars and Lasso using coordinate descent give the,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,289,same results when early stopping is used.,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,290,"(test : before, in the middle, and in the last part of the path)",
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,302,"same test, with normalization",
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,314,Test that the path length of the LassoLars is right,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,320,Also check that the sequence of alphas is always decreasing,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,325,"Test lasso lars on a very ill-conditioned design, and check that",
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,326,"it does not blow up, and stays somewhat close to a solution given",
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,327,by the coordinate descent solver,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,328,Also test that lasso_path (using lars_path output style) gives,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,329,the same result as lars_path and previous lasso output style,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,330,under these conditions.,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,333,Generate data,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,357,Create an ill-conditioned situation in which the LARS has to go,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,358,"far in the path to converge, and check that LARS and coordinate",
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,359,descent give the same answers,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,360,Note it used to be the case that Lars had to use the drop for good,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,361,strategy for this but this is no longer the case with the,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,362,equality_tolerance checks,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,386,assure that at least some features get added if necessary,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,387,test for 6d2b4c,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,388,Hilbert matrix,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,400,The path should be of length 6 + 1 in a Lars going down to 6,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,401,non-zero coefs,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,407,Assure that estimators receiving multidimensional y do the right thing,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,413,regression test for gh-1615,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,434,Test the LassoLarsCV object by checking that the optimal alpha,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,435,increases as the number of samples increases.,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,436,This property is not actually guaranteed in general and is just a,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,437,"property of the given dataset, with the given steps chosen.",
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,457,add correlated features,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,460,Check that there is no warning in general and no ConvergenceWarning,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,461,in particular.,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,462,Materialize the string representation of the warning to get a more,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,463,informative error message in case of AssertionError.,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,469,Test the LassoLarsIC object by checking that,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,470,- some good features are selected.,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,471,- alpha_bic > alpha_aic,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,472,- n_nonzero_bic < n_nonzero_aic,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,477,add 5 bad features,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,486,test error on unknown IC,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,492,"When using automated memory mapping on large input, the",
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,493,fold data is in read-only mode,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,494,This is a non-regression test for:,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,495,https://github.com/scikit-learn/scikit-learn/issues/4597,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,498,The following should not fail despite copy=False,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,503,this is the main test for the positive parameter on the lars_path method,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,504,the estimator classes just make use of this function,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,506,we do the test on the diabetes dataset,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,508,ensure that we get negative coefficients when positive=False,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,509,and all positive when positive=True,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,510,for method 'lar' (default) and lasso,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,529,now we gonna test the positive option for all estimator classes,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,539,testing the transmissibility for the positive option of all estimator,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,540,classes in this same function here,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,558,Test that LassoLars and Lasso using coordinate descent give the,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,559,same results when using the positive option,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,561,This test is basically a copy of the above with additional positive,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,562,"option. However for the middle part, the comparison of coefficient values",
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,563,"for a range of alphas, we had to make an adaptations. See below.",
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,565,not normalized data,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,579,The range of alphas chosen for coefficient comparison here is restricted,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,580,as compared with the above test without the positive option. This is due,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,581,to the circumstance that the Lars-Lasso algorithm does not converge to,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,582,"the least-squares-solution for small alphas, see 'Least Angle Regression'",
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,583,by Efron et al 2004. The coefficients are typically in congruence up to,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,584,the smallest alpha reached by the Lars-Lasso algorithm and start to,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,585,diverge thereafter.  See,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,586,https://gist.github.com/michigraber/7e7d7c75eca694c7a6ff,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,596,normalized data,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,602,don't include alpha=0,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,610,Test that sklearn LassoLars implementation agrees with the LassoLars,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,611,implementation available in R (lars library) under the following,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,612,scenarios:,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,613,1) fit_intercept=False and normalize=False,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,614,2) fit_intercept=True and normalize=True,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,616,Let's generate the data used in the bug report 7778,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,628,,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,629,Scenario 1: Let's compare R vs sklearn when fit_intercept=False and,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,630,normalize=False,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,631,,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,632,,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,633,The R result was obtained using the following code:,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,634,,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,635,library(lars),
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,636,"model_lasso_lars = lars(X, t(y), type=""lasso"", intercept=FALSE,",
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,637,"trace=TRUE, normalize=FALSE)",
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,638,r = t(model_lasso_lars$beta),
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,639,,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,664,,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,666,,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,667,Scenario 2: Let's compare R vs sklearn when fit_intercept=True and,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,668,normalize=True,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,669,,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,670,"Note: When normalize is equal to True, R returns the coefficients in",
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,671,"their original units, that is, they are rescaled back, whereas sklearn",
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,672,"does not do that, therefore, we need to do this step before comparing",
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,673,their results.,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,674,,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,675,,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,676,The R result was obtained using the following code:,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,677,,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,678,library(lars),
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,679,"model_lasso_lars2 = lars(X, t(y), type=""lasso"", intercept=TRUE,",
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,680,"trace=TRUE, normalize=TRUE)",
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,681,r2 = t(model_lasso_lars2$beta),
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,695,Let's rescale back the coefficients returned by sklearn before comparing,
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,696,against the R result (read the note above),
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,702,,
scikit-learn/sklearn/linear_model/tests/test_base.py,1,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/sklearn/linear_model/tests/test_base.py,2,Fabian Pedregosa <fabian.pedregosa@inria.fr>,
scikit-learn/sklearn/linear_model/tests/test_base.py,3,,
scikit-learn/sklearn/linear_model/tests/test_base.py,4,License: BSD 3 clause,
scikit-learn/sklearn/linear_model/tests/test_base.py,33,Test LinearRegression on a simple dataset.,
scikit-learn/sklearn/linear_model/tests/test_base.py,34,a simple dataset,
scikit-learn/sklearn/linear_model/tests/test_base.py,45,test it also for degenerate input,
scikit-learn/sklearn/linear_model/tests/test_base.py,57,TODO: loop over sparse data as well,
scikit-learn/sklearn/linear_model/tests/test_base.py,61,It would not work with under-determined systems,
scikit-learn/sklearn/linear_model/tests/test_base.py,70,LinearRegression with explicit sample_weight,
scikit-learn/sklearn/linear_model/tests/test_base.py,76,sanity checks,
scikit-learn/sklearn/linear_model/tests/test_base.py,79,Closed form of the weighted least square,
scikit-learn/sklearn/linear_model/tests/test_base.py,80,theta = (X^T W X)^(-1) * X^T W y,
scikit-learn/sklearn/linear_model/tests/test_base.py,99,Sample weights must be either scalar or 1D,
scikit-learn/sklearn/linear_model/tests/test_base.py,113,"make sure the ""OK"" sample weights actually work",
scikit-learn/sklearn/linear_model/tests/test_base.py,120,Test assertions on betas shape.,
scikit-learn/sklearn/linear_model/tests/test_base.py,142,Test that linear regression also works with sparse data,
scikit-learn/sklearn/linear_model/tests/test_base.py,160,Test that linear regression agrees between sparse and dense,
scikit-learn/sklearn/linear_model/tests/test_base.py,178,Test multiple-outcome linear regressions,
scikit-learn/sklearn/linear_model/tests/test_base.py,194,Test multiple-outcome linear regressions with sparse data,
scikit-learn/sklearn/linear_model/tests/test_base.py,212,restrict the pd versions < '0.24.0' as they have a bug in is_sparse func,
scikit-learn/sklearn/linear_model/tests/test_base.py,216,Warning is raised only when some of the columns is sparse,
scikit-learn/sklearn/linear_model/tests/test_base.py,221,all columns but the first column is sparse,
scikit-learn/sklearn/linear_model/tests/test_base.py,231,does not warn when the whole dataframe is sparse,
scikit-learn/sklearn/linear_model/tests/test_base.py,309,"XXX: if normalize=True, should we expect a weighted standard deviation?",
scikit-learn/sklearn/linear_model/tests/test_base.py,310,"Currently not weighted, but calculated with respect to weighted mean",
scikit-learn/sklearn/linear_model/tests/test_base.py,336,random_state not supported yet in sparse.rand,
scikit-learn/sklearn/linear_model/tests/test_base.py,337,", random_state=rng",
scikit-learn/sklearn/linear_model/tests/test_base.py,372,"Test output format of _preprocess_data, when input is csr",
scikit-learn/sklearn/linear_model/tests/test_base.py,502,array,
scikit-learn/sklearn/linear_model/tests/test_base.py,514,csr,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,1,Authors: Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,2,Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,3,License: BSD 3 clause,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,70,Check that the lasso can handle zero data without crashing,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,81,Test Lasso on a toy example for various values of alpha.,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,82,When validating this against glmnet notice that glmnet divides it,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,83,against nobs.,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,86,just a straight line,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,87,test sample,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,119,Test ElasticNet for various parameters of alpha and l1_ratio.,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,120,"Actually, the parameters alpha = 0 should not be allowed. However,",
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,121,we test it as a border case.,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,122,ElasticNet is tested with and without precomputed Gram matrix,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,125,just a straight line,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,126,test sample,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,128,this should be the same as lasso,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,145,with Gram,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,152,with Gram,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,196,Check that the lars and the coordinate descent implementation,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,197,select a similar alpha,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,199,for this we check that they don't fall in the grid of,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,200,clf.alphas further than 1,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,203,check that they also give a similar MSE,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,208,test set,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,234,Ensure the unconstrained fit has a negative coefficient,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,240,"On same data, constrained fit has non-negative coefficients",
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,248,Test that lasso_path with lars_path style output gives the,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,249,same result,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,251,Some toy data,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,256,Use lars_path and lasso_path(new output) with 1D linear interpolation,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,257,to compute the same path,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,272,We use a large number of samples and of informative features so that,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,273,the l1_ratio selected is more toward ridge than lasso,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,278,"Here we have a small number of iterations, and thus the",
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,279,ElasticNet might not converge. This is to speed up tests,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,284,"Well-conditioned settings, we should have selected our",
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,285,smallest penalty,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,287,Non-sparse ground truth: we should have selected an elastic-net,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,288,that is closer to ridge than to lasso,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,296,"Well-conditioned settings, we should have selected our",
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,297,smallest penalty,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,299,Non-sparse ground truth: we should have selected an elastic-net,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,300,that is closer to ridge than to lasso,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,303,We are in well-conditioned settings with low noise: we should,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,304,have a good test-set performance,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,307,Multi-output/target case,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,312,We are in well-conditioned settings with low noise: we should,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,313,have a good test-set performance,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,317,Mono-output should have same cross-validated alpha_ and l1_ratio_,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,318,in both cases.,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,334,new params,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,344,do a second round with 5 iterations,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,353,just a straight line,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,361,just a straight line with negative slope,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,374,just a straight line with negative slope,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,385,Ensure the unconstrained fit has a negative coefficient,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,392,"On same data, constrained fit has non-negative coefficients",
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,433,"Y_test = np.c_[y_test, y_test]",
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,448,just a straight line,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,449,test sample,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,568,Precompute = 'auto' is not supported for ElasticNet,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,578,This dataset is not trivial enough for the model to converge in one pass.,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,581,Check that n_iter_ is invariant to multiple calls to fit,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,582,"when warm_start=False, all else being equal.",
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,587,"Fit the same model again, using a warm start: the optimizer just performs",
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,588,a single pass before checking that it has already converged,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,598,Train a model to converge on a lightly regularized problem,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,602,Fitting a new model on a more regularized version of the same problem.,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,603,Fitting with high regularization is easier it should converge faster,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,604,in general.,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,608,"Fit the solution to the original, less regularized version of the",
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,609,problem but from the solution of the highly regularized variant of,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,610,the problem as a better starting point. This should also converge,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,611,faster than the original model that starts from zero.,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,619,Test that both random and cyclic selection give the same results.,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,620,Ensure that the test models fully converge and check a wide,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,621,range of conditions.,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,623,This uses the coordinate descent algo using the gram trick.,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,632,This uses the descent algo without the gram trick,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,640,Sparse Case,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,648,Multioutput case.,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,658,Raise error when selection is not in cyclic or random.,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,664,Test positive parameter,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,668,For mono output,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,669,Test that the coefs returned by positive=True in enet_path are positive,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,674,"For multi output, positive parameter is not allowed",
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,675,Test that an error is raised,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,681,Test that dense and sparse input give the same input for descent paths.,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,695,Check that no error is raised if data is provided in the right format,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,697,"With check_input=False, an exhaustive check is not made on y but its",
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,698,dtype is still cast in _preprocess_data to X's dtype. So the test should,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,699,pass anyway,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,702,"With no input checking, providing X in C order should result in false",
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,703,computation,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,728,"No copying, X is overwritten",
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,757,Generate dataset,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,759,"Here we have a small number of iterations, and thus the",
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,760,ElasticNet might not converge. This is to speed up tests,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,780,test precompute Gram array,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,791,test multi task enet,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,811,Test that an error message is raised if an estimator that,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,812,uses _alpha_grid is called with l1_ratio=0,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,824,Test that l1_ratio=0 is allowed if we supply a grid manually,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,853,do a second round with 5 iterations,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,882,check that the model fails to converge,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,886,check that the model converges w/o warnings,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,900,check that the model converges w/o warnings,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,935,make it explicit that X is int,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,961,sample_weight=np.ones(..) should be equivalent to sample_weight=None,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,968,sample_weight=None should be equivalent to sample_weight = number,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,975,"scaling of sample_weight should have no effect, cf. np.average()",
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,982,setting one element of sample_weight to 0 is equivalent to removing,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,983,the corresponding sample,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,995,check that multiplying sample_weight by 2 is equivalent,
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,996,to repeating corresponding samples twice,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,65,XXX untested as of v0.22,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,91,Test Data,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,93,test sample 1,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,99,test sample 2; string class labels,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,107,test sample 3,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,114,test sample 4 - two more or less redundant feature groups,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,123,test sample 5 - test sample 1 as binary classification problem,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,129,,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,130,Common Test Case to classification and regression,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,132,a simple implementation of ASGD to use for testing,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,133,uses squared loss to find the gradient,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,145,sparse data has a fixed decay of .01,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,171,Check whether expected ValueError on bad alpha,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,178,Check whether expected ValueError on bad penalty,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,186,Check whether expected ValueError on bad loss,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,191,Test that explicit warm restart...,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,202,... and implicit warm restart are equivalent.,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,228,Input format tests.,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,240,Test whether clone works ok.,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,272,TODO: remove in 0.25,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,335,"Check whether expected ValueError on bad alpha, i.e. 0",
scikit-learn/sklearn/linear_model/tests/test_sgd.py,336,since alpha is used to compute the optimal learning rate,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,393,remove shuffling,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,404,test that n_iter_ increases monotonically with n_iter_no_change,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,417,test an error is raised if the training or validation set is empty,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,423,,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,424,Classification Test Case,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,428,Check that SGD gives any results :-),
scikit-learn/sklearn/linear_model/tests/test_sgd.py,434,"assert_almost_equal(clf.coef_[0], clf.coef_[1], decimal=7)",
scikit-learn/sklearn/linear_model/tests/test_sgd.py,440,Check whether expected ValueError on bad l1_ratio,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,446,Check whether expected ValueError on bad learning_rate,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,452,Check whether expected ValueError on bad eta0,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,459,Test parameter validity check,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,465,Test parameter validity check,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,471,Test parameter validity check,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,477,Test parameter validity check,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,483,Test parameter validity check,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,489,Checks coef_init not allowed as model argument (only fit),
scikit-learn/sklearn/linear_model/tests/test_sgd.py,490,Provided coef_ does not match dataset,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,496,Checks coef_init shape for the warm starts,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,497,Provided coef_ does not match dataset.,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,504,Checks intercept_ shape for the warm starts,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,505,Provided intercept_ does not match dataset.,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,512,Test parameter validity check,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,519,Checks intercept_ shape for the warm starts in binary case,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,525,Checks the SGDClassifier correctly computes the average weights,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,540,simple linear function without noise,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,556,Checks intercept_ shape consistency for the warm starts,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,557,Inconsistent intercept_ shape.,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,566,Target must have at least two labels,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,573,"partial_fit with class_weight='balanced' not supported""""""",
scikit-learn/sklearn/linear_model/tests/test_sgd.py,590,Multi-class test case,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,603,Multi-class average test case,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,626,Multi-class test case,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,638,Multi-class test case with multi-core support,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,649,Checks coef_init and intercept_init shape for multi-class,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,650,problems,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,651,Provided coef_ does not match dataset,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,655,Provided coef_ does match dataset,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,658,Provided intercept_ does not match dataset,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,663,Provided intercept_ does match dataset.,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,669,Checks that SGDClassifier predict_proba and predict_log_proba methods,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,670,can either be accessed or raise an appropriate error message,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,671,otherwise. See,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,672,https://github.com/scikit-learn/scikit-learn/issues/10938 for more,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,673,details.,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,694,Check SGD.predict_proba,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,696,Hinge loss does not allow for conditional prob estimate.,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,697,"We cannot use the factory here, because it defines predict_proba",
scikit-learn/sklearn/linear_model/tests/test_sgd.py,698,anyway.,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,704,log and modified_huber losses can output probability estimates,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,705,binary case,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,719,log loss multiclass probability estimates,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,740,Modified Huber multiclass probability estimates; requires a separate,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,741,test because the hard zero/one probabilities may destroy the,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,742,ordering present in decision_function output.,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,749,XXX the sparse test gets a different X2 (?),
scikit-learn/sklearn/linear_model/tests/test_sgd.py,752,"the following sample produces decision_function values < -1,",
scikit-learn/sklearn/linear_model/tests/test_sgd.py,753,which would cause naive normalization to fail (see comment,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,754,in SGDClassifier.predict_proba),
scikit-learn/sklearn/linear_model/tests/test_sgd.py,757,XXX not true in sparse test case (why?),
scikit-learn/sklearn/linear_model/tests/test_sgd.py,764,Test L1 regularization,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,780,test sparsify with dense inputs,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,786,pickle and unpickle with sparse coef_,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,795,Test class weights.,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,805,we give a small weights to class 1,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,810,now the hyperplane should rotate clock-wise and,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,811,the prediction on this point should shift,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,817,Test if equal class weights approx. equals no class weights.,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,829,should be similar up to some epsilon due to learning rate schedule,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,835,ValueError due to not existing class label.,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,842,ValueError due to wrong class_weight argument type.,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,849,Tests that class_weight and sample_weight are multiplicative,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,868,"Test class weights for imbalanced data""""""",
scikit-learn/sklearn/linear_model/tests/test_sgd.py,869,compute reference metrics on iris dataset that is quite balanced by,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,870,default,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,883,make the same prediction using balanced class_weight,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,890,Make sure that in the balanced case it does not change anything,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,891,"to use ""balanced""",
scikit-learn/sklearn/linear_model/tests/test_sgd.py,894,build an very very imbalanced dataset out of iris data,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,901,fit a model on the imbalanced data without class weight info,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,907,fit a model with balanced class_weight enabled,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,917,Test weights on individual samples,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,926,we give a small weights to class 1,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,929,now the hyperplane should rotate clock-wise and,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,930,the prediction on this point should shift,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,936,Test if ValueError is raised if sample_weight has wrong shape,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,938,provided sample_weight too long,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,945,classes was not specified,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,963,check that coef_ haven't been re-allocated,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,984,check that coef_ haven't been re-allocated,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1005,Partial_fit should work after initial fit in the multiclass case.,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1006,Non-regression test for #2496; fit would previously produce a,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1007,Fortran-ordered coef_ that subsequent partial_fit couldn't handle.,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1010,no exception here,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1067,Test multiple calls of fit w/ different shaped inputs.,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1072,Non-regression test: try fitting with a different label set.,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1077,,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1078,Regression Test Case,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1082,Check that SGD gives any results.,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1090,Tests the average regressor matches the naive implementation,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1100,simple linear function without noise,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1120,Tests whether the partial fit yields the same average as the fit,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1129,simple linear function without noise,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1150,Checks the average weights on data with 0s,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1179,simple linear function without noise,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1188,simple linear function with noise,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1205,simple linear function without noise,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1215,simple linear function with noise,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1233,simple linear function without noise,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1242,simple linear function with noise,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1254,Check that the SGD output is consistent with coordinate descent,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1259,ground_truth linear model that generate y from X and to which the,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1260,models should converge if the regularizer would be set to 0.0,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1264,XXX: alpha = 0.1 seems to cause convergence problems,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1295,check that coef_ haven't been re-allocated,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1327,Test if l1 ratio extremes match L1 and L2 penalty settings.,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1332,test if elasticnet with l1_ratio near 1 gives same result as pure l1,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1340,test if elasticnet with l1_ratio near 0 gives same result as pure l2,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1351,Generate some weird data with hugely unscaled features,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1360,Use MinMaxScaler to scale the data without introducing a numerical,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1361,instability (computing the standard deviation naively is not possible,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1362,on this data),
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1366,Define a ground truth on the scaled data,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1373,smoke test: model is stable on scaled data,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1377,model is numerically unstable on unscaled data,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1385,Non regression test case for numerical stability on scaled problems,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1386,where the gradient can still explode with some losses,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1397,Non regression tests for numerical stability issues caused by large,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1398,regularization parameters,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1408,Test that the tol parameter behaves as expected,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1412,"With tol is None, the number of iteration should be equal to max_iter",
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1418,"If tol is not None, the number of iteration should be less than max_iter",
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1425,A larger tol should yield a smaller number of iteration,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1431,Strict tolerance and small max_iter should trigger a warning,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1438,Test gradient of different loss functions,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1439,"cases is a list of (p, y, expected)",
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1445,Test Hinge (hinge / perceptron),
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1446,hinge,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1449,"(p, y, expected)",
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1456,perceptron,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1459,"(p, y, expected)",
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1468,Test SquaredHinge,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1471,"(p, y, expected)",
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1479,Test Log (logistic loss),
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1482,"(p, y, expected)",
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1496,Test SquaredLoss,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1499,"(p, y, expected)",
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1507,Test Huber,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1510,"(p, y, expected)",
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1518,Test ModifiedHuber,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1521,"(p, y, expected)",
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1530,Test EpsilonInsensitive,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1541,Test SquaredEpsilonInsensitive,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1552,This is a non-regression test for a bad interaction between,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1553,early stopping internal attribute and thread-based parallelism.,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1564,This is a non-regression test for a bad interaction between,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1565,early stopping internal attribute and process-based multi-core,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1566,parallelism.,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1583,"This is a non-regression smoke test. In the multi-class case,",
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1584,SGDClassifier.fit fits each class in a one-versus-all fashion using,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1585,"joblib.Parallel.  However, each OvA step updates the coef_ attribute of",
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1586,"the estimator in-place. Internally, SGDClassifier calls Parallel using",
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1587,require='sharedmem'. This test makes sure SGDClassifier.fit works,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1588,consistently even when the user asks for a backend that does not provide,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1589,sharedmem semantics.,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1591,We further test a case where memmapping would have been used if,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1592,SGDClassifier.fit was called from a loky or multiprocessing backend. In,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1593,"this specific case, in-place modification of clf.coef_ would have caused",
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1594,a segmentation fault when trying to write in a readonly memory mapped,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1595,buffer.,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1602,Create a classification problem with 50000 features and 20 classes. Using,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1603,loky or multiprocessing this make the clf.coef_ exceed the threshold,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1604,above which memmaping is used in joblib and loky (1MB as of 2018/11/1).,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1609,Begin by fitting a SGD classifier sequentially,
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1614,"Fit a SGDClassifier using the specified backend, and make sure the",
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1615,coefficients are equal to those obtained using a sequential fit,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,74,Ridge regression convergence test using score,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,75,"TODO: for this test to be robust, we should use a dataset instead",
scikit-learn/sklearn/linear_model/tests/test_ridge.py,76,of np.random.,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,80,With more samples than features,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,91,Currently the only solvers to support sample_weight.,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,95,With more features than samples,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,104,Currently the only solvers to support sample_weight.,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,119,test on a singular matrix,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,147,Sample weight can be implemented via a simple rescaling,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,148,for the square loss.,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,167,TODO: loop over sparse data as well,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,168,Note: parametrizing this test with pytest results in failed,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,169,"assertions, meaning that is is not extremely robust",
scikit-learn/sklearn/linear_model/tests/test_ridge.py,183,Ridge with explicit sample_weight,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,190,Closed form of the weighted regularized least square,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,191,theta = (X^T W X + alpha I)^(-1) * X^T W y,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,213,Test shape of coef_ and intercept_,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,237,Test intercept with multiple targets GH issue #708,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,255,Test BayesianRegression ridge classifier,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,256,TODO: test also n_samples > n_features,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,277,"On alpha=0., Ridge and OLS yield the same solution.",
scikit-learn/sklearn/linear_model/tests/test_ridge.py,280,we need more samples than features,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,298,Tests the ridge object using individual penalties,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,318,Test error is raised when number of targets and penalties do not match.,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,479,checking on asymmetric scoring,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,534,ignore warning from GridSearchCV: FutureWarning: The default,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,535,of the `iid` parameter will change from True to False in version 0.22,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,536,and will be removed in 0.24,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,593,test that can work with both dense or sparse matrices,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,601,check best alpha,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,606,check that we get same best alpha with custom loss_func,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,613,check that we get same best alpha with custom score_func,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,620,check that we get same best alpha with a scorer,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,626,check that we get same best alpha with sample weights,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,632,simulate several responses,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,679,Check that `cv_values_` is not stored when store_cv_values is False,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,692,check that the best_score_ is store,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,707,simulate several responses,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,741,non-regression test for #14672,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,742,check that RidgeClassifierCV works with all sort of scoring and,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,743,cross-validation,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,746,Smoke test to check that fit/predict does not raise error,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,753,check that custom scoring is working as expected,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,754,check the tie breaking strategy (keep the first alpha tried),
scikit-learn/sklearn/linear_model/tests/test_ridge.py,765,"In case of tie score, the first alphas will be kept",
scikit-learn/sklearn/linear_model/tests/test_ridge.py,782,test dense matrix,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,784,test sparse matrix,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,786,test that the outputs are the same,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,808,Test class weights.,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,817,we give a small weights to class 1,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,821,now the hyperplane should rotate clock-wise and,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,822,the prediction on this point should shift,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,825,check if class_weight = 'balanced' can handle negative labels.,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,830,"class_weight = 'balanced', and class_weight = None should return",
scikit-learn/sklearn/linear_model/tests/test_ridge.py,831,same values when y has equal number of all labels,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,847,"Iris is balanced, so no effect expected for using 'balanced' weights",
scikit-learn/sklearn/linear_model/tests/test_ridge.py,854,"Inflate importance of class 1, check against user-defined weights",
scikit-learn/sklearn/linear_model/tests/test_ridge.py,864,Check that sample_weight and class_weight are multiplicative,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,873,Test class weights for cross validated ridge classifier.,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,881,we give a small weights to class 1,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,904,with len(y.shape) == 1,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,909,with len(y.shape) == 2,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,936,with len(y.shape) == 1,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,941,with len(y.shape) == 2,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,954,There are different algorithms for n_samples > n_features,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,955,"and the opposite, so test them both.",
scikit-learn/sklearn/linear_model/tests/test_ridge.py,965,Check using GridSearchCV directly,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,975,Sample weights must be either scalar or 1D,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,993,"make sure the ""OK"" sample weights actually work",
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1014,Sample weights must work with sparse matrices,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1049,Integers,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1059,Negative integers,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1065,Negative floats,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1073,Tests whether a ValueError is raised if a non-identified solver,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1074,is passed to ridge_regression,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1098,Test that self.n_iter_ is correct.,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1120,for now only sparse_cg can correctly fit an intercept with sparse X with,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1121,default tol and max_iter.,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1122,sag is tested separately in test_ridge_fit_intercept_sparse_sag,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1123,because it requires more iterations and should raise a warning if default,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1124,max_iter is used.,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1125,"other solvers raise an exception, as checked in",
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1126,test_ridge_fit_intercept_sparse_error,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1127,,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1128,"""auto"" should switch to ""sparse_cg"" when X is sparse",
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1129,"so the reference we use for both (""auto"" and ""sparse_cg"") is",
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1130,"Ridge(solver=""sparse_cg""), fitted using the dense representation (note",
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1131,"that ""sparse_cg"" can fit sparse or dense data)",
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1182,test excludes 'svd' solver because it raises exception for sparse inputs,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1240,Check type consistency 32bits,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1245,Check type consistency 64 bits,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1250,Do the actual checks at once for easier debug,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1259,Test different alphas in cholesky solver to ensure full coverage.,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1260,This test is separated from test_dtype_match for clarity.,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1270,Check type consistency 32bits,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1275,Check type consistency 64 bits,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1280,"Do all the checks at once, like this is easier to debug",
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1299,XXX: Sparse CG seems to be far less numerically stable than the,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1300,"others, maybe we should not enable float32 for this one.",
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1320,check that Fortran array are converted when using SAG solver,
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1322,for the order of X and y to not be C-ordered arrays,
scikit-learn/sklearn/linear_model/tests/test_omp.py,1,Author: Vlad Niculae,
scikit-learn/sklearn/linear_model/tests/test_omp.py,2,License: BSD 3 clause,
scikit-learn/sklearn/linear_model/tests/test_omp.py,23,Make X not of norm 1 for testing,
scikit-learn/sklearn/linear_model/tests/test_omp.py,27,"this makes X (n_samples, n_features)",
scikit-learn/sklearn/linear_model/tests/test_omp.py,28,"and y (n_samples, 3)",
scikit-learn/sklearn/linear_model/tests/test_omp.py,106,Non-regression test for:,
scikit-learn/sklearn/linear_model/tests/test_omp.py,107,https://github.com/scikit-learn/scikit-learn/issues/5956,
scikit-learn/sklearn/linear_model/tests/test_omp.py,159,"X[:, 21] should be selected first, then X[:, 0] selected second,",
scikit-learn/sklearn/linear_model/tests/test_omp.py,160,"which will take X[:, 21]'s place in case the algorithm does",
scikit-learn/sklearn/linear_model/tests/test_omp.py,161,column swapping for optimization (which is the case at the moment),
scikit-learn/sklearn/linear_model/tests/test_omp.py,216,Use small simple data; it's a sanity check but OMP can stop early,
scikit-learn/sklearn/linear_model/tests/test_ransac.py,19,Generate coordinates of line,
scikit-learn/sklearn/linear_model/tests/test_ransac.py,24,Add some faulty data,
scikit-learn/sklearn/linear_model/tests/test_ransac.py,39,Estimate parameters of corrupted data,
scikit-learn/sklearn/linear_model/tests/test_ransac.py,42,Ground truth / reference inlier mask,
scikit-learn/sklearn/linear_model/tests/test_ransac.py,92,there is a 1e-9 chance it will take these many trials. No good reason,
scikit-learn/sklearn/linear_model/tests/test_ransac.py,93,"1e-2 isn't enough, can still happen",
scikit-learn/sklearn/linear_model/tests/test_ransac.py,94,2 is the what ransac defines  as min_samples = X.shape[1] + 1,
scikit-learn/sklearn/linear_model/tests/test_ransac.py,153,When residual_threshold=0.0 there are no inliers and a,
scikit-learn/sklearn/linear_model/tests/test_ransac.py,154,ValueError with a message should be raised,
scikit-learn/sklearn/linear_model/tests/test_ransac.py,342,3-D target values,
scikit-learn/sklearn/linear_model/tests/test_ransac.py,345,Estimate parameters of corrupted data,
scikit-learn/sklearn/linear_model/tests/test_ransac.py,348,Ground truth / reference inlier mask,
scikit-learn/sklearn/linear_model/tests/test_ransac.py,373,multi-dimensional,
scikit-learn/sklearn/linear_model/tests/test_ransac.py,382,one-dimensional,
scikit-learn/sklearn/linear_model/tests/test_ransac.py,401,Estimate parameters of corrupted data,
scikit-learn/sklearn/linear_model/tests/test_ransac.py,404,Ground truth / reference inlier mask,
scikit-learn/sklearn/linear_model/tests/test_ransac.py,413,Numbers hand-calculated and confirmed on page 119 (Table 4.3) in,
scikit-learn/sklearn/linear_model/tests/test_ransac.py,414,"Hartley, R.~I. and Zisserman, A., 2004,",
scikit-learn/sklearn/linear_model/tests/test_ransac.py,415,"Multiple View Geometry in Computer Vision, Second Edition,",
scikit-learn/sklearn/linear_model/tests/test_ransac.py,416,"Cambridge University Press, ISBN: 0521540518",
scikit-learn/sklearn/linear_model/tests/test_ransac.py,418,"e = 0%, min_samples = X",
scikit-learn/sklearn/linear_model/tests/test_ransac.py,421,"e = 5%, min_samples = 2",
scikit-learn/sklearn/linear_model/tests/test_ransac.py,423,"e = 10%, min_samples = 2",
scikit-learn/sklearn/linear_model/tests/test_ransac.py,425,"e = 30%, min_samples = 2",
scikit-learn/sklearn/linear_model/tests/test_ransac.py,427,"e = 50%, min_samples = 2",
scikit-learn/sklearn/linear_model/tests/test_ransac.py,430,"e = 5%, min_samples = 8",
scikit-learn/sklearn/linear_model/tests/test_ransac.py,432,"e = 10%, min_samples = 8",
scikit-learn/sklearn/linear_model/tests/test_ransac.py,434,"e = 30%, min_samples = 8",
scikit-learn/sklearn/linear_model/tests/test_ransac.py,436,"e = 50%, min_samples = 8",
scikit-learn/sklearn/linear_model/tests/test_ransac.py,439,"e = 0%, min_samples = 10",
scikit-learn/sklearn/linear_model/tests/test_ransac.py,457,sanity check,
scikit-learn/sklearn/linear_model/tests/test_ransac.py,463,check that mask is correct,
scikit-learn/sklearn/linear_model/tests/test_ransac.py,466,"check that fit(X)  = fit([X1, X2, X3],sample_weight = [n1, n2, n3]) where",
scikit-learn/sklearn/linear_model/tests/test_ransac.py,467,"X = X1 repeated n1 times, X2 repeated n2 times and so forth",
scikit-learn/sklearn/linear_model/tests/test_ransac.py,491,check that if base_estimator.fit doesn't support,
scikit-learn/sklearn/linear_model/tests/test_ransac.py,492,"sample_weight, raises error",
scikit-learn/sklearn/linear_model/tests/test_bayes.py,1,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/sklearn/linear_model/tests/test_bayes.py,2,Fabian Pedregosa <fabian.pedregosa@inria.fr>,
scikit-learn/sklearn/linear_model/tests/test_bayes.py,3,,
scikit-learn/sklearn/linear_model/tests/test_bayes.py,4,License: BSD 3 clause,
scikit-learn/sklearn/linear_model/tests/test_bayes.py,59,check with initial values of alpha and lambda (see code for the values),
scikit-learn/sklearn/linear_model/tests/test_bayes.py,64,value of the parameters of the Gamma hyperpriors,
scikit-learn/sklearn/linear_model/tests/test_bayes.py,70,compute score using formula of docstring,
scikit-learn/sklearn/linear_model/tests/test_bayes.py,78,compute score with BayesianRidge,
scikit-learn/sklearn/linear_model/tests/test_bayes.py,88,Test correctness of lambda_ and alpha_ parameters (GitHub issue #8224),
scikit-learn/sklearn/linear_model/tests/test_bayes.py,92,A Ridge regression model using an alpha value equal to the ratio of,
scikit-learn/sklearn/linear_model/tests/test_bayes.py,93,lambda_ and alpha_ from the Bayesian Ridge model must be identical,
scikit-learn/sklearn/linear_model/tests/test_bayes.py,101,Test correctness of the sample_weights method,
scikit-learn/sklearn/linear_model/tests/test_bayes.py,106,A Ridge regression model using an alpha value equal to the ratio of,
scikit-learn/sklearn/linear_model/tests/test_bayes.py,107,lambda_ and alpha_ from the Bayesian Ridge model must be identical,
scikit-learn/sklearn/linear_model/tests/test_bayes.py,116,Test BayesianRidge on toy,
scikit-learn/sklearn/linear_model/tests/test_bayes.py,122,Check that the model could approximately learn the identity function,
scikit-learn/sklearn/linear_model/tests/test_bayes.py,128,"Test BayesianRidge with initial values (alpha_init, lambda_init)",
scikit-learn/sklearn/linear_model/tests/test_bayes.py,130,y = (x^3 - 6x^2 + 8x) / 3,
scikit-learn/sklearn/linear_model/tests/test_bayes.py,132,"In this case, starting from the default initial values will increase",
scikit-learn/sklearn/linear_model/tests/test_bayes.py,133,"the bias of the fitted curve. So, lambda_init should be small.",
scikit-learn/sklearn/linear_model/tests/test_bayes.py,135,Check the R2 score nearly equals to one.,
scikit-learn/sklearn/linear_model/tests/test_bayes.py,141,Test BayesianRidge and ARDRegression predictions for edge case of,
scikit-learn/sklearn/linear_model/tests/test_bayes.py,142,constant target vectors,
scikit-learn/sklearn/linear_model/tests/test_bayes.py,159,Test BayesianRidge and ARDRegression standard dev. for edge case of,
scikit-learn/sklearn/linear_model/tests/test_bayes.py,160,constant target vector,
scikit-learn/sklearn/linear_model/tests/test_bayes.py,161,The standard dev. should be relatively small (< 0.01 is tested here),
scikit-learn/sklearn/linear_model/tests/test_bayes.py,177,Checks that `sigma_` is updated correctly after the last iteration,
scikit-learn/sklearn/linear_model/tests/test_bayes.py,178,of the ARDRegression algorithm. See issue #10128.,
scikit-learn/sklearn/linear_model/tests/test_bayes.py,184,"With the inputs above, ARDRegression prunes one of the two coefficients",
scikit-learn/sklearn/linear_model/tests/test_bayes.py,185,"in the first iteration. Hence, the expected shape of `sigma_` is (1, 1).",
scikit-learn/sklearn/linear_model/tests/test_bayes.py,187,Ensure that no error is thrown at prediction stage,
scikit-learn/sklearn/linear_model/tests/test_bayes.py,192,Test BayesianRegression ARD classifier,
scikit-learn/sklearn/linear_model/tests/test_bayes.py,198,Check that the model could approximately learn the identity function,
scikit-learn/sklearn/linear_model/tests/test_bayes.py,204,Check that ARD converges with reasonable accuracy on an easy problem,
scikit-learn/sklearn/linear_model/tests/test_bayes.py,205,(Github issue #14055),
scikit-learn/sklearn/linear_model/tests/test_bayes.py,206,This particular seed seems to converge poorly in the failure-case,
scikit-learn/sklearn/linear_model/tests/test_bayes.py,207,"(scipy==1.3.0, sklearn==0.21.2)",
scikit-learn/sklearn/linear_model/tests/test_bayes.py,216,Expect an accuracy of better than 1E-4 in most cases -,
scikit-learn/sklearn/linear_model/tests/test_bayes.py,217,Failure-case produces 0.16!,
scikit-learn/sklearn/linear_model/tests/test_bayes.py,222,Test return_std option for both Bayesian regressors,
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,5,Author: Florian Wilhelm <florian.wilhelm@gmail.com>,
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,6,License: BSD 3 clause,
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,37,"Linear model y = 3*x + N(2, 0.1**2)",
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,48,Add some outliers,
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,66,"Linear model y = 5*x_1 + 10*x_2 + N(1, 0.1**2)",
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,72,Add some outliers,
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,82,"Linear model y = 5*x_1 + 10*x_2  + 42*x_3 + 7*x_4 + N(1, 0.1**2)",
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,88,Add some outliers,
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,97,Check startvalue is element of X and solution,
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,101,Check startvalue is not the solution,
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,106,Check startvalue is not the solution but element of X,
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,111,Check that a single vector is identity,
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,121,Check first two iterations,
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,126,Check fix point,
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,137,Test larger problem and for exact solution in 1d case,
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,153,Check if median is solution of the Fermat-Weber location problem,
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,156,Check when maximum iteration is exceeded a warning is emitted,
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,162,Check that Least Squares fails,
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,165,Check that Theil-Sen works,
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,173,Check that Least Squares fails,
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,176,Check that Theil-Sen works,
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,185,Check that Least Squares fails,
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,188,Check that Theil-Sen works,
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,240,Check for exact the same results as Least Squares,
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,246,Check that Theil-Sen can be verbose,
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,256,Check that Least Squares fails,
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,259,Check that Theil-Sen works,
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,272,Check that Theil-Sen falls back to Least Squares if fit_intercept=False,
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,277,Check fit_intercept=True case. This will not be equal to the Least,
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,278,Squares solution since the intercept is calculated differently.,
scikit-learn/sklearn/linear_model/_glm/glm.py,5,Author: Christian Lorentzen <lorentzen.ch@googlemail.com>,
scikit-learn/sklearn/linear_model/_glm/glm.py,6,some parts and tricks stolen from other sklearn files.,
scikit-learn/sklearn/linear_model/_glm/glm.py,7,License: BSD 3 clause,
scikit-learn/sklearn/linear_model/_glm/glm.py,47,same as X.T @ temp,
scikit-learn/sklearn/linear_model/_glm/glm.py,170,Guarantee that self._link_instance is set to an instance of,
scikit-learn/sklearn/linear_model/_glm/glm.py,171,class BaseLink,
scikit-learn/sklearn/linear_model/_glm/glm.py,234,TODO: if alpha=0 check that X is not rank deficient,
scikit-learn/sklearn/linear_model/_glm/glm.py,236,rescaling of sample_weight,
scikit-learn/sklearn/linear_model/_glm/glm.py,237,,
scikit-learn/sklearn/linear_model/_glm/glm.py,238,IMPORTANT NOTE: Since we want to minimize,
scikit-learn/sklearn/linear_model/_glm/glm.py,239,"1/(2*sum(sample_weight)) * deviance + L2,",
scikit-learn/sklearn/linear_model/_glm/glm.py,240,"deviance = sum(sample_weight * unit_deviance),",
scikit-learn/sklearn/linear_model/_glm/glm.py,241,we rescale weights such that sum(weights) = 1 and this becomes,
scikit-learn/sklearn/linear_model/_glm/glm.py,242,1/2*deviance + L2 with deviance=sum(weights * unit_deviance),
scikit-learn/sklearn/linear_model/_glm/glm.py,258,algorithms for optimization,
scikit-learn/sklearn/linear_model/_glm/glm.py,266,offset if coef[0] is intercept,
scikit-learn/sklearn/linear_model/_glm/glm.py,292,set intercept to zero as the other linear models do,
scikit-learn/sklearn/linear_model/_glm/glm.py,330,check_array is done in _linear_predictor,
scikit-learn/sklearn/linear_model/_glm/glm.py,366,"Note, default score defined in RegressorMixin is R^2 score.",
scikit-learn/sklearn/linear_model/_glm/glm.py,367,TODO: make D^2 a score function in module metrics (and thereby get,
scikit-learn/sklearn/linear_model/_glm/glm.py,368,input validation and so on),
scikit-learn/sklearn/linear_model/_glm/glm.py,377,create the _family_instance if fit wasn't called yet.,
scikit-learn/sklearn/linear_model/_glm/glm.py,443,Make this attribute read-only to avoid mis-uses e.g. in GridSearch.,
scikit-learn/sklearn/linear_model/_glm/glm.py,506,Make this attribute read-only to avoid mis-uses e.g. in GridSearch.,
scikit-learn/sklearn/linear_model/_glm/glm.py,602,We use a property with a setter to make sure that the family is,
scikit-learn/sklearn/linear_model/_glm/glm.py,603,"always a Tweedie distribution, and that self.power and",
scikit-learn/sklearn/linear_model/_glm/glm.py,604,self.family.power are identical by construction.,
scikit-learn/sklearn/linear_model/_glm/glm.py,606,TODO: make the returned object immutable,
scikit-learn/sklearn/linear_model/_glm/link.py,5,Author: Christian Lorentzen <lorentzen.ch@googlemail.com>,
scikit-learn/sklearn/linear_model/_glm/link.py,6,License: BSD 3 clause,
scikit-learn/sklearn/linear_model/_glm/__init__.py,1,License: BSD 3 clause,
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,1,Authors: Christian Lorentzen <lorentzen.ch@gmail.com>,
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,2,,
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,3,License: BSD 3 clause,
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,42,scalar value but not positive,
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,48,Positive weights are accepted,
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,51,2d array,
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,56,1d but wrong length,
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,70,in range of all distributions,
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,85,in range of all distributions,
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,102,Make sure link='auto' delivers the expected link function,
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,103,in range of all distributions,
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,204,sample_weight=np.ones(..) should be equivalent to sample_weight=None,
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,209,"sample_weight are normalized to 1 so, scaling them has no effect",
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,214,setting one element of sample_weight to 0 is equivalent to removing,
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,215,the correspoding sample,
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,223,check that multiplying sample_weight by 2 is equivalent,
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,224,to repeating correspoding samples twice,
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,282,"As we intentionally set max_iter=1, L-BFGS-B will issue a",
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,283,ConvergenceWarning which we here simply ignore.,
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,290,The two model are not exactly identical since the lbfgs solver,
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,291,"computes the approximate hessian from previous iterations, which",
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,292,will not be strictly identical in the case of a warm start.,
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,326,"GLM has 1/(2*n) * Loss + 1/2*L2, Ridge has Loss + L2",
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,348,"library(""glmnet"")",
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,349,options(digits=10),
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,350,"df <- data.frame(a=c(-2,-1,1,2), b=c(0,0,1,1), y=c(0,1,1,2))",
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,351,"x <- data.matrix(df[,c(""a"", ""b"")])",
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,352,y <- df$y,
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,353,"fit <- glmnet(x=x, y=y, alpha=0, intercept=T, family=""poisson"",",
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,354,"standardize=F, thresh=1e-10, nlambda=10000)",
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,355,"coef(fit, s=1)",
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,356,(Intercept) -0.12889386979,
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,357,a            0.29019207995,
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,358,b            0.03741173122,
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,379,Make sure the family attribute is read-only to prevent searching over it,
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,380,e.g. in a grid search,
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,390,Make sure the family attribute is read-only to prevent searching over it,
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,391,e.g. in a grid search,
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,401,Make sure the family attribute is always a TweedieDistribution and that,
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,402,the power attribute is properly updated,
scikit-learn/sklearn/linear_model/_glm/tests/test_link.py,1,Authors: Christian Lorentzen <lorentzen.ch@gmail.com>,
scikit-learn/sklearn/linear_model/_glm/tests/test_link.py,2,,
scikit-learn/sklearn/linear_model/_glm/tests/test_link.py,3,License: BSD 3 clause,
scikit-learn/sklearn/linear_model/_glm/tests/test_link.py,26,"careful for large x, note expit(36) = 1",
scikit-learn/sklearn/linear_model/_glm/tests/test_link.py,27,limit max eta to 15,
scikit-learn/sklearn/linear_model/_glm/tests/test_link.py,30,"if g(h(x)) = x, then g'(h(x)) = 1/h'(x)",
scikit-learn/sklearn/linear_model/_glm/tests/test_link.py,31,"g = link, h = link.inverse",
scikit-learn/sklearn/linear_model/_glm/tests/__init__.py,1,License: BSD 3 clause,
scikit-learn/sklearn/tests/test_calibration.py,1,Authors: Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>,
scikit-learn/sklearn/tests/test_calibration.py,2,License: BSD 3 clause,
scikit-learn/sklearn/tests/test_calibration.py,34,MultinomialNB only allows positive X,
scikit-learn/sklearn/tests/test_calibration.py,36,split train and test,
scikit-learn/sklearn/tests/test_calibration.py,41,Naive-Bayes,
scikit-learn/sklearn/tests/test_calibration.py,48,Naive Bayes with calibration,
scikit-learn/sklearn/tests/test_calibration.py,54,Note that this fit overwrites the fit on the entire training,
scikit-learn/sklearn/tests/test_calibration.py,55,set,
scikit-learn/sklearn/tests/test_calibration.py,59,Check that brier score has improved after calibration,
scikit-learn/sklearn/tests/test_calibration.py,63,"Check invariance against relabeling [0, 1] -> [1, 2]",
scikit-learn/sklearn/tests/test_calibration.py,69,"Check invariance against relabeling [0, 1] -> [-1, 1]",
scikit-learn/sklearn/tests/test_calibration.py,75,"Check invariance against relabeling [0, 1] -> [1, 0]",
scikit-learn/sklearn/tests/test_calibration.py,84,Isotonic calibration is not invariant against relabeling,
scikit-learn/sklearn/tests/test_calibration.py,85,but should improve in both cases,
scikit-learn/sklearn/tests/test_calibration.py,90,Check failure cases:,
scikit-learn/sklearn/tests/test_calibration.py,91,"only ""isotonic"" and ""sigmoid"" should be accepted as methods",
scikit-learn/sklearn/tests/test_calibration.py,95,base-estimators should provide either decision_function or,
scikit-learn/sklearn/tests/test_calibration.py,96,"predict_proba (most regressors, for instance, should fail)",
scikit-learn/sklearn/tests/test_calibration.py,118,"As the weights are used for the calibration, they should still yield",
scikit-learn/sklearn/tests/test_calibration.py,119,a different predictions,
scikit-learn/sklearn/tests/test_calibration.py,129,test multi-class setting with classifier that implements,
scikit-learn/sklearn/tests/test_calibration.py,130,only decision function,
scikit-learn/sklearn/tests/test_calibration.py,135,Use categorical labels to check that CalibratedClassifierCV supports,
scikit-learn/sklearn/tests/test_calibration.py,136,them correctly,
scikit-learn/sklearn/tests/test_calibration.py,150,Check that log-loss of calibrated classifier is smaller than,
scikit-learn/sklearn/tests/test_calibration.py,151,log-loss of naively turned OvR decision function to probabilities,
scikit-learn/sklearn/tests/test_calibration.py,152,via softmax,
scikit-learn/sklearn/tests/test_calibration.py,162,Test that calibration of a multiclass classifier decreases log-loss,
scikit-learn/sklearn/tests/test_calibration.py,163,for RandomForestClassifier,
scikit-learn/sklearn/tests/test_calibration.py,189,MultinomialNB only allows positive X,
scikit-learn/sklearn/tests/test_calibration.py,191,split train and test,
scikit-learn/sklearn/tests/test_calibration.py,199,Naive-Bayes,
scikit-learn/sklearn/tests/test_calibration.py,204,Naive Bayes with calibration,
scikit-learn/sklearn/tests/test_calibration.py,227,computed from my python port of the C++ code in LibSVM,
scikit-learn/sklearn/tests/test_calibration.py,235,check that _SigmoidCalibration().fit only accepts 1d array or 2d column,
scikit-learn/sklearn/tests/test_calibration.py,236,arrays,
scikit-learn/sklearn/tests/test_calibration.py,255,"probabilities outside [0, 1] should not be accepted when normalize",
scikit-learn/sklearn/tests/test_calibration.py,256,is set to False,
scikit-learn/sklearn/tests/test_calibration.py,260,test that quantiles work as expected,
scikit-learn/sklearn/tests/test_calibration.py,271,Check that error is raised when invalid strategy is selected,
scikit-learn/sklearn/tests/test_calibration.py,291,Test that sum of probabilities is 1. A non-regression test for,
scikit-learn/sklearn/tests/test_calibration.py,292,issue #7796,
scikit-learn/sklearn/tests/test_calibration.py,305,Test to check calibration works fine when train set in a test-train,
scikit-learn/sklearn/tests/test_calibration.py,306,split does not contain all classes,
scikit-learn/sklearn/tests/test_calibration.py,307,"Since this test uses LOO, at each iteration train set will not contain a",
scikit-learn/sklearn/tests/test_calibration.py,308,class label,
scikit-learn/sklearn/tests/test_calibration.py,338,toy decision function that just needs to have the right shape:,
scikit-learn/sklearn/tests/test_calibration.py,342,we should be able to fit this classifier with no error,
scikit-learn/sklearn/tests/test_dummy.py,23,We know that we can have division by zero,
scikit-learn/sklearn/tests/test_dummy.py,41,We know that we can have division by zero,
scikit-learn/sklearn/tests/test_dummy.py,46,1d case,
scikit-learn/sklearn/tests/test_dummy.py,47,ignored,
scikit-learn/sklearn/tests/test_dummy.py,54,2d case,
scikit-learn/sklearn/tests/test_dummy.py,66,2d case only,
scikit-learn/sklearn/tests/test_dummy.py,67,ignored,
scikit-learn/sklearn/tests/test_dummy.py,87,ignored,
scikit-learn/sklearn/tests/test_dummy.py,105,non-regression test added in,
scikit-learn/sklearn/tests/test_dummy.py,106,https://github.com/scikit-learn/scikit-learn/pull/13545,
scikit-learn/sklearn/tests/test_dummy.py,121,ignored,
scikit-learn/sklearn/tests/test_dummy.py,140,ignored,
scikit-learn/sklearn/tests/test_dummy.py,154,ignored,
scikit-learn/sklearn/tests/test_dummy.py,177,ignored,
scikit-learn/sklearn/tests/test_dummy.py,191,ignored,
scikit-learn/sklearn/tests/test_dummy.py,270,ignored,
scikit-learn/sklearn/tests/test_dummy.py,290,Correctness oracle,
scikit-learn/sklearn/tests/test_dummy.py,309,ignored,
scikit-learn/sklearn/tests/test_dummy.py,329,Correctness oracle,
scikit-learn/sklearn/tests/test_dummy.py,344,ignored,
scikit-learn/sklearn/tests/test_dummy.py,377,Correctness oracle,
scikit-learn/sklearn/tests/test_dummy.py,387,Correctness oracle,
scikit-learn/sklearn/tests/test_dummy.py,400,ignored,
scikit-learn/sklearn/tests/test_dummy.py,401,ignored,
scikit-learn/sklearn/tests/test_dummy.py,431,ignored,
scikit-learn/sklearn/tests/test_dummy.py,450,test with 2d array,
scikit-learn/sklearn/tests/test_dummy.py,456,Correctness oracle,
scikit-learn/sklearn/tests/test_dummy.py,470,when strategy = 'mean',
scikit-learn/sklearn/tests/test_dummy.py,503,ignored,
scikit-learn/sklearn/tests/test_dummy.py,511,ignored,
scikit-learn/sklearn/tests/test_dummy.py,520,ignored,
scikit-learn/sklearn/tests/test_dummy.py,575,ignored,
scikit-learn/sklearn/tests/test_dummy.py,593,ignored,
scikit-learn/sklearn/tests/test_dummy.py,616,ignored,
scikit-learn/sklearn/tests/test_dummy.py,639,ignored,
scikit-learn/sklearn/tests/test_dummy.py,699,ignored,
scikit-learn/sklearn/tests/test_dummy.py,705,there should be two elements when return_std is True,
scikit-learn/sklearn/tests/test_dummy.py,707,the second element should be all zeros,
scikit-learn/sklearn/tests/test_dummy.py,759,0.24,
scikit-learn/sklearn/tests/test_dummy.py,779,TODO: Remove in 0.24 when DummyClassifier's `strategy` default updates,
scikit-learn/sklearn/tests/test_naive_bayes.py,27,Data is just 6 separable points in the plane,
scikit-learn/sklearn/tests/test_naive_bayes.py,31,A bit more random tests,
scikit-learn/sklearn/tests/test_naive_bayes.py,36,Data is 6 random integer points in a 100 dimensional space classified to,
scikit-learn/sklearn/tests/test_naive_bayes.py,37,three classes.,
scikit-learn/sklearn/tests/test_naive_bayes.py,43,Gaussian Naive Bayes classification.,
scikit-learn/sklearn/tests/test_naive_bayes.py,44,This checks that GaussianNB implements fit and predict and returns,
scikit-learn/sklearn/tests/test_naive_bayes.py,45,correct values for a simple toy dataset.,
scikit-learn/sklearn/tests/test_naive_bayes.py,55,Test whether label mismatch between target y and classes raises,
scikit-learn/sklearn/tests/test_naive_bayes.py,56,an Error,
scikit-learn/sklearn/tests/test_naive_bayes.py,57,FIXME Remove this test once the more general partial_fit tests are merged,
scikit-learn/sklearn/tests/test_naive_bayes.py,62,Test whether class priors are properly set.,
scikit-learn/sklearn/tests/test_naive_bayes.py,67,Check that the class priors sum to 1,
scikit-learn/sklearn/tests/test_naive_bayes.py,73,Sample weights all being 1 should not change results,
scikit-learn/sklearn/tests/test_naive_bayes.py,81,Fitting twice with half sample-weights should result,
scikit-learn/sklearn/tests/test_naive_bayes.py,82,in same result as fitting once with full weights,
scikit-learn/sklearn/tests/test_naive_bayes.py,91,Check that duplicate entries and correspondingly increased sample,
scikit-learn/sklearn/tests/test_naive_bayes.py,92,weights yield the same result,
scikit-learn/sklearn/tests/test_naive_bayes.py,119,"test whether the class prior sum is properly tested""""""",
scikit-learn/sklearn/tests/test_naive_bayes.py,126,smoke test for issue #9633,
scikit-learn/sklearn/tests/test_naive_bayes.py,152,Create an empty array,
scikit-learn/sklearn/tests/test_naive_bayes.py,167,Fit for the first time the GNB,
scikit-learn/sklearn/tests/test_naive_bayes.py,169,Partial fit a second time with an incoherent X,
scikit-learn/sklearn/tests/test_naive_bayes.py,188,Scaling the data should not change the prediction results,
scikit-learn/sklearn/tests/test_naive_bayes.py,199,Test whether class priors are properly set.,
scikit-learn/sklearn/tests/test_naive_bayes.py,221,all categories have to appear in the first partial fit,
scikit-learn/sklearn/tests/test_naive_bayes.py,227,the categories for each feature of CategoricalNB are mapped to an,
scikit-learn/sklearn/tests/test_naive_bayes.py,228,index chronologically with each call of partial fit and therefore,
scikit-learn/sklearn/tests/test_naive_bayes.py,229,the category_count matrices cannot be compared for equality,
scikit-learn/sklearn/tests/test_naive_bayes.py,236,assert category 0 occurs 1x in the first class and 0x in the 2nd,
scikit-learn/sklearn/tests/test_naive_bayes.py,237,class,
scikit-learn/sklearn/tests/test_naive_bayes.py,239,assert category 1 occurs 0x in the first class and 2x in the 2nd,
scikit-learn/sklearn/tests/test_naive_bayes.py,240,class,
scikit-learn/sklearn/tests/test_naive_bayes.py,243,assert category 0 occurs 0x in the first class and 1x in the 2nd,
scikit-learn/sklearn/tests/test_naive_bayes.py,244,class,
scikit-learn/sklearn/tests/test_naive_bayes.py,246,assert category 1 occurs 1x in the first class and 1x in the 2nd,
scikit-learn/sklearn/tests/test_naive_bayes.py,247,class,
scikit-learn/sklearn/tests/test_naive_bayes.py,256,Test picklability of discrete naive Bayes classifiers,
scikit-learn/sklearn/tests/test_naive_bayes.py,267,Test pickling of estimator trained with partial_fit,
scikit-learn/sklearn/tests/test_naive_bayes.py,279,Test input checks for the fit method,
scikit-learn/sklearn/tests/test_naive_bayes.py,281,check shape consistency for number of samples at fit time,
scikit-learn/sklearn/tests/test_naive_bayes.py,284,check shape consistency for number of input features at predict time,
scikit-learn/sklearn/tests/test_naive_bayes.py,291,check shape consistency,
scikit-learn/sklearn/tests/test_naive_bayes.py,295,classes is required for first call to partial fit,
scikit-learn/sklearn/tests/test_naive_bayes.py,298,check consistency of consecutive classes values,
scikit-learn/sklearn/tests/test_naive_bayes.py,304,check consistency of input shape for partial_fit,
scikit-learn/sklearn/tests/test_naive_bayes.py,307,check consistency of input shape for predict,
scikit-learn/sklearn/tests/test_naive_bayes.py,312,Test discrete NB classes' probability scores,
scikit-learn/sklearn/tests/test_naive_bayes.py,314,The 100s below distinguish Bernoulli from multinomial.,
scikit-learn/sklearn/tests/test_naive_bayes.py,315,FIXME: write a test to show this.,
scikit-learn/sklearn/tests/test_naive_bayes.py,319,test binary case (1-d output),
scikit-learn/sklearn/tests/test_naive_bayes.py,320,"2 is regression test for binary case, 02e673",
scikit-learn/sklearn/tests/test_naive_bayes.py,329,"test multiclass case (2-d output, must sum to one)",
scikit-learn/sklearn/tests/test_naive_bayes.py,344,Test whether discrete NB classes fit a uniform prior,
scikit-learn/sklearn/tests/test_naive_bayes.py,345,when fit_prior=False and class_prior=None,
scikit-learn/sklearn/tests/test_naive_bayes.py,356,Test whether discrete NB classes use provided prior,
scikit-learn/sklearn/tests/test_naive_bayes.py,363,Inconsistent number of classes with prior,
scikit-learn/sklearn/tests/test_naive_bayes.py,371,Test whether discrete NB classes use provided prior,
scikit-learn/sklearn/tests/test_naive_bayes.py,372,when using partial_fit,
scikit-learn/sklearn/tests/test_naive_bayes.py,391,check shape consistency for number of samples at fit time,
scikit-learn/sklearn/tests/test_naive_bayes.py,404,Check sample weight using the partial_fit method,
scikit-learn/sklearn/tests/test_naive_bayes.py,415,coef_ and intercept_ should have shapes as in other linear models.,
scikit-learn/sklearn/tests/test_naive_bayes.py,416,Non-regression test for issue #2127.,
scikit-learn/sklearn/tests/test_naive_bayes.py,418,binary classification,
scikit-learn/sklearn/tests/test_naive_bayes.py,428,Test Multinomial Naive Bayes classification.,
scikit-learn/sklearn/tests/test_naive_bayes.py,429,This checks that MultinomialNB implements fit and predict and returns,
scikit-learn/sklearn/tests/test_naive_bayes.py,430,correct values for a simple toy dataset.,
scikit-learn/sklearn/tests/test_naive_bayes.py,437,Check the ability to predict the learning set.,
scikit-learn/sklearn/tests/test_naive_bayes.py,444,Verify that np.log(clf.predict_proba(X)) gives the same results as,
scikit-learn/sklearn/tests/test_naive_bayes.py,445,clf.predict_log_proba(X),
scikit-learn/sklearn/tests/test_naive_bayes.py,450,Check that incremental fitting yields the same results,
scikit-learn/sklearn/tests/test_naive_bayes.py,465,Partial fit on the whole data at once should be the same as fit too,
scikit-learn/sklearn/tests/test_naive_bayes.py,479,test smoothing of prior for yet unobserved targets,
scikit-learn/sklearn/tests/test_naive_bayes.py,481,Create toy training data,
scikit-learn/sklearn/tests/test_naive_bayes.py,495,add a training example with previously unobserved class,
scikit-learn/sklearn/tests/test_naive_bayes.py,517,Tests that BernoulliNB when alpha=1.0 gives the same values as,
scikit-learn/sklearn/tests/test_naive_bayes.py,518,"those given for the toy example in Manning, Raghavan, and",
scikit-learn/sklearn/tests/test_naive_bayes.py,519,"Schuetze's ""Introduction to Information Retrieval"" book:",
scikit-learn/sklearn/tests/test_naive_bayes.py,520,https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html,
scikit-learn/sklearn/tests/test_naive_bayes.py,522,Training data points are:,
scikit-learn/sklearn/tests/test_naive_bayes.py,523,Chinese Beijing Chinese (class: China),
scikit-learn/sklearn/tests/test_naive_bayes.py,524,Chinese Chinese Shanghai (class: China),
scikit-learn/sklearn/tests/test_naive_bayes.py,525,Chinese Macao (class: China),
scikit-learn/sklearn/tests/test_naive_bayes.py,526,Tokyo Japan Chinese (class: Japan),
scikit-learn/sklearn/tests/test_naive_bayes.py,528,"Features are Beijing, Chinese, Japan, Macao, Shanghai, and Tokyo",
scikit-learn/sklearn/tests/test_naive_bayes.py,534,"Classes are China (0), Japan (1)",
scikit-learn/sklearn/tests/test_naive_bayes.py,537,Fit BernoulliBN w/ alpha = 1.0,
scikit-learn/sklearn/tests/test_naive_bayes.py,541,Check the class prior is correct,
scikit-learn/sklearn/tests/test_naive_bayes.py,545,Check the feature probabilities are correct,
scikit-learn/sklearn/tests/test_naive_bayes.py,551,Testing data point is:,
scikit-learn/sklearn/tests/test_naive_bayes.py,552,Chinese Chinese Chinese Tokyo Japan,
scikit-learn/sklearn/tests/test_naive_bayes.py,555,Check the predictive probabilities are correct,
scikit-learn/sklearn/tests/test_naive_bayes.py,563,Test for issue #4268.,
scikit-learn/sklearn/tests/test_naive_bayes.py,564,Tests that the feature log prob value computed by BernoulliNB when,
scikit-learn/sklearn/tests/test_naive_bayes.py,565,"alpha=1.0 is equal to the expression given in Manning, Raghavan,",
scikit-learn/sklearn/tests/test_naive_bayes.py,566,"and Schuetze's ""Introduction to Information Retrieval"" book:",
scikit-learn/sklearn/tests/test_naive_bayes.py,567,http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html,
scikit-learn/sklearn/tests/test_naive_bayes.py,572,Fit Bernoulli NB w/ alpha = 1.0,
scikit-learn/sklearn/tests/test_naive_bayes.py,576,Manually form the (log) numerator and denominator that,
scikit-learn/sklearn/tests/test_naive_bayes.py,577,constitute P(feature presence | class),
scikit-learn/sklearn/tests/test_naive_bayes.py,581,Check manual estimate matches,
scikit-learn/sklearn/tests/test_naive_bayes.py,586,"Tests ComplementNB when alpha=1.0 for the toy example in Manning,",
scikit-learn/sklearn/tests/test_naive_bayes.py,587,"Raghavan, and Schuetze's ""Introduction to Information Retrieval"" book:",
scikit-learn/sklearn/tests/test_naive_bayes.py,588,https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html,
scikit-learn/sklearn/tests/test_naive_bayes.py,590,Training data points are:,
scikit-learn/sklearn/tests/test_naive_bayes.py,591,Chinese Beijing Chinese (class: China),
scikit-learn/sklearn/tests/test_naive_bayes.py,592,Chinese Chinese Shanghai (class: China),
scikit-learn/sklearn/tests/test_naive_bayes.py,593,Chinese Macao (class: China),
scikit-learn/sklearn/tests/test_naive_bayes.py,594,Tokyo Japan Chinese (class: Japan),
scikit-learn/sklearn/tests/test_naive_bayes.py,596,"Features are Beijing, Chinese, Japan, Macao, Shanghai, and Tokyo.",
scikit-learn/sklearn/tests/test_naive_bayes.py,602,"Classes are China (0), Japan (1).",
scikit-learn/sklearn/tests/test_naive_bayes.py,605,Check that weights are correct. See steps 4-6 in Table 4 of,
scikit-learn/sklearn/tests/test_naive_bayes.py,606,Rennie et al. (2003).,
scikit-learn/sklearn/tests/test_naive_bayes.py,631,Verify inputs are nonnegative.,
scikit-learn/sklearn/tests/test_naive_bayes.py,637,Check that counts/weights are correct.,
scikit-learn/sklearn/tests/test_naive_bayes.py,652,Check the ability to predict the training set.,
scikit-learn/sklearn/tests/test_naive_bayes.py,663,Check error is raised for X with negative entries,
scikit-learn/sklearn/tests/test_naive_bayes.py,670,Check error is raised for incorrect X,
scikit-learn/sklearn/tests/test_naive_bayes.py,675,Test alpha,
scikit-learn/sklearn/tests/test_naive_bayes.py,677,alpha=1 increases the count of all categories by one so the final,
scikit-learn/sklearn/tests/test_naive_bayes.py,678,probability for each category is not 50/50 but 1/3 to 2/3,
scikit-learn/sklearn/tests/test_naive_bayes.py,684,Assert category_count has counted all features,
scikit-learn/sklearn/tests/test_naive_bayes.py,687,Check sample_weight,
scikit-learn/sklearn/tests/test_naive_bayes.py,704,Setting alpha=0 should not output nan results when p(x_i|y_j)=0 is a case,
scikit-learn/sklearn/tests/test_naive_bayes.py,724,Test sparse X,
scikit-learn/sklearn/tests/test_naive_bayes.py,736,Test for alpha < 0,
scikit-learn/sklearn/tests/test_naive_bayes.py,760,Setting alpha=np.array with same length,
scikit-learn/sklearn/tests/test_naive_bayes.py,761,as number of features should be fine,
scikit-learn/sklearn/tests/test_naive_bayes.py,766,Test feature probabilities uses pseudo-counts (alpha),
scikit-learn/sklearn/tests/test_naive_bayes.py,770,Test predictions,
scikit-learn/sklearn/tests/test_naive_bayes.py,774,Test alpha non-negative,
scikit-learn/sklearn/tests/test_naive_bayes.py,781,Test that too small pseudo-counts are replaced,
scikit-learn/sklearn/tests/test_naive_bayes.py,790,Test correct dimensions,
scikit-learn/sklearn/tests/test_naive_bayes.py,799,Non regression test to make sure that any further refactoring / optim,
scikit-learn/sklearn/tests/test_naive_bayes.py,800,of the NB models do not harm the performance on a slightly non-linearly,
scikit-learn/sklearn/tests/test_naive_bayes.py,801,separable dataset,
scikit-learn/sklearn/tests/test_naive_bayes.py,806,Multinomial NB,
scikit-learn/sklearn/tests/test_naive_bayes.py,813,Bernoulli NB,
scikit-learn/sklearn/tests/test_naive_bayes.py,820,Gaussian NB,
scikit-learn/sklearn/tests/test_naive_bayes.py,831,TODO: remove in 0.24,
scikit-learn/sklearn/tests/test_isotonic.py,21,check that fit is permutation invariant.,
scikit-learn/sklearn/tests/test_isotonic.py,22,regression test of missing sorting of sample-weights,
scikit-learn/sklearn/tests/test_isotonic.py,47,Check that we got increasing=True and no warnings,
scikit-learn/sklearn/tests/test_isotonic.py,56,Check that we got increasing=True and no warnings,
scikit-learn/sklearn/tests/test_isotonic.py,65,Check that we got increasing=False and no warnings,
scikit-learn/sklearn/tests/test_isotonic.py,74,Check that we got increasing=False and no warnings,
scikit-learn/sklearn/tests/test_isotonic.py,83,Check that we got increasing=False and CI interval warning,
scikit-learn/sklearn/tests/test_isotonic.py,106,check that it is immune to permutation,
scikit-learn/sklearn/tests/test_isotonic.py,113,check we don't crash when all x are equal:,
scikit-learn/sklearn/tests/test_isotonic.py,119,Setup examples with ties on minimum,
scikit-learn/sklearn/tests/test_isotonic.py,124,Check that we get identical results for fit/transform and fit_transform,
scikit-learn/sklearn/tests/test_isotonic.py,132,Setup examples with ties on maximum,
scikit-learn/sklearn/tests/test_isotonic.py,137,Check that we get identical results for fit/transform and fit_transform,
scikit-learn/sklearn/tests/test_isotonic.py,167,"Check fit, transform and fit_transform",
scikit-learn/sklearn/tests/test_isotonic.py,206,Set y and x for decreasing,
scikit-learn/sklearn/tests/test_isotonic.py,210,Create model and fit_transform,
scikit-learn/sklearn/tests/test_isotonic.py,215,work-around for pearson divide warnings in scipy <= 0.17.0,
scikit-learn/sklearn/tests/test_isotonic.py,219,Check that relationship decreases,
scikit-learn/sklearn/tests/test_isotonic.py,225,Set y and x for decreasing,
scikit-learn/sklearn/tests/test_isotonic.py,229,Create model and fit_transform,
scikit-learn/sklearn/tests/test_isotonic.py,234,work-around for pearson divide warnings in scipy <= 0.17.0,
scikit-learn/sklearn/tests/test_isotonic.py,238,Check that relationship increases,
scikit-learn/sklearn/tests/test_isotonic.py,253,check if default value of sample_weight parameter is one,
scikit-learn/sklearn/tests/test_isotonic.py,255,random test data,
scikit-learn/sklearn/tests/test_isotonic.py,260,check if value is correctly used,
scikit-learn/sklearn/tests/test_isotonic.py,269,check if min value is used correctly,
scikit-learn/sklearn/tests/test_isotonic.py,291,Set y and x,
scikit-learn/sklearn/tests/test_isotonic.py,295,Create model and fit,
scikit-learn/sklearn/tests/test_isotonic.py,299,Check that an exception is thrown,
scikit-learn/sklearn/tests/test_isotonic.py,304,Set y and x,
scikit-learn/sklearn/tests/test_isotonic.py,308,Create model and fit,
scikit-learn/sklearn/tests/test_isotonic.py,312,Predict from  training and test x and check that min/max match.,
scikit-learn/sklearn/tests/test_isotonic.py,320,Set y and x,
scikit-learn/sklearn/tests/test_isotonic.py,324,Create model and fit,
scikit-learn/sklearn/tests/test_isotonic.py,328,Predict from  training and test x and check that we have two NaNs.,
scikit-learn/sklearn/tests/test_isotonic.py,334,Set y and x,
scikit-learn/sklearn/tests/test_isotonic.py,338,Create model and fit,
scikit-learn/sklearn/tests/test_isotonic.py,341,Make sure that we throw an error for bad out_of_bounds value,
scikit-learn/sklearn/tests/test_isotonic.py,346,Set y and x,
scikit-learn/sklearn/tests/test_isotonic.py,350,Create model and fit,
scikit-learn/sklearn/tests/test_isotonic.py,353,Make sure that we throw an error for bad out_of_bounds value in transform,
scikit-learn/sklearn/tests/test_isotonic.py,363,Create model and fit,
scikit-learn/sklearn/tests/test_isotonic.py,383,Test from @NelleV's issue:,
scikit-learn/sklearn/tests/test_isotonic.py,384,https://github.com/scikit-learn/scikit-learn/issues/6921,
scikit-learn/sklearn/tests/test_isotonic.py,393,Also test decreasing case since the logic there is different,
scikit-learn/sklearn/tests/test_isotonic.py,399,"Finally, test with only one bound",
scikit-learn/sklearn/tests/test_isotonic.py,406,Test from @ogrisel's issue:,
scikit-learn/sklearn/tests/test_isotonic.py,407,https://github.com/scikit-learn/scikit-learn/issues/4297,
scikit-learn/sklearn/tests/test_isotonic.py,409,Get deterministic RNG with seed,
scikit-learn/sklearn/tests/test_isotonic.py,412,Create regression and samples,
scikit-learn/sklearn/tests/test_isotonic.py,418,Get some random weights and zero out,
scikit-learn/sklearn/tests/test_isotonic.py,423,This will hang in failure case.,
scikit-learn/sklearn/tests/test_isotonic.py,428,test that the faster prediction change doesn't,
scikit-learn/sklearn/tests/test_isotonic.py,429,affect out-of-sample predictions:,
scikit-learn/sklearn/tests/test_isotonic.py,430,https://github.com/scikit-learn/scikit-learn/pull/6206,
scikit-learn/sklearn/tests/test_isotonic.py,433,"X values over the -10,10 range",
scikit-learn/sklearn/tests/test_isotonic.py,439,we also want to test that everything still works when some weights are 0,
scikit-learn/sklearn/tests/test_isotonic.py,445,"Build interpolation function with ALL input data, not just the",
scikit-learn/sklearn/tests/test_isotonic.py,446,non-redundant subset. The following 2 lines are taken from the,
scikit-learn/sklearn/tests/test_isotonic.py,447,".fit() method, without removing unnecessary points",
scikit-learn/sklearn/tests/test_isotonic.py,453,fit with just the necessary data,
scikit-learn/sklearn/tests/test_isotonic.py,464,https://github.com/scikit-learn/scikit-learn/issues/6628,
scikit-learn/sklearn/tests/test_isotonic.py,494,regression test for #15004,
scikit-learn/sklearn/tests/test_isotonic.py,495,check that data are converted when X and y dtype differ,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,23,Data is just 6 separable points in the plane,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,28,Degenerate data with only one feature (still should be separable),
scikit-learn/sklearn/tests/test_discriminant_analysis.py,31,Data is just 9 separable points in the plane,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,37,Degenerate data with 1 feature (still should be separable),
scikit-learn/sklearn/tests/test_discriminant_analysis.py,41,Data that has zero variance in one dimension and needs regularization,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,45,One element class,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,48,Data with less samples in a class than n_features,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,58,Test LDA classification.,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,59,This checks that LDA implements fit and predict and returns correct,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,60,values for simple toy data.,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,67,Assert that it works with 1D data,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,71,Test probability estimates,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,79,"Primarily test for commit 2f34950 -- ""reuse"" of priors",
scikit-learn/sklearn/tests/test_discriminant_analysis.py,81,LDA shouldn't be able to separate those,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,84,Test invalid shrinkages,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,91,Test unknown solver,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,118,check that the empirical means and covariances are close enough to the,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,119,one used to generate the data,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,123,implement the method to compute the probability given in The Elements,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,124,"of Statistical Learning (cf. p.127, Sect. 4.4.5 ""Logistic Regression",
scikit-learn/sklearn/tests/test_discriminant_analysis.py,125,"or LDA?"")",
scikit-learn/sklearn/tests/test_discriminant_analysis.py,150,check the consistency of the computed probability,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,151,all probabilities should sum to one,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,158,check that the probability of LDA are close to the theoretical,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,159,probabilties,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,166,Test priors (negative priors),
scikit-learn/sklearn/tests/test_discriminant_analysis.py,172,Test that priors passed as a list are correctly handled (run to see if,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,173,failure),
scikit-learn/sklearn/tests/test_discriminant_analysis.py,177,Test that priors always sum to 1,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,186,Test if the coefficients of the solvers are approximately the same.,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,207,Test LDA transform.,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,222,"Test if the sum of the normalized eigen vectors values equals 1,",
scikit-learn/sklearn/tests/test_discriminant_analysis.py,223,Also tests whether the explained_variance_ratio_ formed by the,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,224,eigen solver is the same as the explained_variance_ratio_ formed,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,225,by the svd solver,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,248,arrange four classes with their means in a kite-shaped pattern,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,249,"the longer distance should be transformed to the first component, and",
scikit-learn/sklearn/tests/test_discriminant_analysis.py,250,the shorter distance to the second component.,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,253,"We construct perfectly symmetric distributions, so the LDA can estimate",
scikit-learn/sklearn/tests/test_discriminant_analysis.py,254,precise means.,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,261,Fit LDA and transform the means,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,270,the transformed within-class covariance should be the identity matrix,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,273,the means of classes 0 and 3 should lie on the first component,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,276,the means of classes 1 and 2 should lie on the second component,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,281,Test if classification works correctly with differently scaled features.,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,284,use uniform distribution of features to make sure there is absolutely no,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,285,overlap between classes.,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,293,should be able to separate the data perfectly,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,299,Test for solver 'lsqr' and 'eigen',
scikit-learn/sklearn/tests/test_discriminant_analysis.py,300,'store_covariance' has no effect on 'lsqr' and 'eigen' solvers,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,305,Test the actual attribute:,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,315,"Test for SVD solver, the default is to not set the covariances_ attribute",
scikit-learn/sklearn/tests/test_discriminant_analysis.py,319,Test the actual attribute:,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,336,we create n_classes labels by repeating and truncating a,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,337,range(n_classes) until n_samples,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,342,"if n_components <= min(n_classes - 1, n_features), no warning",
scikit-learn/sklearn/tests/test_discriminant_analysis.py,348,"if n_components > min(n_classes - 1, n_features), raise error.",
scikit-learn/sklearn/tests/test_discriminant_analysis.py,349,"We test one unit higher than max_components, and then something",
scikit-learn/sklearn/tests/test_discriminant_analysis.py,350,larger than both n_features and n_classes - 1 to ensure the test,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,351,works for any value of n_component,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,378,Check value consistency between types,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,384,QDA classification.,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,385,This checks that QDA implements fit and predict and returns,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,386,correct values for a simple toy dataset.,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,391,Assure that it works with 1D data,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,395,Test probas estimates,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,402,QDA shouldn't be able to separate those,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,405,Classes should have at least 2 elements,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,423,The default is to not set the covariances_ attribute,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,427,Test the actual attribute:,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,443,the default is reg_param=0. and will cause issues,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,444,when there is a constant variable,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,450,adding a little regularization fixes the problem,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,457,Case n_samples_in_a_class < n_features,
scikit-learn/sklearn/tests/test_discriminant_analysis.py,469,make features correlated,
scikit-learn/sklearn/tests/test_random_projection.py,37,Make some random data with uniformly located non zero entries with,
scikit-learn/sklearn/tests/test_random_projection.py,38,Gaussian distributed values,
scikit-learn/sklearn/tests/test_random_projection.py,61,,
scikit-learn/sklearn/tests/test_random_projection.py,62,test on JL lemma,
scikit-learn/sklearn/tests/test_random_projection.py,63,,
scikit-learn/sklearn/tests/test_random_projection.py,82,,
scikit-learn/sklearn/tests/test_random_projection.py,83,tests random matrix generation,
scikit-learn/sklearn/tests/test_random_projection.py,84,,
scikit-learn/sklearn/tests/test_random_projection.py,101,All random matrix should produce a transformation matrix,
scikit-learn/sklearn/tests/test_random_projection.py,102,with zero mean and unit norm for each columns,
scikit-learn/sklearn/tests/test_random_projection.py,120,Check basic properties of random matrix generation,
scikit-learn/sklearn/tests/test_random_projection.py,136,Check some statical properties of Gaussian random matrix,
scikit-learn/sklearn/tests/test_random_projection.py,137,Check that the random matrix follow the proper distribution.,
scikit-learn/sklearn/tests/test_random_projection.py,138,Let's say that each element of a_{ij} of A is taken from,
scikit-learn/sklearn/tests/test_random_projection.py,139,"a_ij ~ N(0.0, 1 / n_components).",
scikit-learn/sklearn/tests/test_random_projection.py,140,,
scikit-learn/sklearn/tests/test_random_projection.py,150,Check some statical properties of sparse random matrix,
scikit-learn/sklearn/tests/test_random_projection.py,163,Check possible values,
scikit-learn/sklearn/tests/test_random_projection.py,174,Check that the random matrix follow the proper distribution.,
scikit-learn/sklearn/tests/test_random_projection.py,175,Let's say that each element of a_{ij} of A is taken from,
scikit-learn/sklearn/tests/test_random_projection.py,176,,
scikit-learn/sklearn/tests/test_random_projection.py,177,- -sqrt(s) / sqrt(n_components)   with probability 1 / 2s,
scikit-learn/sklearn/tests/test_random_projection.py,178,-  0                              with probability 1 - 1 / s,
scikit-learn/sklearn/tests/test_random_projection.py,179,- +sqrt(s) / sqrt(n_components)   with probability 1 / 2s,
scikit-learn/sklearn/tests/test_random_projection.py,180,,
scikit-learn/sklearn/tests/test_random_projection.py,198,,
scikit-learn/sklearn/tests/test_random_projection.py,199,tests on random projection transformer,
scikit-learn/sklearn/tests/test_random_projection.py,200,,
scikit-learn/sklearn/tests/test_random_projection.py,248,remove 0 distances to avoid division by 0,
scikit-learn/sklearn/tests/test_random_projection.py,258,remove 0 distances to avoid division by 0,
scikit-learn/sklearn/tests/test_random_projection.py,263,check that the automatically tuned values for the density respect the,
scikit-learn/sklearn/tests/test_random_projection.py,264,contract for eps: pairwise distances are preserved according to the,
scikit-learn/sklearn/tests/test_random_projection.py,265,Johnson-Lindenstrauss lemma,
scikit-learn/sklearn/tests/test_random_projection.py,272,"when using sparse input, the projected data can be forced to be a",
scikit-learn/sklearn/tests/test_random_projection.py,273,dense numpy array,
scikit-learn/sklearn/tests/test_random_projection.py,282,the output can be left to a sparse matrix instead,
scikit-learn/sklearn/tests/test_random_projection.py,286,output for dense input will stay dense:,
scikit-learn/sklearn/tests/test_random_projection.py,289,output for sparse output will be sparse:,
scikit-learn/sklearn/tests/test_random_projection.py,299,the number of components is adjusted from the shape of the training,
scikit-learn/sklearn/tests/test_random_projection.py,300,set,
scikit-learn/sklearn/tests/test_random_projection.py,313,once the RP is 'fitted' the projection is always the same,
scikit-learn/sklearn/tests/test_random_projection.py,317,fit transform with same random seed will lead to the same results,
scikit-learn/sklearn/tests/test_random_projection.py,322,Try to transform with an input X of size different from fitted.,
scikit-learn/sklearn/tests/test_random_projection.py,325,it is also possible to fix the number of components and the density,
scikit-learn/sklearn/tests/test_random_projection.py,326,level,
scikit-learn/sklearn/tests/test_random_projection.py,333,close to 1% density,
scikit-learn/sklearn/tests/test_random_projection.py,334,close to 1% density,
scikit-learn/sklearn/tests/test_random_projection.py,359,TODO remove in 0.24,
scikit-learn/sklearn/tests/test_multioutput.py,79,Test multi target regression raises,
scikit-learn/sklearn/tests/test_multioutput.py,111,no exception should be raised if the base estimator supports weights,
scikit-learn/sklearn/tests/test_multioutput.py,117,weighted regressor,
scikit-learn/sklearn/tests/test_multioutput.py,124,weighted with different weights,
scikit-learn/sklearn/tests/test_multioutput.py,133,weighted regressor,
scikit-learn/sklearn/tests/test_multioutput.py,140,"unweighted, but with repeated samples",
scikit-learn/sklearn/tests/test_multioutput.py,150,Import the data,
scikit-learn/sklearn/tests/test_multioutput.py,152,create a multiple targets by randomized shuffling and concatenating y.,
scikit-learn/sklearn/tests/test_multioutput.py,172,parallelism requires this to be the case for a sane implementation,
scikit-learn/sklearn/tests/test_multioutput.py,176,check multioutput has predict_proba,
scikit-learn/sklearn/tests/test_multioutput.py,178,default SGDClassifier has loss='hinge',
scikit-learn/sklearn/tests/test_multioutput.py,179,which does not expose a predict_proba method,
scikit-learn/sklearn/tests/test_multioutput.py,185,case where predict_proba attribute exists,
scikit-learn/sklearn/tests/test_multioutput.py,192,check predict_proba passes,
scikit-learn/sklearn/tests/test_multioutput.py,197,inner function for custom scoring,
scikit-learn/sklearn/tests/test_multioutput.py,210,SGDClassifier defaults to loss='hinge' which is not a probabilistic,
scikit-learn/sklearn/tests/test_multioutput.py,211,loss function; therefore it does not expose a predict_proba method,
scikit-learn/sklearn/tests/test_multioutput.py,221,test if multi_target initializes correctly with base estimator and fit,
scikit-learn/sklearn/tests/test_multioutput.py,222,assert predictions work as expected for predict,
scikit-learn/sklearn/tests/test_multioutput.py,227,train the multi_target_linear and also get the predictions.,
scikit-learn/sklearn/tests/test_multioutput.py,239,train the linear classification with each column and assert that,
scikit-learn/sklearn/tests/test_multioutput.py,240,predictions are equal after first partial_fit and second partial_fit,
scikit-learn/sklearn/tests/test_multioutput.py,242,create a clone with the same state,
scikit-learn/sklearn/tests/test_multioutput.py,260,test if multi_target initializes correctly with base estimator and fit,
scikit-learn/sklearn/tests/test_multioutput.py,261,"assert predictions work as expected for predict, prodict_proba and score",
scikit-learn/sklearn/tests/test_multioutput.py,266,train the multi_target_forest and also get the predictions.,
scikit-learn/sklearn/tests/test_multioutput.py,281,train the forest with each column and assert that predictions are equal,
scikit-learn/sklearn/tests/test_multioutput.py,283,create a clone with the same state,
scikit-learn/sklearn/tests/test_multioutput.py,291,test to check meta of meta estimators,
scikit-learn/sklearn/tests/test_multioutput.py,301,train the forest with each column and assert that predictions are equal,
scikit-learn/sklearn/tests/test_multioutput.py,303,create a clone,
scikit-learn/sklearn/tests/test_multioutput.py,312,make test deterministic,
scikit-learn/sklearn/tests/test_multioutput.py,315,random features,
scikit-learn/sklearn/tests/test_multioutput.py,318,random labels,
scikit-learn/sklearn/tests/test_multioutput.py,319,2 classes,
scikit-learn/sklearn/tests/test_multioutput.py,320,3 classes,
scikit-learn/sklearn/tests/test_multioutput.py,346,weighted classifier,
scikit-learn/sklearn/tests/test_multioutput.py,354,"unweighted, but with repeated samples",
scikit-learn/sklearn/tests/test_multioutput.py,366,weighted classifier,
scikit-learn/sklearn/tests/test_multioutput.py,374,"unweighted, but with repeated samples",
scikit-learn/sklearn/tests/test_multioutput.py,385,"NotFittedError when fit is not done but score, predict and",
scikit-learn/sklearn/tests/test_multioutput.py,386,and predict_proba are called,
scikit-learn/sklearn/tests/test_multioutput.py,392,ValueError when number of outputs is different,
scikit-learn/sklearn/tests/test_multioutput.py,393,for fit and score,
scikit-learn/sklearn/tests/test_multioutput.py,397,ValueError when y is continuous,
scikit-learn/sklearn/tests/test_multioutput.py,402,Generate a multilabel data set from a multiclass dataset as a way of,
scikit-learn/sklearn/tests/test_multioutput.py,403,by representing the integer number of the original class using a binary,
scikit-learn/sklearn/tests/test_multioutput.py,404,encoding.,
scikit-learn/sklearn/tests/test_multioutput.py,417,Fit classifier chain and verify predict performance using LinearSVC,
scikit-learn/sklearn/tests/test_multioutput.py,433,Fit classifier chain with sparse data,
scikit-learn/sklearn/tests/test_multioutput.py,449,Verify that an ensemble of classifier chains (each of length,
scikit-learn/sklearn/tests/test_multioutput.py,450,N) can achieve a higher Jaccard similarity score than N independent,
scikit-learn/sklearn/tests/test_multioutput.py,451,models,
scikit-learn/sklearn/tests/test_multioutput.py,471,Fit base chain and verify predict performance,
scikit-learn/sklearn/tests/test_multioutput.py,490,Fit base chain with sparse data cross_val_predict,
scikit-learn/sklearn/tests/test_multioutput.py,502,Fit base chain with random order,
scikit-learn/sklearn/tests/test_multioutput.py,514,Randomly ordered chain should behave identically to a fixed order,
scikit-learn/sklearn/tests/test_multioutput.py,515,chain with the same order.,
scikit-learn/sklearn/tests/test_multioutput.py,522,Fit chain with cross_val_predict and verify predict,
scikit-learn/sklearn/tests/test_multioutput.py,523,performance,
scikit-learn/sklearn/tests/test_multioutput.py,549,Tests classes_ attribute of multioutput classifiers,
scikit-learn/sklearn/tests/test_multioutput.py,550,RandomForestClassifier supports multioutput out-of-the-box,
scikit-learn/sklearn/tests/test_multioutput.py,559,TODO: remove in 0.24,
scikit-learn/sklearn/tests/test_multioutput.py,595,Make sure fit_params are properly propagated to the sub-estimators,
scikit-learn/sklearn/tests/test_multioutput.py,608,Fitting with params,
scikit-learn/sklearn/tests/test_kernel_approximation.py,15,generate data,
scikit-learn/sklearn/tests/test_kernel_approximation.py,28,test that AdditiveChi2Sampler approximates kernel on random data,
scikit-learn/sklearn/tests/test_kernel_approximation.py,30,compute exact kernel,
scikit-learn/sklearn/tests/test_kernel_approximation.py,31,abbreviations for easier formula,
scikit-learn/sklearn/tests/test_kernel_approximation.py,37,reduce to n_samples_x x n_samples_y by summing over features,
scikit-learn/sklearn/tests/test_kernel_approximation.py,40,approximate kernel mapping,
scikit-learn/sklearn/tests/test_kernel_approximation.py,55,test error is raised on negative input,
scikit-learn/sklearn/tests/test_kernel_approximation.py,60,test error on invalid sample_steps,
scikit-learn/sklearn/tests/test_kernel_approximation.py,64,test that the sample interval is set correctly,
scikit-learn/sklearn/tests/test_kernel_approximation.py,68,test that the sample_interval is initialized correctly,
scikit-learn/sklearn/tests/test_kernel_approximation.py,72,test that the sample_interval is changed in the fit method,
scikit-learn/sklearn/tests/test_kernel_approximation.py,76,test that the sample_interval is set correctly,
scikit-learn/sklearn/tests/test_kernel_approximation.py,86,test that RBFSampler approximates kernel on random data,
scikit-learn/sklearn/tests/test_kernel_approximation.py,88,compute exact kernel,
scikit-learn/sklearn/tests/test_kernel_approximation.py,90,set on negative component but greater than c to ensure that the kernel,
scikit-learn/sklearn/tests/test_kernel_approximation.py,91,approximation is valid on the group (-c; +\infty) endowed with the skewed,
scikit-learn/sklearn/tests/test_kernel_approximation.py,92,multiplication.,
scikit-learn/sklearn/tests/test_kernel_approximation.py,95,abbreviations for easier formula,
scikit-learn/sklearn/tests/test_kernel_approximation.py,99,we do it in log-space in the hope that it's more stable,
scikit-learn/sklearn/tests/test_kernel_approximation.py,100,this array is n_samples_x x n_samples_y big x n_features,
scikit-learn/sklearn/tests/test_kernel_approximation.py,103,reduce to n_samples_x x n_samples_y by summing over features in log-space,
scikit-learn/sklearn/tests/test_kernel_approximation.py,106,approximate kernel mapping,
scikit-learn/sklearn/tests/test_kernel_approximation.py,119,test error is raised on when inputs contains values smaller than -c,
scikit-learn/sklearn/tests/test_kernel_approximation.py,138,test that RBFSampler approximates kernel on random data,
scikit-learn/sklearn/tests/test_kernel_approximation.py,139,compute exact kernel,
scikit-learn/sklearn/tests/test_kernel_approximation.py,143,approximate kernel mapping,
scikit-learn/sklearn/tests/test_kernel_approximation.py,150,close to unbiased,
scikit-learn/sklearn/tests/test_kernel_approximation.py,152,nothing too far off,
scikit-learn/sklearn/tests/test_kernel_approximation.py,153,mean is fairly close,
scikit-learn/sklearn/tests/test_kernel_approximation.py,157,Regression test: kernel approx. transformers should work on lists,
scikit-learn/sklearn/tests/test_kernel_approximation.py,158,No assertions; the old versions would simply crash,
scikit-learn/sklearn/tests/test_kernel_approximation.py,169,some basic tests,
scikit-learn/sklearn/tests/test_kernel_approximation.py,173,With n_components = n_samples this is exact,
scikit-learn/sklearn/tests/test_kernel_approximation.py,182,test callable kernel,
scikit-learn/sklearn/tests/test_kernel_approximation.py,187,test that available kernels fit and transform,
scikit-learn/sklearn/tests/test_kernel_approximation.py,199,rbf kernel should behave as gamma=None by default,
scikit-learn/sklearn/tests/test_kernel_approximation.py,200,aka gamma = 1 / n_features,
scikit-learn/sklearn/tests/test_kernel_approximation.py,207,chi2 kernel should behave as gamma=1 by default,
scikit-learn/sklearn/tests/test_kernel_approximation.py,216,test that nystroem works with singular kernel matrix,
scikit-learn/sklearn/tests/test_kernel_approximation.py,219,duplicate samples,
scikit-learn/sklearn/tests/test_kernel_approximation.py,232,Non-regression: Nystroem should pass other parameters beside gamma.,
scikit-learn/sklearn/tests/test_kernel_approximation.py,244,Test Nystroem on a callable.,
scikit-learn/sklearn/tests/test_kernel_approximation.py,255,test input validation,
scikit-learn/sklearn/tests/test_kernel_approximation.py,261,"if degree, gamma or coef0 is passed, we raise a warning",
scikit-learn/sklearn/tests/test_kernel_approximation.py,271,Non-regression: test Nystroem on precomputed kernel.,
scikit-learn/sklearn/tests/test_kernel_approximation.py,272,PR - 14706,
scikit-learn/sklearn/tests/test_kernel_approximation.py,281,"if degree, gamma or coef0 is passed, we raise a ValueError",
scikit-learn/sklearn/tests/test_kernel_ridge.py,40,"alpha=0 causes a LinAlgError in computing the dual coefficients,",
scikit-learn/sklearn/tests/test_kernel_ridge.py,41,which causes a fallback to a lstsq solver. This is tested here.,
scikit-learn/sklearn/tests/test_kernel_ridge.py,65,precomputed kernel,
scikit-learn/sklearn/tests/test_init.py,1,Basic unittests to test functioning of module's top-level,
scikit-learn/sklearn/tests/test_init.py,9,noqa,
scikit-learn/sklearn/tests/test_init.py,16,Test either above import has failed for some reason,
scikit-learn/sklearn/tests/test_init.py,17,"""import *"" is discouraged outside of the module level, hence we",
scikit-learn/sklearn/tests/test_init.py,18,rely on setting up the variable above,
scikit-learn/sklearn/tests/test_common.py,5,Authors: Andreas Mueller <amueller@ais.uni-bonn.de>,
scikit-learn/sklearn/tests/test_common.py,6,Gael Varoquaux gael.varoquaux@normalesup.org,
scikit-learn/sklearn/tests/test_common.py,7,License: BSD 3 clause,
scikit-learn/sklearn/tests/test_common.py,44,test that all_estimators doesn't find abstract classes.,
scikit-learn/sklearn/tests/test_common.py,52,Non-regression test for #16707 to ensure that parametrize_with_checks,
scikit-learn/sklearn/tests/test_common.py,53,works with estimator classes,
scikit-learn/sklearn/tests/test_common.py,55,Using the generator does not raise,
scikit-learn/sklearn/tests/test_common.py,74,Test that estimators are default-constructible,
scikit-learn/sklearn/tests/test_common.py,110,Common tests for estimator instances,
scikit-learn/sklearn/tests/test_common.py,129,all classes checks include check_parameters_default_constructible,
scikit-learn/sklearn/tests/test_common.py,132,TODO: meta-estimators like GridSearchCV has required parameters,
scikit-learn/sklearn/tests/test_common.py,133,that do not have default values. This is expected to change in the future,
scikit-learn/sklearn/tests/test_common.py,141,"ignore deprecated open(.., 'U') in numpy distutils",
scikit-learn/sklearn/tests/test_common.py,143,"Smoke test the 'configure' step of setup, this tests all the",
scikit-learn/sklearn/tests/test_common.py,144,'configure' functions in the setup.pys in scikit-learn,
scikit-learn/sklearn/tests/test_common.py,145,This test requires Cython which is not necessarily there when running,
scikit-learn/sklearn/tests/test_common.py,146,the tests of an installed version of scikit-learn or when scikit-learn,
scikit-learn/sklearn/tests/test_common.py,147,is installed in editable mode by pip build isolation enabled.,
scikit-learn/sklearn/tests/test_common.py,154,XXX unreached code as of v0.22,
scikit-learn/sklearn/tests/test_common.py,161,The configuration spits out warnings when not finding,
scikit-learn/sklearn/tests/test_common.py,162,Blas/Atlas development headers,
scikit-learn/sklearn/tests/test_common.py,178,FIXME,
scikit-learn/sklearn/tests/test_common.py,194,Smoke test to check that any name in a __all__ list is actually defined,
scikit-learn/sklearn/tests/test_common.py,195,in the namespace of the module or package.,
scikit-learn/sklearn/tests/test_common.py,221,"Ensure that for each contentful subpackage, there is a test directory",
scikit-learn/sklearn/tests/test_common.py,222,within it that is also a subpackage (i.e. a directory with __init__.py),
scikit-learn/sklearn/tests/test_metaestimators.py,49,Ensures specified metaestimators have methods iff subestimator does,
scikit-learn/sklearn/tests/test_metaestimators.py,119,delegation before fit raises a NotFittedError,
scikit-learn/sklearn/tests/test_metaestimators.py,132,smoke test delegation,
scikit-learn/sklearn/tests/test_base.py,1,Author: Gael Varoquaux,
scikit-learn/sklearn/tests/test_base.py,2,License: BSD 3 clause,
scikit-learn/sklearn/tests/test_base.py,29,,
scikit-learn/sklearn/tests/test_base.py,30,A few test classes,
scikit-learn/sklearn/tests/test_base.py,107,,
scikit-learn/sklearn/tests/test_base.py,108,The tests,
scikit-learn/sklearn/tests/test_base.py,111,Tests that clone creates a correct deep copy.,
scikit-learn/sklearn/tests/test_base.py,112,"We create an estimator, make a copy of its original state",
scikit-learn/sklearn/tests/test_base.py,113,"(which, in this case, is the current state of the estimator),",
scikit-learn/sklearn/tests/test_base.py,114,and check that the obtained copy is a correct deep copy.,
scikit-learn/sklearn/tests/test_base.py,129,Tests that clone doesn't copy everything.,
scikit-learn/sklearn/tests/test_base.py,130,"We first create an estimator, give it an own attribute, and",
scikit-learn/sklearn/tests/test_base.py,131,make a copy of its original state. Then we check that the copy doesn't,
scikit-learn/sklearn/tests/test_base.py,132,have the specific attribute we manually added to the initial estimator.,
scikit-learn/sklearn/tests/test_base.py,143,Check that clone raises an error on buggy estimators.,
scikit-learn/sklearn/tests/test_base.py,159,Regression test for cloning estimators with empty arrays,
scikit-learn/sklearn/tests/test_base.py,170,Regression test for cloning estimators with default parameter as np.nan,
scikit-learn/sklearn/tests/test_base.py,191,Check that clone works for parameters that are types rather than,
scikit-learn/sklearn/tests/test_base.py,192,instances,
scikit-learn/sklearn/tests/test_base.py,200,Check that clone raises expected error message when,
scikit-learn/sklearn/tests/test_base.py,201,cloning class rather than instance,
scikit-learn/sklearn/tests/test_base.py,208,Smoke test the repr of the base estimator.,
scikit-learn/sklearn/tests/test_base.py,221,Smoke test the str of the base estimator,
scikit-learn/sklearn/tests/test_base.py,247,test nested estimator parameter setting,
scikit-learn/sklearn/tests/test_base.py,249,non-existing parameter in svc,
scikit-learn/sklearn/tests/test_base.py,251,non-existing parameter of pipeline,
scikit-learn/sklearn/tests/test_base.py,253,we don't currently catch if the things in pipeline are estimators,
scikit-learn/sklearn/tests/test_base.py,254,"bad_pipeline = Pipeline([(""bad"", NoEstimator())])",
scikit-learn/sklearn/tests/test_base.py,255,"assert_raises(AttributeError, bad_pipeline.set_params,",
scikit-learn/sklearn/tests/test_base.py,256,bad__stupid_param=True),
scikit-learn/sklearn/tests/test_base.py,260,Make sure all parameters are passed together to set_params,
scikit-learn/sklearn/tests/test_base.py,261,of nested estimator. Regression test for #9944,
scikit-learn/sklearn/tests/test_base.py,266,expected_kwargs is in test scope,
scikit-learn/sklearn/tests/test_base.py,278,"Check that set_params tries to set SVC().C, not",
scikit-learn/sklearn/tests/test_base.py,279,DecisionTreeClassifier().C,
scikit-learn/sklearn/tests/test_base.py,289,test both ClassifierMixin and RegressorMixin,
scikit-learn/sklearn/tests/test_base.py,297,generate random sample weights,
scikit-learn/sklearn/tests/test_base.py,299,check that the score with and without sample weights are different,
scikit-learn/sklearn/tests/test_base.py,334,build and clone estimator,
scikit-learn/sklearn/tests/test_base.py,340,the test,
scikit-learn/sklearn/tests/test_base.py,352,test that we can predict with the restored decision tree classifier,
scikit-learn/sklearn/tests/test_base.py,388,"TreeNoVersion has no getstate, like pre-0.18",
scikit-learn/sklearn/tests/test_base.py,396,check we got the warning about using pre-0.18 pickle,
scikit-learn/sklearn/tests/test_base.py,484,test that changing tags by inheritance is not allowed,
scikit-learn/sklearn/tests/test_import_deprecations.py,9,We are deprecating importing anything that isn't in an __init__ file and,
scikit-learn/sklearn/tests/test_import_deprecations.py,10,remaming most file.py into _file.py.,
scikit-learn/sklearn/tests/test_import_deprecations.py,11,"This test makes sure imports are still possible but deprecated, with the",
scikit-learn/sklearn/tests/test_import_deprecations.py,12,appropriate error message.,
scikit-learn/sklearn/tests/test_import_deprecations.py,20,"Make sure that ""from deprecated_path import importee"" is still possible",
scikit-learn/sklearn/tests/test_import_deprecations.py,21,but raises a warning,
scikit-learn/sklearn/tests/test_import_deprecations.py,22,"We only need one entry per file, no need to check multiple imports from",
scikit-learn/sklearn/tests/test_import_deprecations.py,23,the same file.,
scikit-learn/sklearn/tests/test_import_deprecations.py,25,TODO: remove in 0.24,
scikit-learn/sklearn/tests/test_import_deprecations.py,27,Special case for:,
scikit-learn/sklearn/tests/test_import_deprecations.py,28,https://github.com/scikit-learn/scikit-learn/issues/15842,
scikit-learn/sklearn/tests/test_build.py,10,Check that sklearn is built with OpenMP-based parallelism enabled.,
scikit-learn/sklearn/tests/test_build.py,11,This test can be skipped by setting the environment variable,
scikit-learn/sklearn/tests/test_build.py,12,``SKLEARN_SKIP_OPENMP_TEST``.,
scikit-learn/sklearn/tests/test_check_build.py,5,Author: G Varoquaux,
scikit-learn/sklearn/tests/test_check_build.py,6,License: BSD 3 clause,
scikit-learn/sklearn/tests/test_multiclass.py,45,Fail on multioutput data,
scikit-learn/sklearn/tests/test_multiclass.py,55,Test that check_classification_target return correct type. #5782,
scikit-learn/sklearn/tests/test_multiclass.py,62,A classifier which implements decision_function.,
scikit-learn/sklearn/tests/test_multiclass.py,71,A classifier which implements predict_proba.,
scikit-learn/sklearn/tests/test_multiclass.py,78,Test if partial_fit is working as intended,
scikit-learn/sklearn/tests/test_multiclass.py,91,Test when mini batches doesn't have all classes,
scikit-learn/sklearn/tests/test_multiclass.py,92,with SGDClassifier,
scikit-learn/sklearn/tests/test_multiclass.py,106,test partial_fit only exists if estimator has it:,
scikit-learn/sklearn/tests/test_multiclass.py,116,A new class value which was not in the first call of partial_fit,
scikit-learn/sklearn/tests/test_multiclass.py,117,It should raise ValueError,
scikit-learn/sklearn/tests/test_multiclass.py,125,test that ovr and ovo work on regressors which don't have a decision_,
scikit-learn/sklearn/tests/test_multiclass.py,126,function,
scikit-learn/sklearn/tests/test_multiclass.py,131,we are doing something sensible,
scikit-learn/sklearn/tests/test_multiclass.py,138,we are doing something sensible,
scikit-learn/sklearn/tests/test_multiclass.py,168,Test predict_proba,
scikit-learn/sklearn/tests/test_multiclass.py,171,predict assigns a label if the probability that the,
scikit-learn/sklearn/tests/test_multiclass.py,172,sample has the label is greater than 0.5.,
scikit-learn/sklearn/tests/test_multiclass.py,176,Test decision_function,
scikit-learn/sklearn/tests/test_multiclass.py,184,Test that ovr works with classes that are always present or absent.,
scikit-learn/sklearn/tests/test_multiclass.py,185,Note: tests is the case where _ConstantPredictor is utilised,
scikit-learn/sklearn/tests/test_multiclass.py,189,Build an indicator matrix where two features are always on.,
scikit-learn/sklearn/tests/test_multiclass.py,190,"As list of lists, it would be: [[int(i >= 5), 2, 3] for i in range(10)]",
scikit-learn/sklearn/tests/test_multiclass.py,205,y has a constantly absent label,
scikit-learn/sklearn/tests/test_multiclass.py,207,variable label,
scikit-learn/sklearn/tests/test_multiclass.py,215,Toy dataset where features correspond directly to labels.,
scikit-learn/sklearn/tests/test_multiclass.py,234,test input as label indicator matrix,
scikit-learn/sklearn/tests/test_multiclass.py,241,Toy dataset where features correspond directly to labels.,
scikit-learn/sklearn/tests/test_multiclass.py,264,test input as label indicator matrix,
scikit-learn/sklearn/tests/test_multiclass.py,279,Toy dataset where features correspond directly to labels.,
scikit-learn/sklearn/tests/test_multiclass.py,341,Decision function only estimator.,
scikit-learn/sklearn/tests/test_multiclass.py,345,"Estimator with predict_proba disabled, depending on parameters.",
scikit-learn/sklearn/tests/test_multiclass.py,352,Estimator which can get predict_proba enabled after fitting,
scikit-learn/sklearn/tests/test_multiclass.py,363,predict assigns a label if the probability that the,
scikit-learn/sklearn/tests/test_multiclass.py,364,sample has the label is greater than 0.5.,
scikit-learn/sklearn/tests/test_multiclass.py,376,Decision function only estimator.,
scikit-learn/sklearn/tests/test_multiclass.py,384,predict assigns a label if the probability that the,
scikit-learn/sklearn/tests/test_multiclass.py,385,sample has the label is greater than 0.5.,
scikit-learn/sklearn/tests/test_multiclass.py,426,Test with pipeline of length one,
scikit-learn/sklearn/tests/test_multiclass.py,427,This test is needed because the multiclass estimators may fail to detect,
scikit-learn/sklearn/tests/test_multiclass.py,428,the presence of predict_proba or decision_function.,
scikit-learn/sklearn/tests/test_multiclass.py,440,SVC has sparse coef with sparse input data,
scikit-learn/sklearn/tests/test_multiclass.py,444,test with dense and sparse coef,
scikit-learn/sklearn/tests/test_multiclass.py,449,don't densify sparse coefficients,
scikit-learn/sklearn/tests/test_multiclass.py,455,Not fitted exception!,
scikit-learn/sklearn/tests/test_multiclass.py,457,lambda is needed because we don't want coef_ to be evaluated right away,
scikit-learn/sklearn/tests/test_multiclass.py,460,Doesn't have coef_ exception!,
scikit-learn/sklearn/tests/test_multiclass.py,472,Test that OneVsOne fitting works with a list of targets and yields the,
scikit-learn/sklearn/tests/test_multiclass.py,473,same output as predict from an array,
scikit-learn/sklearn/tests/test_multiclass.py,483,A classifier which implements decision_function.,
scikit-learn/sklearn/tests/test_multiclass.py,488,A classifier which implements predict_proba.,
scikit-learn/sklearn/tests/test_multiclass.py,509,Test when mini-batches have binary target classes,
scikit-learn/sklearn/tests/test_multiclass.py,531,raises error when mini-batch does not have classes from all_classes,
scikit-learn/sklearn/tests/test_multiclass.py,540,test partial_fit only exists if estimator has it:,
scikit-learn/sklearn/tests/test_multiclass.py,549,first binary,
scikit-learn/sklearn/tests/test_multiclass.py,554,then multi-class,
scikit-learn/sklearn/tests/test_multiclass.py,561,Compute the votes,
scikit-learn/sklearn/tests/test_multiclass.py,572,Extract votes and verify,
scikit-learn/sklearn/tests/test_multiclass.py,576,"For each sample and each class, there only 3 possible vote levels",
scikit-learn/sklearn/tests/test_multiclass.py,577,because they are only 3 distinct class pairs thus 3 distinct,
scikit-learn/sklearn/tests/test_multiclass.py,578,binary classifiers.,
scikit-learn/sklearn/tests/test_multiclass.py,579,"Therefore, sorting predictions based on votes would yield",
scikit-learn/sklearn/tests/test_multiclass.py,580,mostly tied predictions:,
scikit-learn/sklearn/tests/test_multiclass.py,583,The OVO decision function on the other hand is able to resolve,
scikit-learn/sklearn/tests/test_multiclass.py,584,most of the ties on this data as it combines both the vote counts,
scikit-learn/sklearn/tests/test_multiclass.py,585,and the aggregated confidence levels of the binary classifiers,
scikit-learn/sklearn/tests/test_multiclass.py,586,to compute the aggregate decision function. The iris dataset,
scikit-learn/sklearn/tests/test_multiclass.py,587,has 150 samples with a couple of duplicates. The OvO decisions,
scikit-learn/sklearn/tests/test_multiclass.py,588,can resolve most of the ties:,
scikit-learn/sklearn/tests/test_multiclass.py,602,"Test that ties are broken using the decision function,",
scikit-learn/sklearn/tests/test_multiclass.py,603,not defaulting to the smallest label,
scikit-learn/sklearn/tests/test_multiclass.py,611,"Classifiers are in order 0-1, 0-2, 1-2",
scikit-learn/sklearn/tests/test_multiclass.py,612,Use decision_function to compute the votes and the normalized,
scikit-learn/sklearn/tests/test_multiclass.py,613,"sum_of_confidences, which is used to disambiguate when there is a tie in",
scikit-learn/sklearn/tests/test_multiclass.py,614,votes.,
scikit-learn/sklearn/tests/test_multiclass.py,618,"For the first point, there is one vote per class",
scikit-learn/sklearn/tests/test_multiclass.py,620,"For the rest, there is no tie and the prediction is the argmax",
scikit-learn/sklearn/tests/test_multiclass.py,622,"For the tie, the prediction is the class with the highest score",
scikit-learn/sklearn/tests/test_multiclass.py,627,test that ties can not only be won by the first two labels,
scikit-learn/sklearn/tests/test_multiclass.py,631,cycle through labels so that each label wins once,
scikit-learn/sklearn/tests/test_multiclass.py,641,Test that the OvO doesn't mess up the encoding of string labels,
scikit-learn/sklearn/tests/test_multiclass.py,651,Test error for OvO with one class,
scikit-learn/sklearn/tests/test_multiclass.py,660,Test that the OvO errors on float targets,
scikit-learn/sklearn/tests/test_multiclass.py,674,A classifier which implements decision_function.,
scikit-learn/sklearn/tests/test_multiclass.py,680,A classifier which implements predict_proba.,
scikit-learn/sklearn/tests/test_multiclass.py,697,Test that the OCC errors on float targets,
scikit-learn/sklearn/tests/test_config.py,9,Not using as a context manager affects nothing,
scikit-learn/sklearn/tests/test_config.py,30,global setting will not be retained outside of context that,
scikit-learn/sklearn/tests/test_config.py,31,did not modify this setting,
scikit-learn/sklearn/tests/test_config.py,42,No positional arguments,
scikit-learn/sklearn/tests/test_config.py,44,No unknown arguments,
scikit-learn/sklearn/tests/test_config.py,70,No unknown arguments,
scikit-learn/sklearn/tests/test_pipeline.py,37,noqa,
scikit-learn/sklearn/tests/test_pipeline.py,144,store timestamp to figure out whether the result of 'fit' has been,
scikit-learn/sklearn/tests/test_pipeline.py,145,cached or not,
scikit-learn/sklearn/tests/test_pipeline.py,162,Test the various init parameters of the pipeline.,
scikit-learn/sklearn/tests/test_pipeline.py,164,Check that we can't instantiate pipelines with objects without fit,
scikit-learn/sklearn/tests/test_pipeline.py,165,method,
scikit-learn/sklearn/tests/test_pipeline.py,171,Smoke test with only an estimator,
scikit-learn/sklearn/tests/test_pipeline.py,178,Check that params are set,
scikit-learn/sklearn/tests/test_pipeline.py,182,Smoke test the repr:,
scikit-learn/sklearn/tests/test_pipeline.py,185,Test with two objects,
scikit-learn/sklearn/tests/test_pipeline.py,190,Check that estimators are not cloned on pipeline construction,
scikit-learn/sklearn/tests/test_pipeline.py,194,Check that we can't instantiate with non-transformers on the way,
scikit-learn/sklearn/tests/test_pipeline.py,195,"Note that NoTrans implements fit, but not transform",
scikit-learn/sklearn/tests/test_pipeline.py,201,Check that params are set,
scikit-learn/sklearn/tests/test_pipeline.py,204,Smoke test the repr:,
scikit-learn/sklearn/tests/test_pipeline.py,207,Check that params are not set when naming them wrong,
scikit-learn/sklearn/tests/test_pipeline.py,210,Test clone,
scikit-learn/sklearn/tests/test_pipeline.py,214,"Check that apart from estimators, the parameters are the same",
scikit-learn/sklearn/tests/test_pipeline.py,224,Remove estimators that where copied,
scikit-learn/sklearn/tests/test_pipeline.py,233,Pipeline accepts steps as tuple,
scikit-learn/sklearn/tests/test_pipeline.py,245,Test the various methods of the pipeline (anova).,
scikit-learn/sklearn/tests/test_pipeline.py,248,Test with Anova + LogisticRegression,
scikit-learn/sklearn/tests/test_pipeline.py,260,Test that the pipeline can take fit parameters,
scikit-learn/sklearn/tests/test_pipeline.py,263,classifier should return True,
scikit-learn/sklearn/tests/test_pipeline.py,265,and transformer params should not be changed,
scikit-learn/sklearn/tests/test_pipeline.py,268,invalid parameters should raise an error message,
scikit-learn/sklearn/tests/test_pipeline.py,277,Pipeline should pass sample_weight,
scikit-learn/sklearn/tests/test_pipeline.py,288,When sample_weight is None it shouldn't be passed,
scikit-learn/sklearn/tests/test_pipeline.py,302,Test pipeline raises set params error message for nested models.,
scikit-learn/sklearn/tests/test_pipeline.py,305,expected error message,
scikit-learn/sklearn/tests/test_pipeline.py,315,nested model check,
scikit-learn/sklearn/tests/test_pipeline.py,323,Test the various methods of the pipeline (pca + svm).,
scikit-learn/sklearn/tests/test_pipeline.py,326,Test with PCA + SVC,
scikit-learn/sklearn/tests/test_pipeline.py,339,Test that the score_samples method is implemented on a pipeline.,
scikit-learn/sklearn/tests/test_pipeline.py,340,Test that the score_samples method on pipeline yields same results as,
scikit-learn/sklearn/tests/test_pipeline.py,341,applying transform and score_samples steps separately.,
scikit-learn/sklearn/tests/test_pipeline.py,346,Check the shapes,
scikit-learn/sklearn/tests/test_pipeline.py,348,Check the values,
scikit-learn/sklearn/tests/test_pipeline.py,356,Test that a pipeline does not have score_samples method when the final,
scikit-learn/sklearn/tests/test_pipeline.py,357,step of the pipeline does not have score_samples defined.,
scikit-learn/sklearn/tests/test_pipeline.py,367,Test the various methods of the pipeline (preprocessing + svm).,
scikit-learn/sklearn/tests/test_pipeline.py,380,check shapes of various prediction functions,
scikit-learn/sklearn/tests/test_pipeline.py,397,test that the fit_predict method is implemented on a pipeline,
scikit-learn/sklearn/tests/test_pipeline.py,398,test that the fit_predict on pipeline yields same results as applying,
scikit-learn/sklearn/tests/test_pipeline.py,399,transform and clustering steps separately,
scikit-learn/sklearn/tests/test_pipeline.py,402,"As pipeline doesn't clone estimators on construction,",
scikit-learn/sklearn/tests/test_pipeline.py,403,it must have its own estimators,
scikit-learn/sklearn/tests/test_pipeline.py,407,first compute the transform and clustering step separately,
scikit-learn/sklearn/tests/test_pipeline.py,411,use a pipeline to do the transform and clustering in one step,
scikit-learn/sklearn/tests/test_pipeline.py,422,tests that a pipeline does not have fit_predict method when final,
scikit-learn/sklearn/tests/test_pipeline.py,423,step of pipeline does not have fit_predict defined,
scikit-learn/sklearn/tests/test_pipeline.py,433,tests that Pipeline passes fit_params to intermediate steps,
scikit-learn/sklearn/tests/test_pipeline.py,434,when fit_predict is invoked,
scikit-learn/sklearn/tests/test_pipeline.py,446,tests that Pipeline passes predict_params to the final estimator,
scikit-learn/sklearn/tests/test_pipeline.py,447,when predict is invoked,
scikit-learn/sklearn/tests/test_pipeline.py,456,basic sanity check for feature union,
scikit-learn/sklearn/tests/test_pipeline.py,467,check if it does the expected thing,
scikit-learn/sklearn/tests/test_pipeline.py,472,test if it also works for sparse input,
scikit-learn/sklearn/tests/test_pipeline.py,473,We use a different svd object to control the random_state stream,
scikit-learn/sklearn/tests/test_pipeline.py,479,Test clone,
scikit-learn/sklearn/tests/test_pipeline.py,483,test setting parameters,
scikit-learn/sklearn/tests/test_pipeline.py,487,test it works with transformers missing fit_transform,
scikit-learn/sklearn/tests/test_pipeline.py,492,test error if some elements do not support transform,
scikit-learn/sklearn/tests/test_pipeline.py,499,test that init accepts tuples,
scikit-learn/sklearn/tests/test_pipeline.py,519,invalid keyword parameters should raise an error message,
scikit-learn/sklearn/tests/test_pipeline.py,528,Test whether pipeline works with a transformer at the end.,
scikit-learn/sklearn/tests/test_pipeline.py,529,Also test pipeline.transform and pipeline.inverse_transform,
scikit-learn/sklearn/tests/test_pipeline.py,534,test transform and fit_transform:,
scikit-learn/sklearn/tests/test_pipeline.py,547,Test whether pipeline works with a transformer missing fit_transform,
scikit-learn/sklearn/tests/test_pipeline.py,553,test fit_transform:,
scikit-learn/sklearn/tests/test_pipeline.py,588,Directly setting attr,
scikit-learn/sklearn/tests/test_pipeline.py,594,Using set_params,
scikit-learn/sklearn/tests/test_pipeline.py,598,Using set_params to replace single step,
scikit-learn/sklearn/tests/test_pipeline.py,602,With invalid data,
scikit-learn/sklearn/tests/test_pipeline.py,613,Test access via named_steps bunch object,
scikit-learn/sklearn/tests/test_pipeline.py,619,Test bunch with conflict attribute of dict,
scikit-learn/sklearn/tests/test_pipeline.py,686,"for other methods, ensure no AttributeErrors on None:",
scikit-learn/sklearn/tests/test_pipeline.py,700,mult2 and mult3 are active,
scikit-learn/sklearn/tests/test_pipeline.py,709,Check 'passthrough' step at construction time,
scikit-learn/sklearn/tests/test_pipeline.py,768,test feature union with transformer weights,
scikit-learn/sklearn/tests/test_pipeline.py,773,test using fit followed by transform,
scikit-learn/sklearn/tests/test_pipeline.py,778,test using fit_transform,
scikit-learn/sklearn/tests/test_pipeline.py,782,test it works with transformers missing fit_transform,
scikit-learn/sklearn/tests/test_pipeline.py,786,check against expected result,
scikit-learn/sklearn/tests/test_pipeline.py,788,We use a different pca object to control the random_state stream,
scikit-learn/sklearn/tests/test_pipeline.py,800,test that n_jobs work for FeatureUnion,
scikit-learn/sklearn/tests/test_pipeline.py,830,fit_transform should behave the same,
scikit-learn/sklearn/tests/test_pipeline.py,837,transformers should stay fit after fit_transform,
scikit-learn/sklearn/tests/test_pipeline.py,887,Directly setting attr,
scikit-learn/sklearn/tests/test_pipeline.py,892,Using set_params,
scikit-learn/sklearn/tests/test_pipeline.py,897,Using set_params to replace single step,
scikit-learn/sklearn/tests/test_pipeline.py,903,TODO: Remove parametrization in 0.24 when None is removed for FeatureUnion,
scikit-learn/sklearn/tests/test_pipeline.py,932,check we can change back,
scikit-learn/sklearn/tests/test_pipeline.py,938,Check 'drop' step at construction time,
scikit-learn/sklearn/tests/test_pipeline.py,951,we validate in construction (despite scikit-learn convention),
scikit-learn/sklearn/tests/test_pipeline.py,959,three ways to make invalid:,
scikit-learn/sklearn/tests/test_pipeline.py,960,- construction,
scikit-learn/sklearn/tests/test_pipeline.py,964,- setattr,
scikit-learn/sklearn/tests/test_pipeline.py,971,- set_params,
scikit-learn/sklearn/tests/test_pipeline.py,990,Test that an error is raised when memory is not a string or a Memory,
scikit-learn/sklearn/tests/test_pipeline.py,991,instance,
scikit-learn/sklearn/tests/test_pipeline.py,994,Define memory as an integer,
scikit-learn/sklearn/tests/test_pipeline.py,1031,Deal with change of API in joblib,
scikit-learn/sklearn/tests/test_pipeline.py,1035,Test with Transformer + SVC,
scikit-learn/sklearn/tests/test_pipeline.py,1042,Memoize the transformer at the first fit,
scikit-learn/sklearn/tests/test_pipeline.py,1045,Get the time stamp of the transformer in the cached pipeline,
scikit-learn/sklearn/tests/test_pipeline.py,1047,Check that cached_pipe and pipe yield identical results,
scikit-learn/sklearn/tests/test_pipeline.py,1056,Check that we are reading the cache while fitting,
scikit-learn/sklearn/tests/test_pipeline.py,1057,a second time,
scikit-learn/sklearn/tests/test_pipeline.py,1059,Check that cached_pipe and pipe yield identical results,
scikit-learn/sklearn/tests/test_pipeline.py,1068,Create a new pipeline with cloned estimators,
scikit-learn/sklearn/tests/test_pipeline.py,1069,Check that even changing the name step does not affect the cache hit,
scikit-learn/sklearn/tests/test_pipeline.py,1076,Check that cached_pipe and pipe yield identical results,
scikit-learn/sklearn/tests/test_pipeline.py,1093,Deal with change of API in joblib,
scikit-learn/sklearn/tests/test_pipeline.py,1167,make sure pipelines delegate n_features_in to the first step,
scikit-learn/sklearn/tests/test_pipeline.py,1179,if the first step has the n_features_in attribute then the pipeline also,
scikit-learn/sklearn/tests/test_pipeline.py,1180,"has it, even though it isn't fitted.",
scikit-learn/sklearn/tests/test_pipeline.py,1190,make sure FeatureUnion delegates n_features_in to the first transformer,
scikit-learn/sklearn/tests/test_pipeline.py,1201,if the first step has the n_features_in attribute then the feature_union,
scikit-learn/sklearn/tests/test_pipeline.py,1202,"also has it, even though it isn't fitted.",
scikit-learn/sklearn/tests/test_pipeline.py,1210,Regression test for issue: #15117,
scikit-learn/sklearn/tests/test_pipeline.py,1232,TODO: Remove in 0.24 when None is removed,
scikit-learn/sklearn/tests/test_docstring_parameters.py,1,Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/sklearn/tests/test_docstring_parameters.py,2,Raghav RV <rvraghav93@gmail.com>,
scikit-learn/sklearn/tests/test_docstring_parameters.py,3,License: BSD 3 clause,
scikit-learn/sklearn/tests/test_docstring_parameters.py,30,"walk_packages() ignores DeprecationWarnings, now we need to ignore",
scikit-learn/sklearn/tests/test_docstring_parameters.py,31,FutureWarnings,
scikit-learn/sklearn/tests/test_docstring_parameters.py,37,"mypy error: Module has no attribute ""__path__""",
scikit-learn/sklearn/tests/test_docstring_parameters.py,38,type: ignore  # mypy issue #1422,
scikit-learn/sklearn/tests/test_docstring_parameters.py,42,functions to ignore args / docstring of,
scikit-learn/sklearn/tests/test_docstring_parameters.py,51,Methods where y param should be ignored if y=None by default,
scikit-learn/sklearn/tests/test_docstring_parameters.py,62,numpydoc 0.8.0's docscrape tool raises because of collections.abc under,
scikit-learn/sklearn/tests/test_docstring_parameters.py,63,Python 3.7,
scikit-learn/sklearn/tests/test_docstring_parameters.py,68,Test module docstring formatting,
scikit-learn/sklearn/tests/test_docstring_parameters.py,70,Skip test if numpydoc is not found,
scikit-learn/sklearn/tests/test_docstring_parameters.py,74,XXX unreached code as of v0.22,
scikit-learn/sklearn/tests/test_docstring_parameters.py,80,We cannot always control these docstrings,
scikit-learn/sklearn/tests/test_docstring_parameters.py,85,Exclude imported classes,
scikit-learn/sklearn/tests/test_docstring_parameters.py,112,Now skip docstring test for y when y is None,
scikit-learn/sklearn/tests/test_docstring_parameters.py,113,by default for API reason,
scikit-learn/sklearn/tests/test_docstring_parameters.py,118,ignore y for fit and score,
scikit-learn/sklearn/tests/test_docstring_parameters.py,126,Exclude imported functions,
scikit-learn/sklearn/tests/test_docstring_parameters.py,129,Don't test private methods / functions,
scikit-learn/sklearn/tests/test_docstring_parameters.py,146,Test that there are no tabs in our source files,
scikit-learn/sklearn/tests/test_docstring_parameters.py,154,because we don't import,
scikit-learn/sklearn/tests/test_docstring_parameters.py,157,TODO: Remove when minimum python version is 3.7,
scikit-learn/sklearn/tests/test_docstring_parameters.py,158,unwrap to get module because Pep562 backport wraps the original,
scikit-learn/sklearn/tests/test_docstring_parameters.py,159,module,
scikit-learn/sklearn/tests/test_docstring_parameters.py,165,"user probably should have run ""make clean""",
scikit-learn/sklearn/tests/test_docstring_parameters.py,222,"As certain attributes are present ""only"" if a certain parameter is",
scikit-learn/sklearn/tests/test_docstring_parameters.py,223,"provided, this checks if the word ""only"" is present in the attribute",
scikit-learn/sklearn/tests/test_docstring_parameters.py,224,"description, and if not the attribute is required to be present.",
scikit-learn/sklearn/neighbors/_regression.py,3,Authors: Jake Vanderplas <vanderplas@astro.washington.edu>,
scikit-learn/sklearn/neighbors/_regression.py,4,Fabian Pedregosa <fabian.pedregosa@inria.fr>,
scikit-learn/sklearn/neighbors/_regression.py,5,Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/sklearn/neighbors/_regression.py,6,Sparseness support by Lars Buitinck,
scikit-learn/sklearn/neighbors/_regression.py,7,Multi-output support by Arnaud Joly <a.joly@ulg.ac.be>,
scikit-learn/sklearn/neighbors/_regression.py,8,Empty radius support by Andreas Bjerre-Nielsen,
scikit-learn/sklearn/neighbors/_regression.py,9,,
scikit-learn/sklearn/neighbors/_regression.py,10,"License: BSD 3 clause (C) INRIA, University of Amsterdam,",
scikit-learn/sklearn/neighbors/_regression.py,11,University of Copenhagen,
scikit-learn/sklearn/neighbors/_regression.py,155,For cross-validation routines to split data correctly,
scikit-learn/sklearn/neighbors/_classification.py,3,Authors: Jake Vanderplas <vanderplas@astro.washington.edu>,
scikit-learn/sklearn/neighbors/_classification.py,4,Fabian Pedregosa <fabian.pedregosa@inria.fr>,
scikit-learn/sklearn/neighbors/_classification.py,5,Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/sklearn/neighbors/_classification.py,6,Sparseness support by Lars Buitinck,
scikit-learn/sklearn/neighbors/_classification.py,7,Multi-output support by Arnaud Joly <a.joly@ulg.ac.be>,
scikit-learn/sklearn/neighbors/_classification.py,8,,
scikit-learn/sklearn/neighbors/_classification.py,9,"License: BSD 3 clause (C) INRIA, University of Amsterdam",
scikit-learn/sklearn/neighbors/_classification.py,237,a simple ':' index doesn't work right,
scikit-learn/sklearn/neighbors/_classification.py,238,loop is O(n_neighbors),
scikit-learn/sklearn/neighbors/_classification.py,241,"normalize 'votes' into real [0,1] probabilities",
scikit-learn/sklearn/neighbors/_classification.py,419,"iterate over multi-output, get the most frequest label for each",
scikit-learn/sklearn/neighbors/_classification.py,420,output.,
scikit-learn/sklearn/neighbors/_classification.py,440,ensure the outlier lable for each output is a scalar.,
scikit-learn/sklearn/neighbors/_classification.py,445,ensure the dtype of outlier label is consistent with y.,
scikit-learn/sklearn/neighbors/_classification.py,480,"iterate over multi-output, assign labels based on probabilities",
scikit-learn/sklearn/neighbors/_classification.py,481,of each output.,
scikit-learn/sklearn/neighbors/_classification.py,539,"iterate over multi-output, measure probabilities of the k-th output.",
scikit-learn/sklearn/neighbors/_classification.py,547,samples have different size of neighbors within the same radius,
scikit-learn/sklearn/neighbors/_classification.py,570,"normalize 'votes' into real [0,1] probabilities",
scikit-learn/sklearn/neighbors/_kde.py,5,Author: Jake Vanderplas <jakevdp@cs.washington.edu>,
scikit-learn/sklearn/neighbors/_kde.py,23,TODO: implement a brute force version for testing purposes,
scikit-learn/sklearn/neighbors/_kde.py,24,TODO: bandwidth estimation,
scikit-learn/sklearn/neighbors/_kde.py,25,TODO: create a density estimation base class?,
scikit-learn/sklearn/neighbors/_kde.py,105,run the choose algorithm code so that exceptions will happen here,
scikit-learn/sklearn/neighbors/_kde.py,106,"we're using clone() in the GenerativeBayes classifier,",
scikit-learn/sklearn/neighbors/_kde.py,107,so we can't do this kind of logic in __init__,
scikit-learn/sklearn/neighbors/_kde.py,116,"given the algorithm string + metric string, choose the optimal",
scikit-learn/sklearn/neighbors/_kde.py,117,algorithm to compute the result.,
scikit-learn/sklearn/neighbors/_kde.py,119,use KD Tree if possible,
scikit-learn/sklearn/neighbors/_kde.py,188,The returned density is normalized to the number of points.,
scikit-learn/sklearn/neighbors/_kde.py,189,"For it to be a probability, we must scale it.  For this reason",
scikit-learn/sklearn/neighbors/_kde.py,190,we'll also scale atol.,
scikit-learn/sklearn/neighbors/_kde.py,246,TODO: implement sampling for other valid kernel shapes,
scikit-learn/sklearn/neighbors/_kde.py,264,"we first draw points from a d-dimensional normal distribution,",
scikit-learn/sklearn/neighbors/_kde.py,265,then use an incomplete gamma function to map them to a uniform,
scikit-learn/sklearn/neighbors/_kde.py,266,d-dimensional tophat distribution.,
scikit-learn/sklearn/neighbors/_graph.py,3,Author: Jake Vanderplas <vanderplas@astro.washington.edu>,
scikit-learn/sklearn/neighbors/_graph.py,4,Tom Dupre la Tour,
scikit-learn/sklearn/neighbors/_graph.py,5,,
scikit-learn/sklearn/neighbors/_graph.py,6,"License: BSD 3 clause (C) INRIA, University of Amsterdam",
scikit-learn/sklearn/neighbors/_graph.py,33,it does not include each sample as its own neighbors,
scikit-learn/sklearn/neighbors/_nca.py,1,coding: utf-8,
scikit-learn/sklearn/neighbors/_nca.py,6,Authors: William de Vazelhes <wdevazelhes@gmail.com>,
scikit-learn/sklearn/neighbors/_nca.py,7,John Chiotellis <ioannis.chiotellis@in.tum.de>,
scikit-learn/sklearn/neighbors/_nca.py,8,License: BSD 3 clause,
scikit-learn/sklearn/neighbors/_nca.py,193,"Verify inputs X and y and NCA parameters, and transform a copy if",
scikit-learn/sklearn/neighbors/_nca.py,194,needed,
scikit-learn/sklearn/neighbors/_nca.py,197,Initialize the random generator,
scikit-learn/sklearn/neighbors/_nca.py,200,Measure the total training time,
scikit-learn/sklearn/neighbors/_nca.py,203,Compute a mask that stays fixed during optimization:,
scikit-learn/sklearn/neighbors/_nca.py,205,"(n_samples, n_samples)",
scikit-learn/sklearn/neighbors/_nca.py,207,Initialize the transformation,
scikit-learn/sklearn/neighbors/_nca.py,210,Create a dictionary of parameters to be passed to the optimizer,
scikit-learn/sklearn/neighbors/_nca.py,222,Call the optimizer,
scikit-learn/sklearn/neighbors/_nca.py,226,Reshape the solution found by the optimizer,
scikit-learn/sklearn/neighbors/_nca.py,229,Stop timer,
scikit-learn/sklearn/neighbors/_nca.py,234,Warn the user if the algorithm did not converge,
scikit-learn/sklearn/neighbors/_nca.py,301,"Validate the inputs X and y, and converts y to numerical classes.",
scikit-learn/sklearn/neighbors/_nca.py,306,Check the preferred dimensionality of the projected space,
scikit-learn/sklearn/neighbors/_nca.py,318,"If warm_start is enabled, check that the inputs are consistent",
scikit-learn/sklearn/neighbors/_nca.py,336,Check how the linear transformation should be initialized,
scikit-learn/sklearn/neighbors/_nca.py,342,Assert that init.shape[1] = X.shape[1],
scikit-learn/sklearn/neighbors/_nca.py,350,Assert that init.shape[0] <= init.shape[1],
scikit-learn/sklearn/neighbors/_nca.py,359,Assert that self.n_components = init.shape[0],
scikit-learn/sklearn/neighbors/_nca.py,495,"(n_samples, n_components)",
scikit-learn/sklearn/neighbors/_nca.py,497,Compute softmax distances,
scikit-learn/sklearn/neighbors/_nca.py,500,"(n_samples, n_samples)",
scikit-learn/sklearn/neighbors/_nca.py,502,Compute loss,
scikit-learn/sklearn/neighbors/_nca.py,504,"(n_samples, 1)",
scikit-learn/sklearn/neighbors/_nca.py,507,Compute gradient of loss w.r.t. `transform`,
scikit-learn/sklearn/neighbors/_nca.py,512,time complexity of the gradient: O(n_components x n_samples x (,
scikit-learn/sklearn/neighbors/_nca.py,513,n_samples + n_features)),
scikit-learn/sklearn/neighbors/_lof.py,1,Authors: Nicolas Goix <nicolas.goix@telecom-paristech.fr>,
scikit-learn/sklearn/neighbors/_lof.py,2,Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>,
scikit-learn/sklearn/neighbors/_lof.py,3,License: BSD 3 clause,
scikit-learn/sklearn/neighbors/_lof.py,199,"As fit_predict would be different from fit.predict, fit_predict is",
scikit-learn/sklearn/neighbors/_lof.py,200,only available for outlier detection (novelty=False),
scikit-learn/sklearn/neighbors/_lof.py,227,"As fit_predict would be different from fit.predict, fit_predict is",
scikit-learn/sklearn/neighbors/_lof.py,228,only available for outlier detection (novelty=False),
scikit-learn/sklearn/neighbors/_lof.py,270,Compute lof score over training samples to define offset_:,
scikit-learn/sklearn/neighbors/_lof.py,277,"inliers score around -1 (the higher, the less abnormal).",
scikit-learn/sklearn/neighbors/_lof.py,481,as bigger is better:,
scikit-learn/sklearn/neighbors/_lof.py,509,1e-10 to avoid `nan' when nb of duplicates > n_neighbors_:,
scikit-learn/sklearn/neighbors/_nearest_centroid.py,1,-*- coding: utf-8 -*-,
scikit-learn/sklearn/neighbors/_nearest_centroid.py,6,Author: Robert Layton <robertlayton@gmail.com>,
scikit-learn/sklearn/neighbors/_nearest_centroid.py,7,Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/sklearn/neighbors/_nearest_centroid.py,8,,
scikit-learn/sklearn/neighbors/_nearest_centroid.py,9,License: BSD 3 clause,
scikit-learn/sklearn/neighbors/_nearest_centroid.py,104,"If X is sparse and the metric is ""manhattan"", store it in a csc",
scikit-learn/sklearn/neighbors/_nearest_centroid.py,105,format is easier to calculate the median.,
scikit-learn/sklearn/neighbors/_nearest_centroid.py,125,Mask mapping each class to its members.,
scikit-learn/sklearn/neighbors/_nearest_centroid.py,127,Number of clusters in each class.,
scikit-learn/sklearn/neighbors/_nearest_centroid.py,136,XXX: Update other averaging methods according to the metrics.,
scikit-learn/sklearn/neighbors/_nearest_centroid.py,138,NumPy does not calculate median of sparse matrices.,
scikit-learn/sklearn/neighbors/_nearest_centroid.py,154,m parameter for determining deviation,
scikit-learn/sklearn/neighbors/_nearest_centroid.py,156,Calculate deviation using the standard deviation of centroids.,
scikit-learn/sklearn/neighbors/_nearest_centroid.py,160,To deter outliers from affecting the results.,
scikit-learn/sklearn/neighbors/_nearest_centroid.py,161,Reshape to allow broadcasting.,
scikit-learn/sklearn/neighbors/_nearest_centroid.py,164,"Soft thresholding: if the deviation crosses 0 during shrinking,",
scikit-learn/sklearn/neighbors/_nearest_centroid.py,165,it becomes zero.,
scikit-learn/sklearn/neighbors/_nearest_centroid.py,170,Now adjust the centroids using the deviation,
scikit-learn/sklearn/neighbors/_base.py,2,Authors: Jake Vanderplas <vanderplas@astro.washington.edu>,
scikit-learn/sklearn/neighbors/_base.py,3,Fabian Pedregosa <fabian.pedregosa@inria.fr>,
scikit-learn/sklearn/neighbors/_base.py,4,Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/sklearn/neighbors/_base.py,5,Sparseness support by Lars Buitinck,
scikit-learn/sklearn/neighbors/_base.py,6,Multi-output support by Arnaud Joly <a.joly@ulg.ac.be>,
scikit-learn/sklearn/neighbors/_base.py,7,,
scikit-learn/sklearn/neighbors/_base.py,8,"License: BSD 3 clause (C) INRIA, University of Amsterdam",
scikit-learn/sklearn/neighbors/_base.py,35,The following list comes from the,
scikit-learn/sklearn/neighbors/_base.py,36,sklearn.metrics.pairwise doc string,
scikit-learn/sklearn/neighbors/_base.py,82,if user attempts to classify a point that was zero distance from one,
scikit-learn/sklearn/neighbors/_base.py,83,"or more training points, those training points are weighted as 1.0",
scikit-learn/sklearn/neighbors/_base.py,84,and the other points as 0.0,
scikit-learn/sklearn/neighbors/_base.py,87,check if point_dist is iterable,
scikit-learn/sklearn/neighbors/_base.py,88,(ex: RadiusNeighborClassifier.predict may set an element of,
scikit-learn/sklearn/neighbors/_base.py,89,dist to 1e-6 to represent an 'outlier'),
scikit-learn/sklearn/neighbors/_base.py,172,if each sample has the same number of provided neighbors,
scikit-learn/sklearn/neighbors/_base.py,217,number of neighbors by samples,
scikit-learn/sklearn/neighbors/_base.py,228,if each sample has the same number of provided neighbors,
scikit-learn/sklearn/neighbors/_base.py,325,callable metric is only valid for brute force and ball_tree,
scikit-learn/sklearn/neighbors/_base.py,361,"For minkowski distance, use more efficient methods where available",
scikit-learn/sklearn/neighbors/_base.py,407,Precomputed matrix X must be squared,
scikit-learn/sklearn/neighbors/_base.py,436,"A tree approach is better for small number of neighbors,",
scikit-learn/sklearn/neighbors/_base.py,437,and KDTree is generally faster when available,
scikit-learn/sklearn/neighbors/_base.py,482,For cross-validation routines to split data correctly,
scikit-learn/sklearn/neighbors/_base.py,521,"argpartition doesn't guarantee sorted order, so we sort again",
scikit-learn/sklearn/neighbors/_base.py,610,Include an extra neighbor to account for the sample itself being,
scikit-learn/sklearn/neighbors/_base.py,611,"returned, which is removed later",
scikit-learn/sklearn/neighbors/_base.py,635,"for efficiency, use squared euclidean distances",
scikit-learn/sklearn/neighbors/_base.py,654,Deal with change of API in joblib,
scikit-learn/sklearn/neighbors/_base.py,680,"If the query data is the same as the indexed data, we would like",
scikit-learn/sklearn/neighbors/_base.py,681,"to ignore the first nearest neighbor of every sample, i.e",
scikit-learn/sklearn/neighbors/_base.py,682,the sample itself.,
scikit-learn/sklearn/neighbors/_base.py,692,Corner case: When the number of duplicates are more,
scikit-learn/sklearn/neighbors/_base.py,693,"than the number of neighbors, the first NN will not",
scikit-learn/sklearn/neighbors/_base.py,694,"be the sample, but a duplicate.",
scikit-learn/sklearn/neighbors/_base.py,695,In that case mask the first duplicate.,
scikit-learn/sklearn/neighbors/_base.py,755,check the input only in self.kneighbors,
scikit-learn/sklearn/neighbors/_base.py,757,construct CSR matrix representation of the k-NN graph,
scikit-learn/sklearn/neighbors/_base.py,926,"for efficiency, use squared euclidean distances",
scikit-learn/sklearn/neighbors/_base.py,960,Deal with change of API in joblib,
scikit-learn/sklearn/neighbors/_base.py,985,"If the query data is the same as the indexed data, we would like",
scikit-learn/sklearn/neighbors/_base.py,986,"to ignore the first nearest neighbor of every sample, i.e",
scikit-learn/sklearn/neighbors/_base.py,987,the sample itself.,
scikit-learn/sklearn/neighbors/_base.py,1059,check the input only in self.radius_neighbors,
scikit-learn/sklearn/neighbors/_base.py,1064,construct CSR matrix representation of the NN graph,
scikit-learn/sklearn/neighbors/tests/test_kde.py,16,"XXX Duplicated in test_neighbors_tree, test_kde",
scikit-learn/sklearn/neighbors/tests/test_kde.py,76,draw a tophat sample,
scikit-learn/sklearn/neighbors/tests/test_kde.py,81,check that samples are in the right range,
scikit-learn/sklearn/neighbors/tests/test_kde.py,88,"5 standard deviations is safe for 100 samples, but there's a",
scikit-learn/sklearn/neighbors/tests/test_kde.py,89,very small chance this test could fail.,
scikit-learn/sklearn/neighbors/tests/test_kde.py,92,check unsupported kernels,
scikit-learn/sklearn/neighbors/tests/test_kde.py,97,non-regression test: used to return a scalar,
scikit-learn/sklearn/neighbors/tests/test_kde.py,108,Smoke test for various metrics and algorithms,
scikit-learn/sklearn/neighbors/tests/test_kde.py,110,2 features required for haversine dist.,
scikit-learn/sklearn/neighbors/tests/test_kde.py,125,FIXME,
scikit-learn/sklearn/neighbors/tests/test_kde.py,126,rng = np.random.RandomState(0),
scikit-learn/sklearn/neighbors/tests/test_kde.py,127,"X = rng.random_sample((n_samples, n_features))",
scikit-learn/sklearn/neighbors/tests/test_kde.py,128,"Y = rng.random_sample((n_samples, n_features))",
scikit-learn/sklearn/neighbors/tests/test_kde.py,150,test that kde plays nice in pipelines and grid-searches,
scikit-learn/sklearn/neighbors/tests/test_kde.py,178,Test that adding a constant sample weight has no effect,
scikit-learn/sklearn/neighbors/tests/test_kde.py,188,Test equivalence between sampling and (integer) weights,
scikit-learn/sklearn/neighbors/tests/test_kde.py,198,Test that sample weights has a non-trivial effect,
scikit-learn/sklearn/neighbors/tests/test_kde.py,202,Test invariance with respect to arbitrary scaling,
scikit-learn/sklearn/neighbors/tests/test_kde.py,210,Check sample weighting raises errors.,
scikit-learn/sklearn/neighbors/tests/test_kde.py,222,Make sure that predictions are the same before and after pickling. Used,
scikit-learn/sklearn/neighbors/tests/test_kde.py,223,to be a bug because sample_weights wasn't pickled and the resulting tree,
scikit-learn/sklearn/neighbors/tests/test_kde.py,224,would miss some info.,
scikit-learn/sklearn/neighbors/tests/test_kde.py,243,Check that predict raises an exception in an unfitted estimator.,
scikit-learn/sklearn/neighbors/tests/test_kde.py,244,Unfitted estimators should raise a NotFittedError.,
scikit-learn/sklearn/neighbors/tests/test_neighbors_tree.py,1,License: BSD 3 clause,
scikit-learn/sklearn/neighbors/tests/test_neighbors_tree.py,100,roundoff error can cause test to fail,
scikit-learn/sklearn/neighbors/tests/test_neighbors_tree.py,121,roundoff error can cause test to fail,
scikit-learn/sklearn/neighbors/tests/test_neighbors_tree.py,197,simultaneous sort rows using function,
scikit-learn/sklearn/neighbors/tests/test_neighbors_tree.py,200,simultaneous sort rows using numpy,
scikit-learn/sklearn/neighbors/tests/test_neighbors_tree.py,212,Compare gaussian KDE results to scipy.stats.gaussian_kde,
scikit-learn/sklearn/neighbors/tests/test_neighbors_tree.py,248,"don't check indices here: if there are any duplicate distances,",
scikit-learn/sklearn/neighbors/tests/test_neighbors_tree.py,249,the indices may not match.  Distances should not have this problem.,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,33,load and shuffle iris dataset,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,39,load and shuffle digits,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,53,Filter deprecation warnings.,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,64,"Dist could be multidimensional, flatten it so all values",
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,65,can be looped,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,73,Test unsupervised neighbors methods,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,99,test the types of valid input into NearestNeighbors,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,118,Test to check whether n_neighbors is integer,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,142,Note: smaller samples may result in spurious test success,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,148,"TODO: also test radius_neighbors, but requires different assertion",
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,150,As a feature matrix (n_samples by n_features),
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,155,As a dense distance matrix (n_samples by n_samples),
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,163,Check auto works too,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,171,Check X=None in prediction,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,177,Must raise a ValueError if the matrix is not of correct shape,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,209,We do not test RadiusNeighborsClassifier and RadiusNeighborsRegressor,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,210,since the precomputed neighbors graph is built with k neighbors only.,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,227,We do not test KNeighborsClassifier and KNeighborsRegressor,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,228,since the precomputed neighbors graph is built with a radius.,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,237,"Test that _is_sorted_by_data works as expected. In CSR sparse matrix,",
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,238,"entries in each row can be sorted by indices, by data, or unsorted.",
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,239,"_is_sorted_by_data should return True when entries are sorted by data,",
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,240,and False in all other cases.,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,242,Test with sorted 1D array,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,245,Test with unsorted 1D array,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,249,"Test when the data is sorted in each sample, but not necessarily",
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,250,between samples,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,254,Test with duplicates entries in X.indptr,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,262,Test that _check_precomputed returns a graph sorted by data,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,268,est with a different number of nonzero entries for each sample,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,287,Ensures enough number of nearest neighbors,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,294,Checks error with inconsistent distance matrix,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,303,Ensure array is split correctly,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,320,Test unsupervised radius-based query,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,338,sort the results: this is not done automatically for,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,339,radius searches,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,363,Test k-neighbors classification,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,380,Test prediction with y_str,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,389,Test k-neighbors classification,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,402,Test KNeighborsClassifier.predict_proba() method,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,410,cityblock dist,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,420,Check that it also works with non integer labels,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,424,Check that it works with weights='distance',
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,438,Test radius-based classification,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,461,Test radius-based classifier when no neighbors found.,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,462,In this case it should rise an informative exception,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,468,no outliers,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,469,one outlier,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,487,Test radius-based classifier when no neighbors found and outliers,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,488,are labeled.,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,495,no outliers,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,496,one outlier,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,514,test outlier_labeling of using predict_proba(),
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,519,test outlier_label scalar verification,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,525,test invalid outlier_label dtype,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,531,test most frequent,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,537,test manual label in y,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,545,test manual label out of y warning,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,552,test multi output same outlier label,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,562,test multi output different outlier label,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,573,test inconsistent outlier label list length,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,581,"Test radius-based classifier, when distance to a sample is zero.",
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,602,"Test radius-based regressor, when distance to a sample is zero.",
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,615,we don't test for weights=_weight_func since user will be expected,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,616,to handle zero distances themselves in the function.,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,653,check that we can pass precomputed distances to,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,654,NearestNeighbors.radius_neighbors(),
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,655,non-regression test for,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,656,https://github.com/scikit-learn/scikit-learn/issues/16036,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,677,Test k-NN classifier on multioutput data,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,691,Stack single output prediction,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,702,Multioutput prediction,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,717,Test k-NN classifier on sparse matrices,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,718,"Like the above, but with various types of sparse matrices",
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,736,Test k-NN classifier on multioutput data,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,750,Stack single output prediction,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,764,Multioutput prediction,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,773,Check proba,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,786,Test k-neighbors regression,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,808,Test k-neighbors in multi-output regression with uniform weight,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,839,Test k-neighbors in multi-output regression,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,866,Test radius-based neighbors regression,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,886,test that nan is returned when no nearby observations,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,903,Test radius neighbors in multi-output regression (uniform weight),
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,937,Test k-neighbors in multi-output regression with various weight,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,965,Test radius-based regression on sparse matrices,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,966,"Like the above, but with various types of sparse matrices",
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,989,Sanity checks on the iris dataset,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,990,Puts three points of each label in the plane and performs a,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,991,nearest neighbor query on points near the decision boundary.,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1010,Sanity check on the digits dataset,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1011,the 'brute' algorithm has been observed to fail if the input,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1012,dtype is uint8 due to overflow in distance calculations.,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1030,Test kneighbors_graph to build the k-Nearest Neighbor graph.,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1033,n_neighbors = 1,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1045,n_neighbors = 2,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1061,n_neighbors = 3,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1070,Test kneighbors_graph to build the k-Nearest Neighbor graph,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1071,for sparse input.,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1088,Test radius_neighbors_graph to build the Nearest Neighbor graph.,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1108,Test radius_neighbors_graph to build the Nearest Neighbor graph,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1109,for sparse input.,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1126,Test bad argument values: these should all raise ValueErrors,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1187,Test computing the neighbors for various metrics,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1188,create a symmetric matrix,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1212,KD tree doesn't support all metrics,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1225,Haversine distance only accepts 2D data,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1266,check that there is a metric that is valid for brute,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1267,but not ball_tree (so we actually test something),
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1271,Metric which don't required any additional parameter,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1300,Metric with parameter,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1332,Find a reasonable radius.,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1337,Test kneighbors_graph,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1345,Test radiusneighbors_graph,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1353,"Raise error when wrong parameters are supplied,",
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1370,Test kneighbors et.al when query is not training data,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1380,Test neighbors.,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1388,Test the graph variants.,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1399,Test kneighbors et.al when query is None,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1414,Test the graph variants.,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1431,Test behavior of kneighbors when duplicates are present in query,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1437,Do not do anything special to duplicates.,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1459,Mask the first duplicates when n_duplicates > n_neighbors.,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1467,Test that zeros are explicitly marked in kneighbors_graph.,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1479,Test include_self parameter in neighbors_graph,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1549,Non-regression test which ensure the knn methods are properly working,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1550,even when forcing the global joblib backend.,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1576,Metric accepting sparse matrix input (only),
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1580,Population matrix,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1586,Query matrix,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1595,GS indices of nearest neighbours in `X` for `sparse_metric`,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1604,ignore conversion to boolean in pairwise_distances,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1607,Non-regression test for #4523,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1608,'brute': uses scipy.spatial.distance through pairwise_distances,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1609,'ball_tree': uses sklearn.neighbors._dist_metrics,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1638,Test chaining KNeighborsTransformer and classifiers/regressors,
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1646,"We precompute more neighbors than necessary, to have equivalence between",
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1647,"k-neighbors estimator after radius-neighbors transformer, and vice-versa.",
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1667,compare the chained version and the compact version,
scikit-learn/sklearn/neighbors/tests/test_nca.py,1,coding: utf-8,
scikit-learn/sklearn/neighbors/tests/test_nca.py,6,Authors: William de Vazelhes <wdevazelhes@gmail.com>,
scikit-learn/sklearn/neighbors/tests/test_nca.py,7,John Chiotellis <ioannis.chiotellis@in.tum.de>,
scikit-learn/sklearn/neighbors/tests/test_nca.py,8,License: BSD 3 clause,
scikit-learn/sklearn/neighbors/tests/test_nca.py,26,load and shuffle iris dataset,
scikit-learn/sklearn/neighbors/tests/test_nca.py,72,initialize the loss to very high,
scikit-learn/sklearn/neighbors/tests/test_nca.py,73,Initialize a fake NCA and variables needed to compute the loss:,
scikit-learn/sklearn/neighbors/tests/test_nca.py,91,test that points are collapsed into one point,
scikit-learn/sklearn/neighbors/tests/test_nca.py,102,"Initialize the transformation `M`, as well as `X` and `y` and `NCA`",
scikit-learn/sklearn/neighbors/tests/test_nca.py,117,compute relative error,
scikit-learn/sklearn/neighbors/tests/test_nca.py,123,Test that invalid parameters raise value error,
scikit-learn/sklearn/neighbors/tests/test_nca.py,129,TypeError,
scikit-learn/sklearn/neighbors/tests/test_nca.py,136,ValueError,
scikit-learn/sklearn/neighbors/tests/test_nca.py,168,Fail if transformation input dimension does not match inputs dimensions,
scikit-learn/sklearn/neighbors/tests/test_nca.py,174,Fail if transformation output dimension is larger than,
scikit-learn/sklearn/neighbors/tests/test_nca.py,175,transformation input dimension,
scikit-learn/sklearn/neighbors/tests/test_nca.py,177,len(transformation) > len(transformation[0]),
scikit-learn/sklearn/neighbors/tests/test_nca.py,182,Pass otherwise,
scikit-learn/sklearn/neighbors/tests/test_nca.py,194,n_components = X.shape[1] != transformation.shape[0],
scikit-learn/sklearn/neighbors/tests/test_nca.py,205,n_components > X.shape[1],
scikit-learn/sklearn/neighbors/tests/test_nca.py,216,n_components < X.shape[1],
scikit-learn/sklearn/neighbors/tests/test_nca.py,225,Start learning from scratch,
scikit-learn/sklearn/neighbors/tests/test_nca.py,229,Initialize with random,
scikit-learn/sklearn/neighbors/tests/test_nca.py,233,Initialize with auto,
scikit-learn/sklearn/neighbors/tests/test_nca.py,237,Initialize with PCA,
scikit-learn/sklearn/neighbors/tests/test_nca.py,241,Initialize with LDA,
scikit-learn/sklearn/neighbors/tests/test_nca.py,249,init.shape[1] must match X.shape[1],
scikit-learn/sklearn/neighbors/tests/test_nca.py,259,init.shape[0] must be <= init.shape[1],
scikit-learn/sklearn/neighbors/tests/test_nca.py,269,init.shape[0] must match n_components,
scikit-learn/sklearn/neighbors/tests/test_nca.py,287,Test that auto choose the init as expected with every configuration,
scikit-learn/sklearn/neighbors/tests/test_nca.py,288,"of order of n_samples, n_features, n_classes and n_components.",
scikit-learn/sklearn/neighbors/tests/test_nca.py,296,"n_classes > n_samples is impossible, and n_classes == n_samples",
scikit-learn/sklearn/neighbors/tests/test_nca.py,297,throws an error from lda but is an absurd case,
scikit-learn/sklearn/neighbors/tests/test_nca.py,302,"this would return a ValueError, which is already tested in",
scikit-learn/sklearn/neighbors/tests/test_nca.py,303,test_params_validation,
scikit-learn/sklearn/neighbors/tests/test_nca.py,338,A 1-iteration second fit on same data should give almost same result,
scikit-learn/sklearn/neighbors/tests/test_nca.py,339,"with warm starting, and quite different result without warm starting.",
scikit-learn/sklearn/neighbors/tests/test_nca.py,370,"assert there is proper output when verbose = 1, for every initialization",
scikit-learn/sklearn/neighbors/tests/test_nca.py,371,except auto because auto will call one of the others,
scikit-learn/sklearn/neighbors/tests/test_nca.py,385,check output,
scikit-learn/sklearn/neighbors/tests/test_nca.py,387,"if pca or lda init, an additional line is printed, so we test",
scikit-learn/sklearn/neighbors/tests/test_nca.py,388,it and remove it to test the rest equally among initializations,
scikit-learn/sklearn/neighbors/tests/test_nca.py,399,The following regex will match for instance:,
scikit-learn/sklearn/neighbors/tests/test_nca.py,400,'[NeighborhoodComponentsAnalysis]  0    6.988936e+01   0.01',
scikit-learn/sklearn/neighbors/tests/test_nca.py,409,assert by default there is no output (verbose=0),
scikit-learn/sklearn/neighbors/tests/test_nca.py,413,check output,
scikit-learn/sklearn/neighbors/tests/test_nca.py,421,one singleton class,
scikit-learn/sklearn/neighbors/tests/test_nca.py,430,One non-singleton class,
scikit-learn/sklearn/neighbors/tests/test_nca.py,441,Only singleton classes,
scikit-learn/sklearn/neighbors/tests/test_nca.py,478,assert that my_cb is called,
scikit-learn/sklearn/neighbors/tests/test_nca.py,484,check output,
scikit-learn/sklearn/neighbors/tests/test_nca.py,496,Initialize a fake NCA and variables needed to call the loss,
scikit-learn/sklearn/neighbors/tests/test_nca.py,497,function:,
scikit-learn/sklearn/neighbors/tests/test_nca.py,527,check that no error is raised when parameters have numpy integer or,
scikit-learn/sklearn/neighbors/tests/test_nca.py,528,floating types.,
scikit-learn/sklearn/neighbors/tests/test_dist_metrics.py,29,make boolean arrays: ones and zeros,
scikit-learn/sklearn/neighbors/tests/test_dist_metrics.py,106,Based on https://github.com/scipy/scipy/pull/7373,
scikit-learn/sklearn/neighbors/tests/test_dist_metrics.py,107,"When comparing two all-zero vectors, scipy>=1.2.0 jaccard metric",
scikit-learn/sklearn/neighbors/tests/test_dist_metrics.py,108,"was changed to return 0, instead of nan.",
scikit-learn/sklearn/neighbors/tests/test_dist_metrics.py,167,Check if both callable metric and predefined metric initialized,
scikit-learn/sklearn/neighbors/tests/test_dist_metrics.py,168,DistanceMetric object is picklable,
scikit-learn/sklearn/neighbors/tests/test_dist_metrics.py,193,Regression test for #6288,
scikit-learn/sklearn/neighbors/tests/test_dist_metrics.py,194,"Previously, a metric requiring a particular input dimension would fail",
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,11,Introduce a point into a quad tree with boundaries not easy to compute.,
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,14,check a random case,
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,16,check the case where only 0 are inserted,
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,18,check the case where only negative are inserted,
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,20,check the case where only small numbers are inserted,
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,30,Introduce a point into a quad tree where a similar point already exists.,
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,31,Test will hang if it doesn't complete.,
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,34,check the case where points are actually different,
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,36,check the case where points are the same on X axis,
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,38,check the case where points are arbitrarily close on X axis,
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,40,check the case where points are the same on Y axis,
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,42,check the case where points are arbitrarily close on Y axis,
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,44,check the case where points are arbitrarily close on both axes,
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,48,check the case where points are arbitrarily close on both axes,
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,49,close to machine epsilon - x axis,
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,53,check the case where points are arbitrarily close on both axes,
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,54,close to machine epsilon - y axis,
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,95,Assert that the first 5 are indeed duplicated and that the next,
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,96,ones are single point leaf,
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,13,toy sample,
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,15,Sparse matrix,
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,21,also load the iris dataset,
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,22,and randomly permute it,
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,31,"Check classification on a toy dataset, including sparse versions.",
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,36,"Same test, but with a sparse matrix to fit and test.",
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,41,"Fit with sparse, test with non-sparse",
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,46,"Fit with non-sparse, test with sparse",
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,51,Fit and predict with non-CSR sparse matrices,
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,64,Check consistency on dataset iris.,
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,72,"Check consistency on dataset iris, when using shrinkage.",
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,85,classification,
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,100,Ensure that the shrinking is correct.,
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,101,"The expected result is calculated by R (pamr),",
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,102,which is implemented by the author of the original paper.,
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,103,(One need to modify the code to output the new centroid in pamr.predict),
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,124,Test that NearestCentroid gives same results on translated data,
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,141,Test the manhattan metric.,
scikit-learn/sklearn/neighbors/tests/test_lof.py,1,Authors: Nicolas Goix <nicolas.goix@telecom-paristech.fr>,
scikit-learn/sklearn/neighbors/tests/test_lof.py,2,Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>,
scikit-learn/sklearn/neighbors/tests/test_lof.py,3,License: BSD 3 clause,
scikit-learn/sklearn/neighbors/tests/test_lof.py,27,load the iris dataset,
scikit-learn/sklearn/neighbors/tests/test_lof.py,28,and randomly permute it,
scikit-learn/sklearn/neighbors/tests/test_lof.py,37,Toy sample (the last two samples are outliers):,
scikit-learn/sklearn/neighbors/tests/test_lof.py,40,Test LocalOutlierFactor:,
scikit-learn/sklearn/neighbors/tests/test_lof.py,45,Assert largest outlier score is smaller than smallest inlier score:,
scikit-learn/sklearn/neighbors/tests/test_lof.py,48,Assert predict() works:,
scikit-learn/sklearn/neighbors/tests/test_lof.py,56,Generate train/test data,
scikit-learn/sklearn/neighbors/tests/test_lof.py,61,Generate some abnormal novel observations,
scikit-learn/sklearn/neighbors/tests/test_lof.py,66,fit the model for novelty detection,
scikit-learn/sklearn/neighbors/tests/test_lof.py,69,"predict scores (the lower, the more normal)",
scikit-learn/sklearn/neighbors/tests/test_lof.py,72,check that roc_auc is good,
scikit-learn/sklearn/neighbors/tests/test_lof.py,77,toy samples:,
scikit-learn/sklearn/neighbors/tests/test_lof.py,86,check predict(),
scikit-learn/sklearn/neighbors/tests/test_lof.py,89,check predict(one sample not in train),
scikit-learn/sklearn/neighbors/tests/test_lof.py,92,check predict(one sample already in train),
scikit-learn/sklearn/neighbors/tests/test_lof.py,99,Note: smaller samples may result in spurious test success,
scikit-learn/sklearn/neighbors/tests/test_lof.py,105,As a feature matrix (n_samples by n_features),
scikit-learn/sklearn/neighbors/tests/test_lof.py,111,As a dense distance matrix (n_samples by n_samples),
scikit-learn/sklearn/neighbors/tests/test_lof.py,158,check errors for novelty=False,
scikit-learn/sklearn/neighbors/tests/test_lof.py,161,"predict, decision_function and score_samples raise ValueError",
scikit-learn/sklearn/neighbors/tests/test_lof.py,166,check errors for novelty=True,
scikit-learn/sklearn/neighbors/tests/test_lof.py,173,check that the scores of the training samples are still accessible,
scikit-learn/sklearn/neighbors/tests/test_lof.py,174,when novelty=True through the negative_outlier_factor_ attribute,
scikit-learn/sklearn/neighbors/tests/test_lof.py,177,fit with novelty=False,
scikit-learn/sklearn/neighbors/tests/test_lof.py,182,fit with novelty=True,
scikit-learn/sklearn/neighbors/tests/test_lof.py,191,check availability of prediction methods depending on novelty value.,
scikit-learn/sklearn/neighbors/tests/test_lof.py,194,when novelty=True,
scikit-learn/sklearn/neighbors/tests/test_lof.py,202,when novelty=False,
scikit-learn/sklearn/neighbors/tests/test_lof.py,213,the common tests are run for the default LOF (novelty=False).,
scikit-learn/sklearn/neighbors/tests/test_lof.py,214,here we run these common tests for LOF when novelty=True,
scikit-learn/sklearn/neighbors/tests/test_lof.py,220,the number of predicted outliers should be equal to the number of,
scikit-learn/sklearn/neighbors/tests/test_lof.py,221,expected outliers unless there are ties in the abnormality scores.,
scikit-learn/sklearn/neighbors/tests/test_graph.py,9,Test the number of neighbors returned,
scikit-learn/sklearn/neighbors/tests/test_graph.py,20,with n_neighbors,
scikit-learn/sklearn/neighbors/tests/test_graph.py,36,with radius,
scikit-learn/sklearn/neighbors/tests/test_graph.py,61,Test that the diagonal is explicitly stored in the sparse graph,
scikit-learn/sklearn/neighbors/tests/test_graph.py,77,Using transform on new data should not always have zero diagonal,
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,30,Test chaining KNeighborsTransformer and SpectralClustering,
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,34,compare the chained version and the compact version,
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,47,Test chaining KNeighborsTransformer and SpectralEmbedding,
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,59,compare the chained version and the compact version,
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,72,Test chaining RadiusNeighborsTransformer and DBSCAN,
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,77,compare the chained version and the compact version,
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,89,Test chaining KNeighborsTransformer and Isomap with,
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,90,neighbors_algorithm='precomputed',
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,97,compare the chained version and the compact version,
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,115,Test chaining KNeighborsTransformer and TSNE,
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,125,compare the chained version and the compact version,
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,140,Test chaining KNeighborsTransformer and LocalOutlierFactor,
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,146,compare the chained version and the compact version,
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,160,Test chaining KNeighborsTransformer and LocalOutlierFactor,
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,167,compare the chained version and the compact version,
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,181,Test chaining KNeighborsTransformer and classifiers/regressors,
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,189,"We precompute more neighbors than necessary, to have equivalence between",
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,190,"k-neighbors estimator after radius-neighbors transformer, and vice-versa.",
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,212,compare the chained version and the compact version,
scikit-learn/sklearn/decomposition/_nmf.py,3,Author: Vlad Niculae,
scikit-learn/sklearn/decomposition/_nmf.py,4,Lars Buitinck,
scikit-learn/sklearn/decomposition/_nmf.py,5,Mathieu Blondel <mathieu@mblondel.org>,
scikit-learn/sklearn/decomposition/_nmf.py,6,Tom Dupre la Tour,
scikit-learn/sklearn/decomposition/_nmf.py,7,License: BSD 3 clause,
scikit-learn/sklearn/decomposition/_nmf.py,92,The method can be called with scalars,
scikit-learn/sklearn/decomposition/_nmf.py,98,Frobenius norm,
scikit-learn/sklearn/decomposition/_nmf.py,100,"Avoid the creation of the dense np.dot(W, H) if X is sparse.",
scikit-learn/sklearn/decomposition/_nmf.py,115,"compute np.dot(W, H) only where X is nonzero",
scikit-learn/sklearn/decomposition/_nmf.py,123,do not affect the zeros: here 0 ** (-1) = 0 and not infinity,
scikit-learn/sklearn/decomposition/_nmf.py,128,used to avoid division by zero,
scikit-learn/sklearn/decomposition/_nmf.py,131,generalized Kullback-Leibler divergence,
scikit-learn/sklearn/decomposition/_nmf.py,133,"fast and memory efficient computation of np.sum(np.dot(W, H))",
scikit-learn/sklearn/decomposition/_nmf.py,135,computes np.sum(X * log(X / WH)) only where X is nonzero,
scikit-learn/sklearn/decomposition/_nmf.py,138,"add full np.sum(np.dot(W, H)) - np.sum(X)",
scikit-learn/sklearn/decomposition/_nmf.py,141,Itakura-Saito divergence,
scikit-learn/sklearn/decomposition/_nmf.py,146,"beta-divergence, beta not in (0, 1, 2)",
scikit-learn/sklearn/decomposition/_nmf.py,149,"slow loop, but memory efficient computation of :",
scikit-learn/sklearn/decomposition/_nmf.py,150,"np.sum(np.dot(W, H) ** beta)",
scikit-learn/sklearn/decomposition/_nmf.py,218,'mu' is the only solver that handles other beta losses than 'frobenius',
scikit-learn/sklearn/decomposition/_nmf.py,325,Random initialization,
scikit-learn/sklearn/decomposition/_nmf.py,337,NNDSVD initialization,
scikit-learn/sklearn/decomposition/_nmf.py,342,The leading singular triplet is non-negative,
scikit-learn/sklearn/decomposition/_nmf.py,343,so it can be used as is for initialization.,
scikit-learn/sklearn/decomposition/_nmf.py,350,extract positive and negative parts of column vectors,
scikit-learn/sklearn/decomposition/_nmf.py,354,and their norms,
scikit-learn/sklearn/decomposition/_nmf.py,360,choose update,
scikit-learn/sklearn/decomposition/_nmf.py,410,L2 regularization corresponds to increase of the diagonal of HHt,
scikit-learn/sklearn/decomposition/_nmf.py,412,adds l2_reg only on the diagonal,
scikit-learn/sklearn/decomposition/_nmf.py,414,L1 regularization corresponds to decrease of each element of XHt,
scikit-learn/sklearn/decomposition/_nmf.py,422,The following seems to be required on 64-bit Windows w/ Python 3.5.,
scikit-learn/sklearn/decomposition/_nmf.py,499,so W and Ht are both in C order in memory,
scikit-learn/sklearn/decomposition/_nmf.py,508,Update W,
scikit-learn/sklearn/decomposition/_nmf.py,511,Update H,
scikit-learn/sklearn/decomposition/_nmf.py,537,Numerator,
scikit-learn/sklearn/decomposition/_nmf.py,541,"avoid a copy of XHt, which will be re-computed (update_H=True)",
scikit-learn/sklearn/decomposition/_nmf.py,544,"preserve the XHt, which is not re-computed (update_H=False)",
scikit-learn/sklearn/decomposition/_nmf.py,547,Denominator,
scikit-learn/sklearn/decomposition/_nmf.py,553,Numerator,
scikit-learn/sklearn/decomposition/_nmf.py,554,"if X is sparse, compute WH only where X is non zero",
scikit-learn/sklearn/decomposition/_nmf.py,562,copy used in the Denominator,
scikit-learn/sklearn/decomposition/_nmf.py,567,to avoid taking a negative power of zero,
scikit-learn/sklearn/decomposition/_nmf.py,574,speeds up computation time,
scikit-learn/sklearn/decomposition/_nmf.py,575,refer to /numpy/numpy/issues/9363,
scikit-learn/sklearn/decomposition/_nmf.py,578,element-wise multiplication,
scikit-learn/sklearn/decomposition/_nmf.py,582,element-wise multiplication,
scikit-learn/sklearn/decomposition/_nmf.py,585,"here numerator = dot(X * (dot(W, H) ** (beta_loss - 2)), H.T)",
scikit-learn/sklearn/decomposition/_nmf.py,588,Denominator,
scikit-learn/sklearn/decomposition/_nmf.py,591,"shape(n_components, )",
scikit-learn/sklearn/decomposition/_nmf.py,595,"computation of WHHt = dot(dot(W, H) ** beta_loss - 1, H.T)",
scikit-learn/sklearn/decomposition/_nmf.py,597,memory efficient computation,
scikit-learn/sklearn/decomposition/_nmf.py,598,"(compute row by row, avoiding the dense matrix WH)",
scikit-learn/sklearn/decomposition/_nmf.py,611,Add L1 and L2 regularization,
scikit-learn/sklearn/decomposition/_nmf.py,621,"gamma is in ]0, 1]",
scikit-learn/sklearn/decomposition/_nmf.py,635,Numerator,
scikit-learn/sklearn/decomposition/_nmf.py,643,copy used in the Denominator,
scikit-learn/sklearn/decomposition/_nmf.py,648,to avoid division by zero,
scikit-learn/sklearn/decomposition/_nmf.py,655,speeds up computation time,
scikit-learn/sklearn/decomposition/_nmf.py,656,refer to /numpy/numpy/issues/9363,
scikit-learn/sklearn/decomposition/_nmf.py,659,element-wise multiplication,
scikit-learn/sklearn/decomposition/_nmf.py,663,element-wise multiplication,
scikit-learn/sklearn/decomposition/_nmf.py,666,"here numerator = dot(W.T, (dot(W, H) ** (beta_loss - 2)) * X)",
scikit-learn/sklearn/decomposition/_nmf.py,669,Denominator,
scikit-learn/sklearn/decomposition/_nmf.py,671,"shape(n_components, )",
scikit-learn/sklearn/decomposition/_nmf.py,675,"beta_loss not in (1, 2)",
scikit-learn/sklearn/decomposition/_nmf.py,677,"computation of WtWH = dot(W.T, dot(W, H) ** beta_loss - 1)",
scikit-learn/sklearn/decomposition/_nmf.py,679,memory efficient computation,
scikit-learn/sklearn/decomposition/_nmf.py,680,"(compute column by column, avoiding the dense matrix WH)",
scikit-learn/sklearn/decomposition/_nmf.py,693,Add L1 and L2 regularization,
scikit-learn/sklearn/decomposition/_nmf.py,703,"gamma is in ]0, 1]",
scikit-learn/sklearn/decomposition/_nmf.py,784,gamma for Maximization-Minimization (MM) algorithm [Fevotte 2011],
scikit-learn/sklearn/decomposition/_nmf.py,792,used for the convergence criterion,
scikit-learn/sklearn/decomposition/_nmf.py,798,update W,
scikit-learn/sklearn/decomposition/_nmf.py,799,"H_sum, HHt and XHt are saved and reused if not update_H",
scikit-learn/sklearn/decomposition/_nmf.py,805,necessary for stability with beta_loss < 1,
scikit-learn/sklearn/decomposition/_nmf.py,809,update H,
scikit-learn/sklearn/decomposition/_nmf.py,815,These values will be recomputed since H changed,
scikit-learn/sklearn/decomposition/_nmf.py,818,necessary for stability with beta_loss < 1,
scikit-learn/sklearn/decomposition/_nmf.py,822,test convergence criterion every 10 iterations,
scikit-learn/sklearn/decomposition/_nmf.py,835,do not print if we have already printed in the convergence test,
scikit-learn/sklearn/decomposition/_nmf.py,1031,"check W and H, or initialize them",
scikit-learn/sklearn/decomposition/_nmf.py,1044,'mu' solver should not be initialized by zeros,
scikit-learn/sklearn/decomposition/_sparse_pca.py,2,"Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort",
scikit-learn/sklearn/decomposition/_sparse_pca.py,3,License: BSD 3 clause,
scikit-learn/sklearn/decomposition/_sparse_pca.py,17,FIXME: remove in 0.24,
scikit-learn/sklearn/decomposition/_pca.py,4,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/sklearn/decomposition/_pca.py,5,Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/sklearn/decomposition/_pca.py,6,Mathieu Blondel <mathieu@mblondel.org>,
scikit-learn/sklearn/decomposition/_pca.py,7,Denis A. Engemann <denis-alexander.engemann@inria.fr>,
scikit-learn/sklearn/decomposition/_pca.py,8,Michael Eickenberg <michael.eickenberg@inria.fr>,
scikit-learn/sklearn/decomposition/_pca.py,9,Giorgio Patrini <giorgio.patrini@anu.edu.au>,
scikit-learn/sklearn/decomposition/_pca.py,10,,
scikit-learn/sklearn/decomposition/_pca.py,11,License: BSD 3 clause,
scikit-learn/sklearn/decomposition/_pca.py,66,"When the tested rank is associated with a small eigenvalue, there's",
scikit-learn/sklearn/decomposition/_pca.py,67,no point in computing the log-likelihood: it's going to be very,
scikit-learn/sklearn/decomposition/_pca.py,68,"small and won't be the max anyway. Also, it can lead to numerical",
scikit-learn/sklearn/decomposition/_pca.py,69,"issues below when computing pa, in particular in log((spectrum[i] -",
scikit-learn/sklearn/decomposition/_pca.py,70,spectrum[j]) because this will take the log of something very small.,
scikit-learn/sklearn/decomposition/_pca.py,106,we don't want to return n_components = 0,
scikit-learn/sklearn/decomposition/_pca.py,380,X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples),
scikit-learn/sklearn/decomposition/_pca.py,383,X_new = X * V = U * S * V^T * V = U * S,
scikit-learn/sklearn/decomposition/_pca.py,391,Raise an error for sparse input.,
scikit-learn/sklearn/decomposition/_pca.py,392,This is more informative than the generic one raised by check_array.,
scikit-learn/sklearn/decomposition/_pca.py,400,Handle n_components==None,
scikit-learn/sklearn/decomposition/_pca.py,409,Handle svd_solver,
scikit-learn/sklearn/decomposition/_pca.py,412,"Small problem or n_components == 'mle', just call full PCA",
scikit-learn/sklearn/decomposition/_pca.py,417,"This is also the case of n_components in (0,1)",
scikit-learn/sklearn/decomposition/_pca.py,421,Call different fits for either full or truncated SVD,
scikit-learn/sklearn/decomposition/_pca.py,450,Center data,
scikit-learn/sklearn/decomposition/_pca.py,455,flip eigenvectors' sign to enforce deterministic output,
scikit-learn/sklearn/decomposition/_pca.py,460,Get variance explained by singular values,
scikit-learn/sklearn/decomposition/_pca.py,464,Store the singular values.,
scikit-learn/sklearn/decomposition/_pca.py,466,Postprocess the number of components required,
scikit-learn/sklearn/decomposition/_pca.py,471,number of components for which the cumulated explained,
scikit-learn/sklearn/decomposition/_pca.py,472,variance percentage is superior to the desired threshold,
scikit-learn/sklearn/decomposition/_pca.py,473,side='right' ensures that number of features selected,
scikit-learn/sklearn/decomposition/_pca.py,474,their variance is always greater than n_components float,
scikit-learn/sklearn/decomposition/_pca.py,475,passed. More discussion in issue: #15669,
scikit-learn/sklearn/decomposition/_pca.py,479,Compute noise covariance using Probabilistic PCA model,
scikit-learn/sklearn/decomposition/_pca.py,480,The sigma2 maximum likelihood (cf. eq. 12.46),
scikit-learn/sklearn/decomposition/_pca.py,526,Center data,
scikit-learn/sklearn/decomposition/_pca.py,531,"random init solution, as ARPACK does it internally",
scikit-learn/sklearn/decomposition/_pca.py,534,svds doesn't abide by scipy.linalg.svd/randomized_svd,
scikit-learn/sklearn/decomposition/_pca.py,535,"conventions, so reverse its outputs.",
scikit-learn/sklearn/decomposition/_pca.py,537,flip eigenvectors' sign to enforce deterministic output,
scikit-learn/sklearn/decomposition/_pca.py,541,sign flipping is done inside,
scikit-learn/sklearn/decomposition/_pca.py,551,Get variance explained by singular values,
scikit-learn/sklearn/decomposition/_pca.py,556,Store the singular values.,
scikit-learn/sklearn/decomposition/_factor_analysis.py,16,Author: Christian Osendorfer <osendorf@gmail.com>,
scikit-learn/sklearn/decomposition/_factor_analysis.py,17,Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/sklearn/decomposition/_factor_analysis.py,18,Denis A. Engemann <denis-alexander.engemann@inria.fr>,
scikit-learn/sklearn/decomposition/_factor_analysis.py,20,License: BSD3,
scikit-learn/sklearn/decomposition/_factor_analysis.py,182,some constant terms,
scikit-learn/sklearn/decomposition/_factor_analysis.py,200,we'll modify svd outputs to return unexplained variance,
scikit-learn/sklearn/decomposition/_factor_analysis.py,201,to allow for unified computation of loglikelihood,
scikit-learn/sklearn/decomposition/_factor_analysis.py,220,SMALL helps numerics,
scikit-learn/sklearn/decomposition/_factor_analysis.py,224,Use 'maximum' here to avoid sqrt problems.,
scikit-learn/sklearn/decomposition/_factor_analysis.py,229,loglikelihood,
scikit-learn/sklearn/decomposition/_factor_analysis.py,294,modify diag inplace,
scikit-learn/sklearn/decomposition/_factor_analysis.py,309,handle corner cases first,
scikit-learn/sklearn/decomposition/_factor_analysis.py,315,Get precision using matrix inversion lemma,
scikit-learn/sklearn/decomposition/_incremental_pca.py,3,Author: Kyle Kastner <kastnerkyle@gmail.com>,
scikit-learn/sklearn/decomposition/_incremental_pca.py,4,Giorgio Patrini,
scikit-learn/sklearn/decomposition/_incremental_pca.py,5,License: BSD 3 clause,
scikit-learn/sklearn/decomposition/_incremental_pca.py,271,This is the first partial_fit,
scikit-learn/sklearn/decomposition/_incremental_pca.py,277,Update stats - they are 0 if this is the first step,
scikit-learn/sklearn/decomposition/_incremental_pca.py,284,Whitening,
scikit-learn/sklearn/decomposition/_incremental_pca.py,286,"If it is the first step, simply whiten X",
scikit-learn/sklearn/decomposition/_incremental_pca.py,291,Build matrix of combined previous basis and new data,
scikit-learn/sklearn/decomposition/_kernel_pca.py,3,Author: Mathieu Blondel <mathieu@mblondel.org>,
scikit-learn/sklearn/decomposition/_kernel_pca.py,4,License: BSD 3 clause,
scikit-learn/sklearn/decomposition/_kernel_pca.py,184,center kernel,
scikit-learn/sklearn/decomposition/_kernel_pca.py,192,compute eigenvectors,
scikit-learn/sklearn/decomposition/_kernel_pca.py,206,"initialize with [-1,1] as in ARPACK",
scikit-learn/sklearn/decomposition/_kernel_pca.py,214,make sure that the eigenvalues are ok and fix numerical issues,
scikit-learn/sklearn/decomposition/_kernel_pca.py,218,flip eigenvectors' sign to enforce deterministic output,
scikit-learn/sklearn/decomposition/_kernel_pca.py,222,sort eigenvectors in descending order,
scikit-learn/sklearn/decomposition/_kernel_pca.py,227,remove eigenvectors with a zero eigenvalue (null space) if required,
scikit-learn/sklearn/decomposition/_kernel_pca.py,232,Maintenance note on Eigenvectors normalization,
scikit-learn/sklearn/decomposition/_kernel_pca.py,233,----------------------------------------------,
scikit-learn/sklearn/decomposition/_kernel_pca.py,234,there is a link between,
scikit-learn/sklearn/decomposition/_kernel_pca.py,235,the eigenvectors of K=Phi(X)'Phi(X) and the ones of Phi(X)Phi(X)',
scikit-learn/sklearn/decomposition/_kernel_pca.py,236,if v is an eigenvector of K,
scikit-learn/sklearn/decomposition/_kernel_pca.py,237,then Phi(X)v  is an eigenvector of Phi(X)Phi(X)',
scikit-learn/sklearn/decomposition/_kernel_pca.py,238,if u is an eigenvector of Phi(X)Phi(X)',
scikit-learn/sklearn/decomposition/_kernel_pca.py,239,then Phi(X)'u is an eigenvector of Phi(X)Phi(X)',
scikit-learn/sklearn/decomposition/_kernel_pca.py,240,,
scikit-learn/sklearn/decomposition/_kernel_pca.py,241,"At this stage our self.alphas_ (the v) have norm 1, we need to scale",
scikit-learn/sklearn/decomposition/_kernel_pca.py,242,them so that eigenvectors in kernel feature space (the u) have norm=1,
scikit-learn/sklearn/decomposition/_kernel_pca.py,243,instead,
scikit-learn/sklearn/decomposition/_kernel_pca.py,244,,
scikit-learn/sklearn/decomposition/_kernel_pca.py,245,We COULD scale them here:,
scikit-learn/sklearn/decomposition/_kernel_pca.py,246,self.alphas_ = self.alphas_ / np.sqrt(self.lambdas_),
scikit-learn/sklearn/decomposition/_kernel_pca.py,247,,
scikit-learn/sklearn/decomposition/_kernel_pca.py,248,"But choose to perform that LATER when needed, in `fit()` and in",
scikit-learn/sklearn/decomposition/_kernel_pca.py,249,`transform()`.,
scikit-learn/sklearn/decomposition/_kernel_pca.py,284,"no need to use the kernel to transform X, use shortcut expression",
scikit-learn/sklearn/decomposition/_kernel_pca.py,307,"no need to use the kernel to transform X, use shortcut expression",
scikit-learn/sklearn/decomposition/_kernel_pca.py,328,Compute centered gram matrix between X and training data X_fit_,
scikit-learn/sklearn/decomposition/_kernel_pca.py,331,scale eigenvectors (properly account for null-space for dot product),
scikit-learn/sklearn/decomposition/_kernel_pca.py,337,Project with a scalar product between K and the scaled eigenvectors,
scikit-learn/sklearn/decomposition/_lda.py,11,Author: Chyi-Kwei Yau,
scikit-learn/sklearn/decomposition/_lda.py,12,Author: Matthew D. Hoffman (original onlineldavb implementation),
scikit-learn/sklearn/decomposition/_lda.py,83,"In the literature, this is `exp(E[log(theta)])`",
scikit-learn/sklearn/decomposition/_lda.py,86,diff on `component_` (only calculate it when `cal_diff` is True),
scikit-learn/sklearn/decomposition/_lda.py,103,"The next one is a copy, since the inner loop overwrites it.",
scikit-learn/sklearn/decomposition/_lda.py,107,Iterate between `doc_topic_d` and `norm_phi` until convergence,
scikit-learn/sklearn/decomposition/_lda.py,111,The optimal phi_{dwk} is proportional to,
scikit-learn/sklearn/decomposition/_lda.py,112,exp(E[log(theta_{dk})]) * exp(E[log(beta_{dw})]).,
scikit-learn/sklearn/decomposition/_lda.py,117,"Note: adds doc_topic_prior to doc_topic_d, in-place.",
scikit-learn/sklearn/decomposition/_lda.py,125,Contribution of document d to the expected sufficient,
scikit-learn/sklearn/decomposition/_lda.py,126,statistics for the M step.,
scikit-learn/sklearn/decomposition/_lda.py,345,"In the literature, this is called `lambda`",
scikit-learn/sklearn/decomposition/_lda.py,349,"In the literature, this is `exp(E[log(beta)])`",
scikit-learn/sklearn/decomposition/_lda.py,383,Run e-step in parallel,
scikit-learn/sklearn/decomposition/_lda.py,386,TODO: make Parallel._effective_n_jobs public instead?,
scikit-learn/sklearn/decomposition/_lda.py,400,merge result,
scikit-learn/sklearn/decomposition/_lda.py,405,This step finishes computing the sufficient statistics for the,
scikit-learn/sklearn/decomposition/_lda.py,406,M-step.,
scikit-learn/sklearn/decomposition/_lda.py,443,E-step,
scikit-learn/sklearn/decomposition/_lda.py,447,M-step,
scikit-learn/sklearn/decomposition/_lda.py,451,online update,
scikit-learn/sklearn/decomposition/_lda.py,452,"In the literature, the weight is `rho`",
scikit-learn/sklearn/decomposition/_lda.py,460,update `component_` related variables,
scikit-learn/sklearn/decomposition/_lda.py,501,"In theory reset should be equal to `first_time`, but there are tests",
scikit-learn/sklearn/decomposition/_lda.py,502,checking the input number of feature and they expect a specific,
scikit-learn/sklearn/decomposition/_lda.py,503,"string, which is not the same one raised by check_n_features. So we",
scikit-learn/sklearn/decomposition/_lda.py,504,don't check n_features_in_ here for now (it's done with adhoc code in,
scikit-learn/sklearn/decomposition/_lda.py,505,the estimator anyway).,
scikit-learn/sklearn/decomposition/_lda.py,506,TODO: set reset=first_time when addressing reset in,
scikit-learn/sklearn/decomposition/_lda.py,507,predict/transform/etc.,
scikit-learn/sklearn/decomposition/_lda.py,514,initialize parameters or check,
scikit-learn/sklearn/decomposition/_lda.py,562,initialize parameters,
scikit-learn/sklearn/decomposition/_lda.py,564,change to perplexity later,
scikit-learn/sklearn/decomposition/_lda.py,575,batch update,
scikit-learn/sklearn/decomposition/_lda.py,579,check perplexity,
scikit-learn/sklearn/decomposition/_lda.py,598,calculate final perplexity value on train set,
scikit-learn/sklearn/decomposition/_lda.py,622,make sure feature size is the same in fitted model and in X,
scikit-learn/sklearn/decomposition/_lda.py,685,calculate log-likelihood,
scikit-learn/sklearn/decomposition/_lda.py,706,"E[log p(docs | theta, beta)]",
scikit-learn/sklearn/decomposition/_lda.py,719,compute E[log p(theta | alpha) - log q(theta | gamma)],
scikit-learn/sklearn/decomposition/_lda.py,723,Compensate for the subsampling of the population of documents,
scikit-learn/sklearn/decomposition/_lda.py,728,E[log p(beta | eta) - log q (beta | lambda)],
scikit-learn/sklearn/decomposition/__init__.py,7,TODO: remove me in 0.24 (as well as the noqa markers) and,
scikit-learn/sklearn/decomposition/__init__.py,8,import the dict_learning func directly from the ._dict_learning,
scikit-learn/sklearn/decomposition/__init__.py,9,module instead.,
scikit-learn/sklearn/decomposition/__init__.py,10,Pre-cache the import of the deprecated module so that import,
scikit-learn/sklearn/decomposition/__init__.py,11,sklearn.decomposition.dict_learning returns the function as in,
scikit-learn/sklearn/decomposition/__init__.py,12,"0.21, instead of the module.",
scikit-learn/sklearn/decomposition/__init__.py,13,https://github.com/scikit-learn/scikit-learn/issues/15842,
scikit-learn/sklearn/decomposition/__init__.py,20,noqa,
scikit-learn/sklearn/decomposition/__init__.py,21,noqa,
scikit-learn/sklearn/decomposition/__init__.py,22,noqa,
scikit-learn/sklearn/decomposition/__init__.py,23,noqa,
scikit-learn/sklearn/decomposition/__init__.py,24,noqa,
scikit-learn/sklearn/decomposition/__init__.py,25,noqa,
scikit-learn/sklearn/decomposition/__init__.py,26,noqa,
scikit-learn/sklearn/decomposition/__init__.py,29,noqa,
scikit-learn/sklearn/decomposition/__init__.py,30,noqa,
scikit-learn/sklearn/decomposition/__init__.py,31,noqa,
scikit-learn/sklearn/decomposition/__init__.py,32,noqa,
scikit-learn/sklearn/decomposition/_truncated_svd.py,4,Author: Lars Buitinck,
scikit-learn/sklearn/decomposition/_truncated_svd.py,5,Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/sklearn/decomposition/_truncated_svd.py,6,Michael Becker <mike@beckerfuffle.com>,
scikit-learn/sklearn/decomposition/_truncated_svd.py,7,License: 3-clause BSD.,
scikit-learn/sklearn/decomposition/_truncated_svd.py,170,svds doesn't abide by scipy.linalg.svd/randomized_svd,
scikit-learn/sklearn/decomposition/_truncated_svd.py,171,"conventions, so reverse its outputs.",
scikit-learn/sklearn/decomposition/_truncated_svd.py,189,Calculate explained variance & explained variance ratio,
scikit-learn/sklearn/decomposition/_truncated_svd.py,198,Store the singular values.,
scikit-learn/sklearn/decomposition/_base.py,3,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/sklearn/decomposition/_base.py,4,Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/sklearn/decomposition/_base.py,5,Mathieu Blondel <mathieu@mblondel.org>,
scikit-learn/sklearn/decomposition/_base.py,6,Denis A. Engemann <denis-alexander.engemann@inria.fr>,
scikit-learn/sklearn/decomposition/_base.py,7,Kyle Kastner <kastnerkyle@gmail.com>,
scikit-learn/sklearn/decomposition/_base.py,8,,
scikit-learn/sklearn/decomposition/_base.py,9,License: BSD 3 clause,
scikit-learn/sklearn/decomposition/_base.py,44,modify diag inplace,
scikit-learn/sklearn/decomposition/_base.py,60,handle corner cases first,
scikit-learn/sklearn/decomposition/_base.py,66,Get precision using matrix inversion lemma,
scikit-learn/sklearn/decomposition/_dict_learning.py,3,"Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort",
scikit-learn/sklearn/decomposition/_dict_learning.py,4,License: BSD 3 clause,
scikit-learn/sklearn/decomposition/_dict_learning.py,115,overwriting cov is safe,
scikit-learn/sklearn/decomposition/_dict_learning.py,122,account for scaling,
scikit-learn/sklearn/decomposition/_dict_learning.py,126,"Not passing in verbose=max(0, verbose-1) because Lars.fit already",
scikit-learn/sklearn/decomposition/_dict_learning.py,127,corrects the verbosity level.,
scikit-learn/sklearn/decomposition/_dict_learning.py,138,account for scaling,
scikit-learn/sklearn/decomposition/_dict_learning.py,140,TODO: Make verbosity argument for Lasso?,
scikit-learn/sklearn/decomposition/_dict_learning.py,141,sklearn.linear_model.coordinate_descent.enet_path has a verbosity,
scikit-learn/sklearn/decomposition/_dict_learning.py,142,argument that we could pass in from Lasso.,
scikit-learn/sklearn/decomposition/_dict_learning.py,157,"Not passing in verbose=max(0, verbose-1) because Lars.fit already",
scikit-learn/sklearn/decomposition/_dict_learning.py,158,corrects the verbosity level.,
scikit-learn/sklearn/decomposition/_dict_learning.py,187,XXX : could be moved to the linear_model module,
scikit-learn/sklearn/decomposition/_dict_learning.py,321,Enter parallel code block,
scikit-learn/sklearn/decomposition/_dict_learning.py,383,Get BLAS functions,
scikit-learn/sklearn/decomposition/_dict_learning.py,387,"Residuals, computed with BLAS for speed and efficiency",
scikit-learn/sklearn/decomposition/_dict_learning.py,388,R <- -1.0 * U * V^T + 1.0 * Y,
scikit-learn/sklearn/decomposition/_dict_learning.py,389,Outputs R as Fortran array for efficiency,
scikit-learn/sklearn/decomposition/_dict_learning.py,392,R <- 1.0 * U_k * V_k^T + R,
scikit-learn/sklearn/decomposition/_dict_learning.py,397,Scale k'th atom,
scikit-learn/sklearn/decomposition/_dict_learning.py,398,(U_k * U_k) ** 0.5,
scikit-learn/sklearn/decomposition/_dict_learning.py,409,Setting corresponding coefs to 0,
scikit-learn/sklearn/decomposition/_dict_learning.py,411,(U_k * U_k) ** 0.5,
scikit-learn/sklearn/decomposition/_dict_learning.py,416,R <- -1.0 * U_k * V_k^T + R,
scikit-learn/sklearn/decomposition/_dict_learning.py,539,Avoid integer division problems,
scikit-learn/sklearn/decomposition/_dict_learning.py,543,Init the code and the dictionary with SVD of Y,
scikit-learn/sklearn/decomposition/_dict_learning.py,546,"Don't copy V, it will happen below",
scikit-learn/sklearn/decomposition/_dict_learning.py,552,True even if n_components=None,
scikit-learn/sklearn/decomposition/_dict_learning.py,560,"Fortran-order dict, as we are going to access its row vectors",
scikit-learn/sklearn/decomposition/_dict_learning.py,571,"If max_iter is 0, number of iterations returned should be zero",
scikit-learn/sklearn/decomposition/_dict_learning.py,584,Update code,
scikit-learn/sklearn/decomposition/_dict_learning.py,588,Update dictionary,
scikit-learn/sklearn/decomposition/_dict_learning.py,595,Cost function,
scikit-learn/sklearn/decomposition/_dict_learning.py,601,assert(dE >= -tol * errors[-1]),
scikit-learn/sklearn/decomposition/_dict_learning.py,604,A line return,
scikit-learn/sklearn/decomposition/_dict_learning.py,761,Avoid integer division problems,
scikit-learn/sklearn/decomposition/_dict_learning.py,765,Init V with SVD of X,
scikit-learn/sklearn/decomposition/_dict_learning.py,797,The covariance of the dictionary,
scikit-learn/sklearn/decomposition/_dict_learning.py,800,The data approximation,
scikit-learn/sklearn/decomposition/_dict_learning.py,806,"If n_iter is zero, we need to return zero.",
scikit-learn/sklearn/decomposition/_dict_learning.py,826,Update the auxiliary variables,
scikit-learn/sklearn/decomposition/_dict_learning.py,838,Update dictionary,
scikit-learn/sklearn/decomposition/_dict_learning.py,842,XXX: Can the residuals be of any use?,
scikit-learn/sklearn/decomposition/_dict_learning.py,844,Maybe we need a stopping criteria based on the amount of,
scikit-learn/sklearn/decomposition/_dict_learning.py,845,modification in the dictionary,
scikit-learn/sklearn/decomposition/_dict_learning.py,924,feature vector is split into a positive and negative side,
scikit-learn/sklearn/decomposition/_dict_learning.py,1449,Keep track of the state of the algorithm to be able to do,
scikit-learn/sklearn/decomposition/_dict_learning.py,1450,some online fitting (partial_fit),
scikit-learn/sklearn/decomposition/_dict_learning.py,1501,Keep track of the state of the algorithm to be able to do,
scikit-learn/sklearn/decomposition/_dict_learning.py,1502,some online fitting (partial_fit),
scikit-learn/sklearn/decomposition/_fastica.py,8,"Authors: Pierre Lafaye de Micheaux, Stefan van der Walt, Gael Varoquaux,",
scikit-learn/sklearn/decomposition/_fastica.py,9,"Bertrand Thirion, Alexandre Gramfort, Denis A. Engemann",
scikit-learn/sklearn/decomposition/_fastica.py,10,License: BSD 3 clause,
scikit-learn/sklearn/decomposition/_fastica.py,58,u (resp. s) contains the eigenvectors (resp. square roots of,
scikit-learn/sklearn/decomposition/_fastica.py,59,the eigenvalues) of W * W.T,
scikit-learn/sklearn/decomposition/_fastica.py,73,j is the index of the extracted component,
scikit-learn/sklearn/decomposition/_fastica.py,112,"builtin max, abs are faster than numpy counter parts.",
scikit-learn/sklearn/decomposition/_fastica.py,125,Some standard non-linear functions.,
scikit-learn/sklearn/decomposition/_fastica.py,126,"XXX: these should be optimized, as they can be a bottleneck.",
scikit-learn/sklearn/decomposition/_fastica.py,128,comment it out?,
scikit-learn/sklearn/decomposition/_fastica.py,131,apply the tanh inplace,
scikit-learn/sklearn/decomposition/_fastica.py,133,XXX compute in chunks to avoid extra allocation,
scikit-learn/sklearn/decomposition/_fastica.py,134,please don't vectorize.,
scikit-learn/sklearn/decomposition/_fastica.py,473,Centering the columns (ie the variables),
scikit-learn/sklearn/decomposition/_fastica.py,477,Whitening and preprocessing by PCA,
scikit-learn/sklearn/decomposition/_fastica.py,481,see (6.33) p.140,
scikit-learn/sklearn/decomposition/_fastica.py,484,see (13.6) p.267 Here X1 is white and data,
scikit-learn/sklearn/decomposition/_fastica.py,485,in X has been projected onto a subspace by PCA,
scikit-learn/sklearn/decomposition/_fastica.py,488,X must be casted to floats to avoid typing issues with numpy,
scikit-learn/sklearn/decomposition/_fastica.py,489,2.0 and the line below,
scikit-learn/sklearn/decomposition/_fastica.py,490,copy has been taken care of,
scikit-learn/sklearn/decomposition/tests/test_fastica.py,37,Test gram schmidt orthonormalization,
scikit-learn/sklearn/decomposition/tests/test_fastica.py,38,generate a random orthogonal  matrix,
scikit-learn/sklearn/decomposition/tests/test_fastica.py,53,Test the FastICA algorithm on very simple data.,
scikit-learn/sklearn/decomposition/tests/test_fastica.py,55,scipy.stats uses the global RNG:,
scikit-learn/sklearn/decomposition/tests/test_fastica.py,57,Generate two sources:,
scikit-learn/sklearn/decomposition/tests/test_fastica.py,64,Mixing angle,
scikit-learn/sklearn/decomposition/tests/test_fastica.py,75,function as fun arg,
scikit-learn/sklearn/decomposition/tests/test_fastica.py,96,Check that the mixing model described in the docstring holds:,
scikit-learn/sklearn/decomposition/tests/test_fastica.py,102,Check to see if the sources have been estimated,
scikit-learn/sklearn/decomposition/tests/test_fastica.py,103,in the wrong order,
scikit-learn/sklearn/decomposition/tests/test_fastica.py,109,Check that we have estimated the original sources,
scikit-learn/sklearn/decomposition/tests/test_fastica.py,117,Test FastICA class,
scikit-learn/sklearn/decomposition/tests/test_fastica.py,142,test for issue #697,
scikit-learn/sklearn/decomposition/tests/test_fastica.py,149,Test the FastICA algorithm on very simple data,
scikit-learn/sklearn/decomposition/tests/test_fastica.py,150,(see test_non_square_fastica).,
scikit-learn/sklearn/decomposition/tests/test_fastica.py,151,Ensure a ConvergenceWarning raised if the tolerance is sufficiently low.,
scikit-learn/sklearn/decomposition/tests/test_fastica.py,155,Generate two sources:,
scikit-learn/sklearn/decomposition/tests/test_fastica.py,162,Mixing matrix,
scikit-learn/sklearn/decomposition/tests/test_fastica.py,166,Do fastICA with tolerance 0. to ensure failing convergence,
scikit-learn/sklearn/decomposition/tests/test_fastica.py,174,Test the FastICA algorithm on very simple data.,
scikit-learn/sklearn/decomposition/tests/test_fastica.py,178,Generate two sources:,
scikit-learn/sklearn/decomposition/tests/test_fastica.py,186,Mixing matrix,
scikit-learn/sklearn/decomposition/tests/test_fastica.py,198,Check that the mixing model described in the docstring holds:,
scikit-learn/sklearn/decomposition/tests/test_fastica.py,203,Check to see if the sources have been estimated,
scikit-learn/sklearn/decomposition/tests/test_fastica.py,204,in the wrong order,
scikit-learn/sklearn/decomposition/tests/test_fastica.py,210,Check that we have estimated the original sources,
scikit-learn/sklearn/decomposition/tests/test_fastica.py,217,Test FastICA.fit_transform,
scikit-learn/sklearn/decomposition/tests/test_fastica.py,238,Test FastICA.inverse_transform,
scikit-learn/sklearn/decomposition/tests/test_fastica.py,255,"catch ""n_components ignored"" warning",
scikit-learn/sklearn/decomposition/tests/test_fastica.py,262,reversibility test in non-reduction case,
scikit-learn/sklearn/decomposition/tests/test_factor_analysis.py,1,Author: Christian Osendorfer <osendorf@gmail.com>,
scikit-learn/sklearn/decomposition/tests/test_factor_analysis.py,2,Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/sklearn/decomposition/tests/test_factor_analysis.py,3,License: BSD3,
scikit-learn/sklearn/decomposition/tests/test_factor_analysis.py,16,Ignore warnings from switching to more power iterations in randomized_svd,
scikit-learn/sklearn/decomposition/tests/test_factor_analysis.py,19,Test FactorAnalysis ability to recover the data covariance structure,
scikit-learn/sklearn/decomposition/tests/test_factor_analysis.py,23,Some random settings for the generative model,
scikit-learn/sklearn/decomposition/tests/test_factor_analysis.py,25,"latent variable of dim 3, 20 of it",
scikit-learn/sklearn/decomposition/tests/test_factor_analysis.py,27,using gamma to model different noise variance,
scikit-learn/sklearn/decomposition/tests/test_factor_analysis.py,28,per component,
scikit-learn/sklearn/decomposition/tests/test_factor_analysis.py,31,generate observations,
scikit-learn/sklearn/decomposition/tests/test_factor_analysis.py,32,"wlog, mean is 0",
scikit-learn/sklearn/decomposition/tests/test_factor_analysis.py,56,Sample Covariance,
scikit-learn/sklearn/decomposition/tests/test_factor_analysis.py,59,Model Covariance,
scikit-learn/sklearn/decomposition/tests/test_factor_analysis.py,68,sign will not be equal,
scikit-learn/sklearn/decomposition/tests/test_factor_analysis.py,77,Test get_covariance and get_precision with n_components == n_features,
scikit-learn/sklearn/decomposition/tests/test_factor_analysis.py,78,with n_components < n_features and with n_components == 0,
scikit-learn/sklearn/decomposition/tests/test_pca.py,24,check the shape of fit.transform,
scikit-learn/sklearn/decomposition/tests/test_pca.py,28,check the equivalence of fit.transform and fit_transform,
scikit-learn/sklearn/decomposition/tests/test_pca.py,34,Test get_covariance and get_precision,
scikit-learn/sklearn/decomposition/tests/test_pca.py,41,test if we avoid numpy warnings for computing over empty arrays,
scikit-learn/sklearn/decomposition/tests/test_pca.py,43,anything > n_comps triggered it in 0.16,
scikit-learn/sklearn/decomposition/tests/test_pca.py,54,Check that PCA output has unit-variance,
scikit-learn/sklearn/decomposition/tests/test_pca.py,61,some low rank data with correlated features,
scikit-learn/sklearn/decomposition/tests/test_pca.py,65,the component-wise variance of the first 50 features is 3 times the,
scikit-learn/sklearn/decomposition/tests/test_pca.py,66,mean component-wise variance of the remaining 30 features,
scikit-learn/sklearn/decomposition/tests/test_pca.py,71,the component-wise variance is thus highly varying:,
scikit-learn/sklearn/decomposition/tests/test_pca.py,74,whiten the data while projecting to the lower dim subspace,
scikit-learn/sklearn/decomposition/tests/test_pca.py,75,make sure we keep an original across iterations.,
scikit-learn/sklearn/decomposition/tests/test_pca.py,78,test fit_transform,
scikit-learn/sklearn/decomposition/tests/test_pca.py,95,in that case the output components still have varying variances,
scikit-learn/sklearn/decomposition/tests/test_pca.py,97,"we always center, so no test for non-centering.",
scikit-learn/sklearn/decomposition/tests/test_pca.py,168,compare to the Frobenius norm,
scikit-learn/sklearn/decomposition/tests/test_pca.py,172,Compare to the 2-norms of the score vectors,
scikit-learn/sklearn/decomposition/tests/test_pca.py,177,set the singular values and see what er get back,
scikit-learn/sklearn/decomposition/tests/test_pca.py,193,Test that the projection of data is correct,
scikit-learn/sklearn/decomposition/tests/test_pca.py,208,Test that the projection of data is correct,
scikit-learn/sklearn/decomposition/tests/test_pca.py,220,Test that the projection of data can be inverted,
scikit-learn/sklearn/decomposition/tests/test_pca.py,223,spherical data,
scikit-learn/sklearn/decomposition/tests/test_pca.py,224,make middle component relatively small,
scikit-learn/sklearn/decomposition/tests/test_pca.py,225,make a large mean,
scikit-learn/sklearn/decomposition/tests/test_pca.py,227,same check that we can find the original data from the transformed,
scikit-learn/sklearn/decomposition/tests/test_pca.py,228,signal (since the data is almost of rank n_components),
scikit-learn/sklearn/decomposition/tests/test_pca.py,253,Ensures that solver-specific extreme inputs for the n_components,
scikit-learn/sklearn/decomposition/tests/test_pca.py,254,parameter raise errors,
scikit-learn/sklearn/decomposition/tests/test_pca.py,255,The smallest dimension,
scikit-learn/sklearn/decomposition/tests/test_pca.py,266,Additional case for arpack,
scikit-learn/sklearn/decomposition/tests/test_pca.py,292,Ensure that n_components == 'mle' doesn't raise error for auto/full,
scikit-learn/sklearn/decomposition/tests/test_pca.py,303,Ensure that n_components == 'mle' will raise an error for unsupported,
scikit-learn/sklearn/decomposition/tests/test_pca.py,304,solvers,
scikit-learn/sklearn/decomposition/tests/test_pca.py,316,Check automated dimensionality setting,
scikit-learn/sklearn/decomposition/tests/test_pca.py,327,TODO: explain what this is testing,
scikit-learn/sklearn/decomposition/tests/test_pca.py,328,Or at least use explicit variable names...,
scikit-learn/sklearn/decomposition/tests/test_pca.py,341,TODO: explain what this is testing,
scikit-learn/sklearn/decomposition/tests/test_pca.py,342,Or at least use explicit variable names...,
scikit-learn/sklearn/decomposition/tests/test_pca.py,369,row > col,
scikit-learn/sklearn/decomposition/tests/test_pca.py,370,row > col,
scikit-learn/sklearn/decomposition/tests/test_pca.py,371,row < col,
scikit-learn/sklearn/decomposition/tests/test_pca.py,383,Test that probabilistic PCA scoring yields a reasonable score,
scikit-learn/sklearn/decomposition/tests/test_pca.py,404,Check that probabilistic PCA selects the right model,
scikit-learn/sklearn/decomposition/tests/test_pca.py,422,Sanity check for the noise_variance_. For more details see,
scikit-learn/sklearn/decomposition/tests/test_pca.py,423,https://github.com/scikit-learn/scikit-learn/issues/7568,
scikit-learn/sklearn/decomposition/tests/test_pca.py,424,https://github.com/scikit-learn/scikit-learn/issues/8541,
scikit-learn/sklearn/decomposition/tests/test_pca.py,425,https://github.com/scikit-learn/scikit-learn/issues/8544,
scikit-learn/sklearn/decomposition/tests/test_pca.py,434,Check the consistency of score between solvers,
scikit-learn/sklearn/decomposition/tests/test_pca.py,443,"arpack raises ValueError for n_components == min(n_samples,  n_features)",
scikit-learn/sklearn/decomposition/tests/test_pca.py,446,ensure that noise_variance_ is 0 in edge cases,
scikit-learn/sklearn/decomposition/tests/test_pca.py,447,"when n_components == min(n_samples, n_features)",
scikit-learn/sklearn/decomposition/tests/test_pca.py,462,"case: n_components in (0,1) => 'full'",
scikit-learn/sklearn/decomposition/tests/test_pca.py,464,case: max(X.shape) <= 500 => 'full',
scikit-learn/sklearn/decomposition/tests/test_pca.py,466,case: n_components >= .8 * min(X.shape) => 'full',
scikit-learn/sklearn/decomposition/tests/test_pca.py,468,n_components >= 1 and n_components < .8*min(X.shape) => 'randomized',
scikit-learn/sklearn/decomposition/tests/test_pca.py,521,Ensure that PCA does not upscale the dtype when input is float32,
scikit-learn/sklearn/decomposition/tests/test_pca.py,536,the rtol is set such that the test passes on all platforms tested on,
scikit-learn/sklearn/decomposition/tests/test_pca.py,537,conda-forge: PR#15775,
scikit-learn/sklearn/decomposition/tests/test_pca.py,538,see: https://github.com/conda-forge/scikit-learn-feedstock/pull/113,
scikit-learn/sklearn/decomposition/tests/test_pca.py,543,Ensure that all int types will be upcast to float64,
scikit-learn/sklearn/decomposition/tests/test_pca.py,562,when n_components is the second highest cumulative sum of the,
scikit-learn/sklearn/decomposition/tests/test_pca.py,563,"explained_variance_ratio_, then n_components_ should equal the",
scikit-learn/sklearn/decomposition/tests/test_pca.py,564,number of features in the dataset #15669,
scikit-learn/sklearn/decomposition/tests/test_pca.py,574,"Test error when tested rank not in [1, n_features - 1]",
scikit-learn/sklearn/decomposition/tests/test_pca.py,584,Test rank associated with tiny eigenvalues are given a log-likelihood of,
scikit-learn/sklearn/decomposition/tests/test_pca.py,585,-inf. The inferred rank will be 1,
scikit-learn/sklearn/decomposition/tests/test_pca.py,597,Test 'mle' with pathological X: only one relevant feature should give a,
scikit-learn/sklearn/decomposition/tests/test_pca.py,598,rank of 1,
scikit-learn/sklearn/decomposition/tests/test_pca.py,608,Tests that an error is raised when the number of samples is smaller,
scikit-learn/sklearn/decomposition/tests/test_pca.py,609,than the number of features during an mle fit,
scikit-learn/sklearn/decomposition/tests/test_pca.py,621,non-regression test for issue,
scikit-learn/sklearn/decomposition/tests/test_pca.py,622,https://github.com/scikit-learn/scikit-learn/issues/16730,
scikit-learn/sklearn/decomposition/tests/test_pca.py,625,true X dim is ndim - 1,
scikit-learn/sklearn/decomposition/tests/test_pca.py,632,Make sure assess_dimension works properly on a matrix of rank 1,
scikit-learn/sklearn/decomposition/tests/test_pca.py,634,rank 1 matrix,
scikit-learn/sklearn/decomposition/tests/test_pca.py,636,"except for rank 1, all eigenvalues are 0",
scikit-learn/sklearn/decomposition/tests/test_nmf.py,6,For testing internals,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,32,Test that initialization does not return negative values,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,72,Test NNDSVD error,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,73,Test that _initialize_nmf error is less than the standard deviation of,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,74,the entries in the matrix.,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,84,Test NNDSVD variants correctness,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,85,Test that the variants 'nndsvda' and 'nndsvdar' differ from basic,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,86,'nndsvd' only where the basic version has zeros.,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,98,ignore UserWarning raised when both solver='mu' and init='nndsvd',
scikit-learn/sklearn/decomposition/tests/test_nmf.py,101,Test that the decomposition does not contain negative values,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,116,Test that the fit is not too far away,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,125,Test that NMF.transform returns close values,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,136,Smoke test that checks if NMF.transform works with custom initialization,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,152,Test that NMF.inverse_transform returns close values,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,163,Smoke test for the case of more components than features.,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,170,Test that sparse matrices are accepted as input,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,193,Test that transform works on sparse data.  Issue #2124,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,208,"Test that the function is called in the same way, either directly",
scikit-learn/sklearn/decomposition/tests/test_nmf.py,209,or through the NMF class,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,233,Test parameters checking is public function,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,283,Compare _beta_divergence with the reference _beta_divergence_dense,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,289,initialization,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,306,"Test the function that computes np.dot(W, H), only where X is non zero.",
scikit-learn/sklearn/decomposition/tests/test_nmf.py,321,"test that both results have same values, in X_csr nonzero elements",
scikit-learn/sklearn/decomposition/tests/test_nmf.py,326,test that WH_safe and X_csr have the same sparse structure,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,334,Compare sparse and dense input in multiplicative update NMF,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,335,Also test continuity of the results with respect to beta_loss parameter,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,343,initialization,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,352,Reference with dense array X,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,359,Compare with sparse X,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,369,"Compare with almost same beta_loss, since some values have a specific",
scikit-learn/sklearn/decomposition/tests/test_nmf.py,370,"behavior, but the results should be continuous w.r.t beta_loss",
scikit-learn/sklearn/decomposition/tests/test_nmf.py,383,Test that an error is raised if beta_loss < 0 and X contains zeros.,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,384,Test that the output has not NaN values when the input contains zeros.,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,412,Test the effect of L1 and L2 regularizations,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,419,L1 regularization should increase the number of zeros,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,441,L2 regularization should decrease the mean of the coefficients,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,461,test that the objective function is decreasing at each iteration,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,469,initialization,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,479,not implemented,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,484,one more iteration starting from the previous results,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,498,Regression test for an underflow issue in _beta_divergence,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,519,Check that NMF preserves dtype (float32 and float64),
scikit-learn/sklearn/decomposition/tests/test_nmf.py,531,Check that the result of NMF is the same between float32 and float64,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,543,Check that an error is raise if custom H and/or W don't have the same,
scikit-learn/sklearn/decomposition/tests/test_nmf.py,544,dtype as X.,
scikit-learn/sklearn/decomposition/tests/test_truncated_svd.py,17,Make an X that looks somewhat like a small tf-idf matrix.,
scikit-learn/sklearn/decomposition/tests/test_truncated_svd.py,37,"All elements are equal, but some elements are more equal than others.",
scikit-learn/sklearn/decomposition/tests/test_truncated_svd.py,73,"We need a lot of components for the reconstruction to be ""almost",
scikit-learn/sklearn/decomposition/tests/test_truncated_svd.py,74,"equal"" in all positions. XXX Test means or sums instead?",
scikit-learn/sklearn/decomposition/tests/test_truncated_svd.py,96,Assert that all the values are greater than 0,
scikit-learn/sklearn/decomposition/tests/test_truncated_svd.py,99,Assert that total explained variance is less than 1,
scikit-learn/sklearn/decomposition/tests/test_truncated_svd.py,102,Test that explained_variance is correct,
scikit-learn/sklearn/decomposition/tests/test_truncated_svd.py,120,Assert the 1st component is equal,
scikit-learn/sklearn/decomposition/tests/test_truncated_svd.py,127,Assert that 20 components has higher explained variance than 10,
scikit-learn/sklearn/decomposition/tests/test_truncated_svd.py,136,Check that the TruncatedSVD output has the correct singular values,
scikit-learn/sklearn/decomposition/tests/test_truncated_svd.py,144,Compare to the Frobenius norm,
scikit-learn/sklearn/decomposition/tests/test_truncated_svd.py,149,Compare to the 2-norms of the score vectors,
scikit-learn/sklearn/decomposition/tests/test_truncated_svd.py,156,Set the singular values and see what we get back,
scikit-learn/sklearn/decomposition/tests/test_truncated_svd.py,177,TruncatedSVD should be equal to PCA on centered data,
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,1,Author: Vlad Niculae,
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,2,License: BSD 3 clause,
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,32,Y is defined by : Y = UV + noise,
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,34,Add noise,
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,37,"SparsePCA can be a bit slow. To avoid having test times go up, we",
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,38,test different aspects of the code in the same test,
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,48,test overcomplete decomposition,
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,58,wide array,
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,63,Test that CD gives similar results,
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,74,wide array,
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,79,Test multiple CPUs,
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,88,Test that SparsePCA won't return NaN when there is 0 feature in all,
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,89,samples.,
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,91,wide array,
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,99,tall array,
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,125,test overcomplete decomposition,
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,132,XXX: test always skipped,
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,137,wide array,
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,141,Test multiple CPUs,
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,142,fake parallelism for win32,
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,152,we can efficiently use parallelism,
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,158,Test that CD gives similar results,
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,77,subsampling factor,
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,80,Compute a wavelet dictionary,
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,91,check that the underlying model fails to converge,
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,97,check that the underlying model converges w/o warnings,
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,176,"used to test lars here too, but there's no guarantee the number of",
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,177,nonzero atoms is right.,
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,181,regression test that parallel reconstruction works with n_jobs>1,
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,318,test verbosity,
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,376,random init,
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,397,random init,
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,413,random init,
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,426,random init,
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,437,random init,
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,449,random init,
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,468,random init,
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,476,random init,
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,485,Non-regression test for:,
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,486,https://github.com/scikit-learn/scikit-learn/issues/5956,
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,487,Test that SparseCoder does not error by passing reading only,
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,488,arrays to child processes,
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,493,Ensure that `data` is >2M. Joblib memory maps arrays,
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,494,if they are larger than 1MB. The 4 accounts for float32,
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,495,data type,
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,24,Histogram kernel implemented as a callable.,
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,25,no kernel_params that we didn't ask for,
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,30,histogram kernel produces singular matrix inside linalg.solve,
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,31,XXX use a least-squares approximation?,
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,34,transform fit data,
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,42,"non-regression test: previously, gamma would be 0 by default,",
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,43,forcing all eigenvalues to 0 under the poly kernel,
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,46,transform new data,
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,51,inverse transform,
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,63,"X_fit_ needs to retain the old, unmodified copy of X",
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,97,transform fit data,
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,105,transform new data,
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,110,inverse transform,
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,111,X_pred2 = kpca.inverse_transform(X_pred_transformed),
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,112,assert X_pred2.shape == X_pred.shape),
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,120,"for a linear kernel, kernel PCA should find the same projection as PCA",
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,121,modulo the sign (direction),
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,122,"fit only the first four components: fifth is near zero eigenvalue, so",
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,123,can be trimmed due to roundoff error,
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,145,n_components=None (default) => remove_zero_eig is True,
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,165,"Assert that even with all np warnings on, there is no div by zero warning",
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,170,"Fit, then transform",
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,172,Do both at once,
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,174,Compare,
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,178,"There might be warnings about the kernel being badly conditioned,",
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,179,but there should not be warnings about division by zero.,
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,180,"(Numpy division by zero warning can have many message variants, but",
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,181,at least we know that it is a RuntimeWarning so lets check only this),
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,220,Test if we can do a grid-search to find parameters to separate,
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,221,circles with a perceptron model.,
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,234,Test if we can do a grid-search to find parameters to separate,
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,235,circles with a perceptron model using a precomputed kernel.,
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,249,Test the linear separability of the first 2D KPCA transform,
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,253,2D nested circles are not linearly separable,
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,257,Project the circles data into the first 2 components of a RBF Kernel,
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,258,PCA model.,
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,259,Note that the gamma value is data dependent. If this test breaks,
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,260,"and the gamma value has to be updated, the Kernel PCA example will",
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,261,have to be updated too.,
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,266,The data is perfectly linearly separable in that space,
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,275,create a pathological X leading to small non-zero eigenvalue,
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,283,check that the small non-zero eigenvalue was correctly set to zero,
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,24,Create 3 topics and each topic has 3 distinct words.,
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,25,(Each word only belongs to a single topic.),
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,35,default prior parameter should be `1 / topics`,
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,36,and verbose params should not affect result,
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,50,Test LDA batch learning_offset (`fit` method with 'batch' learning),
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,60,Find top 3 words in each LDA component,
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,66,Test LDA online learning (`fit` method with 'online' learning),
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,76,Find top 3 words in each LDA component,
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,82,Test LDA online learning (`partial_fit` method),
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,83,(same as test_lda_batch),
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,99,Test LDA with dense input.,
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,108,Find top 3 words in each LDA component,
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,114,Test LDA transform.,
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,115,Transform result cannot be negative and should be normalized,
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,129,Test LDA fit_transform & transform,
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,130,fit_transform and transform result should be the same,
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,141,test `n_features` mismatch in `partial_fit`,
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,156,test `_check_params` method,
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,173,test pass dense matrix with sparse negative input.,
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,182,test `perplexity` before `fit`,
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,194,test `n_features` mismatch in partial_fit and transform,
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,211,Test LDA batch training with multi CPU,
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,226,Test LDA online training with multi CPU,
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,242,test dimension mismatch in `perplexity` method,
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,251,invalid samples,
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,255,invalid topic number,
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,263,Test LDA perplexity for batch training,
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,264,perplexity should be lower after each iteration,
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,286,Test LDA score for batch training,
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,287,score should be higher after each iteration,
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,304,Test LDA perplexity for sparse and dense input,
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,305,score should be the same for both dense and sparse input,
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,317,Test the relationship between LDA score and perplexity,
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,330,Test that the perplexity computed during fit is consistent with what is,
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,331,returned by the perplexity method,
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,338,Perplexity computed at end of fit method,
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,341,Result of perplexity method on the train set,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,18,Incremental PCA on dense arrays.,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,44,Incremental PCA on sparse arrays.,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,75,Test that the projection of data is correct.,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,82,Get the reconstruction of the generated data X,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,83,"Note that Xt has the same ""components"" as X, just separated",
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,84,This is what we want to ensure is recreated correctly,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,87,Normalize,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,90,"Make sure that the first element of Yt is ~1, this means",
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,91,the reconstruction worked as expected,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,96,Test that the projection of data can be inverted.,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,99,spherical data,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,100,make middle component relatively small,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,101,make a large mean,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,103,same check that we can find the original data from the transformed,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,104,signal (since the data is almost of rank n_components),
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,112,Test that n_components is >=1 and <= n_features.,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,123,Tests that n_components is also <= n_samples.,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,132,Ensures that n_components == None is handled correctly,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,138,"First partial_fit call, ipca.n_components_ is inferred from",
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,139,min(X.shape),
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,143,"Second partial_fit call, ipca.n_components_ is inferred from",
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,144,ipca.components_ computed from the first partial_fit call,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,150,Test that components_ sign is stable over batch sizes.,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,159,Decreasing number of components,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,163,Increasing number of components,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,167,Returning to original setting,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,173,Test that changing n_components will raise an error.,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,185,Test that components_ sign is stable over batch sizes.,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,201,Test that components_ values are stable over batch sizes.,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,217,Test sample size in each batch is always larger or equal to n_components,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,234,Test that fit and partial_fit get equivalent results.,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,237,spherical data,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,238,make middle component relatively small,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,239,make a large mean,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,241,same check that we can find the original data from the transformed,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,242,signal (since the data is almost of rank n_components),
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,246,Add one to make sure endpoint is included,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,254,Test that IncrementalPCA and PCA are approximate (to a sign flip).,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,264,Test that IncrementalPCA and PCA are approximate (to a sign flip).,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,277,Test that PCA and IncrementalPCA calculations match,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,294,Check that the IncrementalPCA output has the correct singular values,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,307,Compare to the Frobenius norm,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,315,Compare to the 2-norms of the score vectors,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,321,Set the singular values and see what we get back,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,345,Test that PCA and IncrementalPCA transforms match to sign flip.,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,366,Test to ensure float division is used in all versions of Python,
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,367,(non-regression test for issue #9489),
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,375,Set n_samples_seen_ to be a floating point number instead of an int,
scikit-learn/sklearn/_build_utils/pre_build_helpers.py,19,extra_(pre/post)args can be a callable to make it possible to get its,
scikit-learn/sklearn/_build_utils/pre_build_helpers.py,20,value from the compiler,
scikit-learn/sklearn/_build_utils/pre_build_helpers.py,32,Write test program,
scikit-learn/sklearn/_build_utils/pre_build_helpers.py,38,"Compile, test program",
scikit-learn/sklearn/_build_utils/pre_build_helpers.py,42,Link test program,
scikit-learn/sklearn/_build_utils/pre_build_helpers.py,49,Run test program,
scikit-learn/sklearn/_build_utils/pre_build_helpers.py,50,will raise a CalledProcessError if return code was non-zero,
scikit-learn/sklearn/_build_utils/openmp_helpers.py,3,"This code is adapted for a large part from the astropy openmp helpers, which",
scikit-learn/sklearn/_build_utils/openmp_helpers.py,4,can be found at: https://github.com/astropy/astropy-helpers/blob/master/astropy_helpers/openmp_helpers.py  # noqa,
scikit-learn/sklearn/_build_utils/openmp_helpers.py,31,-fopenmp can't be passed as compile flag when using Apple-clang.,
scikit-learn/sklearn/_build_utils/openmp_helpers.py,32,OpenMP support has to be enabled during preprocessing.,
scikit-learn/sklearn/_build_utils/openmp_helpers.py,33,,
scikit-learn/sklearn/_build_utils/openmp_helpers.py,34,"For example, our macOS wheel build jobs use the following environment",
scikit-learn/sklearn/_build_utils/openmp_helpers.py,35,"variables to build with Apple-clang and the brew installed ""libomp"":",
scikit-learn/sklearn/_build_utils/openmp_helpers.py,36,,
scikit-learn/sklearn/_build_utils/openmp_helpers.py,37,"export CPPFLAGS=""$CPPFLAGS -Xpreprocessor -fopenmp""",
scikit-learn/sklearn/_build_utils/openmp_helpers.py,38,"export CFLAGS=""$CFLAGS -I/usr/local/opt/libomp/include""",
scikit-learn/sklearn/_build_utils/openmp_helpers.py,39,"export CXXFLAGS=""$CXXFLAGS -I/usr/local/opt/libomp/include""",
scikit-learn/sklearn/_build_utils/openmp_helpers.py,40,"export LDFLAGS=""$LDFLAGS -Wl,-rpath,/usr/local/opt/libomp/lib",
scikit-learn/sklearn/_build_utils/openmp_helpers.py,41,"-L/usr/local/opt/libomp/lib -lomp""",
scikit-learn/sklearn/_build_utils/openmp_helpers.py,43,Default flag for GCC and clang:,
scikit-learn/sklearn/_build_utils/__init__.py,4,"author: Andy Mueller, Gael Varoquaux",
scikit-learn/sklearn/_build_utils/__init__.py,5,license: BSD,
scikit-learn/sklearn/_build_utils/__init__.py,20,The following places need to be in sync with regard to Cython version:,
scikit-learn/sklearn/_build_utils/__init__.py,21,- .circleci config file,
scikit-learn/sklearn/_build_utils/__init__.py,22,- sklearn/_build_utils/__init__.py,
scikit-learn/sklearn/_build_utils/__init__.py,23,- advanced installation guide,
scikit-learn/sklearn/_build_utils/__init__.py,34,Re-raise with more informative error message instead:,
scikit-learn/sklearn/_build_utils/__init__.py,48,Fast fail before cythonization if compiler fails compiling basic test,
scikit-learn/sklearn/_build_utils/__init__.py,49,code even without OpenMP,
scikit-learn/sklearn/_build_utils/__init__.py,52,check simple compilation with OpenMP. If it fails scikit-learn will be,
scikit-learn/sklearn/_build_utils/__init__.py,53,built without OpenMP and the test test_openmp_supported in the test suite,
scikit-learn/sklearn/_build_utils/__init__.py,54,will fail.,
scikit-learn/sklearn/_build_utils/__init__.py,55,`check_openmp_support` compiles a small test program to see if the,
scikit-learn/sklearn/_build_utils/__init__.py,56,compilers are properly configured to build with OpenMP. This is expensive,
scikit-learn/sklearn/_build_utils/__init__.py,57,and we only want to call this function once.,
scikit-learn/sklearn/_build_utils/__init__.py,58,The result of this check is cached as a private attribute on the sklearn,
scikit-learn/sklearn/_build_utils/__init__.py,59,module (only at build-time) to be used twice:,
scikit-learn/sklearn/_build_utils/__init__.py,60,"- First to set the value of SKLEARN_OPENMP_PARALLELISM_ENABLED, the",
scikit-learn/sklearn/_build_utils/__init__.py,61,cython build-time variable passed to the cythonize() call.,
scikit-learn/sklearn/_build_utils/__init__.py,62,- Then in the build_ext subclass defined in the top-level setup.py file,
scikit-learn/sklearn/_build_utils/__init__.py,63,to actually build the compiled extensions with OpenMP flags if needed.,
scikit-learn/sklearn/_build_utils/__init__.py,70,earlier joblib versions don't account for CPU affinity,
scikit-learn/sklearn/_build_utils/__init__.py,71,"constraints, and may over-estimate the number of available",
scikit-learn/sklearn/_build_utils/__init__.py,72,CPU particularly in CI (cf loky#114),
scikit-learn/sklearn/_build_utils/__init__.py,85,Lazy import because cython is not a runtime dependency.,
scikit-learn/sklearn/_build_utils/__init__.py,91,"if the template is not updated, no need to output the cython file",
scikit-learn/sklearn/_build_utils/deprecated_modules.py,6,TODO: Remove the whole file in 0.24,
scikit-learn/sklearn/_build_utils/deprecated_modules.py,8,This is a set of 4-tuples consisting of,
scikit-learn/sklearn/_build_utils/deprecated_modules.py,9,"(new_module_name, deprecated_path, correct_import_path, importee)",
scikit-learn/sklearn/_build_utils/deprecated_modules.py,10,importee is used by test_import_deprecations to check for DeprecationWarnings,
scikit-learn/sklearn/cross_decomposition/_pls.py,5,Author: Edouard Duchesnay <edouard.duchesnay@cea.fr>,
scikit-learn/sklearn/cross_decomposition/_pls.py,6,License: BSD 3 clause,
scikit-learn/sklearn/cross_decomposition/_pls.py,46,Uses condition from scipy<1.3 in pinv2 which was changed in,
scikit-learn/sklearn/cross_decomposition/_pls.py,47,"https://github.com/scipy/scipy/pull/10067. In scipy 1.3, the",
scikit-learn/sklearn/cross_decomposition/_pls.py,48,condition was changed to depend on the largest singular value,
scikit-learn/sklearn/cross_decomposition/_pls.py,56,Inner loop of the Wold algo.,
scikit-learn/sklearn/cross_decomposition/_pls.py,58,1.1 Update u: the X weights,
scikit-learn/sklearn/cross_decomposition/_pls.py,61,We use slower pinv2 (same as np.linalg.pinv) for stability,
scikit-learn/sklearn/cross_decomposition/_pls.py,62,reasons,
scikit-learn/sklearn/cross_decomposition/_pls.py,65,mode A,
scikit-learn/sklearn/cross_decomposition/_pls.py,66,Mode A regress each X column on y_score,
scikit-learn/sklearn/cross_decomposition/_pls.py,68,If y_score only has zeros x_weights will only have zeros. In,
scikit-learn/sklearn/cross_decomposition/_pls.py,69,this case add an epsilon to converge to a more acceptable,
scikit-learn/sklearn/cross_decomposition/_pls.py,70,solution,
scikit-learn/sklearn/cross_decomposition/_pls.py,73,1.2 Normalize u,
scikit-learn/sklearn/cross_decomposition/_pls.py,75,1.3 Update x_score: the X latent scores,
scikit-learn/sklearn/cross_decomposition/_pls.py,77,2.1 Update y_weights,
scikit-learn/sklearn/cross_decomposition/_pls.py,80,compute once pinv(Y),
scikit-learn/sklearn/cross_decomposition/_pls.py,84,Mode A regress each Y column on x_score,
scikit-learn/sklearn/cross_decomposition/_pls.py,86,2.2 Normalize y_weights,
scikit-learn/sklearn/cross_decomposition/_pls.py,89,2.3 Update y_score: the Y latent scores,
scikit-learn/sklearn/cross_decomposition/_pls.py,91,"y_score = np.dot(Y, y_weights) / np.dot(y_score.T, y_score) ## BUG",
scikit-learn/sklearn/cross_decomposition/_pls.py,119,center,
scikit-learn/sklearn/cross_decomposition/_pls.py,124,scale,
scikit-learn/sklearn/cross_decomposition/_pls.py,280,copy since this will contains the residuals (deflated) matrices,
scikit-learn/sklearn/cross_decomposition/_pls.py,303,Scale (in place),
scikit-learn/sklearn/cross_decomposition/_pls.py,306,Residuals (deflated) matrices,
scikit-learn/sklearn/cross_decomposition/_pls.py,309,Results matrices,
scikit-learn/sklearn/cross_decomposition/_pls.py,318,"NIPALS algo: outer loop, over components",
scikit-learn/sklearn/cross_decomposition/_pls.py,322,Yk constant,
scikit-learn/sklearn/cross_decomposition/_pls.py,325,1) weights estimation (inner loop),
scikit-learn/sklearn/cross_decomposition/_pls.py,326,-----------------------------------,
scikit-learn/sklearn/cross_decomposition/_pls.py,328,Replace columns that are all close to zero with zeros,
scikit-learn/sklearn/cross_decomposition/_pls.py,339,Forces sign stability of x_weights and y_weights,
scikit-learn/sklearn/cross_decomposition/_pls.py,340,"Sign undeterminacy issue from svd if algorithm == ""svd""",
scikit-learn/sklearn/cross_decomposition/_pls.py,341,and from platform dependent computation if algorithm == 'nipals',
scikit-learn/sklearn/cross_decomposition/_pls.py,344,compute scores,
scikit-learn/sklearn/cross_decomposition/_pls.py,351,test for null variance,
scikit-learn/sklearn/cross_decomposition/_pls.py,355,2) Deflation (in place),
scikit-learn/sklearn/cross_decomposition/_pls.py,356,----------------------,
scikit-learn/sklearn/cross_decomposition/_pls.py,357,Possible memory footprint reduction may done here: in order to,
scikit-learn/sklearn/cross_decomposition/_pls.py,358,avoid the allocation of a data chunk for the rank-one,
scikit-learn/sklearn/cross_decomposition/_pls.py,359,"approximations matrix which is then subtracted to Xk, we suggest",
scikit-learn/sklearn/cross_decomposition/_pls.py,360,to perform a column-wise deflation.,
scikit-learn/sklearn/cross_decomposition/_pls.py,361,,
scikit-learn/sklearn/cross_decomposition/_pls.py,362,- regress Xk's on x_score,
scikit-learn/sklearn/cross_decomposition/_pls.py,364,- subtract rank-one approximations to obtain remainder matrix,
scikit-learn/sklearn/cross_decomposition/_pls.py,367,"- regress Yk's on y_score, then subtract rank-one approx.",
scikit-learn/sklearn/cross_decomposition/_pls.py,372,"- regress Yk's on x_score, then subtract rank-one approx.",
scikit-learn/sklearn/cross_decomposition/_pls.py,376,"3) Store weights, scores and loadings # Notation:",
scikit-learn/sklearn/cross_decomposition/_pls.py,377,T,
scikit-learn/sklearn/cross_decomposition/_pls.py,378,U,
scikit-learn/sklearn/cross_decomposition/_pls.py,379,W,
scikit-learn/sklearn/cross_decomposition/_pls.py,380,C,
scikit-learn/sklearn/cross_decomposition/_pls.py,381,P,
scikit-learn/sklearn/cross_decomposition/_pls.py,382,Q,
scikit-learn/sklearn/cross_decomposition/_pls.py,383,Such that: X = TP' + Err and Y = UQ' + Err,
scikit-learn/sklearn/cross_decomposition/_pls.py,385,4) rotations from input space to transformed space (scores),
scikit-learn/sklearn/cross_decomposition/_pls.py,386,T = X W(P'W)^-1 = XW* (W* : p x k matrix),
scikit-learn/sklearn/cross_decomposition/_pls.py,387,U = Y C(Q'C)^-1 = YC* (W* : q x k matrix),
scikit-learn/sklearn/cross_decomposition/_pls.py,401,FIXME what's with the if?,
scikit-learn/sklearn/cross_decomposition/_pls.py,402,Estimate regression coefficient,
scikit-learn/sklearn/cross_decomposition/_pls.py,403,Regress Y on T,
scikit-learn/sklearn/cross_decomposition/_pls.py,404,"Y = TQ' + Err,",
scikit-learn/sklearn/cross_decomposition/_pls.py,405,Then express in function of X,
scikit-learn/sklearn/cross_decomposition/_pls.py,406,Y = X W(P'W)^-1Q' + Err = XB + Err,
scikit-learn/sklearn/cross_decomposition/_pls.py,407,=> B = W*Q' (p x q),
scikit-learn/sklearn/cross_decomposition/_pls.py,434,Normalize,
scikit-learn/sklearn/cross_decomposition/_pls.py,437,Apply rotation,
scikit-learn/sklearn/cross_decomposition/_pls.py,469,From pls space to original space,
scikit-learn/sklearn/cross_decomposition/_pls.py,472,Denormalize,
scikit-learn/sklearn/cross_decomposition/_pls.py,496,Normalize,
scikit-learn/sklearn/cross_decomposition/_pls.py,892,copy since this will contains the centered data,
scikit-learn/sklearn/cross_decomposition/_pls.py,905,Scale (in place),
scikit-learn/sklearn/cross_decomposition/_pls.py,908,svd(X'Y),
scikit-learn/sklearn/cross_decomposition/_pls.py,911,The arpack svds solver only works if the number of extracted,
scikit-learn/sklearn/cross_decomposition/_pls.py,912,"components is smaller than rank(X) - 1. Hence, if we want to extract",
scikit-learn/sklearn/cross_decomposition/_pls.py,913,"all the components (C.shape[1]), we have to use another one. Else,",
scikit-learn/sklearn/cross_decomposition/_pls.py,914,let's use arpacks to compute only the interesting components.,
scikit-learn/sklearn/cross_decomposition/_pls.py,919,Deterministic output,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,19,1) Canonical (symmetric) PLS (PLS 2 blocks canonical mode A),
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,20,===========================================================,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,21,Compare 2 algo.: nipals vs. svd,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,22,------------------------------,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,27,check equalities of loading (up to the sign of the second column),
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,38,Check PLS properties (with n_components=X.shape[1]),
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,39,---------------------------------------------------,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,53,Orthogonality of weights,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,54,~~~~~~~~~~~~~~~~~~~~~~~~,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,58,Orthogonality of latent scores,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,59,~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,63,Check X = TP' and Y = UQ' (with (p == q) components),
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,64,~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,65,"center scale X, Y",
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,71,Check that rotations on training data lead to scores,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,72,~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,82,Check that inverse_transform works,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,83,~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,88,"""Non regression test"" on canonical PLS",
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,89,--------------------------------------,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,90,The results were checked against the R-package plspm,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,98,"x_weights_sign_flip holds columns of 1 or -1, depending on sign flip",
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,99,between R and python,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,120,x_weights = X.dot(x_rotation),
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,121,Hence R/python sign flip should be the same in x_weight and x_rotation,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,123,This test that R / python give the same result up to column,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,124,sign indeterminacy,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,133,"2) Regression PLS (PLS2): ""Non regression test""",
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,134,===============================================,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,135,"The results were checked against the R-packages plspm, misOmics and pls",
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,163,"x_loadings[:, i] = Xi.dot(x_weights[:, i]) \forall i",
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,172,3) Another non-regression test of Canonical PLS on random dataset,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,173,=================================================================,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,174,The results were checked against the R-package plspm,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,178,2 latents vars:,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,260,Orthogonality of weights,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,261,~~~~~~~~~~~~~~~~~~~~~~~~,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,265,Orthogonality of latent scores,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,266,~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,270,"4) Another ""Non regression test"" of PLS Regression (PLS2):",
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,271,Checking behavior when the first column of Y is constant,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,272,===============================================,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,273,The results were compared against a modified version of plsreg2,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,274,from the R-package plsdepot,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,298,R/python sign flip should be the same in x_weight and x_rotation,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,301,This test that R / python give the same result up to column,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,302,sign indeterminacy,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,306,"For the PLSRegression with default parameters, it holds that",
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,307,y_loadings==y_weights. In this case we only test that R/python,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,308,give the same result for the y_loadings irrespective of the sign,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,322,Let's check the PLSSVD doesn't return all possible component but just,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,323,the specified number,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,335,Ensure 1d Y is correctly interpreted,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,341,Compare 1d to column vector,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,348,"check that the ""copy"" keyword works",
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,356,check that results are identical with copy,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,360,check also if passing Y,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,363,check that copy doesn't destroy,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,364,we do want to check exact equality here,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,367,also check that mean wasn't zero before (to make sure we didn't touch it),
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,372,We test scale=True parameter,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,373,This allows to check numerical stability over platforms as well,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,378,"causes X[:, -1].std() to be zero",
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,381,From bug #2821,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,382,"Test with X2, T2 s.t. clf.x_score[:, 1] == 0, clf.y_score[:, 1] == 0",
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,383,This test robustness of algorithm when dealing with value close to 0,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,410,Scaling should be idempotent,
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,429,sanity check for scale=True,
scikit-learn/sklearn/_loss/glm_distribution.py,5,Author: Christian Lorentzen <lorentzen.ch@googlemail.com>,
scikit-learn/sklearn/_loss/glm_distribution.py,6,License: BSD 3 clause,
scikit-learn/sklearn/_loss/glm_distribution.py,57,Note that currently supported distributions have +inf upper bound,
scikit-learn/sklearn/_loss/glm_distribution.py,212,"We use a property with a setter, to update lower and",
scikit-learn/sklearn/_loss/glm_distribution.py,213,upper bound when the power parameter is updated e.g. in grid,
scikit-learn/sklearn/_loss/glm_distribution.py,214,search.,
scikit-learn/sklearn/_loss/glm_distribution.py,220,Extreme Stable or Normal distribution,
scikit-learn/sklearn/_loss/glm_distribution.py,226,Poisson or Compound Poisson distribution,
scikit-learn/sklearn/_loss/glm_distribution.py,229,"Gamma, Positive Stable, Inverse Gaussian distributions",
scikit-learn/sklearn/_loss/glm_distribution.py,231,pragma: no cover,
scikit-learn/sklearn/_loss/glm_distribution.py,232,this branch should be unreachable.,
scikit-learn/sklearn/_loss/glm_distribution.py,278,"'Extreme stable', y any realy number, y_pred > 0",
scikit-learn/sklearn/_loss/glm_distribution.py,282,"Normal, y and y_pred can be any real number",
scikit-learn/sklearn/_loss/glm_distribution.py,288,"Poisson and Compount poisson distribution, y >= 0, y_pred > 0",
scikit-learn/sklearn/_loss/glm_distribution.py,293,"Gamma and Extreme stable distribution, y and y_pred > 0",
scikit-learn/sklearn/_loss/glm_distribution.py,297,pragma: nocover,
scikit-learn/sklearn/_loss/glm_distribution.py,298,Unreachable statement,
scikit-learn/sklearn/_loss/glm_distribution.py,302,"'Extreme stable', y any realy number, y_pred > 0",
scikit-learn/sklearn/_loss/glm_distribution.py,308,"Normal distribution, y and y_pred any real number",
scikit-learn/sklearn/_loss/glm_distribution.py,314,Poisson distribution,
scikit-learn/sklearn/_loss/glm_distribution.py,317,Gamma distribution,
scikit-learn/sklearn/_loss/tests/test_glm_distribution.py,1,Authors: Christian Lorentzen <lorentzen.ch@gmail.com>,
scikit-learn/sklearn/_loss/tests/test_glm_distribution.py,2,,
scikit-learn/sklearn/_loss/tests/test_glm_distribution.py,3,License: BSD 3 clause,
scikit-learn/sklearn/_loss/tests/test_glm_distribution.py,97,make data positive,
scikit-learn/sklearn/impute/_knn.py,1,Authors: Ashim Bhattarai <ashimb9@gmail.com>,
scikit-learn/sklearn/impute/_knn.py,2,Thomas J Fan <thomasjpfan@gmail.com>,
scikit-learn/sklearn/impute/_knn.py,3,License: BSD 3 clause,
scikit-learn/sklearn/impute/_knn.py,135,Get donors,
scikit-learn/sklearn/impute/_knn.py,139,Get weight matrix from from distance matrix,
scikit-learn/sklearn/impute/_knn.py,145,fill nans with zeros,
scikit-learn/sklearn/impute/_knn.py,149,Retrieve donor values and calculate kNN average,
scikit-learn/sklearn/impute/_knn.py,169,Check data integrity and calling arguments,
scikit-learn/sklearn/impute/_knn.py,224,No missing values in X,
scikit-learn/sklearn/impute/_knn.py,225,Remove columns where the training data is all nan,
scikit-learn/sklearn/impute/_knn.py,232,Maps from indices from X to indices in dist matrix,
scikit-learn/sklearn/impute/_knn.py,239,Find and impute missing by column,
scikit-learn/sklearn/impute/_knn.py,242,column was all missing during training,
scikit-learn/sklearn/impute/_knn.py,247,column has no missing values,
scikit-learn/sklearn/impute/_knn.py,252,receivers_idx are indices in X,
scikit-learn/sklearn/impute/_knn.py,255,distances for samples that needed imputation for column,
scikit-learn/sklearn/impute/_knn.py,259,receivers with all nan distances impute with mean,
scikit-learn/sklearn/impute/_knn.py,269,all receivers imputed with mean,
scikit-learn/sklearn/impute/_knn.py,272,receivers with at least one defined distance,
scikit-learn/sklearn/impute/_knn.py,286,process in fixed-memory chunks,
scikit-learn/sklearn/impute/_knn.py,295,process_chunk modifies X in place. No return value.,
scikit-learn/sklearn/impute/_iterative.py,304,"if no missing values, don't predict",
scikit-learn/sklearn/impute/_iterative.py,308,get posterior samples if there is at least one missing value,
scikit-learn/sklearn/impute/_iterative.py,314,two types of problems: (1) non-positive sigmas,
scikit-learn/sklearn/impute/_iterative.py,315,(2) mus outside legal range of min_value and max_value,
scikit-learn/sklearn/impute/_iterative.py,316,(results in inf sample),
scikit-learn/sklearn/impute/_iterative.py,323,the rest can be sampled without statistical issues,
scikit-learn/sklearn/impute/_iterative.py,340,update the feature,
scikit-learn/sklearn/impute/_iterative.py,456,if a feature in the neighboorhood has only a single value,
scikit-learn/sklearn/impute/_iterative.py,457,"(e.g., categorical feature), the std. dev. will be null and",
scikit-learn/sklearn/impute/_iterative.py,458,np.corrcoef will raise a warning due to a division by zero,
scikit-learn/sklearn/impute/_iterative.py,460,np.corrcoef is not defined for features with zero std,
scikit-learn/sklearn/impute/_iterative.py,462,"ensures exploration, i.e. at least some probability of sampling",
scikit-learn/sklearn/impute/_iterative.py,464,features are not their own neighbors,
scikit-learn/sklearn/impute/_iterative.py,466,needs to sum to 1 for np.random.choice sampling,
scikit-learn/sklearn/impute/_iterative.py,599,Edge case: a single feature. We return the initial ...,
scikit-learn/sklearn/impute/_iterative.py,613,order in which to impute,
scikit-learn/sklearn/impute/_iterative.py,614,note this is probably too slow for large feature data (d > 100000),
scikit-learn/sklearn/impute/_iterative.py,615,and a better way would be good.,
scikit-learn/sklearn/impute/_iterative.py,616,see: https://goo.gl/KyCNwj and subsequent comments,
scikit-learn/sklearn/impute/__init__.py,8,Avoid errors in type checkers (e.g. mypy) for experimental estimators.,
scikit-learn/sklearn/impute/__init__.py,9,TODO: remove this check once the estimator is no longer experimental.,
scikit-learn/sklearn/impute/__init__.py,10,noqa,
scikit-learn/sklearn/impute/_base.py,1,Authors: Nicolas Tresegnie <nicolas.tresegnie@gmail.com>,
scikit-learn/sklearn/impute/_base.py,2,Sergey Feldman <sergeyfeldman@gmail.com>,
scikit-learn/sklearn/impute/_base.py,3,License: BSD 3 clause,
scikit-learn/sklearn/impute/_base.py,34,Compute the most frequent value in array only,
scikit-learn/sklearn/impute/_base.py,37,stats.mode raises a warning when input array contains objects due,
scikit-learn/sklearn/impute/_base.py,38,to incapacity to detect NaNs. Irrelevant here since input array,
scikit-learn/sklearn/impute/_base.py,39,has already been NaN-masked.,
scikit-learn/sklearn/impute/_base.py,49,Compare to array + [extra_value] * n_repeat,
scikit-learn/sklearn/impute/_base.py,57,Ties the breaks. Copy the behaviour of scipy.stats.mode,
scikit-learn/sklearn/impute/_base.py,276,"default fill_value is 0 for numerical input and ""missing_value""",
scikit-learn/sklearn/impute/_base.py,277,otherwise,
scikit-learn/sklearn/impute/_base.py,286,fill_value should be numerical in case of numerical input,
scikit-learn/sklearn/impute/_base.py,295,missing_values = 0 not allowed with sparse data as it would,
scikit-learn/sklearn/impute/_base.py,296,force densification,
scikit-learn/sklearn/impute/_base.py,321,"for constant strategy, self.statistcs_ is used to store",
scikit-learn/sklearn/impute/_base.py,322,fill_value in each column,
scikit-learn/sklearn/impute/_base.py,330,combine explicit and implicit zeros,
scikit-learn/sklearn/impute/_base.py,355,Mean,
scikit-learn/sklearn/impute/_base.py,358,"Avoid the warning ""Warning: converting a masked element to nan.""",
scikit-learn/sklearn/impute/_base.py,364,Median,
scikit-learn/sklearn/impute/_base.py,367,"Avoid the warning ""Warning: converting a masked element to nan.""",
scikit-learn/sklearn/impute/_base.py,373,Most frequent,
scikit-learn/sklearn/impute/_base.py,375,Avoid use of scipy.stats.mstats.mode due to the required,
scikit-learn/sklearn/impute/_base.py,376,additional overhead and slow benchmarking performance.,
scikit-learn/sklearn/impute/_base.py,377,See Issue 14325 and PR 14399 for full discussion.,
scikit-learn/sklearn/impute/_base.py,379,To be able access the elements by columns,
scikit-learn/sklearn/impute/_base.py,395,Constant,
scikit-learn/sklearn/impute/_base.py,397,"for constant strategy, self.statistcs_ is used to store",
scikit-learn/sklearn/impute/_base.py,398,fill_value in each column,
scikit-learn/sklearn/impute/_base.py,420,Delete the invalid columns if strategy is not constant,
scikit-learn/sklearn/impute/_base.py,424,same as np.isnan but also works for object dtypes,
scikit-learn/sklearn/impute/_base.py,437,Do actual imputation,
scikit-learn/sklearn/impute/_base.py,559,The imputer mask will be constructed with the same sparse format,
scikit-learn/sklearn/impute/_base.py,560,as X.,
scikit-learn/sklearn/impute/_base.py,609,missing_values = 0 not allowed with sparse data as it would,
scikit-learn/sklearn/impute/_base.py,610,force densification,
scikit-learn/sklearn/impute/tests/test_impute.py,16,make IterativeImputer available,
scikit-learn/sklearn/impute/tests/test_impute.py,17,noqa,
scikit-learn/sklearn/impute/tests/test_impute.py,50,Normal matrix,
scikit-learn/sklearn/impute/tests/test_impute.py,57,Sparse matrix,
scikit-learn/sklearn/impute/tests/test_impute.py,73,Verify the shapes of the imputed matrix for different strategies.,
scikit-learn/sklearn/impute/tests/test_impute.py,111,check that error are raised when missing_values = 0 and input is sparse,
scikit-learn/sklearn/impute/tests/test_impute.py,126,np.median([]) raises a TypeError for numpy >= 1.10.1,
scikit-learn/sklearn/impute/tests/test_impute.py,132,np.mean([]) raises a RuntimeWarning for numpy >= 1.10.1,
scikit-learn/sklearn/impute/tests/test_impute.py,138,"Test imputation using the mean and median strategies, when",
scikit-learn/sklearn/impute/tests/test_impute.py,139,missing_values != 0.,
scikit-learn/sklearn/impute/tests/test_impute.py,159,Create a matrix X with columns,
scikit-learn/sklearn/impute/tests/test_impute.py,160,"- with only zeros,",
scikit-learn/sklearn/impute/tests/test_impute.py,161,- with only missing values,
scikit-learn/sklearn/impute/tests/test_impute.py,162,"- with zeros, missing values and values",
scikit-learn/sklearn/impute/tests/test_impute.py,163,And a matrix X_true containing all true values,
scikit-learn/sklearn/impute/tests/test_impute.py,176,Create the columns,
scikit-learn/sklearn/impute/tests/test_impute.py,180,XXX unreached code as of v0.22,
scikit-learn/sklearn/impute/tests/test_impute.py,191,Shuffle them the same way,
scikit-learn/sklearn/impute/tests/test_impute.py,195,"Mean doesn't support columns containing NaNs, median does",
scikit-learn/sklearn/impute/tests/test_impute.py,208,Test median imputation with sparse boundary cases,
scikit-learn/sklearn/impute/tests/test_impute.py,210,odd: implicit zero,
scikit-learn/sklearn/impute/tests/test_impute.py,211,odd: explicit nonzero,
scikit-learn/sklearn/impute/tests/test_impute.py,212,even: average two zeros,
scikit-learn/sklearn/impute/tests/test_impute.py,213,even: avg zero and neg,
scikit-learn/sklearn/impute/tests/test_impute.py,214,even: avg zero and pos,
scikit-learn/sklearn/impute/tests/test_impute.py,215,even: avg nonzeros,
scikit-learn/sklearn/impute/tests/test_impute.py,216,even: avg negatives,
scikit-learn/sklearn/impute/tests/test_impute.py,217,even: crossing neg and pos,
scikit-learn/sklearn/impute/tests/test_impute.py,266,"Test imputation on non-numeric data using ""most_frequent"" and ""constant""",
scikit-learn/sklearn/impute/tests/test_impute.py,267,strategy,
scikit-learn/sklearn/impute/tests/test_impute.py,282,Test imputation using the most-frequent strategy.,
scikit-learn/sklearn/impute/tests/test_impute.py,297,"scipy.stats.mode, used in SimpleImputer, doesn't return the first most",
scikit-learn/sklearn/impute/tests/test_impute.py,298,frequent as promised in the doc but the lowest most frequent. When this,
scikit-learn/sklearn/impute/tests/test_impute.py,299,"test will fail after an update of scipy, SimpleImputer will need to be",
scikit-learn/sklearn/impute/tests/test_impute.py,300,updated to be consistent with the new (correct) behaviour,
scikit-learn/sklearn/impute/tests/test_impute.py,306,Test imputation using the most-frequent strategy.,
scikit-learn/sklearn/impute/tests/test_impute.py,330,Test imputation using the most frequent strategy on pandas df,
scikit-learn/sklearn/impute/tests/test_impute.py,356,Verify that exceptions are raised on invalid fill_value type,
scikit-learn/sklearn/impute/tests/test_impute.py,368,Test imputation using the constant strategy on integers,
scikit-learn/sklearn/impute/tests/test_impute.py,392,Test imputation using the constant strategy on floats,
scikit-learn/sklearn/impute/tests/test_impute.py,419,Test imputation using the constant strategy on objects,
scikit-learn/sklearn/impute/tests/test_impute.py,443,Test imputation using the constant strategy on pandas df,
scikit-learn/sklearn/impute/tests/test_impute.py,469,check we exit early when there is a single feature,
scikit-learn/sklearn/impute/tests/test_impute.py,480,Test imputation within a pipeline + gridsearch.,
scikit-learn/sklearn/impute/tests/test_impute.py,499,Test imputation with copy,
scikit-learn/sklearn/impute/tests/test_impute.py,502,"copy=True, dense => copy",
scikit-learn/sklearn/impute/tests/test_impute.py,509,"copy=True, sparse csr => copy",
scikit-learn/sklearn/impute/tests/test_impute.py,517,"copy=False, dense => no copy",
scikit-learn/sklearn/impute/tests/test_impute.py,524,"copy=False, sparse csc => no copy",
scikit-learn/sklearn/impute/tests/test_impute.py,532,"copy=False, sparse csr => copy",
scikit-learn/sklearn/impute/tests/test_impute.py,540,"Note: If X is sparse and if missing_values=0, then a (dense) copy of X is",
scikit-learn/sklearn/impute/tests/test_impute.py,541,"made, even if copy=False.",
scikit-learn/sklearn/impute/tests/test_impute.py,555,"with max_iter=0, only initial imputation is performed",
scikit-learn/sklearn/impute/tests/test_impute.py,558,repeat but force n_iter_ to 0,
scikit-learn/sklearn/impute/tests/test_impute.py,560,transformed should not be equal to initial imputation,
scikit-learn/sklearn/impute/tests/test_impute.py,565,now they should be equal as only initial imputation is done,
scikit-learn/sklearn/impute/tests/test_impute.py,603,this column should not be discarded by IterativeImputer,
scikit-learn/sklearn/impute/tests/test_impute.py,650,check that types are correct for estimators,
scikit-learn/sklearn/impute/tests/test_impute.py,658,check that each estimator is unique,
scikit-learn/sklearn/impute/tests/test_impute.py,704,test that the values that are imputed using `sample_posterior=True`,
scikit-learn/sklearn/impute/tests/test_impute.py,705,with boundaries (`min_value` and `max_value` are not None) are drawn,
scikit-learn/sklearn/impute/tests/test_impute.py,706,from a distribution that looks gaussian via the Kolmogorov Smirnov test.,
scikit-learn/sklearn/impute/tests/test_impute.py,707,note that starting from the wrong random seed will make this test fail,
scikit-learn/sklearn/impute/tests/test_impute.py,708,because random sampling doesn't occur at all when the imputation,
scikit-learn/sklearn/impute/tests/test_impute.py,709,"is outside of the (min_value, max_value) range",
scikit-learn/sklearn/impute/tests/test_impute.py,721,generate multiple imputations for the single missing value,
scikit-learn/sklearn/impute/tests/test_impute.py,732,we want to fail to reject null hypothesis,
scikit-learn/sklearn/impute/tests/test_impute.py,733,null hypothesis: distributions are the same,
scikit-learn/sklearn/impute/tests/test_impute.py,749,definitely no missing values in 0th column,
scikit-learn/sklearn/impute/tests/test_impute.py,750,definitely missing value in 0th column,
scikit-learn/sklearn/impute/tests/test_impute.py,759,"if there were no missing values at time of fit, then imputer will",
scikit-learn/sklearn/impute/tests/test_impute.py,760,only use the initial imputer for that feature at transform,
scikit-learn/sklearn/impute/tests/test_impute.py,773,"when sample_posterior=True, two transforms shouldn't be equal",
scikit-learn/sklearn/impute/tests/test_impute.py,783,sufficient to assert that the means are not the same,
scikit-learn/sklearn/impute/tests/test_impute.py,786,"when sample_posterior=False, and n_nearest_features=None",
scikit-learn/sklearn/impute/tests/test_impute.py,787,and imputation_order is not random,
scikit-learn/sklearn/impute/tests/test_impute.py,788,the two transforms should be identical even if rng are different,
scikit-learn/sklearn/impute/tests/test_impute.py,821,should exclude the first column entirely,
scikit-learn/sklearn/impute/tests/test_impute.py,823,fit and fit_transform should both be identical,
scikit-learn/sklearn/impute/tests/test_impute.py,859,split up data in half,
scikit-learn/sklearn/impute/tests/test_impute.py,883,a quarter is randomly missing,
scikit-learn/sklearn/impute/tests/test_impute.py,888,split up data,
scikit-learn/sklearn/impute/tests/test_impute.py,948,check that we catch a RuntimeWarning due to a division by zero when a,
scikit-learn/sklearn/impute/tests/test_impute.py,949,feature is constant in the dataset,
scikit-learn/sklearn/impute/tests/test_impute.py,953,simulate that a feature only contain one category during fit,
scikit-learn/sklearn/impute/tests/test_impute.py,956,add some missing values,
scikit-learn/sklearn/impute/tests/test_impute.py,985,check that passing scalar or array-like,
scikit-learn/sklearn/impute/tests/test_impute.py,986,for min_value and max_value in IterativeImputer works,
scikit-learn/sklearn/impute/tests/test_impute.py,1006,check that passing scalar or array-like,
scikit-learn/sklearn/impute/tests/test_impute.py,1007,for min_value and max_value in IterativeImputer works,
scikit-learn/sklearn/impute/tests/test_impute.py,1020,Test that None/inf and scalar/vector give the same imputation,
scikit-learn/sklearn/impute/tests/test_impute.py,1045,check the imputing strategy when missing data are present in the,
scikit-learn/sklearn/impute/tests/test_impute.py,1046,testing set only.,
scikit-learn/sklearn/impute/tests/test_impute.py,1047,taken from: https://github.com/scikit-learn/scikit-learn/issues/14383,
scikit-learn/sklearn/impute/tests/test_impute.py,1065,impute with the initial strategy: 'mean',
scikit-learn/sklearn/impute/tests/test_impute.py,1122,convert the input to the right array format and right dtype,
scikit-learn/sklearn/impute/tests/test_impute.py,1163,test for sparse input and missing_value == 0,
scikit-learn/sklearn/impute/tests/test_impute.py,1171,convert the input to the right array format,
scikit-learn/sklearn/impute/tests/test_impute.py,1196,check the format of the output with different sparse parameter,
scikit-learn/sklearn/impute/tests/test_impute.py,1268,regression test for issue #11390. Comparison between incoherent dtype,
scikit-learn/sklearn/impute/tests/test_impute.py,1269,for X and missing_values was not raising a proper error.,
scikit-learn/sklearn/impute/tests/test_impute.py,1281,check that all features are dropped if there are no missing values when,
scikit-learn/sklearn/impute/tests/test_impute.py,1282,features='missing-only' (#13491),
scikit-learn/sklearn/impute/tests/test_impute.py,1293,Check that non missing values don't become explicit zeros in the mask,
scikit-learn/sklearn/impute/tests/test_impute.py,1294,generated by missing indicator when X is sparse. (#13491),
scikit-learn/sklearn/impute/tests/test_impute.py,1353,regression test for #15393,
scikit-learn/sklearn/impute/tests/test_common.py,10,noqa,
scikit-learn/sklearn/impute/tests/test_common.py,21,ConvergenceWarning will be raised by the IterativeImputer,
scikit-learn/sklearn/impute/tests/test_common.py,25,[Non Regression Test for issue #13968] Missing value in test set should,
scikit-learn/sklearn/impute/tests/test_common.py,26,not throw an error and return a finite dataset,
scikit-learn/sklearn/impute/tests/test_common.py,33,ConvergenceWarning will be raised by the IterativeImputer,
scikit-learn/sklearn/impute/tests/test_common.py,61,ConvergenceWarning will be raised by the IterativeImputer,
scikit-learn/sklearn/impute/tests/test_knn.py,15,Verify the shapes of the imputed matrix for different weights and,
scikit-learn/sklearn/impute/tests/test_knn.py,16,number of neighbors.,
scikit-learn/sklearn/impute/tests/test_knn.py,29,Test imputation with default values and invalid input,
scikit-learn/sklearn/impute/tests/test_knn.py,31,Test with inf present,
scikit-learn/sklearn/impute/tests/test_knn.py,43,Test with inf present in matrix passed in transform(),
scikit-learn/sklearn/impute/tests/test_knn.py,65,negative n_neighbors,
scikit-learn/sklearn/impute/tests/test_knn.py,69,Test with missing_values=0 when NaN present,
scikit-learn/sklearn/impute/tests/test_knn.py,87,Test with a metric type without NaN support,
scikit-learn/sklearn/impute/tests/test_knn.py,115,Test with an imputable matrix and compare with different missing_values,
scikit-learn/sklearn/impute/tests/test_knn.py,150,Test with an imputable matrix,
scikit-learn/sklearn/impute/tests/test_knn.py,174,Test when there is not enough neighbors,
scikit-learn/sklearn/impute/tests/test_knn.py,186,"Not enough neighbors, use column mean from training",
scikit-learn/sklearn/impute/tests/test_knn.py,202,Test when data in fit() and transform() are different,
scikit-learn/sklearn/impute/tests/test_knn.py,303,"Test with ""uniform"" weight (or unweighted)",
scikit-learn/sklearn/impute/tests/test_knn.py,317,"Test with ""callable"" weight",
scikit-learn/sklearn/impute/tests/test_knn.py,324,"Test with ""callable"" uniform weight",
scikit-learn/sklearn/impute/tests/test_knn.py,344,"Test with ""distance"" weight",
scikit-learn/sklearn/impute/tests/test_knn.py,350,Manual calculation,
scikit-learn/sklearn/impute/tests/test_knn.py,366,NearestNeighbor calculation,
scikit-learn/sklearn/impute/tests/test_knn.py,381,"Test with weights = ""distance"" and n_neighbors=2",
scikit-learn/sklearn/impute/tests/test_knn.py,389,"neighbors are rows 1, 2, the nan_euclidean_distances are:",
scikit-learn/sklearn/impute/tests/test_knn.py,404,Test with varying missingness patterns,
scikit-learn/sklearn/impute/tests/test_knn.py,415,Get weights of donor neighbors,
scikit-learn/sklearn/impute/tests/test_knn.py,425,Collect donor values,
scikit-learn/sklearn/impute/tests/test_knn.py,429,Final imputed values,
scikit-learn/sklearn/impute/tests/test_knn.py,461,Calculate weights,
scikit-learn/sklearn/impute/tests/test_knn.py,467,Calculate weighted averages,
scikit-learn/sklearn/impute/tests/test_knn.py,490,Define callable metric that returns the l1 norm:,
scikit-learn/sklearn/impute/tests/test_knn.py,519,"Note that we use working_memory=0 to ensure that chunking is tested, even",
scikit-learn/sklearn/impute/tests/test_knn.py,520,"for a small dataset. However, it should raise a UserWarning that we ignore.",
scikit-learn/sklearn/impute/tests/test_knn.py,560,Samples with needed feature has nan distance,
scikit-learn/sklearn/feature_extraction/image.py,6,Authors: Emmanuelle Gouillart <emmanuelle.gouillart@normalesup.org>,
scikit-learn/sklearn/feature_extraction/image.py,7,Gael Varoquaux <gael.varoquaux@normalesup.org>,
scikit-learn/sklearn/feature_extraction/image.py,8,Olivier Grisel,
scikit-learn/sklearn/feature_extraction/image.py,9,Vlad Niculae,
scikit-learn/sklearn/feature_extraction/image.py,10,License: BSD 3 clause,
scikit-learn/sklearn/feature_extraction/image.py,27,,
scikit-learn/sklearn/feature_extraction/image.py,28,From an image to a graph,
scikit-learn/sklearn/feature_extraction/image.py,64,XXX: Why mask the image after computing the weights?,
scikit-learn/sklearn/feature_extraction/image.py,204,,
scikit-learn/sklearn/feature_extraction/image.py,205,From an image to a set of small image patches,
scikit-learn/sklearn/feature_extraction/image.py,434,remove the color dimension if useless,
scikit-learn/sklearn/feature_extraction/image.py,470,compute the dimensions of the patches array,
scikit-learn/sklearn/feature_extraction/image.py,478,divide by the amount of overlap,
scikit-learn/sklearn/feature_extraction/image.py,479,"XXX: is this the most efficient way? memory-wise yes, cpu wise?",
scikit-learn/sklearn/feature_extraction/image.py,569,compute the dimensions of the patches array,
scikit-learn/sklearn/feature_extraction/image.py,576,extract the patches,
scikit-learn/sklearn/feature_extraction/_stop_words.py,1,"This list of English stop words is taken from the ""Glasgow Information",
scikit-learn/sklearn/feature_extraction/_stop_words.py,2,"Retrieval Group"". The original list can be found at",
scikit-learn/sklearn/feature_extraction/_stop_words.py,3,http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words,
scikit-learn/sklearn/feature_extraction/_hash.py,1,Author: Lars Buitinck,
scikit-learn/sklearn/feature_extraction/_hash.py,2,License: BSD 3 clause,
scikit-learn/sklearn/feature_extraction/_hash.py,99,"strangely, np.int16 instances are not instances of Integral,",
scikit-learn/sklearn/feature_extraction/_hash.py,100,while np.int64 instances are...,
scikit-learn/sklearn/feature_extraction/_hash.py,126,repeat input validation for grid search (which calls set_params),
scikit-learn/sklearn/feature_extraction/_hash.py,163,also sorts the indices,
scikit-learn/sklearn/feature_extraction/_dict_vectorizer.py,1,Authors: Lars Buitinck,
scikit-learn/sklearn/feature_extraction/_dict_vectorizer.py,2,Dan Blanchard <dblanchard@ets.org>,
scikit-learn/sklearn/feature_extraction/_dict_vectorizer.py,3,License: BSD 3 clause,
scikit-learn/sklearn/feature_extraction/_dict_vectorizer.py,18,single sample,
scikit-learn/sklearn/feature_extraction/_dict_vectorizer.py,135,Sanity check: Python's array has no way of explicitly requesting the,
scikit-learn/sklearn/feature_extraction/_dict_vectorizer.py,136,"signed 32-bit integers that scipy.sparse needs, so we use the next",
scikit-learn/sklearn/feature_extraction/_dict_vectorizer.py,137,"best thing: typecode ""i"" (int). However, if that gives larger or",
scikit-learn/sklearn/feature_extraction/_dict_vectorizer.py,138,"smaller integers than 32-bit ones, np.frombuffer screws up.",
scikit-learn/sklearn/feature_extraction/_dict_vectorizer.py,152,Process everything as sparse regardless of setting,
scikit-learn/sklearn/feature_extraction/_dict_vectorizer.py,157,"XXX we could change values to an array.array as well, but it",
scikit-learn/sklearn/feature_extraction/_dict_vectorizer.py,158,would require (heuristic) conversion of dtype to typecode...,
scikit-learn/sklearn/feature_extraction/_dict_vectorizer.py,161,collect all the possible feature names and build sparse matrix at,
scikit-learn/sklearn/feature_extraction/_dict_vectorizer.py,162,same time,
scikit-learn/sklearn/feature_extraction/_dict_vectorizer.py,189,Sort everything if asked,
scikit-learn/sklearn/feature_extraction/_dict_vectorizer.py,252,COO matrix is not subscriptable,
scikit-learn/sklearn/feature_extraction/text.py,1,-*- coding: utf-8 -*-,
scikit-learn/sklearn/feature_extraction/text.py,2,Authors: Olivier Grisel <olivier.grisel@ensta.org>,
scikit-learn/sklearn/feature_extraction/text.py,3,Mathieu Blondel <mathieu@mblondel.org>,
scikit-learn/sklearn/feature_extraction/text.py,4,Lars Buitinck,
scikit-learn/sklearn/feature_extraction/text.py,5,Robert Layton <robertlayton@gmail.com>,
scikit-learn/sklearn/feature_extraction/text.py,6,Jochen Wersdörfer <jochen@wersdoerfer.de>,
scikit-learn/sklearn/feature_extraction/text.py,7,Roman Sinayev <roman.sinayev@gmail.com>,
scikit-learn/sklearn/feature_extraction/text.py,8,,
scikit-learn/sklearn/feature_extraction/text.py,9,License: BSD 3 clause,
scikit-learn/sklearn/feature_extraction/text.py,133,"If `s` is ASCII-compatible, then it does not contain any accented",
scikit-learn/sklearn/feature_extraction/text.py,134,characters and we can avoid an expensive list comprehension,
scikit-learn/sklearn/feature_extraction/text.py,183,assume it's a collection,
scikit-learn/sklearn/feature_extraction/text.py,225,handle stop words,
scikit-learn/sklearn/feature_extraction/text.py,229,handle token n-grams,
scikit-learn/sklearn/feature_extraction/text.py,234,no need to do any slicing for unigrams,
scikit-learn/sklearn/feature_extraction/text.py,235,just iterate through the original tokens,
scikit-learn/sklearn/feature_extraction/text.py,243,bind method outside of loop to reduce overhead,
scikit-learn/sklearn/feature_extraction/text.py,256,normalize white spaces,
scikit-learn/sklearn/feature_extraction/text.py,262,no need to do any slicing for unigrams,
scikit-learn/sklearn/feature_extraction/text.py,263,iterate through the string,
scikit-learn/sklearn/feature_extraction/text.py,269,bind method outside of loop to reduce overhead,
scikit-learn/sklearn/feature_extraction/text.py,283,normalize white spaces,
scikit-learn/sklearn/feature_extraction/text.py,289,bind method outside of loop to reduce overhead,
scikit-learn/sklearn/feature_extraction/text.py,301,count a short word (w_len < n) only once,
scikit-learn/sklearn/feature_extraction/text.py,316,accent stripping,
scikit-learn/sklearn/feature_extraction/text.py,368,Stop words are were previously validated,
scikit-learn/sklearn/feature_extraction/text.py,371,"NB: stop_words is validated, unlike self.stop_words",
scikit-learn/sklearn/feature_extraction/text.py,388,Failed to check stop words consistency (e.g. because a custom,
scikit-learn/sklearn/feature_extraction/text.py,389,preprocessor or tokenizer was used),
scikit-learn/sklearn/feature_extraction/text.py,726,triggers a parameter validation,
scikit-learn/sklearn/feature_extraction/text.py,1063,Calculate a mask based on document frequencies,
scikit-learn/sklearn/feature_extraction/text.py,1077,maps old indices to new,
scikit-learn/sklearn/feature_extraction/text.py,1097,Add a new value when a new vocabulary item is seen,
scikit-learn/sklearn/feature_extraction/text.py,1117,Ignore out-of-vocabulary items for fixed_vocab=True,
scikit-learn/sklearn/feature_extraction/text.py,1125,disable defaultdict behaviour,
scikit-learn/sklearn/feature_extraction/text.py,1131,= 2**31 - 1,
scikit-learn/sklearn/feature_extraction/text.py,1183,We intentionally don't call the transform method to make,
scikit-learn/sklearn/feature_extraction/text.py,1184,fit_transform overridable without unwanted side effects in,
scikit-learn/sklearn/feature_extraction/text.py,1185,TfidfVectorizer.,
scikit-learn/sklearn/feature_extraction/text.py,1247,use the same matrix-building strategy as fit_transform,
scikit-learn/sklearn/feature_extraction/text.py,1269,We need CSR format for fast row manipulations.,
scikit-learn/sklearn/feature_extraction/text.py,1272,"We need to convert X to a matrix, so that the indexing",
scikit-learn/sklearn/feature_extraction/text.py,1273,returns 2D objects,
scikit-learn/sklearn/feature_extraction/text.py,1438,perform idf smoothing if required,
scikit-learn/sklearn/feature_extraction/text.py,1442,log+1 instead of log makes sure terms with zero idf don't get,
scikit-learn/sklearn/feature_extraction/text.py,1443,suppressed entirely.,
scikit-learn/sklearn/feature_extraction/text.py,1479,"idf_ being a property, the automatic attributes detection",
scikit-learn/sklearn/feature_extraction/text.py,1480,does not work as usual and we need to specify the attribute,
scikit-learn/sklearn/feature_extraction/text.py,1481,name:,
scikit-learn/sklearn/feature_extraction/text.py,1490,*= doesn't work,
scikit-learn/sklearn/feature_extraction/text.py,1500,"if _idf_diag is not set, this will raise an attribute error,",
scikit-learn/sklearn/feature_extraction/text.py,1501,"which means hasattr(self, ""idf_"") is False",
scikit-learn/sklearn/feature_extraction/text.py,1741,Broadcast the TF-IDF parameters to the underlying transformer instance,
scikit-learn/sklearn/feature_extraction/text.py,1742,for easy grid search and repr,
scikit-learn/sklearn/feature_extraction/text.py,1839,X is already a transformed view of raw_documents so,
scikit-learn/sklearn/feature_extraction/text.py,1840,we set copy to False,
scikit-learn/sklearn/feature_extraction/text.py,1870,FIXME Remove copy parameter support in 0.24,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,1,-*- coding: utf-8 -*-,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,80,check some classical latin accentuated symbols,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,89,check some arabic,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,90,alef with a hamza below: إ,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,91,simple alef: ا,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,94,mix letters accentuated and not,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,99,strings that are already decomposed,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,100,o with diaresis,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,104,combining marks by themselves,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,109,Multiple combining marks on one character,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,116,check some classical latin accentuated symbols,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,125,check some arabic,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,126,halef with a hamza below,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,127,halef has no direct ascii match,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,130,mix letters accentuated and not,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,156,with custom preprocessor,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,164,with custom tokenizer,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,187,"decode_error default to strict, so this should fail",
scikit-learn/sklearn/feature_extraction/tests/test_text.py,188,"First, encode (as bytes) a unicode string.",
scikit-learn/sklearn/feature_extraction/tests/test_text.py,192,"Then let the Analyzer try to decode it as ascii. It should fail,",
scikit-learn/sklearn/feature_extraction/tests/test_text.py,193,because we have given it an incorrect encoding.,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,269,Try a few of the supported types.,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,334,fit on stopwords only,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,353,check normalization,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,356,this is robust to features with only zeros,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,373,check normalization,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,376,the lack of smoothing make IDF fragile in the presence of feature with,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,377,only zeros,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,400,raw documents as an iterator,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,405,test without vocabulary,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,412,build a vectorizer v1 with the same vocabulary as the one fitted by v1,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,415,compare that the two vectorizer give the same output on the test sample,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,426,stop word from the fixed list,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,429,stop word found automatically by the vectorizer DF thresholding,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,430,words that are high frequent across the complete corpus are likely,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,431,to be not informative (either real stop words of extraction,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,432,artifacts),
scikit-learn/sklearn/feature_extraction/tests/test_text.py,435,not present in the sample,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,441,test tf-idf,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,447,test tf-idf with new data,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,451,test tf alone,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,456,test idf transform with unlearned idf vector,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,461,test idf transform with incompatible n_features,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,470,L1-normalized term frequencies sum to one,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,473,test the direct tfidf vectorizer,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,474,(equivalent to term count vectorizer + tfidf transformer),
scikit-learn/sklearn/feature_extraction/tests/test_text.py,483,test the direct tfidf vectorizer with new data,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,487,test transform on unfitted vectorizer with empty vocabulary,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,492,ascii preprocessor?,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,501,error on bad strip_accents param,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,506,error with bad analyzer type,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,525,FIXME Remove copy parameter support in 0.24,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,545,By default the hashed values receive a random sign and l2 normalization,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,546,makes the feature values bounded,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,552,Check that the rows are normalized,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,556,Check vectorization with some non-default parameters,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,562,ngrams generate more non zeros,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,567,makes the feature values bounded,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,571,Check that the rows are normalized,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,579,test for Value error on unfitted/empty vocabulary,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,584,test for vocabulary learned from data,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,598,test for custom vocabulary,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,618,test bounded number of extracted features,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,626,Regression test: max_features didn't work correctly in 0.14.,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,640,"The most common feature is ""the"", with frequency 7.",
scikit-learn/sklearn/feature_extraction/tests/test_text.py,645,The most common feature should be the same,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,659,0.5 * 3 documents -> max_doc_count == 1.5,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,661,{ae} ignored,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,662,{bcdt} remain,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,668,{ae} ignored,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,669,{bcdt} remain,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,684,{bcdt} ignored,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,685,{ae} remain,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,689,0.8 * 3 documents -> min_doc_count == 2.4,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,691,{bcdet} ignored,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,692,{a} remains,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,698,by default multiple occurrences are counted as longs,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,706,"using boolean features, we can fetch the binary occurrence info",
scikit-learn/sklearn/feature_extraction/tests/test_text.py,707,instead.,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,713,check the ability to change the dtype,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,722,by default multiple occurrences are counted as longs,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,730,"using boolean features, we can fetch the binary occurrence info",
scikit-learn/sklearn/feature_extraction/tests/test_text.py,731,instead.,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,738,check the ability to change the dtype,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,747,raw documents,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,758,Test that inverse_transform also works with numpy arrays,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,766,raw documents,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,769,"label junk food as -1, the others as +1",
scikit-learn/sklearn/feature_extraction/tests/test_text.py,772,split the dataset for model development and final evaluation,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,784,find the best parameters for both the feature extraction and the,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,785,classifier,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,788,Check that the best model found by grid search is 100% correct on the,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,789,held out evaluation set.,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,793,on this toy dataset bigram representation which is used in the last of,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,794,the grid_search is considered the best estimator since they all converge,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,795,to 100% accuracy models,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,802,raw documents,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,805,"label junk food as -1, the others as +1",
scikit-learn/sklearn/feature_extraction/tests/test_text.py,808,split the dataset for model development and final evaluation,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,821,find the best parameters for both the feature extraction and the,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,822,classifier,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,825,Check that the best model found by grid search is 100% correct on the,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,826,held out evaluation set.,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,830,on this toy dataset bigram representation which is used in the last of,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,831,the grid_search is considered the best estimator since they all converge,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,832,to 100% accuracy models,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,841,raw documents,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,844,"label junk food as -1, the others as +1",
scikit-learn/sklearn/feature_extraction/tests/test_text.py,856,tests that the count vectorizer works with cyrillic.,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,871,No collisions on such a small dataset,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,874,"When norm is None and not alternate_sign, the tokens are counted up to",
scikit-learn/sklearn/feature_extraction/tests/test_text.py,875,collisions,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,880,non regression smoke test for inheritance issues,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,938,ensure that vocabulary of type set is coerced to a list to,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,939,preserve iteration ordering after deserialization,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,969,Ensure that deleting the stop_words_ attribute doesn't affect transform,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,1040,np.nan can appear when using pandas to load text fields from a csv file,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,1041,with missing values.,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,1053,"Non-regression test: TfidfVectorizer used to ignore its ""binary"" param.",
scikit-learn/sklearn/feature_extraction/tests/test_text.py,1139,vectorizers could be initialized with invalid ngram range,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,1140,test for raising error message,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,1177,reset stop word validation,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,1181,Only one warning per stop list,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,1185,Test caching of inconsistency assessment,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,1204,force indices and indptr to int64.,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,1232,checks are cached,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,1292,check if a custom exception from the analyzer is shown to the user,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,1333,setting parameter and checking for corresponding warning messages,
scikit-learn/sklearn/feature_extraction/tests/test_text.py,1350,"For vectorizers, n_features_in_ does not make sense",
scikit-learn/sklearn/feature_extraction/tests/test_text.py,1357,TODO: Remove in 0.24,
scikit-learn/sklearn/feature_extraction/tests/test_feature_hasher.py,26,"mix byte and Unicode strings; note that ""foo"" is a duplicate in row 0",
scikit-learn/sklearn/feature_extraction/tests/test_feature_hasher.py,33,iterable,
scikit-learn/sklearn/feature_extraction/tests/test_feature_hasher.py,49,check the influence of the seed when computing the hashes,
scikit-learn/sklearn/feature_extraction/tests/test_feature_hasher.py,50,import is here to avoid importing on pypy,
scikit-learn/sklearn/feature_extraction/tests/test_feature_hasher.py,134,Test delayed input validation in fit (useful for grid search).,
scikit-learn/sklearn/feature_extraction/tests/test_feature_hasher.py,142,Assert that no zeros are materialized in the output.,
scikit-learn/sklearn/feature_extraction/tests/test_feature_hasher.py,165,check that some of the hashed tokens are added,
scikit-learn/sklearn/feature_extraction/tests/test_feature_hasher.py,166,with an opposite sign and cancel out,
scikit-learn/sklearn/feature_extraction/tests/test_image.py,1,Authors: Emmanuelle Gouillart <emmanuelle.gouillart@normalesup.org>,
scikit-learn/sklearn/feature_extraction/tests/test_image.py,2,Gael Varoquaux <gael.varoquaux@normalesup.org>,
scikit-learn/sklearn/feature_extraction/tests/test_image.py,3,License: BSD 3 clause,
scikit-learn/sklearn/feature_extraction/tests/test_image.py,23,Negative elements are the diagonal: the elements of the original,
scikit-learn/sklearn/feature_extraction/tests/test_image.py,24,"image. Positive elements are the values of the gradient, they",
scikit-learn/sklearn/feature_extraction/tests/test_image.py,25,should all be equal on grad_x and grad_y,
scikit-learn/sklearn/feature_extraction/tests/test_image.py,31,Checking that the function works with graphs containing no edges,
scikit-learn/sklearn/feature_extraction/tests/test_image.py,34,Generating two convex parts with one vertex,
scikit-learn/sklearn/feature_extraction/tests/test_image.py,35,"Thus, edges will be empty in _to_graph",
scikit-learn/sklearn/feature_extraction/tests/test_image.py,43,Checking that the function works whatever the type of mask is,
scikit-learn/sklearn/feature_extraction/tests/test_image.py,48,Checking dtype of the graph,
scikit-learn/sklearn/feature_extraction/tests/test_image.py,59,scipy deprecation inside face,
scikit-learn/sklearn/feature_extraction/tests/test_image.py,64,Newer versions of scipy have face in misc,
scikit-learn/sklearn/feature_extraction/tests/test_image.py,67,subsample by 4 to reduce run time,
scikit-learn/sklearn/feature_extraction/tests/test_image.py,75,scipy deprecation inside face,
scikit-learn/sklearn/feature_extraction/tests/test_image.py,80,Newer versions of scipy have face in misc,
scikit-learn/sklearn/feature_extraction/tests/test_image.py,84,subsample by 4 to reduce run time,
scikit-learn/sklearn/feature_extraction/tests/test_image.py,100,Newer versions of scipy have face in misc,
scikit-learn/sklearn/feature_extraction/tests/test_image.py,124,make a collection of faces,
scikit-learn/sklearn/feature_extraction/tests/test_image.py,185,Request patches of the same size as image,
scikit-learn/sklearn/feature_extraction/tests/test_image.py,186,Should return just the single patch a.k.a. the image,
scikit-learn/sklearn/feature_extraction/tests/test_image.py,195,this is 3185,
scikit-learn/sklearn/feature_extraction/tests/test_image.py,320,test same patch size for all dimensions,
scikit-learn/sklearn/feature_extraction/tests/test_image.py,331,width and height of the patch should be less than the image,
scikit-learn/sklearn/feature_extraction/tests/test_image.py,339,TODO: Remove in 0.24,
scikit-learn/sklearn/feature_extraction/tests/test_dict_vectorizer.py,1,Authors: Lars Buitinck,
scikit-learn/sklearn/feature_extraction/tests/test_dict_vectorizer.py,2,Dan Blanchard <dblanchard@ets.org>,
scikit-learn/sklearn/feature_extraction/tests/test_dict_vectorizer.py,3,License: BSD 3 clause,
scikit-learn/sklearn/feature_extraction/tests/test_dict_vectorizer.py,34,CSR matrices can't be compared for equality,
scikit-learn/sklearn/feature_extraction/tests/test_dict_vectorizer.py,47,make two feature dicts with two useful features and a bunch of useless,
scikit-learn/sklearn/feature_extraction/tests/test_dict_vectorizer.py,48,"ones, in terms of chi2",
scikit-learn/sklearn/feature_extraction/tests/test_dict_vectorizer.py,101,Generate equal dictionaries with different memory layouts,
scikit-learn/sklearn/feature_extraction/tests/test_dict_vectorizer.py,108,check that the memory layout does not impact the resulting vocabulary,
scikit-learn/sklearn/feature_extraction/tests/test_dict_vectorizer.py,116,"For vectorizers, n_features_in_ does not make sense and does not exist.",
scikit-learn/sklearn/compose/_column_transformer.py,6,Author: Andreas Mueller,
scikit-learn/sklearn/compose/_column_transformer.py,7,Joris Van den Bossche,
scikit-learn/sklearn/compose/_column_transformer.py,8,License: BSD,
scikit-learn/sklearn/compose/_column_transformer.py,246,interleave the validated column specifiers,
scikit-learn/sklearn/compose/_column_transformer.py,251,add transformer tuple for remainder,
scikit-learn/sklearn/compose/_column_transformer.py,258,replace 'passthrough' with identity transformer and,
scikit-learn/sklearn/compose/_column_transformer.py,259,skip in case of 'drop',
scikit-learn/sklearn/compose/_column_transformer.py,277,validate names,
scikit-learn/sklearn/compose/_column_transformer.py,280,validate estimators,
scikit-learn/sklearn/compose/_column_transformer.py,317,Make it possible to check for reordered named columns on transform,
scikit-learn/sklearn/compose/_column_transformer.py,341,Use Bunch object to improve autocomplete,
scikit-learn/sklearn/compose/_column_transformer.py,372,transformers are fitted; excludes 'drop' cases,
scikit-learn/sklearn/compose/_column_transformer.py,380,"FunctionTransformer is present in list of transformers,",
scikit-learn/sklearn/compose/_column_transformer.py,381,"so get next transformer, but save original string",
scikit-learn/sklearn/compose/_column_transformer.py,390,sanity check that transformers is exhausted,
scikit-learn/sklearn/compose/_column_transformer.py,485,we use fit_transform to make sure to set sparse_output_ (for which we,
scikit-learn/sklearn/compose/_column_transformer.py,486,need the transformed data) to have consistent output type in predict,
scikit-learn/sklearn/compose/_column_transformer.py,512,TODO: this should be `feature_names_in_` when we start having it,
scikit-learn/sklearn/compose/_column_transformer.py,518,set n_features_in_ attribute,
scikit-learn/sklearn/compose/_column_transformer.py,528,All transformers are None,
scikit-learn/sklearn/compose/_column_transformer.py,533,determine if concatenated output will be sparse or not,
scikit-learn/sklearn/compose/_column_transformer.py,580,No column reordering allowed for named cols combined with remainder,
scikit-learn/sklearn/compose/_column_transformer.py,581,"TODO: remove this mechanism in 0.24, once we enforce strict column",
scikit-learn/sklearn/compose/_column_transformer.py,582,name order and count. See #14237 for details.,
scikit-learn/sklearn/compose/_column_transformer.py,594,TODO: also call _check_n_features(reset=False) in 0.24,
scikit-learn/sklearn/compose/_column_transformer.py,600,All transformers are None,
scikit-learn/sklearn/compose/_column_transformer.py,617,since all columns should be numeric before stacking them,
scikit-learn/sklearn/compose/_column_transformer.py,618,"in a sparse matrix, `check_array` is used for the",
scikit-learn/sklearn/compose/_column_transformer.py,619,dtype conversion if necessary.,
scikit-learn/sklearn/compose/_column_transformer.py,752,transformer_weights keyword is not passed through because the user,
scikit-learn/sklearn/compose/_column_transformer.py,753,would need to know the automatically generated names of the transformers,
scikit-learn/sklearn/compose/_column_transformer.py,769,TODO: remove in v0.24,
scikit-learn/sklearn/compose/_target.py,1,Authors: Andreas Mueller <andreas.mueller@columbia.edu>,
scikit-learn/sklearn/compose/_target.py,2,Guillaume Lemaitre <guillaume.lemaitre@inria.fr>,
scikit-learn/sklearn/compose/_target.py,3,License: BSD 3 clause,
scikit-learn/sklearn/compose/_target.py,139,XXX: sample_weight is not currently passed to the,
scikit-learn/sklearn/compose/_target.py,140,"transformer. However, if transformer starts using sample_weight, the",
scikit-learn/sklearn/compose/_target.py,141,code should be modified accordingly. At the time to consider the,
scikit-learn/sklearn/compose/_target.py,142,"sample_prop feature, it is also a good use case to be considered.",
scikit-learn/sklearn/compose/_target.py,179,store the number of dimension of the target to predict an array of,
scikit-learn/sklearn/compose/_target.py,180,similar shape at predict,
scikit-learn/sklearn/compose/_target.py,183,"transformers are designed to modify X which is 2d dimensional, we",
scikit-learn/sklearn/compose/_target.py,184,need to modify y accordingly.,
scikit-learn/sklearn/compose/_target.py,191,transform y and convert back to 1d array if needed,
scikit-learn/sklearn/compose/_target.py,193,FIXME: a FunctionTransformer can return a 1D array even when validate,
scikit-learn/sklearn/compose/_target.py,194,"is set to True. Therefore, we need to check the number of dimension",
scikit-learn/sklearn/compose/_target.py,195,first.,
scikit-learn/sklearn/compose/_target.py,244,For consistency with other estimators we raise a AttributeError so,
scikit-learn/sklearn/compose/_target.py,245,that hasattr() returns False the estimator isn't fitted.,
scikit-learn/sklearn/compose/tests/test_target.py,30,provide a transformer and functions at the same time,
scikit-learn/sklearn/compose/tests/test_target.py,38,fit with sample_weight with a regressor which does not support it,
scikit-learn/sklearn/compose/tests/test_target.py,45,func is given but inverse_func is not,
scikit-learn/sklearn/compose/tests/test_target.py,81,check the transformer output,
scikit-learn/sklearn/compose/tests/test_target.py,88,check the regressor output,
scikit-learn/sklearn/compose/tests/test_target.py,99,check the transformer output,
scikit-learn/sklearn/compose/tests/test_target.py,105,check the regressor output,
scikit-learn/sklearn/compose/tests/test_target.py,115,All transformer in scikit-learn expect 2D data. FunctionTransformer with,
scikit-learn/sklearn/compose/tests/test_target.py,116,validate=False lift this constraint without checking that the input is a,
scikit-learn/sklearn/compose/tests/test_target.py,117,2D vector. We check the consistency of the data shape using a 1D and 2D y,
scikit-learn/sklearn/compose/tests/test_target.py,118,array.,
scikit-learn/sklearn/compose/tests/test_target.py,125,consistency forward transform,
scikit-learn/sklearn/compose/tests/test_target.py,129,consistency inverse transform,
scikit-learn/sklearn/compose/tests/test_target.py,132,consistency of the regressor,
scikit-learn/sklearn/compose/tests/test_target.py,146,Check consistency with transformer accepting only 2D array and a 1D/2D y,
scikit-learn/sklearn/compose/tests/test_target.py,147,array.,
scikit-learn/sklearn/compose/tests/test_target.py,153,consistency forward transform,
scikit-learn/sklearn/compose/tests/test_target.py,154,create a 2D array and squeeze results,
scikit-learn/sklearn/compose/tests/test_target.py,160,consistency inverse transform,
scikit-learn/sklearn/compose/tests/test_target.py,163,consistency of the regressor,
scikit-learn/sklearn/compose/tests/test_target.py,166,create a 2D array and squeeze results,
scikit-learn/sklearn/compose/tests/test_target.py,176,Check consistency with transformer accepting only 2D array and a 2D y,
scikit-learn/sklearn/compose/tests/test_target.py,177,array.,
scikit-learn/sklearn/compose/tests/test_target.py,185,consistency forward transform,
scikit-learn/sklearn/compose/tests/test_target.py,189,consistency inverse transform,
scikit-learn/sklearn/compose/tests/test_target.py,192,consistency of the regressor,
scikit-learn/sklearn/compose/tests/test_target.py,218,force that the function only return a 1D array,
scikit-learn/sklearn/compose/tests/test_target.py,258,check that the target ``y`` passed to the transformer will always be a,
scikit-learn/sklearn/compose/tests/test_target.py,259,"numpy array. Similarly, if ``X`` is passed as a list, we check that the",
scikit-learn/sklearn/compose/tests/test_target.py,260,predictor receive as it is.,
scikit-learn/sklearn/compose/tests/test_target.py,291,regression test for gh-issue #11618,
scikit-learn/sklearn/compose/tests/test_target.py,292,check that we only call a single time fit for the transformer,
scikit-learn/sklearn/compose/tests/test_target.py,303,"on the test below we force this to false, we make sure this is",
scikit-learn/sklearn/compose/tests/test_target.py,304,actually passed to the regressor,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,33,1D Series -> 2D DataFrame,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,36,1D array -> 2D array,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,85,single column 1D / 2D,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,88,list-like,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,91,slice,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,94,boolean mask,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,104,callable that returns any of the allowed specifiers,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,116,test with transformer_weights,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,144,String keys: label based,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,146,scalar,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,148,list,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,151,slice,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,154,int keys: positional,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,156,scalar,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,158,list,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,162,slice,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,166,boolean mask,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,177,callable that returns any of the allowed specifiers,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,197,test with transformer_weights,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,209,test multiple columns,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,224,ensure pandas object is passes through,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,243,integer column spec + integer column names -> still use positional,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,260,test case that ensures that the column transformer does also work when,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,261,a given transformer doesn't have any columns to work on,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,289,including remainder,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,297,including remainder,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,304,no distinction between 1D and 2D,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,381,"this shouldn't fail, since boolean can be coerced into a numeric",
scikit-learn/sklearn/compose/tests/test_column_transformer.py,382,See: https://github.com/scikit-learn/scikit-learn/issues/11912,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,395,this fails since strings `a` and `b` cannot be,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,396,coerced into a numeric.,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,402,above data has sparsity of 4 / 8 = 0.5,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,404,apply threshold even if all sparse,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,412,mixed -> sparsity of (4 + 2) / 8 = 0.75,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,431,if nothing is sparse -> no sparse,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,459,"if one transformer is dropped, test that name is still correct",
scikit-learn/sklearn/compose/tests/test_column_transformer.py,464,"because fit is also doing transform, this raises already on fit",
scikit-learn/sklearn/compose/tests/test_column_transformer.py,475,"if one transformer is dropped, test that name is still correct",
scikit-learn/sklearn/compose/tests/test_column_transformer.py,479,"because fit is also doing transform, this raises already on fit",
scikit-learn/sklearn/compose/tests/test_column_transformer.py,488,general invalid,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,494,invalid for arrays,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,500,transformed n_features does not match fitted n_features,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,508,"Should accept added columns, for now",
scikit-learn/sklearn/compose/tests/test_column_transformer.py,563,invalid keyword parameters should raise an error message,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,633,check it are fitted transformers,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,654,raise correct error when not fitted,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,657,raise correct error when no feature names are available,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,663,working example,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,671,passthrough transformers not supported,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,685,drop transformer,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,694,one 'drop' -> ignore,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,704,all 'drop' -> return shape 0 array,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,712,'passthrough',
scikit-learn/sklearn/compose/tests/test_column_transformer.py,722,None itself / other string is not valid,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,739,default drop,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,748,specify passthrough,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,757,column order is not preserved (passed through added to end),
scikit-learn/sklearn/compose/tests/test_column_transformer.py,767,passthrough when all actual transformers are skipped,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,777,error on invalid arg,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,788,check default for make_column_transformer,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,796,test different ways that columns are specified with passthrough,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,815,test different ways that columns are specified with passthrough,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,842,second and third columns are doubled when remainder = DoubleTrans,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,875,columns are doubled when remainder = DoubleTrans,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,900,SparseMatrixTrans creates 3 features for each column. There is,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,901,"one column in ``transformers``, thus:",
scikit-learn/sklearn/compose/tests/test_column_transformer.py,924,"SparseMatrixTrans creates 3 features for each column, thus:",
scikit-learn/sklearn/compose/tests/test_column_transformer.py,1043,assert that function gets the full array,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,1060,assert that function gets the full dataframe,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,1119,No error for added columns if ordering is identical,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,1123,"No error should be raised, for now",
scikit-learn/sklearn/compose/tests/test_column_transformer.py,1125,No 'columns' AttributeError when transform input is a numpy array,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,1184,Regression test for #14510,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,1185,Boolean array-like does not behave as boolean array with NumPy < 1.12,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,1186,and sparse matrices as well,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,1197,make sure n_features_in is what is passed as input to the column,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,1198,transformer.,
scikit-learn/sklearn/compose/tests/test_column_transformer.py,1240,Functional test for column transformer + column selector,
scikit-learn/sklearn/gaussian_process/_gpc.py,3,Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,
scikit-learn/sklearn/gaussian_process/_gpc.py,4,,
scikit-learn/sklearn/gaussian_process/_gpc.py,5,License: BSD 3 clause,
scikit-learn/sklearn/gaussian_process/_gpc.py,24,Values required for approximating the logistic sigmoid by,
scikit-learn/sklearn/gaussian_process/_gpc.py,25,error functions. coefs are obtained via:,
scikit-learn/sklearn/gaussian_process/_gpc.py,26,"x = np.array([0, 0.6, 2, 3.5, 4.5, np.inf])",
scikit-learn/sklearn/gaussian_process/_gpc.py,27,b = logistic(x),
scikit-learn/sklearn/gaussian_process/_gpc.py,28,"A = (erf(np.dot(x, self.lambdas)) + 1) / 2",
scikit-learn/sklearn/gaussian_process/_gpc.py,29,"coefs = lstsq(A, b)[0]",
scikit-learn/sklearn/gaussian_process/_gpc.py,173,Use an RBF kernel as default,
scikit-learn/sklearn/gaussian_process/_gpc.py,183,Encode class labels and check that it is a binary classification,
scikit-learn/sklearn/gaussian_process/_gpc.py,184,problem,
scikit-learn/sklearn/gaussian_process/_gpc.py,198,Choose hyperparameters based on maximizing the log-marginal,
scikit-learn/sklearn/gaussian_process/_gpc.py,199,likelihood (potentially starting from several initial values),
scikit-learn/sklearn/gaussian_process/_gpc.py,209,First optimize starting from theta specified in kernel,
scikit-learn/sklearn/gaussian_process/_gpc.py,214,Additional runs are performed from log-uniform chosen initial,
scikit-learn/sklearn/gaussian_process/_gpc.py,215,theta,
scikit-learn/sklearn/gaussian_process/_gpc.py,228,Select result from run with minimal (negative) log-marginal,
scikit-learn/sklearn/gaussian_process/_gpc.py,229,likelihood,
scikit-learn/sklearn/gaussian_process/_gpc.py,237,Precompute quantities required for predictions which are independent,
scikit-learn/sklearn/gaussian_process/_gpc.py,238,of actual query points,
scikit-learn/sklearn/gaussian_process/_gpc.py,261,"As discussed on Section 3.4.2 of GPML, for making hard binary",
scikit-learn/sklearn/gaussian_process/_gpc.py,262,"decisions, it is enough to compute the MAP of the posterior and",
scikit-learn/sklearn/gaussian_process/_gpc.py,263,pass it through the link function,
scikit-learn/sklearn/gaussian_process/_gpc.py,264,K_star =k(x_star),
scikit-learn/sklearn/gaussian_process/_gpc.py,265,"Algorithm 3.2,Line 4",
scikit-learn/sklearn/gaussian_process/_gpc.py,286,Based on Algorithm 3.2 of GPML,
scikit-learn/sklearn/gaussian_process/_gpc.py,287,K_star =k(x_star),
scikit-learn/sklearn/gaussian_process/_gpc.py,288,Line 4,
scikit-learn/sklearn/gaussian_process/_gpc.py,289,Line 5,
scikit-learn/sklearn/gaussian_process/_gpc.py,290,Line 6 (compute np.diag(v.T.dot(v)) via einsum),
scikit-learn/sklearn/gaussian_process/_gpc.py,293,Line 7:,
scikit-learn/sklearn/gaussian_process/_gpc.py,294,"Approximate \int log(z) * N(z | f_star, var_f_star)",
scikit-learn/sklearn/gaussian_process/_gpc.py,295,"Approximation is due to Williams & Barber, ""Bayesian Classification",
scikit-learn/sklearn/gaussian_process/_gpc.py,296,"with Gaussian Processes"", Appendix A: Approximate the logistic",
scikit-learn/sklearn/gaussian_process/_gpc.py,297,sigmoid by a linear combination of 5 error functions.,
scikit-learn/sklearn/gaussian_process/_gpc.py,298,For information on how this integral can be computed see,
scikit-learn/sklearn/gaussian_process/_gpc.py,299,blitiri.blogspot.de/2012/11/gaussian-integral-of-error-function.html,
scikit-learn/sklearn/gaussian_process/_gpc.py,357,Compute log-marginal-likelihood Z and also store some temporaries,
scikit-learn/sklearn/gaussian_process/_gpc.py,358,which can be reused for computing Z's gradient,
scikit-learn/sklearn/gaussian_process/_gpc.py,365,Compute gradient based on Algorithm 5.1 of GPML,
scikit-learn/sklearn/gaussian_process/_gpc.py,367,XXX: Get rid of the np.diag() in the next line,
scikit-learn/sklearn/gaussian_process/_gpc.py,368,Line 7,
scikit-learn/sklearn/gaussian_process/_gpc.py,369,Line 8,
scikit-learn/sklearn/gaussian_process/_gpc.py,370,Line 9: (use einsum to compute np.diag(C.T.dot(C)))),
scikit-learn/sklearn/gaussian_process/_gpc.py,372,third derivative,
scikit-learn/sklearn/gaussian_process/_gpc.py,375,Line 11,
scikit-learn/sklearn/gaussian_process/_gpc.py,376,Line 12: (R.T.ravel().dot(C.ravel()) = np.trace(R.dot(C))),
scikit-learn/sklearn/gaussian_process/_gpc.py,379,Line 13,
scikit-learn/sklearn/gaussian_process/_gpc.py,380,Line 14,
scikit-learn/sklearn/gaussian_process/_gpc.py,382,Line 15,
scikit-learn/sklearn/gaussian_process/_gpc.py,393,Based on Algorithm 3.1 of GPML,
scikit-learn/sklearn/gaussian_process/_gpc.py,395,"If warm_start are enabled, we reuse the last solution for the",
scikit-learn/sklearn/gaussian_process/_gpc.py,396,"posterior mode as initialization; otherwise, we initialize with 0",
scikit-learn/sklearn/gaussian_process/_gpc.py,403,Use Newton's iteration method to find mode of Laplace approximation,
scikit-learn/sklearn/gaussian_process/_gpc.py,406,Line 4,
scikit-learn/sklearn/gaussian_process/_gpc.py,409,Line 5,
scikit-learn/sklearn/gaussian_process/_gpc.py,414,Line 6,
scikit-learn/sklearn/gaussian_process/_gpc.py,416,Line 7,
scikit-learn/sklearn/gaussian_process/_gpc.py,418,Line 8,
scikit-learn/sklearn/gaussian_process/_gpc.py,421,Line 10: Compute log marginal likelihood in loop and use as,
scikit-learn/sklearn/gaussian_process/_gpc.py,422,convergence criterion,
scikit-learn/sklearn/gaussian_process/_gpc.py,426,Check if we have converged (log marginal likelihood does,
scikit-learn/sklearn/gaussian_process/_gpc.py,427,not decrease),
scikit-learn/sklearn/gaussian_process/_gpc.py,428,XXX: more complex convergence criterion,
scikit-learn/sklearn/gaussian_process/_gpc.py,433,Remember solution for later warm-starts,
scikit-learn/sklearn/gaussian_process/_gpc.py,777,use same theta for all sub-kernels,
scikit-learn/sklearn/gaussian_process/_gpc.py,783,theta for compound kernel,
scikit-learn/sklearn/gaussian_process/_gpr.py,3,Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,
scikit-learn/sklearn/gaussian_process/_gpr.py,4,Modified by: Pete Green <p.l.green@liverpool.ac.uk>,
scikit-learn/sklearn/gaussian_process/_gpr.py,5,License: BSD 3 clause,
scikit-learn/sklearn/gaussian_process/_gpr.py,178,Use an RBF kernel as default,
scikit-learn/sklearn/gaussian_process/_gpr.py,193,Normalize target value,
scikit-learn/sklearn/gaussian_process/_gpr.py,198,Remove mean and make unit variance,
scikit-learn/sklearn/gaussian_process/_gpr.py,218,Choose hyperparameters based on maximizing the log-marginal,
scikit-learn/sklearn/gaussian_process/_gpr.py,219,likelihood (potentially starting from several initial values),
scikit-learn/sklearn/gaussian_process/_gpr.py,229,First optimize starting from theta specified in kernel,
scikit-learn/sklearn/gaussian_process/_gpr.py,234,Additional runs are performed from log-uniform chosen initial,
scikit-learn/sklearn/gaussian_process/_gpr.py,235,theta,
scikit-learn/sklearn/gaussian_process/_gpr.py,248,Select result from run with minimal (negative) log-marginal,
scikit-learn/sklearn/gaussian_process/_gpr.py,249,likelihood,
scikit-learn/sklearn/gaussian_process/_gpr.py,258,Precompute quantities required for predictions which are independent,
scikit-learn/sklearn/gaussian_process/_gpr.py,259,of actual query points,
scikit-learn/sklearn/gaussian_process/_gpr.py,263,Line 2,
scikit-learn/sklearn/gaussian_process/_gpr.py,264,"self.L_ changed, self._K_inv needs to be recomputed",
scikit-learn/sklearn/gaussian_process/_gpr.py,273,Line 3,
scikit-learn/sklearn/gaussian_process/_gpr.py,320,Unfitted;predict based on GP prior,
scikit-learn/sklearn/gaussian_process/_gpr.py,335,Predict based on GP posterior,
scikit-learn/sklearn/gaussian_process/_gpr.py,337,Line 4 (y_mean = f_star),
scikit-learn/sklearn/gaussian_process/_gpr.py,339,undo normalisation,
scikit-learn/sklearn/gaussian_process/_gpr.py,343,Line 5,
scikit-learn/sklearn/gaussian_process/_gpr.py,344,Line 6,
scikit-learn/sklearn/gaussian_process/_gpr.py,346,undo normalisation,
scikit-learn/sklearn/gaussian_process/_gpr.py,351,cache result of K_inv computation,
scikit-learn/sklearn/gaussian_process/_gpr.py,353,compute inverse K_inv of K based on its Cholesky,
scikit-learn/sklearn/gaussian_process/_gpr.py,354,decomposition L and its inverse L_inv,
scikit-learn/sklearn/gaussian_process/_gpr.py,359,Compute variance of predictive distribution,
scikit-learn/sklearn/gaussian_process/_gpr.py,364,Check if any of the variances is negative because of,
scikit-learn/sklearn/gaussian_process/_gpr.py,365,numerical issues. If yes: set the variance to 0.,
scikit-learn/sklearn/gaussian_process/_gpr.py,372,undo normalisation,
scikit-learn/sklearn/gaussian_process/_gpr.py,464,Line 2,
scikit-learn/sklearn/gaussian_process/_gpr.py,469,Support multi-dimensional output of self.y_train_,
scikit-learn/sklearn/gaussian_process/_gpr.py,474,Line 3,
scikit-learn/sklearn/gaussian_process/_gpr.py,476,Compute log-likelihood (compare line 7),
scikit-learn/sklearn/gaussian_process/_gpr.py,480,sum over dimensions,
scikit-learn/sklearn/gaussian_process/_gpr.py,482,compare Equation 5.9 from GPML,
scikit-learn/sklearn/gaussian_process/_gpr.py,483,k: output-dimension,
scikit-learn/sklearn/gaussian_process/_gpr.py,485,"Compute ""0.5 * trace(tmp.dot(K_gradient))"" without",
scikit-learn/sklearn/gaussian_process/_gpr.py,486,constructing the full matrix tmp.dot(K_gradient) since only,
scikit-learn/sklearn/gaussian_process/_gpr.py,487,its diagonal is required,
scikit-learn/sklearn/gaussian_process/kernels.py,16,Author: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,
scikit-learn/sklearn/gaussian_process/kernels.py,17,License: BSD 3 clause,
scikit-learn/sklearn/gaussian_process/kernels.py,19,Note: this module is strongly inspired by the kernel module of the george,
scikit-learn/sklearn/gaussian_process/kernels.py,20,package.,
scikit-learn/sklearn/gaussian_process/kernels.py,84,A raw namedtuple is very memory efficient as it packs the attributes,
scikit-learn/sklearn/gaussian_process/kernels.py,85,in a struct to get rid of the __dict__ of attributes in particular it,
scikit-learn/sklearn/gaussian_process/kernels.py,86,does not copy the string for the keys on each instance.,
scikit-learn/sklearn/gaussian_process/kernels.py,87,By deriving a namedtuple class just to introduce the __init__ method we,
scikit-learn/sklearn/gaussian_process/kernels.py,88,would also reintroduce the __dict__ on the instance. By telling the,
scikit-learn/sklearn/gaussian_process/kernels.py,89,Python interpreter that this subclass uses static __slots__ instead of,
scikit-learn/sklearn/gaussian_process/kernels.py,90,dynamic attributes. Furthermore we don't need any additional slot in the,
scikit-learn/sklearn/gaussian_process/kernels.py,91,subclass so we set __slots__ to the empty tuple.,
scikit-learn/sklearn/gaussian_process/kernels.py,97,vector-valued parameter,
scikit-learn/sklearn/gaussian_process/kernels.py,110,This is mainly a testing utility to check that two hyperparameters,
scikit-learn/sklearn/gaussian_process/kernels.py,111,are equal.,
scikit-learn/sklearn/gaussian_process/kernels.py,142,introspect the constructor arguments to find the model parameters,
scikit-learn/sklearn/gaussian_process/kernels.py,143,to represent,
scikit-learn/sklearn/gaussian_process/kernels.py,186,Simple optimisation to gain speed (inspect is slow),
scikit-learn/sklearn/gaussian_process/kernels.py,192,nested objects case,
scikit-learn/sklearn/gaussian_process/kernels.py,202,simple objects case,
scikit-learn/sklearn/gaussian_process/kernels.py,274,vector-valued parameter,
scikit-learn/sklearn/gaussian_process/kernels.py,1426,convert from upper-triangular matrix to square matrix,
scikit-learn/sklearn/gaussian_process/kernels.py,1439,Hyperparameter l kept fixed,
scikit-learn/sklearn/gaussian_process/kernels.py,1446,We need to recompute the pairwise dimension-wise distances,
scikit-learn/sklearn/gaussian_process/kernels.py,1459,isotropic,
scikit-learn/sklearn/gaussian_process/kernels.py,1596,general case; expensive to evaluate,
scikit-learn/sklearn/gaussian_process/kernels.py,1598,strict zeros result in nan,
scikit-learn/sklearn/gaussian_process/kernels.py,1605,convert from upper-triangular matrix to square matrix,
scikit-learn/sklearn/gaussian_process/kernels.py,1611,Hyperparameter l kept fixed,
scikit-learn/sklearn/gaussian_process/kernels.py,1615,We need to recompute the pairwise dimension-wise distances,
scikit-learn/sklearn/gaussian_process/kernels.py,1635,approximate gradient numerically,
scikit-learn/sklearn/gaussian_process/kernels.py,1636,helper function,
scikit-learn/sklearn/gaussian_process/kernels.py,1783,gradient with respect to length_scale,
scikit-learn/sklearn/gaussian_process/kernels.py,1788,l is kept fixed,
scikit-learn/sklearn/gaussian_process/kernels.py,1791,gradient with respect to alpha,
scikit-learn/sklearn/gaussian_process/kernels.py,1797,alpha is kept fixed,
scikit-learn/sklearn/gaussian_process/kernels.py,1925,gradient with respect to length_scale,
scikit-learn/sklearn/gaussian_process/kernels.py,1930,length_scale is kept fixed,
scikit-learn/sklearn/gaussian_process/kernels.py,1932,gradient with respect to p,
scikit-learn/sklearn/gaussian_process/kernels.py,1938,p is kept fixed,
scikit-learn/sklearn/gaussian_process/kernels.py,2089,adapted from scipy/optimize/optimize.py for functions with 2d output,
scikit-learn/sklearn/gaussian_process/kernels.py,2195,approximate gradient numerically,
scikit-learn/sklearn/gaussian_process/kernels.py,2196,helper function,
scikit-learn/sklearn/gaussian_process/kernels.py,2221,We have to fall back to slow way of computing diagonal,
scikit-learn/sklearn/gaussian_process/__init__.py,1,-*- coding: utf-8 -*-,
scikit-learn/sklearn/gaussian_process/__init__.py,3,Author: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,
scikit-learn/sklearn/gaussian_process/__init__.py,4,Vincent Dubourg <vincent.dubourg@gmail.com>,
scikit-learn/sklearn/gaussian_process/__init__.py,5,"(mostly translation, see implementation details)",
scikit-learn/sklearn/gaussian_process/__init__.py,6,License: BSD 3 clause,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,3,Author: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,4,Modified by: Pete Green <p.l.green@liverpool.ac.uk>,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,5,License: BSD 3 clause,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,55,Test the interpolating property for different kernels.,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,64,Test the interpolating property for different kernels.,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,82,Test that hyperparameter-tuning improves log-marginal likelihood.,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,90,Test that lml of optimized kernel is stored correctly.,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,98,Test that lml of optimized kernel is stored correctly.,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,108,Test that we are in local maximum after hyperparameter-optimization.,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,121,Test that hyperparameter-optimization remains in bounds,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,135,Compare analytic and numeric gradient of log marginal likelihood.,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,150,Test that GP prior has mean 0 and identical variances.,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,157,"XXX: quite hacky, works only for current kernels",
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,165,Test that statistics of samples drawn from GP are correct.,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,172,More digits accuracy would require many more samples,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,179,Test that kernel parameters are unmodified when optimizer is None.,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,190,Test that predicted std.-dev. is consistent with cov's diagonal.,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,198,Test that GPR can identify meaningful anisotropic length-scales.,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,199,We learn a function which varies in one dimension ten-times slower,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,200,than in the other. The corresponding length-scales should differ by at,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,201,least a factor 5,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,213,Test that an increasing number of random-starts of GP fitting only,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,214,increases the log marginal likelihood of the chosen theta.,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,251,Fit non-normalizing GP on normalized y,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,255,Fit normalizing GP on unnormalized y,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,259,"Compare predicted mean, std-devs and covariances",
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,299,Here we utilise a larger variance version of the training data,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,302,Standard GP with normalize_y=True,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,309,'Gold standard' mean predictions from GPy,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,316,'Gold standard' std predictions from GPy,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,323,"Based on numerical experiments, it's reasonable to expect our",
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,324,GP's mean predictions to get within 7% of predictions of those,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,325,made by GPy.,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,328,"Based on numerical experiments, it's reasonable to expect our",
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,329,GP's std predictions to get within 15% of predictions of those,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,330,made by GPy.,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,335,Test that GPR can deal with multi-dimensional target values,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,338,Test for fixed kernel that first dimension of 2d GP equals the output,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,339,of 1d GP and that second dimension is twice as large,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,358,Standard deviation and covariance do not depend on output,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,366,Test hyperparameter optimization,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,379,Test that GPR can use externally defined optimizers.,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,380,Define a dummy optimizer that simply tests 50 random hyperparameters,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,395,Checks that optimizer improved marginal likelihood,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,415,Test GPR can handle two different output-values for the same input.,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,438,Test that GPR predictions without fit does not break by default.,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,457,Test that self._K_inv is reset after a new fit,
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,468,the value of K_inv should be independent of the first fit,
scikit-learn/sklearn/gaussian_process/tests/test_gpc.py,3,Author: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,
scikit-learn/sklearn/gaussian_process/tests/test_gpc.py,4,License: BSD 3 clause,
scikit-learn/sklearn/gaussian_process/tests/test_gpc.py,27,multi-class,
scikit-learn/sklearn/gaussian_process/tests/test_gpc.py,44,Check binary predict decision has also predicted probability above 0.5.,
scikit-learn/sklearn/gaussian_process/tests/test_gpc.py,51,Check binary predict decision has also predicted probability above 0.5.,
scikit-learn/sklearn/gaussian_process/tests/test_gpc.py,62,Test that hyperparameter-tuning improves log-marginal likelihood.,
scikit-learn/sklearn/gaussian_process/tests/test_gpc.py,70,Test that lml of optimized kernel is stored correctly.,
scikit-learn/sklearn/gaussian_process/tests/test_gpc.py,78,Test that clone_kernel=False has side-effects of kernel.theta.,
scikit-learn/sklearn/gaussian_process/tests/test_gpc.py,88,Test that we are in local maximum after hyperparameter-optimization.,
scikit-learn/sklearn/gaussian_process/tests/test_gpc.py,101,Compare analytic and numeric gradient of log marginal likelihood.,
scikit-learn/sklearn/gaussian_process/tests/test_gpc.py,115,Test that an increasing number of random-starts of GP fitting only,
scikit-learn/sklearn/gaussian_process/tests/test_gpc.py,116,increases the log marginal likelihood of the chosen theta.,
scikit-learn/sklearn/gaussian_process/tests/test_gpc.py,137,Test that GPC can use externally defined optimizers.,
scikit-learn/sklearn/gaussian_process/tests/test_gpc.py,138,Define a dummy optimizer that simply tests 10 random hyperparameters,
scikit-learn/sklearn/gaussian_process/tests/test_gpc.py,153,Checks that optimizer improved marginal likelihood,
scikit-learn/sklearn/gaussian_process/tests/test_gpc.py,160,Test GPC for multi-class classification problems.,
scikit-learn/sklearn/gaussian_process/tests/test_gpc.py,173,Test that multi-class GPC produces identical results with n_jobs>1.,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,3,Author: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,4,License: BSD 3 clause,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,54,Compare analytic and numeric gradient of kernels.,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,75,skip non-basic kernels,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,79,Check that parameter vector theta of kernel is set correctly.,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,83,Determine kernel parameters that contribute to theta,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,93,Check that values returned in theta are consistent with,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,94,hyperparameter values (being their logarithms),
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,98,Fixed kernel parameters must be excluded from theta and gradient.,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,100,create copy with certain hyperparameter fixed,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,105,Check that theta and K_gradient are identical with the fixed,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,106,dimension left out,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,119,Check that values of theta are modified correctly,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,131,Identity is not satisfied on diagonal,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,134,Auto-correlation and cross-correlation should be consistent.,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,142,Test that diag method of kernel returns consistent results.,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,149,Adding kernels and multiplying kernels should be commutative.,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,150,Check addition,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,154,Check multiplication,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,160,Anisotropic kernel should be consistent with isotropic kernels.,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,174,Check getting and setting via theta,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,184,Test stationarity of kernels.,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,191,Test whether kernels is for vectors or structured data,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,211,Check that hyperparameters of two kernels are equal,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,221,Test that sklearn's clone works correctly on kernels.,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,224,XXX: Should this be fixed?,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,225,This differs from the sklearn's estimators equality check.,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,229,Check that all constructor parameters are equal.,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,232,Check that all hyperparameters are equal.,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,238,This test is to verify that using set_params does not,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,239,break clone on kernels.,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,240,"This used to break because in kernels such as the RBF, non-trivial",
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,241,logic that modified the length scale used to be in the constructor,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,242,See https://github.com/scikit-learn/scikit-learn/issues/6961,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,243,for more details.,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,247,RationalQuadratic kernel is isotropic.,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,253,XXX unreached code as of v0.22,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,267,Test consistency of Matern kernel for special values of nu.,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,269,the diagonal elements of a matern kernel are 1,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,271,matern kernel for coef0==0.5 is equal to absolute exponential kernel,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,275,matern kernel with coef0==inf is equal to RBF kernel,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,280,"test that special cases of matern kernel (coef0 in [0.5, 1.5, 2.5])",
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,281,result in nearly identical results as the general case for coef0 in,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,282,"[0.5 + tiny, 1.5 + tiny, 2.5 + tiny]",
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,288,test that coef0==large is close to RBF,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,297,Check that GP kernels can also be used as pairwise kernels.,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,299,Test auto-kernel,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,301,"For WhiteKernel: k(X) != k(X,X). This is assumed by",
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,302,pairwise_kernels,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,307,Test cross-kernel,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,315,Check that set_params()/get_params() is consistent with kernel.theta.,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,317,Test get_params(),
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,325,anisotropic kernels,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,333,Test set_params(),
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,335,arbitrary value,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,341,anisotropic kernels,
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,354,Smoke-test for repr in kernels.,
scikit-learn/sklearn/externals/_scipy_linalg.py,1,This should remained pinned to version 1.2 and not updated like other,
scikit-learn/sklearn/externals/_scipy_linalg.py,2,externals.,
scikit-learn/sklearn/externals/_scipy_linalg.py,108,"For Hermitian matrices, singular values equal abs(eigenvalues)",
scikit-learn/sklearn/externals/conftest.py,1,Do not collect any tests in externals. This is more robust than using,
scikit-learn/sklearn/externals/conftest.py,2,--ignore because --ignore needs a path and it is not convenient to pass in,
scikit-learn/sklearn/externals/conftest.py,3,the externals path (very long install-dependent path in site-packages) when,
scikit-learn/sklearn/externals/conftest.py,4,using --pyargs,
scikit-learn/sklearn/externals/_pilutil.py,51,Modification of original scipy pilutil.py to make this module importable if,
scikit-learn/sklearn/externals/_pilutil.py,52,"pillow is not installed. If pillow is not installed, functions will raise",
scikit-learn/sklearn/externals/_pilutil.py,53,ImportError when called.,
scikit-learn/sklearn/externals/_pilutil.py,299,"Mode 'P' means there is an indexed ""palette"".  If we leave the mode",
scikit-learn/sklearn/externals/_pilutil.py,300,"as 'P', then when we do `a = array(im)` below, `a` will be a 2-D",
scikit-learn/sklearn/externals/_pilutil.py,301,"containing the indices into the palette, and not a 3-D array",
scikit-learn/sklearn/externals/_pilutil.py,302,containing the RGB or RGBA values.,
scikit-learn/sklearn/externals/_pilutil.py,311,"Workaround for crash in PIL. When im is 1-bit, the call array(im)",
scikit-learn/sklearn/externals/_pilutil.py,312,"can cause a seg. fault, or generate garbage. See",
scikit-learn/sklearn/externals/_pilutil.py,313,https://github.com/scipy/scipy/issues/2138 and,
scikit-learn/sklearn/externals/_pilutil.py,314,https://github.com/python-pillow/Pillow/issues/350.,
scikit-learn/sklearn/externals/_pilutil.py,315,,
scikit-learn/sklearn/externals/_pilutil.py,316,This converts im from a 1-bit image to an 8-bit image.,
scikit-learn/sklearn/externals/_pilutil.py,369,columns show up first,
scikit-learn/sklearn/externals/_pilutil.py,380,Becomes a mode='P' automagically.,
scikit-learn/sklearn/externals/_pilutil.py,381,default gray-scale,
scikit-learn/sklearn/externals/_pilutil.py,386,high input gives threshold for 1,
scikit-learn/sklearn/externals/_pilutil.py,402,if here then 3-d array with a 3 or a 4 in the shape length.,
scikit-learn/sklearn/externals/_pilutil.py,403,Check for 3 in datacube shape --- 'RGB' or 'YCbCr',
scikit-learn/sklearn/externals/_pilutil.py,446,Here we know data and mode is correct,
scikit-learn/sklearn/externals/_lobpcg.py,41,Used only when verbosity level > 10.,
scikit-learn/sklearn/externals/_lobpcg.py,69,Assume 1!,
scikit-learn/sklearn/externals/_lobpcg.py,105,Shared data!!!,
scikit-learn/sklearn/externals/_lobpcg.py,110,VBV is a Cholesky factor from now on...,
scikit-learn/sklearn/externals/_lobpcg.py,114,"blockVectorV = (cho_solve((VBV.T, True), blockVectorV.T)).T",
scikit-learn/sklearn/externals/_lobpcg.py,117,"blockVectorBV = (cho_solve((VBV.T, True), blockVectorBV.T)).T",
scikit-learn/sklearn/externals/_lobpcg.py,121,raise ValueError('Cholesky has failed'),
scikit-learn/sklearn/externals/_lobpcg.py,312,Block size.,
scikit-learn/sklearn/externals/_lobpcg.py,344,warn('The problem size is small compared to the block size.' \,
scikit-learn/sklearn/externals/_lobpcg.py,345,' Using dense eigensolver instead of LOBPCG.'),
scikit-learn/sklearn/externals/_lobpcg.py,353,Define the closed range of indices of eigenvalues to return.,
scikit-learn/sklearn/externals/_lobpcg.py,365,Reverse order to be compatible with eigs() in 'LM' mode.,
scikit-learn/sklearn/externals/_lobpcg.py,374,Apply constraints to X.,
scikit-learn/sklearn/externals/_lobpcg.py,382,gramYBY is a dense array.,
scikit-learn/sklearn/externals/_lobpcg.py,385,gramYBY is a Cholesky factor from now on...,
scikit-learn/sklearn/externals/_lobpcg.py,392,,
scikit-learn/sklearn/externals/_lobpcg.py,393,B-orthonormalize X.,
scikit-learn/sklearn/externals/_lobpcg.py,396,,
scikit-learn/sklearn/externals/_lobpcg.py,397,Compute the initial Ritz vectors: solve the eigenproblem.,
scikit-learn/sklearn/externals/_lobpcg.py,411,,
scikit-learn/sklearn/externals/_lobpcg.py,412,Active index set.,
scikit-learn/sklearn/externals/_lobpcg.py,422,,
scikit-learn/sklearn/externals/_lobpcg.py,423,Main iteration loop.,
scikit-learn/sklearn/externals/_lobpcg.py,425,set during iteration,
scikit-learn/sklearn/externals/_lobpcg.py,478,Apply preconditioner T to the active residuals.,
scikit-learn/sklearn/externals/_lobpcg.py,481,,
scikit-learn/sklearn/externals/_lobpcg.py,482,Apply constraints to the preconditioned residuals.,
scikit-learn/sklearn/externals/_lobpcg.py,487,,
scikit-learn/sklearn/externals/_lobpcg.py,488,B-orthogonalize the preconditioned residuals to X.,
scikit-learn/sklearn/externals/_lobpcg.py,500,,
scikit-learn/sklearn/externals/_lobpcg.py,501,B-orthonormalize the preconditioned residuals.,
scikit-learn/sklearn/externals/_lobpcg.py,515,Function _b_orthonormalize returns None if Cholesky fails,
scikit-learn/sklearn/externals/_lobpcg.py,523,,
scikit-learn/sklearn/externals/_lobpcg.py,524,Perform the Rayleigh Ritz Procedure:,
scikit-learn/sklearn/externals/_lobpcg.py,525,Compute symmetric Gram matrices:,
scikit-learn/sklearn/externals/_lobpcg.py,537,"Once explicitGramFlag, forever explicitGramFlag.",
scikit-learn/sklearn/externals/_lobpcg.py,540,Shared memory assingments to simplify the code,
scikit-learn/sklearn/externals/_lobpcg.py,547,Common submatrices:,
scikit-learn/sklearn/externals/_lobpcg.py,569,"Note: not documented, but leave it in here for now",
scikit-learn/sklearn/externals/_lobpcg.py,599,try again after dropping the direction vectors P from RR,
scikit-learn/sklearn/externals/_lobpcg.py,628,# Normalize eigenvectors!,
scikit-learn/sklearn/externals/_lobpcg.py,629,"aux = np.sum( eigBlockVector.conj() * eigBlockVector, 0 )",
scikit-learn/sklearn/externals/_lobpcg.py,630,eigVecNorms = np.sqrt( aux ),
scikit-learn/sklearn/externals/_lobpcg.py,631,"eigBlockVector = eigBlockVector / eigVecNorms[np.newaxis, :]",
scikit-learn/sklearn/externals/_lobpcg.py,632,"eigBlockVector, aux = _b_orthonormalize( B, eigBlockVector )",
scikit-learn/sklearn/externals/_lobpcg.py,637,Compute Ritz vectors.,
scikit-learn/sklearn/externals/_lobpcg.py,709,Future work: Need to add Postprocessing here:,
scikit-learn/sklearn/externals/_lobpcg.py,710,"Making sure eigenvectors ""exactly"" satisfy the blockVectorY constrains?",
scikit-learn/sklearn/externals/_lobpcg.py,711,"Making sure eigenvecotrs are ""exactly"" othonormalized by final ""exact"" RR",
scikit-learn/sklearn/externals/_lobpcg.py,712,Computing the actual true residuals,
scikit-learn/sklearn/externals/_arff.py,1,-*- coding: utf-8 -*-,
scikit-learn/sklearn/externals/_arff.py,2,=============================================================================,
scikit-learn/sklearn/externals/_arff.py,3,Federal University of Rio Grande do Sul (UFRGS),
scikit-learn/sklearn/externals/_arff.py,4,Connectionist Artificial Intelligence Laboratory (LIAC),
scikit-learn/sklearn/externals/_arff.py,5,Renato de Pontes Pereira - rppereira@inf.ufrgs.br,
scikit-learn/sklearn/externals/_arff.py,6,=============================================================================,
scikit-learn/sklearn/externals/_arff.py,7,"Copyright (c) 2011 Renato de Pontes Pereira, renato.ppontes at gmail dot com",
scikit-learn/sklearn/externals/_arff.py,8,,
scikit-learn/sklearn/externals/_arff.py,9,"Permission is hereby granted, free of charge, to any person obtaining a copy",
scikit-learn/sklearn/externals/_arff.py,10,"of this software and associated documentation files (the ""Software""), to deal",
scikit-learn/sklearn/externals/_arff.py,11,"in the Software without restriction, including without limitation the rights",
scikit-learn/sklearn/externals/_arff.py,12,"to use, copy, modify, merge, publish, distribute, sublicense, and/or sell",
scikit-learn/sklearn/externals/_arff.py,13,"copies of the Software, and to permit persons to whom the Software is",
scikit-learn/sklearn/externals/_arff.py,14,"furnished to do so, subject to the following conditions:",
scikit-learn/sklearn/externals/_arff.py,15,,
scikit-learn/sklearn/externals/_arff.py,16,The above copyright notice and this permission notice shall be included in,
scikit-learn/sklearn/externals/_arff.py,17,all copies or substantial portions of the Software.,
scikit-learn/sklearn/externals/_arff.py,18,,
scikit-learn/sklearn/externals/_arff.py,19,"THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR",
scikit-learn/sklearn/externals/_arff.py,20,"IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,",
scikit-learn/sklearn/externals/_arff.py,21,FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE,
scikit-learn/sklearn/externals/_arff.py,22,"AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER",
scikit-learn/sklearn/externals/_arff.py,23,"LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,",
scikit-learn/sklearn/externals/_arff.py,24,OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE,
scikit-learn/sklearn/externals/_arff.py,25,SOFTWARE.,
scikit-learn/sklearn/externals/_arff.py,26,=============================================================================,
scikit-learn/sklearn/externals/_arff.py,157,CONSTANTS ===================================================================,
scikit-learn/sklearn/externals/_arff.py,189,"a value is surrounded by "" or by ' or contains no quotables",
scikit-learn/sklearn/externals/_arff.py,197,"This captures (value, error) groups. Because empty values are allowed,",
scikit-learn/sklearn/externals/_arff.py,198,we cannot just look for empty values to handle syntax errors.,
scikit-learn/sklearn/externals/_arff.py,199,"We presume the line has had ',' prepended...",
scikit-learn/sklearn/externals/_arff.py,208,"This captures (key, value) groups and will have an empty key/value",
scikit-learn/sklearn/externals/_arff.py,209,in case of syntax errors.,
scikit-learn/sklearn/externals/_arff.py,210,It does not ensure that the line starts with '{' or ends with '}'.,
scikit-learn/sklearn/externals/_arff.py,271,Fast path for trivial cases (unfortunately we have to handle missing,
scikit-learn/sklearn/externals/_arff.py,272,values because of the empty string case :(.),
scikit-learn/sklearn/externals/_arff.py,276,"_RE_DENSE_VALUES tokenizes despite quoting, whitespace, etc.",
scikit-learn/sklearn/externals/_arff.py,285,an ARFF syntax error in sparse data,
scikit-learn/sklearn/externals/_arff.py,291,an ARFF syntax error,
scikit-learn/sklearn/externals/_arff.py,298,Constant value representing a dense matrix,
scikit-learn/sklearn/externals/_arff.py,299,Constant value representing a sparse matrix in coordinate format,
scikit-learn/sklearn/externals/_arff.py,300,Constant value representing a sparse matrix in list of,
scikit-learn/sklearn/externals/_arff.py,301,dictionaries format,
scikit-learn/sklearn/externals/_arff.py,302,Generator of dictionaries,
scikit-learn/sklearn/externals/_arff.py,303,Generator of dictionaries,
scikit-learn/sklearn/externals/_arff.py,306,=============================================================================,
scikit-learn/sklearn/externals/_arff.py,308,COMPATIBILITY WITH PYTHON 3 =================================================,
scikit-learn/sklearn/externals/_arff.py,315,COMPABILITY WITH PYTHON 2 ===================================================,
scikit-learn/sklearn/externals/_arff.py,316,=============================================================================,
scikit-learn/sklearn/externals/_arff.py,321,EXCEPTIONS ==================================================================,
scikit-learn/sklearn/externals/_arff.py,413,=============================================================================,
scikit-learn/sklearn/externals/_arff.py,415,INTERNAL ====================================================================,
scikit-learn/sklearn/externals/_arff.py,446,Sparse decode,
scikit-learn/sklearn/externals/_arff.py,447,See issue #52: nominals should take their first value when,
scikit-learn/sklearn/externals/_arff.py,448,"unspecified in a sparse matrix. Naturally, this is consistent",
scikit-learn/sklearn/externals/_arff.py,449,with EncodedNominalConversor.,
scikit-learn/sklearn/externals/_arff.py,466,"XXX: int 0 is used for implicit values, not '0'",
scikit-learn/sklearn/externals/_arff.py,545,conversor out of range,
scikit-learn/sklearn/externals/_arff.py,563,Check if the rows are sorted,
scikit-learn/sklearn/externals/_arff.py,570,Add empty rows if necessary,
scikit-learn/sklearn/externals/_arff.py,605,conversor out of range,
scikit-learn/sklearn/externals/_arff.py,651,Probably a scipy.sparse,
scikit-learn/sklearn/externals/_arff.py,662,=============================================================================,
scikit-learn/sklearn/externals/_arff.py,664,ADVANCED INTERFACE ==========================================================,
scikit-learn/sklearn/externals/_arff.py,742,Verify the general structure of declaration,
scikit-learn/sklearn/externals/_arff.py,747,Extracts the raw name and type,
scikit-learn/sklearn/externals/_arff.py,750,Extracts the final name,
scikit-learn/sklearn/externals/_arff.py,753,Extracts the final type,
scikit-learn/sklearn/externals/_arff.py,763,"If not nominal, verify the type name",
scikit-learn/sklearn/externals/_arff.py,773,Make sure this method is idempotent,
scikit-learn/sklearn/externals/_arff.py,776,"If string, convert to a list of lines",
scikit-learn/sklearn/externals/_arff.py,780,Create the return object,
scikit-learn/sklearn/externals/_arff.py,789,Create the data helper object,
scikit-learn/sklearn/externals/_arff.py,792,Read all lines,
scikit-learn/sklearn/externals/_arff.py,797,Ignore empty lines,
scikit-learn/sklearn/externals/_arff.py,803,DESCRIPTION -----------------------------------------------------,
scikit-learn/sklearn/externals/_arff.py,806,-----------------------------------------------------------------,
scikit-learn/sklearn/externals/_arff.py,808,RELATION --------------------------------------------------------,
scikit-learn/sklearn/externals/_arff.py,815,-----------------------------------------------------------------,
scikit-learn/sklearn/externals/_arff.py,817,ATTRIBUTE -------------------------------------------------------,
scikit-learn/sklearn/externals/_arff.py,844,-----------------------------------------------------------------,
scikit-learn/sklearn/externals/_arff.py,846,DATA ------------------------------------------------------------,
scikit-learn/sklearn/externals/_arff.py,852,-----------------------------------------------------------------,
scikit-learn/sklearn/externals/_arff.py,854,COMMENT ---------------------------------------------------------,
scikit-learn/sklearn/externals/_arff.py,857,-----------------------------------------------------------------,
scikit-learn/sklearn/externals/_arff.py,859,Never found @DATA,
scikit-learn/sklearn/externals/_arff.py,866,Ignore empty lines and comment lines.,
scikit-learn/sklearn/externals/_arff.py,870,Alter the data object,
scikit-learn/sklearn/externals/_arff.py,989,DESCRIPTION,
scikit-learn/sklearn/externals/_arff.py,994,RELATION,
scikit-learn/sklearn/externals/_arff.py,1001,ATTRIBUTES,
scikit-learn/sklearn/externals/_arff.py,1007,Verify for bad object format,
scikit-learn/sklearn/externals/_arff.py,1014,Verify for invalid types,
scikit-learn/sklearn/externals/_arff.py,1018,Verify for bad object format,
scikit-learn/sklearn/externals/_arff.py,1022,Verify attribute name is not used twice,
scikit-learn/sklearn/externals/_arff.py,1033,DATA,
scikit-learn/sklearn/externals/_arff.py,1042,=============================================================================,
scikit-learn/sklearn/externals/_arff.py,1044,BASIC INTERFACE =============================================================,
scikit-learn/sklearn/externals/_arff.py,1107,=============================================================================,
scikit-learn/sklearn/covariance/_graph_lasso.py,5,Author: Gael Varoquaux <gael.varoquaux@normalesup.org>,
scikit-learn/sklearn/covariance/_graph_lasso.py,6,License: BSD 3 clause,
scikit-learn/sklearn/covariance/_graph_lasso.py,7,Copyright: INRIA,
scikit-learn/sklearn/covariance/_graph_lasso.py,23,mypy error: Module 'sklearn.linear_model' has no attribute '_cd_fast',
scikit-learn/sklearn/covariance/_graph_lasso.py,24,type: ignore,
scikit-learn/sklearn/covariance/_graph_lasso.py,29,Helper functions to compute the objective and dual objective functions,
scikit-learn/sklearn/covariance/_graph_lasso.py,30,of the l1-penalized estimator,
scikit-learn/sklearn/covariance/_graph_lasso.py,77,The g-lasso algorithm,
scikit-learn/sklearn/covariance/_graph_lasso.py,182,"As a trivial regularization (Tikhonov like), we scale down the",
scikit-learn/sklearn/covariance/_graph_lasso.py,183,"off-diagonal coefficients of our starting point: This is needed, as",
scikit-learn/sklearn/covariance/_graph_lasso.py,184,in the cross-validation the cov_init can easily be,
scikit-learn/sklearn/covariance/_graph_lasso.py,185,"ill-conditioned, and the CV loop blows. Beside, this takes",
scikit-learn/sklearn/covariance/_graph_lasso.py,186,"conservative stand-point on the initial conditions, and it tends to",
scikit-learn/sklearn/covariance/_graph_lasso.py,187,make the convergence go faster.,
scikit-learn/sklearn/covariance/_graph_lasso.py,195,The different l1 regression solver have different numerical errors,
scikit-learn/sklearn/covariance/_graph_lasso.py,201,"be robust to the max_iter=0 edge case, see:",
scikit-learn/sklearn/covariance/_graph_lasso.py,202,https://github.com/scikit-learn/scikit-learn/issues/4134,
scikit-learn/sklearn/covariance/_graph_lasso.py,204,set a sub_covariance buffer,
scikit-learn/sklearn/covariance/_graph_lasso.py,208,To keep the contiguous matrix `sub_covariance` equal to,
scikit-learn/sklearn/covariance/_graph_lasso.py,209,covariance_[indices != idx].T[indices != idx],
scikit-learn/sklearn/covariance/_graph_lasso.py,210,we only need to update 1 column and 1 line when idx changes,
scikit-learn/sklearn/covariance/_graph_lasso.py,220,Use coordinate descent,
scikit-learn/sklearn/covariance/_graph_lasso.py,228,Use LARS,
scikit-learn/sklearn/covariance/_graph_lasso.py,233,Update the precision matrix,
scikit-learn/sklearn/covariance/_graph_lasso.py,386,Covariance does not make sense for a single feature,
scikit-learn/sklearn/covariance/_graph_lasso.py,403,Cross-validation with GraphicalLasso,
scikit-learn/sklearn/covariance/_graph_lasso.py,476,"Capture the errors, and move on",
scikit-learn/sklearn/covariance/_graph_lasso.py,663,Covariance does not make sense for a single feature,
scikit-learn/sklearn/covariance/_graph_lasso.py,674,"List of (alpha, scores, covs)",
scikit-learn/sklearn/covariance/_graph_lasso.py,692,No need to see the convergence warnings on this grid:,
scikit-learn/sklearn/covariance/_graph_lasso.py,693,they will always be points that will not converge,
scikit-learn/sklearn/covariance/_graph_lasso.py,694,during the cross-validation,
scikit-learn/sklearn/covariance/_graph_lasso.py,696,Compute the cross-validated loss on the current grid,
scikit-learn/sklearn/covariance/_graph_lasso.py,698,"NOTE: Warm-restarting graphical_lasso_path has been tried,",
scikit-learn/sklearn/covariance/_graph_lasso.py,699,and this did not allow to gain anything,
scikit-learn/sklearn/covariance/_graph_lasso.py,700,(same execution time with or without).,
scikit-learn/sklearn/covariance/_graph_lasso.py,713,Little danse to transform the list in what we need,
scikit-learn/sklearn/covariance/_graph_lasso.py,720,Find the maximum (avoid using built in 'max' function to,
scikit-learn/sklearn/covariance/_graph_lasso.py,721,have a fully-reproducible selection of the smallest alpha,
scikit-learn/sklearn/covariance/_graph_lasso.py,722,in case of equality),
scikit-learn/sklearn/covariance/_graph_lasso.py,735,Refine the grid,
scikit-learn/sklearn/covariance/_graph_lasso.py,737,We do not need to go back: we have chosen,
scikit-learn/sklearn/covariance/_graph_lasso.py,738,the highest value of alpha for which there are,
scikit-learn/sklearn/covariance/_graph_lasso.py,739,non-zero coefficients,
scikit-learn/sklearn/covariance/_graph_lasso.py,744,We have non-converged models on the upper bound of the,
scikit-learn/sklearn/covariance/_graph_lasso.py,745,"grid, we need to refine the grid there",
scikit-learn/sklearn/covariance/_graph_lasso.py,767,"Finally, compute the score with alpha = 0",
scikit-learn/sklearn/covariance/_graph_lasso.py,777,Finally fit the model with the selected alpha,
scikit-learn/sklearn/covariance/_shrunk_covariance.py,9,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/sklearn/covariance/_shrunk_covariance.py,10,Gael Varoquaux <gael.varoquaux@normalesup.org>,
scikit-learn/sklearn/covariance/_shrunk_covariance.py,11,Virgile Fritsch <virgile.fritsch@inria.fr>,
scikit-learn/sklearn/covariance/_shrunk_covariance.py,12,,
scikit-learn/sklearn/covariance/_shrunk_covariance.py,13,License: BSD 3 clause,
scikit-learn/sklearn/covariance/_shrunk_covariance.py,15,avoid division truncation,
scikit-learn/sklearn/covariance/_shrunk_covariance.py,24,ShrunkCovariance estimator,
scikit-learn/sklearn/covariance/_shrunk_covariance.py,146,"Not calling the parent object to fit, to avoid a potential",
scikit-learn/sklearn/covariance/_shrunk_covariance.py,147,matrix inversion when setting the precision,
scikit-learn/sklearn/covariance/_shrunk_covariance.py,160,Ledoit-Wolf estimator,
scikit-learn/sklearn/covariance/_shrunk_covariance.py,196,"for only one feature, the result is the same whatever the shrinkage",
scikit-learn/sklearn/covariance/_shrunk_covariance.py,207,optionally center data,
scikit-learn/sklearn/covariance/_shrunk_covariance.py,211,A non-blocked version of the computation is present in the tests,
scikit-learn/sklearn/covariance/_shrunk_covariance.py,212,in tests/test_covariance.py,
scikit-learn/sklearn/covariance/_shrunk_covariance.py,214,number of blocks to split the covariance matrix into,
scikit-learn/sklearn/covariance/_shrunk_covariance.py,219,"sum of the coefficients of <X2.T, X2>",
scikit-learn/sklearn/covariance/_shrunk_covariance.py,220,"sum of the *squared* coefficients of <X.T, X>",
scikit-learn/sklearn/covariance/_shrunk_covariance.py,221,starting block computation,
scikit-learn/sklearn/covariance/_shrunk_covariance.py,242,use delta_ to compute beta,
scikit-learn/sklearn/covariance/_shrunk_covariance.py,244,"delta is the sum of the squared coefficients of (<X.T,X> - mu*Id) / p",
scikit-learn/sklearn/covariance/_shrunk_covariance.py,247,get final beta as the min between beta and delta,
scikit-learn/sklearn/covariance/_shrunk_covariance.py,248,"We do this to prevent shrinking more than ""1"", which whould invert",
scikit-learn/sklearn/covariance/_shrunk_covariance.py,249,the value of covariances,
scikit-learn/sklearn/covariance/_shrunk_covariance.py,251,finally get shrinkage,
scikit-learn/sklearn/covariance/_shrunk_covariance.py,294,"for only one feature, the result is the same whatever the shrinkage",
scikit-learn/sklearn/covariance/_shrunk_covariance.py,307,get Ledoit-Wolf shrinkage,
scikit-learn/sklearn/covariance/_shrunk_covariance.py,416,"Not calling the parent object to fit, to avoid computing the",
scikit-learn/sklearn/covariance/_shrunk_covariance.py,417,covariance matrix (and potentially the precision),
scikit-learn/sklearn/covariance/_shrunk_covariance.py,432,OAS estimator,
scikit-learn/sklearn/covariance/_shrunk_covariance.py,469,"for only one feature, the result is the same whatever the shrinkage",
scikit-learn/sklearn/covariance/_shrunk_covariance.py,486,formula from Chen et al.'s **implementation**,
scikit-learn/sklearn/covariance/_shrunk_covariance.py,590,"Not calling the parent object to fit, to avoid computing the",
scikit-learn/sklearn/covariance/_shrunk_covariance.py,591,covariance matrix (and potentially the precision),
scikit-learn/sklearn/covariance/_elliptic_envelope.py,1,Author: Virgile Fritsch <virgile.fritsch@inria.fr>,
scikit-learn/sklearn/covariance/_elliptic_envelope.py,2,,
scikit-learn/sklearn/covariance/_elliptic_envelope.py,3,License: BSD 3 clause,
scikit-learn/sklearn/covariance/_empirical_covariance.py,6,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/sklearn/covariance/_empirical_covariance.py,7,Gael Varoquaux <gael.varoquaux@normalesup.org>,
scikit-learn/sklearn/covariance/_empirical_covariance.py,8,Virgile Fritsch <virgile.fritsch@inria.fr>,
scikit-learn/sklearn/covariance/_empirical_covariance.py,9,,
scikit-learn/sklearn/covariance/_empirical_covariance.py,10,License: BSD 3 clause,
scikit-learn/sklearn/covariance/_empirical_covariance.py,12,avoid division truncation,
scikit-learn/sklearn/covariance/_empirical_covariance.py,164,set covariance,
scikit-learn/sklearn/covariance/_empirical_covariance.py,166,set precision,
scikit-learn/sklearn/covariance/_empirical_covariance.py,235,compute empirical covariance of the test set,
scikit-learn/sklearn/covariance/_empirical_covariance.py,238,compute log likelihood,
scikit-learn/sklearn/covariance/_empirical_covariance.py,274,compute the error,
scikit-learn/sklearn/covariance/_empirical_covariance.py,276,compute the error norm,
scikit-learn/sklearn/covariance/_empirical_covariance.py,284,optionally scale the error norm,
scikit-learn/sklearn/covariance/_empirical_covariance.py,287,finally get either the squared norm or the norm,
scikit-learn/sklearn/covariance/_empirical_covariance.py,311,compute mahalanobis distances,
scikit-learn/sklearn/covariance/_robust_covariance.py,7,Author: Virgile Fritsch <virgile.fritsch@inria.fr>,
scikit-learn/sklearn/covariance/_robust_covariance.py,8,,
scikit-learn/sklearn/covariance/_robust_covariance.py,9,License: BSD 3 clause,
scikit-learn/sklearn/covariance/_robust_covariance.py,23,Minimum Covariance Determinant,
scikit-learn/sklearn/covariance/_robust_covariance.py,24,Implementing of an algorithm by Rousseeuw & Van Driessen described in,
scikit-learn/sklearn/covariance/_robust_covariance.py,25,"(A Fast Algorithm for the Minimum Covariance Determinant Estimator,",
scikit-learn/sklearn/covariance/_robust_covariance.py,26,"1999, American Statistical Association and the American Society",
scikit-learn/sklearn/covariance/_robust_covariance.py,27,"for Quality, TECHNOMETRICS)",
scikit-learn/sklearn/covariance/_robust_covariance.py,28,XXX Is this really a public function? It's not listed in the docs or,
scikit-learn/sklearn/covariance/_robust_covariance.py,29,exported by sklearn.covariance. Deprecate?,
scikit-learn/sklearn/covariance/_robust_covariance.py,103,Initialisation,
scikit-learn/sklearn/covariance/_robust_covariance.py,106,compute initial robust estimates from a random subset,
scikit-learn/sklearn/covariance/_robust_covariance.py,109,get initial robust estimates from the function parameters,
scikit-learn/sklearn/covariance/_robust_covariance.py,112,run a special iteration for that case (to get an initial support),
scikit-learn/sklearn/covariance/_robust_covariance.py,116,compute new estimates,
scikit-learn/sklearn/covariance/_robust_covariance.py,123,Iterative procedure for Minimum Covariance Determinant computation,
scikit-learn/sklearn/covariance/_robust_covariance.py,125,"If the data already has singular covariance, calculate the precision,",
scikit-learn/sklearn/covariance/_robust_covariance.py,126,as the loop below will not be entered.,
scikit-learn/sklearn/covariance/_robust_covariance.py,133,save old estimates values,
scikit-learn/sklearn/covariance/_robust_covariance.py,138,compute a new support from the full data set mahalanobis distances,
scikit-learn/sklearn/covariance/_robust_covariance.py,142,compute new estimates,
scikit-learn/sklearn/covariance/_robust_covariance.py,149,update remaining iterations for early stopping,
scikit-learn/sklearn/covariance/_robust_covariance.py,154,"Check if best fit already found (det => 0, logdet => -inf)",
scikit-learn/sklearn/covariance/_robust_covariance.py,157,Check convergence,
scikit-learn/sklearn/covariance/_robust_covariance.py,159,c_step procedure converged,
scikit-learn/sklearn/covariance/_robust_covariance.py,165,determinant has increased (should not happen),
scikit-learn/sklearn/covariance/_robust_covariance.py,175,Check early stopping,
scikit-learn/sklearn/covariance/_robust_covariance.py,279,compute `n_trials` location and shape estimates candidates in the subset,
scikit-learn/sklearn/covariance/_robust_covariance.py,282,perform `n_trials` computations from random initial supports,
scikit-learn/sklearn/covariance/_robust_covariance.py,290,perform computations from every given initial estimates,
scikit-learn/sklearn/covariance/_robust_covariance.py,300,find the `n_best` best results among the `n_trials` ones,
scikit-learn/sklearn/covariance/_robust_covariance.py,382,minimum breakdown value,
scikit-learn/sklearn/covariance/_robust_covariance.py,388,1-dimensional case quick computation,
scikit-learn/sklearn/covariance/_robust_covariance.py,389,"(Rousseeuw, P. J. and Leroy, A. M. (2005) References, in Robust",
scikit-learn/sklearn/covariance/_robust_covariance.py,390,"Regression and Outlier Detection, John Wiley & Sons, chapter 4)",
scikit-learn/sklearn/covariance/_robust_covariance.py,393,find the sample shortest halves,
scikit-learn/sklearn/covariance/_robust_covariance.py,397,take the middle points' mean to get the robust location estimate,
scikit-learn/sklearn/covariance/_robust_covariance.py,405,get precision matrix in an optimized way,
scikit-learn/sklearn/covariance/_robust_covariance.py,413,get precision matrix in an optimized way,
scikit-learn/sklearn/covariance/_robust_covariance.py,416,Starting FastMCD algorithm for p-dimensional case,
scikit-learn/sklearn/covariance/_robust_covariance.py,418,1. Find candidate supports on subsets,
scikit-learn/sklearn/covariance/_robust_covariance.py,419,a. split the set in subsets of size ~ 300,
scikit-learn/sklearn/covariance/_robust_covariance.py,425,b. perform a total of 500 trials,
scikit-learn/sklearn/covariance/_robust_covariance.py,427,"c. select 10 best (location, covariance) for each subset",
scikit-learn/sklearn/covariance/_robust_covariance.py,436,The above is too big. Let's try with something much small,
scikit-learn/sklearn/covariance/_robust_covariance.py,437,(and less optimal),
scikit-learn/sklearn/covariance/_robust_covariance.py,454,2. Pool the candidate supports into a merged set,
scikit-learn/sklearn/covariance/_robust_covariance.py,455,(possibly the full dataset),
scikit-learn/sklearn/covariance/_robust_covariance.py,463,"find the best couples (location, covariance) on the merged set",
scikit-learn/sklearn/covariance/_robust_covariance.py,472,"3. Finally get the overall best (locations, covariance) couple",
scikit-learn/sklearn/covariance/_robust_covariance.py,474,"directly get the best couple (location, covariance)",
scikit-learn/sklearn/covariance/_robust_covariance.py,482,select the best couple on the full dataset,
scikit-learn/sklearn/covariance/_robust_covariance.py,495,"1. Find the 10 best couples (location, covariance)",
scikit-learn/sklearn/covariance/_robust_covariance.py,496,considering two iterations,
scikit-learn/sklearn/covariance/_robust_covariance.py,503,2. Select the best couple on the full dataset amongst the 10,
scikit-learn/sklearn/covariance/_robust_covariance.py,645,check that the empirical covariance is full rank,
scikit-learn/sklearn/covariance/_robust_covariance.py,649,compute and store raw estimates,
scikit-learn/sklearn/covariance/_robust_covariance.py,658,get precision matrix in an optimized way,
scikit-learn/sklearn/covariance/_robust_covariance.py,667,obtain consistency at normal models,
scikit-learn/sklearn/covariance/_robust_covariance.py,669,re-weight estimator,
scikit-learn/sklearn/covariance/_robust_covariance.py,700,Check that the covariance of the support data is not equal to 0.,
scikit-learn/sklearn/covariance/_robust_covariance.py,701,Otherwise self.dist_ = 0 and thus correction = 0.,
scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py,20,Sample data from a sparse multivariate normal,
scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py,39,Check that the costs always decrease (doesn't hold if alpha == 0),
scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py,42,Check that the 2 approaches give similar results,
scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py,46,Smoke test the estimator,
scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py,52,"For a centered matrix, assume_centered could be chosen True or False",
scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py,53,Check that this returns indeed the same result for centered data,
scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py,64,Hard-coded solution from R glasso package for alpha=1.0,
scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py,65,(need to set penalize.diagonal to FALSE),
scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py,88,Hard-coded solution from Python skggm package,
scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py,89,"obtained by calling `quic(emp_cov, lam=.1, tol=1e-8)`",
scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py,105,Small subset of rows to test the rank-deficient case,
scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py,106,Need to choose samples such that none of the variances are zero,
scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py,109,Hard-coded solution from R glasso package for alpha=0.01,
scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py,132,Sample data from a sparse multivariate normal,
scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py,140,"Capture stdout, to smoke test the verbose mode",
scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py,144,We need verbose very high so that Parallel prints on stdout,
scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py,149,Smoke test with specified alphas,
scikit-learn/sklearn/covariance/tests/test_covariance.py,1,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/sklearn/covariance/tests/test_covariance.py,2,Gael Varoquaux <gael.varoquaux@normalesup.org>,
scikit-learn/sklearn/covariance/tests/test_covariance.py,3,Virgile Fritsch <virgile.fritsch@inria.fr>,
scikit-learn/sklearn/covariance/tests/test_covariance.py,4,,
scikit-learn/sklearn/covariance/tests/test_covariance.py,5,License: BSD 3 clause,
scikit-learn/sklearn/covariance/tests/test_covariance.py,26,Tests Covariance module on a simple dataset.,
scikit-learn/sklearn/covariance/tests/test_covariance.py,27,test covariance fit from data,
scikit-learn/sklearn/covariance/tests/test_covariance.py,43,Mahalanobis distances computation test,
scikit-learn/sklearn/covariance/tests/test_covariance.py,47,test with n_features = 1,
scikit-learn/sklearn/covariance/tests/test_covariance.py,56,test with one sample,
scikit-learn/sklearn/covariance/tests/test_covariance.py,57,Create X with 1 sample and 5 features,
scikit-learn/sklearn/covariance/tests/test_covariance.py,64,test integer type,
scikit-learn/sklearn/covariance/tests/test_covariance.py,69,test centered case,
scikit-learn/sklearn/covariance/tests/test_covariance.py,76,Tests ShrunkCovariance module on a simple dataset.,
scikit-learn/sklearn/covariance/tests/test_covariance.py,77,compare shrunk covariance obtained from data and from MLE estimate,
scikit-learn/sklearn/covariance/tests/test_covariance.py,84,same test with shrinkage not provided,
scikit-learn/sklearn/covariance/tests/test_covariance.py,90,same test with shrinkage = 0 (<==> empirical_covariance),
scikit-learn/sklearn/covariance/tests/test_covariance.py,95,test with n_features = 1,
scikit-learn/sklearn/covariance/tests/test_covariance.py,101,test shrinkage coeff on a simple data set (without saving precision),
scikit-learn/sklearn/covariance/tests/test_covariance.py,108,Tests LedoitWolf module on a simple dataset.,
scikit-learn/sklearn/covariance/tests/test_covariance.py,109,test shrinkage coeff on a simple data set,
scikit-learn/sklearn/covariance/tests/test_covariance.py,122,compare shrunk covariance obtained from data and from MLE estimate,
scikit-learn/sklearn/covariance/tests/test_covariance.py,127,compare estimates given by LW and ShrunkCovariance,
scikit-learn/sklearn/covariance/tests/test_covariance.py,132,test with n_features = 1,
scikit-learn/sklearn/covariance/tests/test_covariance.py,142,test shrinkage coeff on a simple data set (without saving precision),
scikit-learn/sklearn/covariance/tests/test_covariance.py,148,Same tests without assuming centered data,
scikit-learn/sklearn/covariance/tests/test_covariance.py,149,test shrinkage coeff on a simple data set,
scikit-learn/sklearn/covariance/tests/test_covariance.py,156,compare shrunk covariance obtained from data and from MLE estimate,
scikit-learn/sklearn/covariance/tests/test_covariance.py,160,compare estimates given by LW and ShrunkCovariance,
scikit-learn/sklearn/covariance/tests/test_covariance.py,165,test with n_features = 1,
scikit-learn/sklearn/covariance/tests/test_covariance.py,174,test with one sample,
scikit-learn/sklearn/covariance/tests/test_covariance.py,175,warning should be raised when using only 1 sample,
scikit-learn/sklearn/covariance/tests/test_covariance.py,182,test shrinkage coeff on a simple data set (without saving precision),
scikit-learn/sklearn/covariance/tests/test_covariance.py,190,A simple implementation of the formulas from Ledoit & Wolf,
scikit-learn/sklearn/covariance/tests/test_covariance.py,192,The computation below achieves the following computations of the,
scikit-learn/sklearn/covariance/tests/test_covariance.py,193,"""O. Ledoit and M. Wolf, A Well-Conditioned Estimator for",
scikit-learn/sklearn/covariance/tests/test_covariance.py,194,"Large-Dimensional Covariance Matrices""",
scikit-learn/sklearn/covariance/tests/test_covariance.py,195,beta and delta are given in the beginning of section 3.2,
scikit-learn/sklearn/covariance/tests/test_covariance.py,212,Compare our blocked implementation to the naive implementation,
scikit-learn/sklearn/covariance/tests/test_covariance.py,222,test that ledoit_wolf doesn't error on data that is wider than block_size,
scikit-learn/sklearn/covariance/tests/test_covariance.py,224,use a number of features that is larger than the block-size,
scikit-learn/sklearn/covariance/tests/test_covariance.py,227,check that covariance is about diagonal (random normal noise),
scikit-learn/sklearn/covariance/tests/test_covariance.py,231,check that the result is consistent with not splitting data into blocks.,
scikit-learn/sklearn/covariance/tests/test_covariance.py,237,Tests OAS module on a simple dataset.,
scikit-learn/sklearn/covariance/tests/test_covariance.py,238,test shrinkage coeff on a simple data set,
scikit-learn/sklearn/covariance/tests/test_covariance.py,244,compare shrunk covariance obtained from data and from MLE estimate,
scikit-learn/sklearn/covariance/tests/test_covariance.py,249,compare estimates given by OAS and ShrunkCovariance,
scikit-learn/sklearn/covariance/tests/test_covariance.py,254,test with n_features = 1,
scikit-learn/sklearn/covariance/tests/test_covariance.py,263,test shrinkage coeff on a simple data set (without saving precision),
scikit-learn/sklearn/covariance/tests/test_covariance.py,269,Same tests without assuming centered data--------------------------------,
scikit-learn/sklearn/covariance/tests/test_covariance.py,270,test shrinkage coeff on a simple data set,
scikit-learn/sklearn/covariance/tests/test_covariance.py,275,compare shrunk covariance obtained from data and from MLE estimate,
scikit-learn/sklearn/covariance/tests/test_covariance.py,279,compare estimates given by OAS and ShrunkCovariance,
scikit-learn/sklearn/covariance/tests/test_covariance.py,284,test with n_features = 1,
scikit-learn/sklearn/covariance/tests/test_covariance.py,293,test with one sample,
scikit-learn/sklearn/covariance/tests/test_covariance.py,294,warning should be raised when using only 1 sample,
scikit-learn/sklearn/covariance/tests/test_covariance.py,301,test shrinkage coeff on a simple data set (without saving precision),
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,1,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,2,Gael Varoquaux <gael.varoquaux@normalesup.org>,
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,3,Virgile Fritsch <virgile.fritsch@inria.fr>,
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,4,,
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,5,License: BSD 3 clause,
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,25,Tests the FastMCD algorithm implementation,
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,26,Small data set,
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,27,test without outliers (random independent normal data),
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,29,test with a contaminated data set (medium contamination),
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,31,test with a contaminated data set (strong contamination),
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,34,Medium data set,
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,37,Large data set,
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,40,1D data set,
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,62,add some outliers,
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,71,compute MCD by fitting an object,
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,76,compare with the estimates learnt from the inliers,
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,86,"Check that the code does not break with X.shape = (3, 1)",
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,87,(i.e. n_support = n_samples),
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,95,Check that MCD completes when the covariance matrix is singular,
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,96,i.e. one of the rows and columns are all zeros,
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,99,Think of these as the values for X and Y -> 10 values between -5 and 5,
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,101,Get the cartesian product of all possible coordinate pairs from above set,
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,104,Add a third column that's all zeros to make our data a set of point,
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,105,"within a plane, which means that the covariance matrix will be singular",
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,108,The below line of code should raise an exception if the covariance matrix,
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,109,"is singular. As a further test, since we have points in XYZ, the",
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,110,principle components (Eigenvectors) of these directly relate to the,
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,111,"geometry of the points. Since it's a plane, we should be able to test",
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,112,that the Eigenvector that corresponds to the smallest Eigenvalue is the,
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,113,"plane normal, specifically [0, 0, 1], since everything is in the XY plane",
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,114,(as I've set it up above). To do this one would start by:,
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,115,,
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,116,"evals, evecs = np.linalg.eigh(mcd_fit.covariance_)",
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,117,"normal = evecs[:, np.argmin(evals)]",
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,118,,
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,119,"After which we need to assert that our `normal` is equal to [0, 0, 1].",
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,120,"Do note that there is floating point error associated with this, so it's",
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,121,best to subtract the two and then compare some small tolerance (e.g.,
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,122,1e-12).,
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,127,Check that MCD returns a ValueError with informative message when the,
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,128,covariance of the support data is equal to 0.,
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,140,Check that a warning is raised if we observe increasing determinants,
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,141,during the c_step. In theory the sequence of determinants should be,
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,142,decreasing. Increasing determinants are likely due to ill-conditioned,
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,143,covariance matrices that result in poor precision matrices.,
scikit-learn/sklearn/feature_selection/_from_model.py,1,"Authors: Gilles Louppe, Mathieu Blondel, Maheshakya Wijewardena",
scikit-learn/sklearn/feature_selection/_from_model.py,2,License: BSD 3 clause,
scikit-learn/sklearn/feature_selection/_from_model.py,43,determine default from estimator,
scikit-learn/sklearn/feature_selection/_from_model.py,47,the natural default threshold is 0 when l1 penalty was used,
scikit-learn/sklearn/feature_selection/_from_model.py,171,SelectFromModel can directly call on transform.,
scikit-learn/sklearn/feature_selection/_from_model.py,261,For consistency with other estimators we raise a AttributeError so,
scikit-learn/sklearn/feature_selection/_from_model.py,262,that hasattr() fails if the estimator isn't fitted.,
scikit-learn/sklearn/feature_selection/_mutual_info.py,1,Author: Nikolay Mayorov <n59_ru@hotmail.com>,
scikit-learn/sklearn/feature_selection/_mutual_info.py,2,License: 3-clause BSD,
scikit-learn/sklearn/feature_selection/_mutual_info.py,53,Here we rely on NearestNeighbors to select the fastest algorithm.,
scikit-learn/sklearn/feature_selection/_mutual_info.py,60,Algorithm is selected explicitly to allow passing an array as radius,
scikit-learn/sklearn/feature_selection/_mutual_info.py,61,later (not all algorithms support this).,
scikit-learn/sklearn/feature_selection/_mutual_info.py,129,Ignore points with unique labels.,
scikit-learn/sklearn/feature_selection/_mutual_info.py,277,Add small noise to continuous features as advised in Kraskov et. al.,
scikit-learn/sklearn/feature_selection/_rfe.py,1,Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>,
scikit-learn/sklearn/feature_selection/_rfe.py,2,Vincent Michel <vincent.michel@inria.fr>,
scikit-learn/sklearn/feature_selection/_rfe.py,3,Gilles Louppe <g.louppe@gmail.com>,
scikit-learn/sklearn/feature_selection/_rfe.py,4,,
scikit-learn/sklearn/feature_selection/_rfe.py,5,License: BSD 3 clause,
scikit-learn/sklearn/feature_selection/_rfe.py,154,Parameter step_score controls the calculation of self.scores_,
scikit-learn/sklearn/feature_selection/_rfe.py,155,step_score is not exposed to users,
scikit-learn/sklearn/feature_selection/_rfe.py,156,and is used when implementing RFECV,
scikit-learn/sklearn/feature_selection/_rfe.py,157,self.scores_ will not be calculated when calling _fit through fit,
scikit-learn/sklearn/feature_selection/_rfe.py,166,Initialization,
scikit-learn/sklearn/feature_selection/_rfe.py,186,Elimination,
scikit-learn/sklearn/feature_selection/_rfe.py,188,Remaining features,
scikit-learn/sklearn/feature_selection/_rfe.py,191,Rank the remaining features,
scikit-learn/sklearn/feature_selection/_rfe.py,198,Get coefs,
scikit-learn/sklearn/feature_selection/_rfe.py,208,Get ranks,
scikit-learn/sklearn/feature_selection/_rfe.py,214,for sparse case ranks is matrix,
scikit-learn/sklearn/feature_selection/_rfe.py,217,Eliminate the worse features,
scikit-learn/sklearn/feature_selection/_rfe.py,220,Compute step score on the previous selection iteration,
scikit-learn/sklearn/feature_selection/_rfe.py,221,because 'estimator' must use features,
scikit-learn/sklearn/feature_selection/_rfe.py,222,that have not been eliminated yet,
scikit-learn/sklearn/feature_selection/_rfe.py,228,Set final attributes,
scikit-learn/sklearn/feature_selection/_rfe.py,233,Compute step score when only n_features_to_select features left,
scikit-learn/sklearn/feature_selection/_rfe.py,506,Initialization,
scikit-learn/sklearn/feature_selection/_rfe.py,518,"Build an RFE object, which will evaluate and score each possible",
scikit-learn/sklearn/feature_selection/_rfe.py,519,"feature count, down to self.min_features_to_select",
scikit-learn/sklearn/feature_selection/_rfe.py,524,Determine the number of subsets of features by fitting across,
scikit-learn/sklearn/feature_selection/_rfe.py,525,"the train folds and choosing the ""features_to_select"" parameter",
scikit-learn/sklearn/feature_selection/_rfe.py,526,that gives the least averaged error across all folds.,
scikit-learn/sklearn/feature_selection/_rfe.py,528,Note that joblib raises a non-picklable error for bound methods,
scikit-learn/sklearn/feature_selection/_rfe.py,529,even if n_jobs is set to 1 with the default multiprocessing,
scikit-learn/sklearn/feature_selection/_rfe.py,530,backend.,
scikit-learn/sklearn/feature_selection/_rfe.py,531,This branching is done so that to,
scikit-learn/sklearn/feature_selection/_rfe.py,532,make sure that user code that sets n_jobs to 1,
scikit-learn/sklearn/feature_selection/_rfe.py,533,and provides bound methods as scorers is not broken with the,
scikit-learn/sklearn/feature_selection/_rfe.py,534,addition of n_jobs parameter in version 0.18.,
scikit-learn/sklearn/feature_selection/_rfe.py,553,Re-execute an elimination with best_k over the whole set,
scikit-learn/sklearn/feature_selection/_rfe.py,560,Set final attributes,
scikit-learn/sklearn/feature_selection/_rfe.py,567,"Fixing a normalization error, n is equal to get_n_splits(X, y) - 1",
scikit-learn/sklearn/feature_selection/_rfe.py,568,"here, the scores are normalized by get_n_splits(X, y)",
scikit-learn/sklearn/feature_selection/_variance_threshold.py,1,Author: Lars Buitinck,
scikit-learn/sklearn/feature_selection/_variance_threshold.py,2,License: 3-clause BSD,
scikit-learn/sklearn/feature_selection/_variance_threshold.py,71,sparse matrix,
scikit-learn/sklearn/feature_selection/_variance_threshold.py,82,Use peak-to-peak to avoid numeric precision issues,
scikit-learn/sklearn/feature_selection/_variance_threshold.py,83,for constant features,
scikit-learn/sklearn/feature_selection/_base.py,1,-*- coding: utf-8 -*-,
scikit-learn/sklearn/feature_selection/_base.py,4,"Authors: G. Varoquaux, A. Gramfort, L. Buitinck, J. Nothman",
scikit-learn/sklearn/feature_selection/_base.py,5,License: BSD 3 clause,
scikit-learn/sklearn/feature_selection/_base.py,104,insert additional entries in indptr:,
scikit-learn/sklearn/feature_selection/_base.py,105,e.g. if transform changed indptr from [0 2 6 7] to [0 2 3],
scikit-learn/sklearn/feature_selection/_base.py,106,col_nonzeros here will be [2 0 1] so indptr becomes [0 2 2 3],
scikit-learn/sklearn/feature_selection/_univariate_selection.py,3,"Authors: V. Michel, B. Thirion, G. Varoquaux, A. Gramfort, E. Duchesnay.",
scikit-learn/sklearn/feature_selection/_univariate_selection.py,4,"L. Buitinck, A. Joly",
scikit-learn/sklearn/feature_selection/_univariate_selection.py,5,License: BSD 3 clause,
scikit-learn/sklearn/feature_selection/_univariate_selection.py,29,XXX where should this function be called? fit? scoring functions,
scikit-learn/sklearn/feature_selection/_univariate_selection.py,30,themselves?,
scikit-learn/sklearn/feature_selection/_univariate_selection.py,36,,
scikit-learn/sklearn/feature_selection/_univariate_selection.py,37,Scoring functions,
scikit-learn/sklearn/feature_selection/_univariate_selection.py,40,The following function is a rewriting of scipy.stats.f_oneway,
scikit-learn/sklearn/feature_selection/_univariate_selection.py,41,Contrary to the scipy.stats.f_oneway implementation it does not,
scikit-learn/sklearn/feature_selection/_univariate_selection.py,42,copy the data while keeping the inputs unchanged.,
scikit-learn/sklearn/feature_selection/_univariate_selection.py,117,flatten matrix to vector in sparse case,
scikit-learn/sklearn/feature_selection/_univariate_selection.py,163,Reuse f_obs for chi-squared statistics,
scikit-learn/sklearn/feature_selection/_univariate_selection.py,213,XXX: we might want to do some of the following in logspace instead for,
scikit-learn/sklearn/feature_selection/_univariate_selection.py,214,numerical stability.,
scikit-learn/sklearn/feature_selection/_univariate_selection.py,223,n_classes * n_features,
scikit-learn/sklearn/feature_selection/_univariate_selection.py,283,compute centered values,
scikit-learn/sklearn/feature_selection/_univariate_selection.py,284,"note that E[(x - mean(x))*(y - mean(y))] = E[x*(y - mean(y))], so we",
scikit-learn/sklearn/feature_selection/_univariate_selection.py,285,need not center X,
scikit-learn/sklearn/feature_selection/_univariate_selection.py,292,compute the scaled standard deviations via moments,
scikit-learn/sklearn/feature_selection/_univariate_selection.py,298,compute the correlation,
scikit-learn/sklearn/feature_selection/_univariate_selection.py,303,convert to p-value,
scikit-learn/sklearn/feature_selection/_univariate_selection.py,310,,
scikit-learn/sklearn/feature_selection/_univariate_selection.py,311,Base classes,
scikit-learn/sklearn/feature_selection/_univariate_selection.py,367,,
scikit-learn/sklearn/feature_selection/_univariate_selection.py,368,Specific filters,
scikit-learn/sklearn/feature_selection/_univariate_selection.py,369,,
scikit-learn/sklearn/feature_selection/_univariate_selection.py,436,Cater for NaNs,
scikit-learn/sklearn/feature_selection/_univariate_selection.py,529,Request a stable sort. Mergesort takes more memory (~40MB per,
scikit-learn/sklearn/feature_selection/_univariate_selection.py,530,megafeature on x86-64).,
scikit-learn/sklearn/feature_selection/_univariate_selection.py,726,,
scikit-learn/sklearn/feature_selection/_univariate_selection.py,727,Generic filter,
scikit-learn/sklearn/feature_selection/_univariate_selection.py,728,,
scikit-learn/sklearn/feature_selection/_univariate_selection.py,730,"TODO this class should fit on either p-values or scores,",
scikit-learn/sklearn/feature_selection/_univariate_selection.py,731,depending on the mode.,
scikit-learn/sklearn/feature_selection/_univariate_selection.py,799,Now perform some acrobatics to set the right named parameter in,
scikit-learn/sklearn/feature_selection/_univariate_selection.py,800,the selector,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,26,,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,27,Test the score functions,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,30,Test that our f_oneway gives the same result as scipy.stats,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,41,Smoke test f_oneway on integers: that it does raise casting errors,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,42,with recent numpys,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,48,test that is gives the same result as with float,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,55,Test whether the F test yields meaningful results,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,56,on a simple simulated classification problem,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,75,Test whether the F test yields meaningful results,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,76,on a simple simulated regression problem,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,87,"with centering, compare with sparse",
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,93,"again without centering, compare with sparse",
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,101,Test whether f_regression returns the same value,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,102,for any numeric data_type,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,114,Test whether f_regression preserves dof according to 'center' argument,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,115,We use two centered variates so we have a simple relationship between,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,116,F-score with variates centering and F-score without variates centering.,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,117,Create toy example,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,118,X has zero mean,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,122,have Y mean being null,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,127,value from statsmodels OLS,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,131,Test whether the F test yields meaningful results,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,132,on a simple simulated classification problem,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,148,Test whether the relative univariate feature selection,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,149,gets the correct items in a simple classification problem,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,150,with the percentile heuristic,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,169,Test whether the relative univariate feature selection,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,170,gets the correct items in a simple classification problem,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,171,with the percentile heuristic,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,193,Check other columns are empty,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,197,,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,198,Test univariate selection in classification settings,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,201,Test whether the relative univariate feature selection,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,202,gets the correct items in a simple classification problem,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,203,with the k best heuristic,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,222,"Test whether k=""all"" correctly returns all features.",
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,232,Test whether k=0 correctly returns no features.,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,247,Test whether the relative univariate feature selection,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,248,gets the correct items in a simple classification problem,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,249,"with the fdr, fwe and fpr heuristics",
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,268,,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,269,Test univariate selection in regression settings,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,280,Test whether the relative univariate feature selection,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,281,gets the correct items in a simple regression problem,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,282,with the percentile heuristic,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,299,Check inverse_transform respects dtype,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,305,Test whether the relative univariate feature selection,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,306,selects all features when '100%' is asked.,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,336,Test whether the relative univariate feature selection,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,337,gets the correct items in a simple regression problem,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,338,with the k best heuristic,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,355,Test whether the relative univariate feature selection,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,356,gets the correct items in a simple regression problem,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,357,"with the fpr, fdr or fwe heuristics",
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,375,"Test boundary case, and always aim to select 1 feature.",
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,411,Test that fdr heuristic actually has low FDR.,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,418,Warnings can be raised when no features are selected,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,419,(low alpha or very noisy data),
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,436,"As per Benjamini-Hochberg, the expected false discovery rate",
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,437,should be lower than alpha:,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,438,FDR = E(FP / (TP + FP)) <= alpha,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,444,Make sure that the empirical false discovery rate increases,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,445,with alpha:,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,451,Test whether the relative univariate feature selection,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,452,gets the correct items in a simple regression problem,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,453,with the fwe heuristic,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,470,Test whether SelectKBest actually selects k features in case of ties.,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,471,"Prior to 0.11, SelectKBest would return more features than requested.",
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,488,Test if SelectPercentile selects the right n_features in case of ties.,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,505,Test whether k-best and percentiles work with tied pvalues from chi2.,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,506,"chi2 will return the same p-values for the following features, but it",
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,507,will return different scores.,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,523,Test whether k-best and percentiles works with multilabels with chi2.,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,538,Test for stable sorting in k-best with tied scores.,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,549,Assert that SelectKBest and SelectPercentile can handle NaNs.,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,550,First feature has zero variance to confuse f_classif (ANOVA) and,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,551,make it return a NaN.,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,586,Test that f_classif warns if a feature is constant throughout.,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,596,Generate random uncorrelated data: a strict univariate test should,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,597,rejects all the features,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,621,Test in KBest mode.,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,632,Test in Percentile mode.,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,648,Test in KBest mode.,
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,660,Test in Percentile mode.,
scikit-learn/sklearn/feature_selection/tests/test_chi2.py,18,Feature 0 is highly informative for class 1;,
scikit-learn/sklearn/feature_selection/tests/test_chi2.py,19,feature 1 is the same everywhere;,
scikit-learn/sklearn/feature_selection/tests/test_chi2.py,20,feature 2 is a bit informative for class 2.,
scikit-learn/sklearn/feature_selection/tests/test_chi2.py,34,Test Chi2 feature extraction,
scikit-learn/sklearn/feature_selection/tests/test_chi2.py,50,== doesn't work on scipy.sparse matrices,
scikit-learn/sklearn/feature_selection/tests/test_chi2.py,57,Check that chi2 works with a COO matrix,
scikit-learn/sklearn/feature_selection/tests/test_chi2.py,58,"(as returned by CountVectorizer, DictVectorizer)",
scikit-learn/sklearn/feature_selection/tests/test_chi2.py,61,"if we got here without an exception, we're safe",
scikit-learn/sklearn/feature_selection/tests/test_chi2.py,65,Check for proper error on negative numbers in the input X.,
scikit-learn/sklearn/feature_selection/tests/test_chi2.py,73,Unused feature should evaluate to NaN,
scikit-learn/sklearn/feature_selection/tests/test_chi2.py,74,and should issue no runtime warning,
scikit-learn/sklearn/feature_selection/tests/test_chi2.py,86,Test replacement for scipy.stats.chisquare against the original.,
scikit-learn/sklearn/feature_selection/tests/test_chi2.py,91,call SciPy first because our version overwrites obs,
scikit-learn/sklearn/feature_selection/tests/test_variance_threshold.py,17,"Test VarianceThreshold with default setting, zero variance.",
scikit-learn/sklearn/feature_selection/tests/test_variance_threshold.py,30,Test VarianceThreshold with custom variance.,
scikit-learn/sklearn/feature_selection/tests/test_variance_threshold.py,40,Test that VarianceThreshold(0.0).fit eliminates features that have,
scikit-learn/sklearn/feature_selection/tests/test_variance_threshold.py,41,"the same value in every sample, even when floating point errors",
scikit-learn/sklearn/feature_selection/tests/test_variance_threshold.py,42,cause np.var not to be 0 for the feature.,
scikit-learn/sklearn/feature_selection/tests/test_variance_threshold.py,43,See #13691,
scikit-learn/sklearn/feature_selection/tests/test_variance_threshold.py,53,add single NaN and feature should still be included,
scikit-learn/sklearn/feature_selection/tests/test_variance_threshold.py,55,make all values in feature NaN and feature should be rejected,
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,14,In discrete case computations are straightforward and can be done,
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,15,by hand on given vectors.,
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,27,For two continuous variables a good approach is to test on bivariate,
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,28,"normal distribution, where mutual information is known.",
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,30,"Mean of the distribution, irrelevant for mutual information.",
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,33,Setup covariance matrix with correlation coeff. equal 0.5.,
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,42,True theoretical mutual information.,
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,51,"Theory and computed values won't be very close, assert that the",
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,52,first figures after decimal point match.,
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,59,To test define a joint distribution as follows:,
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,60,"p(x, y) = p(x) p(y | x)",
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,61,X ~ Bernoulli(p),
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,62,"(Y | x = 0) ~ Uniform(-1, 1)",
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,63,"(Y | x = 1) ~ Uniform(0, 2)",
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,65,Use the following formula for mutual information:,
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,66,I(X; Y) = H(Y) - H(Y | X),
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,67,Two entropies can be computed by hand:,
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,68,H(Y) = -(1-p)/2 * ln((1-p)/2) - p/2*log(p/2) - 1/2*log(1/2),
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,69,H(Y | X) = ln(2),
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,71,"Now we need to implement sampling from out distribution, which is",
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,72,done easily using conditional distribution logic.,
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,88,Assert the same tolerance.,
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,95,Test that adding unique label doesn't change MI.,
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,113,We are going test that feature ordering by MI matches our expectations.,
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,122,"Here X[:, 0] is the most informative feature, and X[:, 1] is weakly",
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,123,informative.,
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,129,"We generate sample from multivariate normal distribution, using",
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,130,transformation from initially uncorrelated variables. The zero,
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,131,"variables after transformation is selected as the target vector,",
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,132,"it has the strongest correlation with the variable 2, and",
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,133,the weakest correlation with the variable 1.,
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,153,Here the target is discrete and there are two continuous and one,
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,154,discrete feature. The idea of this test is clear from the code.,
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,167,Check that the continuous values have an higher MI with greater,
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,168,n_neighbors,
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,171,The n_neighbors should not have any effect on the discrete value,
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,172,The MI should be the same,
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,13,noqa,
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,51,Test that SelectFromModel fits on a clone of the estimator.,
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,94,Test max_features parameter using various values,
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,110,Test max_features against actual model.,
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,132,Test if max_features can break tie among feature importance,
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,193,Ensure sample weights are passed to underlying estimator,
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,198,Check with sample weights,
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,219,"For the Lasso and related models, the threshold defaults to 1e-5",
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,237,Fit SelectFromModel a multi-class problem,
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,246,Manually check that the norm is correctly performed,
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,269,"check that if est doesn't have partial_fit, neither does SelectFromModel",
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,284,Test all possible combinations of the prefit parameter.,
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,286,Passing a prefit parameter with the selected model,
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,287,and fitting a unfit model with prefit=False should give same results.,
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,297,Check that the model is rewritten if prefit=False and a fitted model is,
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,298,passed,
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,303,Check that prefit=True and calling fit raises a ValueError,
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,315,Calculate the threshold from the estimator directly.,
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,323,Test that the threshold can be set without refitting the model.,
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,330,Set a higher threshold to filter out more features.,
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,336,Test that fit doesn't check for np.inf and np.nan values.,
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,349,Test that transform doesn't check for np.inf and np.nan values.,
scikit-learn/sklearn/feature_selection/tests/test_base.py,48,Check dtype matches,
scikit-learn/sklearn/feature_selection/tests/test_base.py,52,Check 1d list and other dtype:,
scikit-learn/sklearn/feature_selection/tests/test_base.py,56,Check wrong shape raises error,
scikit-learn/sklearn/feature_selection/tests/test_base.py,69,Check dtype matches,
scikit-learn/sklearn/feature_selection/tests/test_base.py,73,Check wrong shape raises error,
scikit-learn/sklearn/feature_selection/tests/test_base.py,83,Check dtype matches,
scikit-learn/sklearn/feature_selection/tests/test_base.py,89,Check 1d list and other dtype:,
scikit-learn/sklearn/feature_selection/tests/test_base.py,93,Check wrong shape raises error,
scikit-learn/sklearn/feature_selection/tests/test_base.py,104,Check dtype matches,
scikit-learn/sklearn/feature_selection/tests/test_base.py,110,Check wrong shape raises error,
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,74,Check if the supports are equal,
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,85,dense model,
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,93,sparse model,
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,113,dense model,
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,127,regression test: list should be supported,
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,129,Test using the score function,
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,132,non-regression test for missing worst feature:,
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,137,All the noisy variable were filtered out,
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,140,same in sparse,
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,147,Test using a customized loss function,
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,154,Test using a scorer,
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,161,Test fix on grid_scores,
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,167,"In the event of cross validation score ties, the expected behavior of",
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,168,RFECV is to return the FEWEST features that maximize the CV score.,
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,169,"Because test_scorer always returns 1.0 in this example, RFECV should",
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,170,reduce the dimensionality to a single feature (i.e. n_features_ = 1),
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,173,"Same as the first two tests, but with step=2",
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,187,Verifying that steps < 1 don't blow up.,
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,199,regression test: list should be supported,
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,201,Test using the score function,
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,204,non-regression test for missing worst feature:,
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,210,Check verbose=1 is producing an output.,
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,232,regression test: list should be supported,
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,234,Non-regression test for varying combinations of step and,
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,235,min_features_to_select.,
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,251,make sure that cross-validation is stratified,
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,263,Test when floor(step * n_features) <= 0,
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,268,"Test when step is between (0,1) and floor(step * n_features) > 0",
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,273,Test when step is an integer,
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,280,"In RFE, 'number_of_subsets_of_features'",
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,281,= the number of iterations in '_fit',
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,282,= max(ranking_),
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,283,= 1 + (n_features + step - n_features_to_select - 1) // step,
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,284,"After optimization #4534, this number",
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,285,= 1 + np.ceil((n_features - n_features_to_select) / float(step)),
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,286,"This test case is to test their equivalence, refer to #4534 and #3824",
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,294,RFE,
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,295,"Case 1, n_features - n_features_to_select is divisible by step",
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,296,"Case 2, n_features - n_features_to_select is not divisible by step",
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,308,this number also equals to the maximum of ranking_,
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,314,"In RFECV, 'fit' calls 'RFE._fit'",
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,315,'number_of_subsets_of_features' of RFE,
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,316,= the size of 'grid_scores' of RFECV,
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,317,= the number of iterations of the for loop before optimization #4534,
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,319,"RFECV, n_features_to_select = 1",
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,320,"Case 1, n_features - 1 is divisible by step",
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,321,"Case 2, n_features - 1 is not divisible by step",
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,383,add nan and inf value to X,
scikit-learn/maint_tools/test_docstrings.py,10,List of whitelisted modules and methods; regexp are supported.,
scikit-learn/maint_tools/test_docstrings.py,38,$ to avoid match w/ predict_proba (regex),
scikit-learn/maint_tools/test_docstrings.py,58,skip private classes,
scikit-learn/maint_tools/test_docstrings.py,80,"We ignore following error code,",
scikit-learn/maint_tools/test_docstrings.py,81,- RT02: The first line of the Returns section,
scikit-learn/maint_tools/test_docstrings.py,82,"should contain only the type, ..",
scikit-learn/maint_tools/test_docstrings.py,83,(as we may need refer to the name of the returned,
scikit-learn/maint_tools/test_docstrings.py,84,object),
scikit-learn/maint_tools/test_docstrings.py,85,- GL01: Docstring text (summary) should start in the line,
scikit-learn/maint_tools/test_docstrings.py,86,"immediately after the opening quotes (not in the same line,",
scikit-learn/maint_tools/test_docstrings.py,87,or leaving a blank line in between),
scikit-learn/maint_tools/test_docstrings.py,92,Following codes are only taken into account for the,
scikit-learn/maint_tools/test_docstrings.py,93,top level class docstrings:,
scikit-learn/maint_tools/test_docstrings.py,94,- ES01: No extended summary found,
scikit-learn/maint_tools/test_docstrings.py,95,- SA01: See Also section not found,
scikit-learn/maint_tools/test_docstrings.py,96,- EX01: No examples section found,
scikit-learn/maint_tools/test_docstrings.py,135,In particular we can't parse the signature of properties,
scikit-learn/maint_tools/test_docstrings.py,201,"When applied to classes, detect class method. For functions",
scikit-learn/maint_tools/test_docstrings.py,202,method = None.,
scikit-learn/maint_tools/test_docstrings.py,203,TODO: this detection can be improved. Currently we assume that we have,
scikit-learn/maint_tools/test_docstrings.py,204,class # methods if the second path element before last is in camel case.,
scikit-learn/maint_tools/check_pxd_in_installation.py,27,A cython test file which cimports all modules corresponding to found,
scikit-learn/maint_tools/check_pxd_in_installation.py,28,pxd files.,
scikit-learn/maint_tools/check_pxd_in_installation.py,29,e.g. sklearn/tree/_utils.pxd becomes `cimport sklearn.tree._utils`,
scikit-learn/maint_tools/check_pxd_in_installation.py,37,A basic setup file to build the test file.,
scikit-learn/maint_tools/check_pxd_in_installation.py,38,We set the language to c++ and we use numpy.get_include() because,
scikit-learn/maint_tools/check_pxd_in_installation.py,39,some modules require it.,
scikit-learn/maint_tools/sort_whats_new.py,1,!/usr/bin/env python,
scikit-learn/maint_tools/sort_whats_new.py,2,Sorts what's new entries with per-module headings.,
scikit-learn/maint_tools/sort_whats_new.py,3,Pass what's new entries on stdin.,
scikit-learn/maint_tools/sort_whats_new.py,20,discard headings and other non-entry lines,
