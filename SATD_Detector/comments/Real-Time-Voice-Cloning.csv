file path,line #,comment,satd
Real-Time-Voice-Cloning/synthesizer_train.py,38,Was 5000,
Real-Time-Voice-Cloning/synthesizer_train.py,40,Was 10000,
Real-Time-Voice-Cloning/synthesizer_train.py,42,Was 100000,
Real-Time-Voice-Cloning/vocoder_train.py,42,Process the arguments,
Real-Time-Voice-Cloning/vocoder_train.py,53,Run the training,
Real-Time-Voice-Cloning/encoder_train.py,41,Process the arguments,
Real-Time-Voice-Cloning/encoder_train.py,44,Run the training,
Real-Time-Voice-Cloning/synthesizer_preprocess_embeds.py,23,Preprocess the dataset,
Real-Time-Voice-Cloning/encoder_preprocess.py,42,Process the arguments,
Real-Time-Voice-Cloning/encoder_preprocess.py,49,Preprocess the datasets,
Real-Time-Voice-Cloning/synthesizer_preprocess_audio.py,29,Process the arguments,
Real-Time-Voice-Cloning/synthesizer_preprocess_audio.py,33,Create directories,
Real-Time-Voice-Cloning/synthesizer_preprocess_audio.py,37,Preprocess the dataset,
Real-Time-Voice-Cloning/demo_toolbox.py,30,Launch the toolbox,
Real-Time-Voice-Cloning/demo_cli.py,15,Info & args,
Real-Time-Voice-Cloning/demo_cli.py,39,Print some environment information (for debugging purposes),
Real-Time-Voice-Cloning/demo_cli.py,59,Load the models one by one.,
Real-Time-Voice-Cloning/demo_cli.py,66,Run a test,
Real-Time-Voice-Cloning/demo_cli.py,68,Forward an audio waveform of zeroes that lasts 1 second. Notice how we can get the encoder's,
Real-Time-Voice-Cloning/demo_cli.py,69,"sampling rate, which may differ.",
Real-Time-Voice-Cloning/demo_cli.py,70,"If you're unfamiliar with digital audio, know that it is encoded as an array of floats",
Real-Time-Voice-Cloning/demo_cli.py,71,"(or sometimes integers, but mostly floats in this projects) ranging from -1 to 1.",
Real-Time-Voice-Cloning/demo_cli.py,72,"The sampling rate is the number of values (samples) recorded per second, it is set to",
Real-Time-Voice-Cloning/demo_cli.py,73,16000 for the encoder. Creating an array of length <sampling_rate> will always correspond,
Real-Time-Voice-Cloning/demo_cli.py,74,to an audio of 1 second.,
Real-Time-Voice-Cloning/demo_cli.py,78,Create a dummy embedding. You would normally use the embedding that encoder.embed_utterance,
Real-Time-Voice-Cloning/demo_cli.py,79,"returns, but here we're going to make one ourselves just for the sake of showing that it's",
Real-Time-Voice-Cloning/demo_cli.py,80,possible.,
Real-Time-Voice-Cloning/demo_cli.py,82,"Embeddings are L2-normalized (this isn't important here, but if you want to make your own",
Real-Time-Voice-Cloning/demo_cli.py,83,embeddings it will be).,
Real-Time-Voice-Cloning/demo_cli.py,85,The synthesizer can handle multiple inputs with batching. Let's create another embedding to,
Real-Time-Voice-Cloning/demo_cli.py,86,illustrate that,
Real-Time-Voice-Cloning/demo_cli.py,92,"The vocoder synthesizes one waveform at a time, but it's more efficient for long ones. We",
Real-Time-Voice-Cloning/demo_cli.py,93,can concatenate the mel spectrograms to a single one.,
Real-Time-Voice-Cloning/demo_cli.py,95,The vocoder can take a callback function to display the generation. More on that later. For,
Real-Time-Voice-Cloning/demo_cli.py,96,now we'll simply hide it like this:,
Real-Time-Voice-Cloning/demo_cli.py,99,"For the sake of making this test short, we'll pass a short target length. The target length",
Real-Time-Voice-Cloning/demo_cli.py,100,is the length of the wav segments that are processed in parallel. E.g. for audio sampled,
Real-Time-Voice-Cloning/demo_cli.py,101,"at 16000 Hertz, a target length of 8000 means that the target audio will be cut in chunks of",
Real-Time-Voice-Cloning/demo_cli.py,102,"0.5 seconds which will all be generated together. The parameters here are absurdly short, and",
Real-Time-Voice-Cloning/demo_cli.py,103,that has a detrimental effect on the quality of the audio. The default parameters are,
Real-Time-Voice-Cloning/demo_cli.py,104,recommended in general.,
Real-Time-Voice-Cloning/demo_cli.py,110,Interactive speech generation,
Real-Time-Voice-Cloning/demo_cli.py,119,Get the reference audio filepath,
Real-Time-Voice-Cloning/demo_cli.py,125,Computing the embedding,
Real-Time-Voice-Cloning/demo_cli.py,126,"First, we load the wav using the function that the speaker encoder provides. This is",
Real-Time-Voice-Cloning/demo_cli.py,127,important: there is preprocessing that must be applied.,
Real-Time-Voice-Cloning/demo_cli.py,129,The following two methods are equivalent:,
Real-Time-Voice-Cloning/demo_cli.py,130,- Directly load from the filepath:,
Real-Time-Voice-Cloning/demo_cli.py,132,- If the wav is already loaded:,
Real-Time-Voice-Cloning/demo_cli.py,137,Then we derive the embedding. There are many functions and parameters that the,
Real-Time-Voice-Cloning/demo_cli.py,138,speaker encoder interfaces. These are mostly for in-depth research. You will typically,
Real-Time-Voice-Cloning/demo_cli.py,139,only use this function (with its default parameters):,
Real-Time-Voice-Cloning/demo_cli.py,144,Generating the spectrogram,
Real-Time-Voice-Cloning/demo_cli.py,147,"The synthesizer works in batch, so you need to put your data in a list or numpy array",
Real-Time-Voice-Cloning/demo_cli.py,150,"If you know what the attention layer alignments are, you can retrieve them here by",
Real-Time-Voice-Cloning/demo_cli.py,151,passing return_alignments=True,
Real-Time-Voice-Cloning/demo_cli.py,157,Generating the waveform,
Real-Time-Voice-Cloning/demo_cli.py,159,Synthesizing the waveform is fairly straightforward. Remember that the longer the,
Real-Time-Voice-Cloning/demo_cli.py,160,"spectrogram, the more time-efficient the vocoder.",
Real-Time-Voice-Cloning/demo_cli.py,164,Post-generation,
Real-Time-Voice-Cloning/demo_cli.py,165,"There's a bug with sounddevice that makes the audio cut one second earlier, so we",
Real-Time-Voice-Cloning/demo_cli.py,166,pad it.,
Real-Time-Voice-Cloning/demo_cli.py,169,Play the audio (non-blocking),
Real-Time-Voice-Cloning/demo_cli.py,174,Save it on the disk,
Real-Time-Voice-Cloning/toolbox/ui.py,12,"from sklearn.manifold import TSNE         # You can try with TSNE if you like, I prefer UMAP",
Real-Time-Voice-Cloning/toolbox/ui.py,65,Embedding,
Real-Time-Voice-Cloning/toolbox/ui.py,66,Clear the plot,
Real-Time-Voice-Cloning/toolbox/ui.py,71,Draw the embed,
Real-Time-Voice-Cloning/toolbox/ui.py,83,Spectrogram,
Real-Time-Voice-Cloning/toolbox/ui.py,84,Draw the spectrogram,
Real-Time-Voice-Cloning/toolbox/ui.py,88,"spec_ax.figure.colorbar(mappable=im, shrink=0.65, orientation=""horizontal"",",
Real-Time-Voice-Cloning/toolbox/ui.py,89,spec_ax=spec_ax),
Real-Time-Voice-Cloning/toolbox/ui.py,105,Display a message if there aren't enough points,
Real-Time-Voice-Cloning/toolbox/ui.py,112,Compute the projections,
Real-Time-Voice-Cloning/toolbox/ui.py,120,reducer = TSNE(),
Real-Time-Voice-Cloning/toolbox/ui.py,131,"self.umap_ax.set_title(""UMAP projections"")",
Real-Time-Voice-Cloning/toolbox/ui.py,134,Draw the plot,
Real-Time-Voice-Cloning/toolbox/ui.py,211,Select a random dataset,
Real-Time-Voice-Cloning/toolbox/ui.py,236,Select a random speaker,
Real-Time-Voice-Cloning/toolbox/ui.py,242,Select a random utterance,
Real-Time-Voice-Cloning/toolbox/ui.py,272,Encoder,
Real-Time-Voice-Cloning/toolbox/ui.py,278,Synthesizer,
Real-Time-Voice-Cloning/toolbox/ui.py,287,Vocoder,
Real-Time-Voice-Cloning/toolbox/ui.py,343,Initialize the application,
Real-Time-Voice-Cloning/toolbox/ui.py,349,Main layouts,
Real-Time-Voice-Cloning/toolbox/ui.py,350,Root,
Real-Time-Voice-Cloning/toolbox/ui.py,354,Browser,
Real-Time-Voice-Cloning/toolbox/ui.py,358,Visualizations,
Real-Time-Voice-Cloning/toolbox/ui.py,362,Generation,
Real-Time-Voice-Cloning/toolbox/ui.py,366,Projections,
Real-Time-Voice-Cloning/toolbox/ui.py,371,Projections,
Real-Time-Voice-Cloning/toolbox/ui.py,372,UMap,
Real-Time-Voice-Cloning/toolbox/ui.py,381,Browser,
Real-Time-Voice-Cloning/toolbox/ui.py,382,"Dataset, speaker and utterance selection",
Real-Time-Voice-Cloning/toolbox/ui.py,399,Random buttons,
Real-Time-Voice-Cloning/toolbox/ui.py,411,Utterance box,
Real-Time-Voice-Cloning/toolbox/ui.py,415,Random & next utterance buttons,
Real-Time-Voice-Cloning/toolbox/ui.py,420,Random & next utterance buttons,
Real-Time-Voice-Cloning/toolbox/ui.py,431,Model selection,
Real-Time-Voice-Cloning/toolbox/ui.py,444,Embed & spectrograms,
Real-Time-Voice-Cloning/toolbox/ui.py,464,Generation,
Real-Time-Voice-Cloning/toolbox/ui.py,488,Set the size of the window and of the elements,
Real-Time-Voice-Cloning/toolbox/ui.py,492,Finalize the display,
Real-Time-Voice-Cloning/toolbox/__init__.py,13,"Use this directory structure for your datasets, or modify it to fit your needs",
Real-Time-Voice-Cloning/toolbox/__init__.py,43,"speaker_name, spec, breaks, wav",
Real-Time-Voice-Cloning/toolbox/__init__.py,45,type: Synthesizer,
Real-Time-Voice-Cloning/toolbox/__init__.py,47,Initialize the events and the interface,
Real-Time-Voice-Cloning/toolbox/__init__.py,58,"Dataset, speaker and utterance selection",
Real-Time-Voice-Cloning/toolbox/__init__.py,69,Model selection,
Real-Time-Voice-Cloning/toolbox/__init__.py,76,Utterance selection,
Real-Time-Voice-Cloning/toolbox/__init__.py,86,Generation,
Real-Time-Voice-Cloning/toolbox/__init__.py,92,UMAP legend,
Real-Time-Voice-Cloning/toolbox/__init__.py,108,Select the next utterance,
Real-Time-Voice-Cloning/toolbox/__init__.py,117,Get the wav from the disk. We take the wav with the vocoder/synthesizer format for,
Real-Time-Voice-Cloning/toolbox/__init__.py,118,"playback, so as to have a fair comparison with the generated audio",
Real-Time-Voice-Cloning/toolbox/__init__.py,135,Compute the mel spectrogram,
Real-Time-Voice-Cloning/toolbox/__init__.py,139,Compute the embedding,
Real-Time-Voice-Cloning/toolbox/__init__.py,145,Add the utterance,
Real-Time-Voice-Cloning/toolbox/__init__.py,150,Plot it,
Real-Time-Voice-Cloning/toolbox/__init__.py,162,Synthesize the spectrogram,
Real-Time-Voice-Cloning/toolbox/__init__.py,185,Synthesize the waveform,
Real-Time-Voice-Cloning/toolbox/__init__.py,203,Add breaks,
Real-Time-Voice-Cloning/toolbox/__init__.py,210,Play it,
Real-Time-Voice-Cloning/toolbox/__init__.py,214,Compute the embedding,
Real-Time-Voice-Cloning/toolbox/__init__.py,215,"TODO: this is problematic with different sampling rates, gotta fix it",
Real-Time-Voice-Cloning/toolbox/__init__.py,221,Add the utterance,
Real-Time-Voice-Cloning/toolbox/__init__.py,226,Plot it,
Real-Time-Voice-Cloning/toolbox/__init__.py,242,Case of Griffin-lim,
Real-Time-Voice-Cloning/synthesizer/audio.py,14,proposed by @dsmiller,
Real-Time-Voice-Cloning/synthesizer/audio.py,30,From https://github.com/r9y9/wavenet_vocoder/blob/master/audio.py,
Real-Time-Voice-Cloning/synthesizer/audio.py,74,Convert back to linear,
Real-Time-Voice-Cloning/synthesizer/audio.py,91,Convert back to linear,
Real-Time-Voice-Cloning/synthesizer/audio.py,126,,
Real-Time-Voice-Cloning/synthesizer/audio.py,127,Those are only correct when using lws!!! (This was messing with Wavenet quality for a long time!),
Real-Time-Voice-Cloning/synthesizer/audio.py,147,,
Real-Time-Voice-Cloning/synthesizer/audio.py,148,Librosa correct padding,
Real-Time-Voice-Cloning/synthesizer/audio.py,152,Conversions,
Real-Time-Voice-Cloning/synthesizer/hparams.py,3,Default hyperparameters,
Real-Time-Voice-Cloning/synthesizer/hparams.py,5,Comma-separated list of cleaners to run on text prior to training and eval. For non-English,
Real-Time-Voice-Cloning/synthesizer/hparams.py,6,"text, you may want to use ""basic_cleaners"" or ""transliteration_cleaners"".",
Real-Time-Voice-Cloning/synthesizer/hparams.py,9,"If you only have 1 GPU or want to use only one GPU, please set num_gpus=0 and specify the",
Real-Time-Voice-Cloning/synthesizer/hparams.py,10,GPU idx on run. example:,
Real-Time-Voice-Cloning/synthesizer/hparams.py,11,"expample 1 GPU of index 2 (train on ""/gpu2"" only): CUDA_VISIBLE_DEVICES=2 python train.py",
Real-Time-Voice-Cloning/synthesizer/hparams.py,12,"--model=""Tacotron"" --hparams=""tacotron_gpu_start_idx=2""",
Real-Time-Voice-Cloning/synthesizer/hparams.py,13,"If you want to train on multiple GPUs, simply specify the number of GPUs available,",
Real-Time-Voice-Cloning/synthesizer/hparams.py,14,and the idx of the first GPU to use. example:,
Real-Time-Voice-Cloning/synthesizer/hparams.py,15,"example 4 GPUs starting from index 0 (train on ""/gpu0""->""/gpu3""): python train.py",
Real-Time-Voice-Cloning/synthesizer/hparams.py,16,"--model=""Tacotron"" --hparams=""tacotron_num_gpus=4, tacotron_gpu_start_idx=0""",
Real-Time-Voice-Cloning/synthesizer/hparams.py,17,The hparams arguments can be directly modified on this hparams.py file instead of being,
Real-Time-Voice-Cloning/synthesizer/hparams.py,18,specified on run if preferred!,
Real-Time-Voice-Cloning/synthesizer/hparams.py,20,If one wants to train both Tacotron and WaveNet in parallel (provided WaveNet will be,
Real-Time-Voice-Cloning/synthesizer/hparams.py,21,"trained on True mel spectrograms), one needs to specify different GPU idxes.",
Real-Time-Voice-Cloning/synthesizer/hparams.py,22,example Tacotron+WaveNet on a machine with 4 or plus GPUs. Two GPUs for each model:,
Real-Time-Voice-Cloning/synthesizer/hparams.py,23,"CUDA_VISIBLE_DEVICES=0,1 python train.py --model=""Tacotron""",
Real-Time-Voice-Cloning/synthesizer/hparams.py,24,"--hparams=""tacotron_gpu_start_idx=0, tacotron_num_gpus=2""",
Real-Time-Voice-Cloning/synthesizer/hparams.py,25,"Cuda_VISIBLE_DEVICES=2,3 python train.py --model=""WaveNet""",
Real-Time-Voice-Cloning/synthesizer/hparams.py,26,"--hparams=""wavenet_gpu_start_idx=2; wavenet_num_gpus=2""",
Real-Time-Voice-Cloning/synthesizer/hparams.py,28,"IMPORTANT NOTE: If using N GPUs, please multiply the tacotron_batch_size by N below in the",
Real-Time-Voice-Cloning/synthesizer/hparams.py,29,hparams! (tacotron_batch_size = 32 * N),
Real-Time-Voice-Cloning/synthesizer/hparams.py,30,Never use lower batch size than 32 on a single GPU!,
Real-Time-Voice-Cloning/synthesizer/hparams.py,31,Same applies for Wavenet: wavenet_batch_size = 8 * N (wavenet_batch_size can be smaller than,
Real-Time-Voice-Cloning/synthesizer/hparams.py,32,"8 if GPU is having OOM, minimum 2)",
Real-Time-Voice-Cloning/synthesizer/hparams.py,33,Please also apply the synthesis batch size modification likewise. (if N GPUs are used for,
Real-Time-Voice-Cloning/synthesizer/hparams.py,34,"synthesis, minimal batch size must be N, minimum of 1 sample per GPU)",
Real-Time-Voice-Cloning/synthesizer/hparams.py,35,We did not add an automatic multi-GPU batch size computation to avoid confusion in the,
Real-Time-Voice-Cloning/synthesizer/hparams.py,36,"user""s mind and to provide more control to the user for",
Real-Time-Voice-Cloning/synthesizer/hparams.py,37,resources related decisions.,
Real-Time-Voice-Cloning/synthesizer/hparams.py,39,Acknowledgement:,
Real-Time-Voice-Cloning/synthesizer/hparams.py,40,Many thanks to @MlWoo for his awesome work on multi-GPU Tacotron which showed to work a,
Real-Time-Voice-Cloning/synthesizer/hparams.py,41,little faster than the original,
Real-Time-Voice-Cloning/synthesizer/hparams.py,42,pipeline for a single GPU as well. Great work!,
Real-Time-Voice-Cloning/synthesizer/hparams.py,44,"Hardware setup: Default supposes user has only one GPU: ""/gpu:0"" (Tacotron only for now!",
Real-Time-Voice-Cloning/synthesizer/hparams.py,45,"WaveNet does not support multi GPU yet, WIP)",
Real-Time-Voice-Cloning/synthesizer/hparams.py,46,Synthesis also uses the following hardware parameters for multi-GPU parallel synthesis.,
Real-Time-Voice-Cloning/synthesizer/hparams.py,47,idx of the first GPU to be used for Tacotron training.,
Real-Time-Voice-Cloning/synthesizer/hparams.py,48,Determines the number of gpus in use for Tacotron training.,
Real-Time-Voice-Cloning/synthesizer/hparams.py,50,Determines whether to split data on CPU or on first GPU. This is automatically True when,
Real-Time-Voice-Cloning/synthesizer/hparams.py,51,more than 1 GPU is used.,
Real-Time-Voice-Cloning/synthesizer/hparams.py,52,,
Real-Time-Voice-Cloning/synthesizer/hparams.py,54,Audio,
Real-Time-Voice-Cloning/synthesizer/hparams.py,55,Audio parameters are the most important parameters to tune when using this work on your,
Real-Time-Voice-Cloning/synthesizer/hparams.py,56,personal data. Below are the beginner steps to adapt,
Real-Time-Voice-Cloning/synthesizer/hparams.py,57,this work to your personal data:,
Real-Time-Voice-Cloning/synthesizer/hparams.py,58,1- Determine my data sample rate: First you need to determine your audio sample_rate (how,
Real-Time-Voice-Cloning/synthesizer/hparams.py,59,"many samples are in a second of audio). This can be done using sox: ""sox --i <filename>""",
Real-Time-Voice-Cloning/synthesizer/hparams.py,60,"(For this small tuto, I will consider 24kHz (24000 Hz), and defaults are 22050Hz,",
Real-Time-Voice-Cloning/synthesizer/hparams.py,61,so there are plenty of examples to refer to),
Real-Time-Voice-Cloning/synthesizer/hparams.py,62,2- set sample_rate parameter to your data correct sample rate,
Real-Time-Voice-Cloning/synthesizer/hparams.py,63,3- Fix win_size and and hop_size accordingly: (Supposing you will follow our advice: 50ms,
Real-Time-Voice-Cloning/synthesizer/hparams.py,64,"window_size, and 12.5ms frame_shift(hop_size))",
Real-Time-Voice-Cloning/synthesizer/hparams.py,65,"a- win_size = 0.05 * sample_rate. In the tuto example, 0.05 * 24000 = 1200",
Real-Time-Voice-Cloning/synthesizer/hparams.py,66,b- hop_size = 0.25 * win_size. Also equal to 0.0125 * sample_rate. In the tuto,
Real-Time-Voice-Cloning/synthesizer/hparams.py,67,"example, 0.25 * 1200 = 0.0125 * 24000 = 300 (Can set frame_shift_ms=12.5 instead)",
Real-Time-Voice-Cloning/synthesizer/hparams.py,68,"4- Fix n_fft, num_freq and upsample_scales parameters accordingly.",
Real-Time-Voice-Cloning/synthesizer/hparams.py,69,a- n_fft can be either equal to win_size or the first power of 2 that comes after,
Real-Time-Voice-Cloning/synthesizer/hparams.py,70,win_size. I usually recommend using the latter,
Real-Time-Voice-Cloning/synthesizer/hparams.py,71,to be more consistent with signal processing friends. No big difference to be seen,
Real-Time-Voice-Cloning/synthesizer/hparams.py,72,however. For the tuto example: n_fft = 2048 = 2**11,
Real-Time-Voice-Cloning/synthesizer/hparams.py,73,b- num_freq = (n_fft / 2) + 1. For the tuto example: num_freq = 2048 / 2 + 1 = 1024 +,
Real-Time-Voice-Cloning/synthesizer/hparams.py,74,1 = 1025.,
Real-Time-Voice-Cloning/synthesizer/hparams.py,75,"c- For WaveNet, upsample_scales products must be equal to hop_size. For the tuto",
Real-Time-Voice-Cloning/synthesizer/hparams.py,76,"example: upsample_scales=[15, 20] where 15 * 20 = 300",
Real-Time-Voice-Cloning/synthesizer/hparams.py,77,"it is also possible to use upsample_scales=[3, 4, 5, 5] instead. One must only",
Real-Time-Voice-Cloning/synthesizer/hparams.py,78,keep in mind that upsample_kernel_size[0] = 2*upsample_scales[0],
Real-Time-Voice-Cloning/synthesizer/hparams.py,79,so the training segments should be long enough (2.8~3x upsample_scales[0] *,
Real-Time-Voice-Cloning/synthesizer/hparams.py,80,hop_size or longer) so that the first kernel size can see the middle,
Real-Time-Voice-Cloning/synthesizer/hparams.py,81,of the samples efficiently. The length of WaveNet training segments is under the,
Real-Time-Voice-Cloning/synthesizer/hparams.py,82,"parameter ""max_time_steps"".",
Real-Time-Voice-Cloning/synthesizer/hparams.py,83,"5- Finally comes the silence trimming. This very much data dependent, so I suggest trying",
Real-Time-Voice-Cloning/synthesizer/hparams.py,84,"preprocessing (or part of it, ctrl-C to stop), then use the",
Real-Time-Voice-Cloning/synthesizer/hparams.py,85,.ipynb provided in the repo to listen to some inverted mel/linear spectrograms. That,
Real-Time-Voice-Cloning/synthesizer/hparams.py,86,"will first give you some idea about your above parameters, and",
Real-Time-Voice-Cloning/synthesizer/hparams.py,87,"it will also give you an idea about trimming. If silences persist, try reducing",
Real-Time-Voice-Cloning/synthesizer/hparams.py,88,"trim_top_db slowly. If samples are trimmed mid words, try increasing it.",
Real-Time-Voice-Cloning/synthesizer/hparams.py,89,6- If audio quality is too metallic or fragmented (or if linear spectrogram plots are,
Real-Time-Voice-Cloning/synthesizer/hparams.py,90,"showing black silent regions on top), then restart from step 2.",
Real-Time-Voice-Cloning/synthesizer/hparams.py,91,Number of mel-spectrogram channels and local conditioning dimensionality,
Real-Time-Voice-Cloning/synthesizer/hparams.py,92,network,
Real-Time-Voice-Cloning/synthesizer/hparams.py,93,Whether to rescale audio prior to preprocessing,
Real-Time-Voice-Cloning/synthesizer/hparams.py,94,Rescaling value,
Real-Time-Voice-Cloning/synthesizer/hparams.py,95,"Whether to clip silence in Audio (at beginning and end of audio only, not the middle)",
Real-Time-Voice-Cloning/synthesizer/hparams.py,96,train samples of lengths between 3sec and 14sec are more than enough to make a model capable,
Real-Time-Voice-Cloning/synthesizer/hparams.py,97,of good parallelization.,
Real-Time-Voice-Cloning/synthesizer/hparams.py,99,"For cases of OOM (Not really recommended, only use if facing unsolvable OOM errors,",
Real-Time-Voice-Cloning/synthesizer/hparams.py,100,also consider clipping your samples to smaller chunks),
Real-Time-Voice-Cloning/synthesizer/hparams.py,102,"Only relevant when clip_mels_length = True, please only use after trying output_per_steps=3",
Real-Time-Voice-Cloning/synthesizer/hparams.py,103,and still getting OOM errors.,
Real-Time-Voice-Cloning/synthesizer/hparams.py,105,Use LWS (https://github.com/Jonathan-LeRoux/lws) for STFT and phase reconstruction,
Real-Time-Voice-Cloning/synthesizer/hparams.py,106,"It""s preferred to set True to use with https://github.com/r9y9/wavenet_vocoder",
Real-Time-Voice-Cloning/synthesizer/hparams.py,107,Does not work if n_ffit is not multiple of hop_size!!,
Real-Time-Voice-Cloning/synthesizer/hparams.py,109,"Only used to set as True if using WaveNet, no difference in performance is observed in",
Real-Time-Voice-Cloning/synthesizer/hparams.py,110,either cases.,
Real-Time-Voice-Cloning/synthesizer/hparams.py,111,silence threshold used for sound trimming for wavenet preprocessing,
Real-Time-Voice-Cloning/synthesizer/hparams.py,113,Mel spectrogram,
Real-Time-Voice-Cloning/synthesizer/hparams.py,114,Extra window size is filled with 0 paddings to match this parameter,
Real-Time-Voice-Cloning/synthesizer/hparams.py,115,"For 16000Hz, 200 = 12.5 ms (0.0125 * sample_rate)",
Real-Time-Voice-Cloning/synthesizer/hparams.py,116,"For 16000Hz, 800 = 50 ms (If None, win_size = n_fft) (0.05 * sample_rate)",
Real-Time-Voice-Cloning/synthesizer/hparams.py,117,16000Hz (corresponding to librispeech) (sox --i <filename>),
Real-Time-Voice-Cloning/synthesizer/hparams.py,119,Can replace hop_size parameter. (Recommended: 12.5),
Real-Time-Voice-Cloning/synthesizer/hparams.py,121,M-AILABS (and other datasets) trim params (these parameters are usually correct for any,
Real-Time-Voice-Cloning/synthesizer/hparams.py,122,"data, but definitely must be tuned for specific speakers)",
Real-Time-Voice-Cloning/synthesizer/hparams.py,127,Mel and Linear spectrograms normalization/scaling and clipping,
Real-Time-Voice-Cloning/synthesizer/hparams.py,129,Whether to normalize mel spectrograms to some predefined range (following below parameters),
Real-Time-Voice-Cloning/synthesizer/hparams.py,130,Only relevant if mel_normalization = True,
Real-Time-Voice-Cloning/synthesizer/hparams.py,132,"Whether to scale the data to be symmetric around 0. (Also multiplies the output range by 2,",
Real-Time-Voice-Cloning/synthesizer/hparams.py,133,faster and cleaner convergence),
Real-Time-Voice-Cloning/synthesizer/hparams.py,135,"max absolute value of data. If symmetric, data will be [-max, max] else [0, max] (Must not",
Real-Time-Voice-Cloning/synthesizer/hparams.py,136,"be too big to avoid gradient explosion,",
Real-Time-Voice-Cloning/synthesizer/hparams.py,137,not too small for fast convergence),
Real-Time-Voice-Cloning/synthesizer/hparams.py,139,"whether to rescale to [0, 1] for wavenet. (better audio quality)",
Real-Time-Voice-Cloning/synthesizer/hparams.py,141,"whether to clip [-max, max] before training/synthesizing with wavenet (better audio quality)",
Real-Time-Voice-Cloning/synthesizer/hparams.py,143,Contribution by @begeekmyfriend,
Real-Time-Voice-Cloning/synthesizer/hparams.py,144,Spectrogram Pre-Emphasis (Lfilter: Reduce spectrogram noise and helps model certitude,
Real-Time-Voice-Cloning/synthesizer/hparams.py,145,levels. Also allows for better G&L phase reconstruction),
Real-Time-Voice-Cloning/synthesizer/hparams.py,146,whether to apply filter,
Real-Time-Voice-Cloning/synthesizer/hparams.py,147,filter coefficient.,
Real-Time-Voice-Cloning/synthesizer/hparams.py,149,Limits,
Real-Time-Voice-Cloning/synthesizer/hparams.py,153,"Set this to 55 if your speaker is male! if female, 95 should help taking off noise. (To",
Real-Time-Voice-Cloning/synthesizer/hparams.py,154,"test depending on dataset. Pitch info: male~[65, 260], female~[100, 525])",
Real-Time-Voice-Cloning/synthesizer/hparams.py,155,To be increased/reduced depending on data.,
Real-Time-Voice-Cloning/synthesizer/hparams.py,157,Griffin Lim,
Real-Time-Voice-Cloning/synthesizer/hparams.py,159,"Only used in G&L inversion, usually values between 1.2 and 1.5 are a good choice.",
Real-Time-Voice-Cloning/synthesizer/hparams.py,161,"Number of G&L iterations, typically 30 is enough but we use 60 to ensure convergence.",
Real-Time-Voice-Cloning/synthesizer/hparams.py,162,,
Real-Time-Voice-Cloning/synthesizer/hparams.py,164,Tacotron,
Real-Time-Voice-Cloning/synthesizer/hparams.py,165,Was 1,
Real-Time-Voice-Cloning/synthesizer/hparams.py,166,number of frames to generate at each decoding step (increase to speed up computation and,
Real-Time-Voice-Cloning/synthesizer/hparams.py,167,"allows for higher batch size, decreases G&L audio quality)",
Real-Time-Voice-Cloning/synthesizer/hparams.py,169,Determines whether the decoder should stop when predicting <stop> to any frame or to all of,
Real-Time-Voice-Cloning/synthesizer/hparams.py,170,them (True works pretty well),
Real-Time-Voice-Cloning/synthesizer/hparams.py,172,dimension of embedding space (these are NOT the speaker embeddings),
Real-Time-Voice-Cloning/synthesizer/hparams.py,174,Encoder parameters,
Real-Time-Voice-Cloning/synthesizer/hparams.py,175,number of encoder convolutional layers,
Real-Time-Voice-Cloning/synthesizer/hparams.py,176,size of encoder convolution filters for each layer,
Real-Time-Voice-Cloning/synthesizer/hparams.py,177,number of encoder convolutions filters for each layer,
Real-Time-Voice-Cloning/synthesizer/hparams.py,178,number of lstm units for each direction (forward and backward),
Real-Time-Voice-Cloning/synthesizer/hparams.py,180,Attention mechanism,
Real-Time-Voice-Cloning/synthesizer/hparams.py,181,Whether to smooth the attention normalization function,
Real-Time-Voice-Cloning/synthesizer/hparams.py,182,dimension of attention space,
Real-Time-Voice-Cloning/synthesizer/hparams.py,183,number of attention convolution filters,
Real-Time-Voice-Cloning/synthesizer/hparams.py,184,kernel size of attention convolution,
Real-Time-Voice-Cloning/synthesizer/hparams.py,186,Whether to cumulate (sum) all previous attention weights or simply feed previous weights (,
Real-Time-Voice-Cloning/synthesizer/hparams.py,187,Recommended: True),
Real-Time-Voice-Cloning/synthesizer/hparams.py,189,Decoder,
Real-Time-Voice-Cloning/synthesizer/hparams.py,190,number of layers and number of units of prenet,
Real-Time-Voice-Cloning/synthesizer/hparams.py,191,number of decoder lstm layers,
Real-Time-Voice-Cloning/synthesizer/hparams.py,192,number of decoder lstm units on each layer,
Real-Time-Voice-Cloning/synthesizer/hparams.py,194,Max decoder steps during inference (Just for safety from infinite loop cases),
Real-Time-Voice-Cloning/synthesizer/hparams.py,196,Residual postnet,
Real-Time-Voice-Cloning/synthesizer/hparams.py,197,number of postnet convolutional layers,
Real-Time-Voice-Cloning/synthesizer/hparams.py,198,size of postnet convolution filters for each layer,
Real-Time-Voice-Cloning/synthesizer/hparams.py,199,number of postnet convolution filters for each layer,
Real-Time-Voice-Cloning/synthesizer/hparams.py,201,CBHG mel->linear postnet,
Real-Time-Voice-Cloning/synthesizer/hparams.py,203,All kernel sizes from 1 to cbhg_kernels will be used in the convolution bank of CBHG to act,
Real-Time-Voice-Cloning/synthesizer/hparams.py,204,"as ""K-grams""",
Real-Time-Voice-Cloning/synthesizer/hparams.py,205,Channels of the convolution bank,
Real-Time-Voice-Cloning/synthesizer/hparams.py,206,pooling size of the CBHG,
Real-Time-Voice-Cloning/synthesizer/hparams.py,208,"projection channels of the CBHG (1st projection, 2nd is automatically set to num_mels)",
Real-Time-Voice-Cloning/synthesizer/hparams.py,209,kernel_size of the CBHG projections,
Real-Time-Voice-Cloning/synthesizer/hparams.py,210,Number of HighwayNet layers,
Real-Time-Voice-Cloning/synthesizer/hparams.py,211,Number of units used in HighwayNet fully connected layers,
Real-Time-Voice-Cloning/synthesizer/hparams.py,213,Number of GRU units used in bidirectional RNN of CBHG block. CBHG output is 2x rnn_units in,
Real-Time-Voice-Cloning/synthesizer/hparams.py,214,shape,
Real-Time-Voice-Cloning/synthesizer/hparams.py,216,Loss params,
Real-Time-Voice-Cloning/synthesizer/hparams.py,218,whether to mask encoder padding while computing attention. Set to True for better prosody,
Real-Time-Voice-Cloning/synthesizer/hparams.py,219,but slower convergence.,
Real-Time-Voice-Cloning/synthesizer/hparams.py,221,"Whether to use loss mask for padded sequences (if False, <stop_token> loss function will not",
Real-Time-Voice-Cloning/synthesizer/hparams.py,222,"be weighted, else recommended pos_weight = 20)",
Real-Time-Voice-Cloning/synthesizer/hparams.py,224,Use class weights to reduce the stop token classes imbalance (by adding more penalty on,
Real-Time-Voice-Cloning/synthesizer/hparams.py,225,False Negatives (FN)) (1 = disabled),
Real-Time-Voice-Cloning/synthesizer/hparams.py,227,Whether to add a post-processing network to the Tacotron to predict linear spectrograms (,
Real-Time-Voice-Cloning/synthesizer/hparams.py,228,True mode Not tested!!),
Real-Time-Voice-Cloning/synthesizer/hparams.py,229,,
Real-Time-Voice-Cloning/synthesizer/hparams.py,231,Tacotron Training,
Real-Time-Voice-Cloning/synthesizer/hparams.py,232,Reproduction seeds,
Real-Time-Voice-Cloning/synthesizer/hparams.py,234,Determines initial graph and operations (i.e: model) random state for reproducibility,
Real-Time-Voice-Cloning/synthesizer/hparams.py,235,random state for train test split repeatability,
Real-Time-Voice-Cloning/synthesizer/hparams.py,237,performance parameters,
Real-Time-Voice-Cloning/synthesizer/hparams.py,239,Whether to use cpu as support to gpu for decoder computation (Not recommended: may cause,
Real-Time-Voice-Cloning/synthesizer/hparams.py,240,major slowdowns! Only use when critical!),
Real-Time-Voice-Cloning/synthesizer/hparams.py,242,"train/test split ratios, mini-batches sizes",
Real-Time-Voice-Cloning/synthesizer/hparams.py,243,number of training samples on each training steps (was 32),
Real-Time-Voice-Cloning/synthesizer/hparams.py,244,Tacotron Batch synthesis supports ~16x the training batch size (no gradients during,
Real-Time-Voice-Cloning/synthesizer/hparams.py,245,testing).,
Real-Time-Voice-Cloning/synthesizer/hparams.py,246,"Training Tacotron with unmasked paddings makes it aware of them, which makes synthesis times",
Real-Time-Voice-Cloning/synthesizer/hparams.py,247,different from training. We thus recommend masking the encoder.,
Real-Time-Voice-Cloning/synthesizer/hparams.py,249,"DO NOT MAKE THIS BIGGER THAN 1 IF YOU DIDN""T TRAIN TACOTRON WITH ""mask_encoder=True""!!",
Real-Time-Voice-Cloning/synthesizer/hparams.py,251,"% of data to keep as test data, if None, tacotron_test_batches must be not None. (5% is",
Real-Time-Voice-Cloning/synthesizer/hparams.py,252,enough to have a good idea about overfit),
Real-Time-Voice-Cloning/synthesizer/hparams.py,253,number of test batches.,
Real-Time-Voice-Cloning/synthesizer/hparams.py,255,Learning rate schedule,
Real-Time-Voice-Cloning/synthesizer/hparams.py,257,"boolean, determines if the learning rate will follow an exponential decay",
Real-Time-Voice-Cloning/synthesizer/hparams.py,258,Step at which learning decay starts,
Real-Time-Voice-Cloning/synthesizer/hparams.py,259,Determines the learning rate decay slope (UNDER TEST),
Real-Time-Voice-Cloning/synthesizer/hparams.py,260,learning rate decay rate (UNDER TEST),
Real-Time-Voice-Cloning/synthesizer/hparams.py,261,starting learning rate,
Real-Time-Voice-Cloning/synthesizer/hparams.py,262,minimal learning rate,
Real-Time-Voice-Cloning/synthesizer/hparams.py,264,Optimization parameters,
Real-Time-Voice-Cloning/synthesizer/hparams.py,265,AdamOptimizer beta1 parameter,
Real-Time-Voice-Cloning/synthesizer/hparams.py,266,AdamOptimizer beta2 parameter,
Real-Time-Voice-Cloning/synthesizer/hparams.py,267,AdamOptimizer Epsilon parameter,
Real-Time-Voice-Cloning/synthesizer/hparams.py,269,Regularization parameters,
Real-Time-Voice-Cloning/synthesizer/hparams.py,270,regularization weight (for L2 regularization),
Real-Time-Voice-Cloning/synthesizer/hparams.py,272,Whether to rescale regularization weight to adapt for outputs range (used when reg_weight is,
Real-Time-Voice-Cloning/synthesizer/hparams.py,273,high and biasing the model),
Real-Time-Voice-Cloning/synthesizer/hparams.py,274,zoneout rate for all LSTM cells in the network,
Real-Time-Voice-Cloning/synthesizer/hparams.py,275,dropout rate for all convolutional layers + prenet,
Real-Time-Voice-Cloning/synthesizer/hparams.py,276,whether to clip gradients,
Real-Time-Voice-Cloning/synthesizer/hparams.py,278,Evaluation parameters,
Real-Time-Voice-Cloning/synthesizer/hparams.py,280,Whether to use 100% natural eval (to evaluate Curriculum Learning performance) or with same,
Real-Time-Voice-Cloning/synthesizer/hparams.py,281,teacher-forcing ratio as in training (just for overfit),
Real-Time-Voice-Cloning/synthesizer/hparams.py,283,Decoder RNN learning can take be done in one of two ways:,
Real-Time-Voice-Cloning/synthesizer/hparams.py,284,"Teacher Forcing: vanilla teacher forcing (usually with ratio = 1). mode=""constant""",
Real-Time-Voice-Cloning/synthesizer/hparams.py,285,Curriculum Learning Scheme: From Teacher-Forcing to sampling from previous outputs is,
Real-Time-Voice-Cloning/synthesizer/hparams.py,286,"function of global step. (teacher forcing ratio decay) mode=""scheduled""",
Real-Time-Voice-Cloning/synthesizer/hparams.py,287,The second approach is inspired by:,
Real-Time-Voice-Cloning/synthesizer/hparams.py,288,Bengio et al. 2015: Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks.,
Real-Time-Voice-Cloning/synthesizer/hparams.py,289,Can be found under: https://arxiv.org/pdf/1506.03099.pdf,
Real-Time-Voice-Cloning/synthesizer/hparams.py,291,"Can be (""constant"" or ""scheduled""). ""scheduled"" mode applies a cosine teacher forcing ratio",
Real-Time-Voice-Cloning/synthesizer/hparams.py,292,decay. (Preference: scheduled),
Real-Time-Voice-Cloning/synthesizer/hparams.py,294,"Value from [0., 1.], 0.=0%, 1.=100%, determines the % of times we force next decoder",
Real-Time-Voice-Cloning/synthesizer/hparams.py,295,"inputs, Only relevant if mode=""constant""",
Real-Time-Voice-Cloning/synthesizer/hparams.py,297,"initial teacher forcing ratio. Relevant if mode=""scheduled""",
Real-Time-Voice-Cloning/synthesizer/hparams.py,299,"final teacher forcing ratio. Relevant if mode=""scheduled""",
Real-Time-Voice-Cloning/synthesizer/hparams.py,301,"starting point of teacher forcing ratio decay. Relevant if mode=""scheduled""",
Real-Time-Voice-Cloning/synthesizer/hparams.py,303,"Determines the teacher forcing ratio decay slope. Relevant if mode=""scheduled""",
Real-Time-Voice-Cloning/synthesizer/hparams.py,305,"teacher forcing ratio decay rate. Relevant if mode=""scheduled""",
Real-Time-Voice-Cloning/synthesizer/hparams.py,306,,
Real-Time-Voice-Cloning/synthesizer/hparams.py,308,Tacotron-2 integration parameters,
Real-Time-Voice-Cloning/synthesizer/hparams.py,310,Whether to use GTA mels to train WaveNet instead of ground truth mels.,
Real-Time-Voice-Cloning/synthesizer/hparams.py,311,,
Real-Time-Voice-Cloning/synthesizer/hparams.py,313,"Eval sentences (if no eval text file was specified during synthesis, these sentences are",
Real-Time-Voice-Cloning/synthesizer/hparams.py,314,used for eval),
Real-Time-Voice-Cloning/synthesizer/hparams.py,316,"From July 8, 2017 New York Times:",
Real-Time-Voice-Cloning/synthesizer/hparams.py,322,"From Google""s Tacotron example page:",
Real-Time-Voice-Cloning/synthesizer/hparams.py,334,From The web (random long utterance),
Real-Time-Voice-Cloning/synthesizer/hparams.py,342,SV2TTS,
Real-Time-Voice-Cloning/synthesizer/hparams.py,344,Duration in seconds of a silence for an utterance to be split,
Real-Time-Voice-Cloning/synthesizer/hparams.py,345,Duration in seconds below which utterances are discarded,
Real-Time-Voice-Cloning/synthesizer/synthesize.py,14,"Create output path if it doesn""t exist",
Real-Time-Voice-Cloning/synthesizer/synthesize.py,23,Set inputs batch wise,
Real-Time-Voice-Cloning/synthesizer/synthesize.py,45,Load the model in memory,
Real-Time-Voice-Cloning/synthesizer/synthesize.py,50,Load the metadata,
Real-Time-Voice-Cloning/synthesizer/synthesize.py,57,Set inputs batch wise,
Real-Time-Voice-Cloning/synthesizer/synthesize.py,60,"TODO: come on big boy, fix this",
Real-Time-Voice-Cloning/synthesizer/synthesize.py,61,Quick and dirty fix to make sure that all batches have the same size,
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,14,Force the batch size to be known in order to use attention masking in batch synthesis,
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,38,pad input sequences with the <pad_token> 0 ( _ ),
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,40,"explicitely setting the padding to a value that doesn""t originally exist in the spectogram",
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,41,"to avoid any possible conflicts, without affecting the output range of the model too much",
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,54,Memory allocation on the GPUs as needed,
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,70,Prepare the input,
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,83,Forward it,
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,89,Trim the output,
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,95,"If no token is generated, we simply do not trim the output",
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,110,Pad inputs according to each GPU max length,
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,128,pad targets according to each GPU max length,
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,134,Not really used but setting it in case for future development maybe?,
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,146,Linearize outputs (1D arrays),
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,152,Natural batch synthesis,
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,153,Get Mel lengths for the entire batch from stop_tokens predictions,
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,156,Take off the batch wise padding,
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,165,Linearize outputs (1D arrays),
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,171,Natural batch synthesis,
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,172,Get Mel/Linear lengths for the entire batch from stop_tokens predictions,
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,173,target_lengths = self._get_output_lengths(stop_tokens),
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,176,Take off the batch wise padding,
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,186,Write the spectrogram to disk,
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,187,"Note: outputs mel-spectrogram files and target ones have same names, just different folders",
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,193,save wav (mel -> wav),
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,197,save alignments,
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,201,save mel spectrogram plot,
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,206,save wav (linear -> wav),
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,210,save linear spectrogram plot,
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,236,Determine each mel length by the stop token predictions. (len = first occurence of 1 in stop_tokens row wise),
Real-Time-Voice-Cloning/synthesizer/train.py,20,Create tensorboard projector,
Real-Time-Voice-Cloning/synthesizer/train.py,25,Initialize config,
Real-Time-Voice-Cloning/synthesizer/train.py,27,Specifiy the embedding variable and the metadata,
Real-Time-Voice-Cloning/synthesizer/train.py,31,Project the embeddings to space dimensions for visualization,
Real-Time-Voice-Cloning/synthesizer/train.py,52,Control learning rate decay speed,
Real-Time-Voice-Cloning/synthesizer/train.py,54,Control teacher forcing,
Real-Time-Voice-Cloning/synthesizer/train.py,55,"ratio decay when mode = ""scheduled""",
Real-Time-Voice-Cloning/synthesizer/train.py,58,visualize,
Real-Time-Voice-Cloning/synthesizer/train.py,59,gradients (in case of explosion),
Real-Time-Voice-Cloning/synthesizer/train.py,138,Start by setting a seed for repeatability,
Real-Time-Voice-Cloning/synthesizer/train.py,141,Set up data feeder,
Real-Time-Voice-Cloning/synthesizer/train.py,146,Set up model:,
Real-Time-Voice-Cloning/synthesizer/train.py,151,Embeddings metadata,
Real-Time-Voice-Cloning/synthesizer/train.py,157,"For visual purposes, swap space with \s",
Real-Time-Voice-Cloning/synthesizer/train.py,163,Book keeping,
Real-Time-Voice-Cloning/synthesizer/train.py,171,Memory allocation on the GPU as needed,
Real-Time-Voice-Cloning/synthesizer/train.py,176,Train,
Real-Time-Voice-Cloning/synthesizer/train.py,183,saved model restoring,
Real-Time-Voice-Cloning/synthesizer/train.py,185,"Restore saved model if the user requested it, default = True",
Real-Time-Voice-Cloning/synthesizer/train.py,204,initializing feeder,
Real-Time-Voice-Cloning/synthesizer/train.py,207,Training loop,
Real-Time-Voice-Cloning/synthesizer/train.py,227,Run eval and save eval stats,
Real-Time-Voice-Cloning/synthesizer/train.py,289,Save some log to monitor model improvement on same unseen sequence,
Real-Time-Voice-Cloning/synthesizer/train.py,329,Save model and current global step,
Real-Time-Voice-Cloning/synthesizer/train.py,341,save predicted mel spectrogram to disk (debug),
Real-Time-Voice-Cloning/synthesizer/train.py,346,save griffin lim inverted wav for debug (mel -> wav),
Real-Time-Voice-Cloning/synthesizer/train.py,352,save alignment plot to disk (control purposes),
Real-Time-Voice-Cloning/synthesizer/train.py,359,save real and predicted mel-spectrogram plot to disk (control purposes),
Real-Time-Voice-Cloning/synthesizer/train.py,371,Get current checkpoint state,
Real-Time-Voice-Cloning/synthesizer/train.py,374,Update Projector,
Real-Time-Voice-Cloning/synthesizer/__init__.py,1,,
Real-Time-Voice-Cloning/synthesizer/feeder.py,25,Load metadata,
Real-Time-Voice-Cloning/synthesizer/feeder.py,34,Train test split,
Real-Time-Voice-Cloning/synthesizer/feeder.py,44,Make sure test_indices is a multiple of batch_size else round up,
Real-Time-Voice-Cloning/synthesizer/feeder.py,58,pad input sequences with the <pad_token> 0 ( _ ),
Real-Time-Voice-Cloning/synthesizer/feeder.py,60,"explicitely setting the padding to a value that doesn""t originally exist in the spectogram",
Real-Time-Voice-Cloning/synthesizer/feeder.py,61,"to avoid any possible conflicts, without affecting the output range of the model too much",
Real-Time-Voice-Cloning/synthesizer/feeder.py,66,Mark finished sequences with 1s,
Real-Time-Voice-Cloning/synthesizer/feeder.py,70,"Create placeholders for inputs and targets. Don""t specify batch size because we want",
Real-Time-Voice-Cloning/synthesizer/feeder.py,71,to be able to feed different batch sizes at eval time.,
Real-Time-Voice-Cloning/synthesizer/feeder.py,82,SV2TTS,
Real-Time-Voice-Cloning/synthesizer/feeder.py,87,Create queue for buffering data,
Real-Time-Voice-Cloning/synthesizer/feeder.py,102,Create eval queue for buffering eval data,
Real-Time-Voice-Cloning/synthesizer/feeder.py,122,Thread will close when parent quits,
Real-Time-Voice-Cloning/synthesizer/feeder.py,126,Thread will close when parent quits,
Real-Time-Voice-Cloning/synthesizer/feeder.py,137,Create parallel sequences containing zeros to represent a non finished sequence,
Real-Time-Voice-Cloning/synthesizer/feeder.py,145,Read a group of examples,
Real-Time-Voice-Cloning/synthesizer/feeder.py,149,Test on entire test set,
Real-Time-Voice-Cloning/synthesizer/feeder.py,152,Bucket examples based on similar output sequence length for efficiency,
Real-Time-Voice-Cloning/synthesizer/feeder.py,164,Read a group of examples,
Real-Time-Voice-Cloning/synthesizer/feeder.py,169,Bucket examples based on similar output sequence length for efficiency,
Real-Time-Voice-Cloning/synthesizer/feeder.py,180,Create test batches once and evaluate on them for all test steps,
Real-Time-Voice-Cloning/synthesizer/feeder.py,201,Create parallel sequences containing zeros to represent a non finished sequence,
Real-Time-Voice-Cloning/synthesizer/feeder.py,217,Used to mask loss,
Real-Time-Voice-Cloning/synthesizer/feeder.py,227,Pad sequences with 1 to infer that the sequence is done,
Real-Time-Voice-Cloning/synthesizer/feeder.py,234,SV2TTS,
Real-Time-Voice-Cloning/synthesizer/feeder.py,238,,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,15,Gather the input directories,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,22,Create the output directories for each output file type,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,26,Create a metadata file,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,30,Preprocess the dataset,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,40,Verify the contents of the metadata file,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,57,Gather the utterance audios and texts,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,63,A few alignment files will be missing,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,66,Iterate over each entry in the alignments file,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,73,Process each sub-utterance,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,84,Load the audio waveform,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,95,Find pauses that are too long,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,100,Profile the noise from the silences and perform noise reduction on the waveform,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,108,Re-attach segments that are too short,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,114,See if the segment can be re-attached with the right or the left segment,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,119,Do not re-attach if it causes the joined utterance to be too long,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,124,Re-attach the segment with the neighbour of shortest duration,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,132,Split the utterance,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,138,# DEBUG: play the audio segments (run with -n=1),
Real-Time-Voice-Cloning/synthesizer/preprocess.py,139,import sounddevice as sd,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,140,if len(wavs) > 1:,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,141,"print(""This sentence was split in %d segments:"" % len(wavs))",
Real-Time-Voice-Cloning/synthesizer/preprocess.py,142,else:,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,143,"print(""There are no silences long enough for this sentence to be split:"")",
Real-Time-Voice-Cloning/synthesizer/preprocess.py,144,"for wav, text in zip(wavs, texts):",
Real-Time-Voice-Cloning/synthesizer/preprocess.py,145,# Pad the waveform with 1 second of silence because sounddevice tends to cut them early,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,146,# when playing them. You shouldn't need to do that in your parsers.,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,147,"wav = np.concatenate((wav, [0] * 16000))",
Real-Time-Voice-Cloning/synthesizer/preprocess.py,148,"print(""\t%s"" % text)",
Real-Time-Voice-Cloning/synthesizer/preprocess.py,149,"sd.play(wav, 16000, blocking=True)",
Real-Time-Voice-Cloning/synthesizer/preprocess.py,150,"print("""")",
Real-Time-Voice-Cloning/synthesizer/preprocess.py,157,FOR REFERENCE:,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,158,For you not to lose your head if you ever wish to change things here or implement your own,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,159,synthesizer.,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,160,- Both the audios and the mel spectrograms are saved as numpy arrays,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,161,- There is no processing done to the audios that will be saved to disk beyond volume,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,162,normalization (in split_on_silences),
Real-Time-Voice-Cloning/synthesizer/preprocess.py,163,"- However, pre-emphasis is applied to the audios before computing the mel spectrogram. This",
Real-Time-Voice-Cloning/synthesizer/preprocess.py,164,is why we re-apply it on the audio on the side of the vocoder.,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,165,"- Librosa pads the waveform before computing the mel spectrogram. Here, the waveform is saved",
Real-Time-Voice-Cloning/synthesizer/preprocess.py,166,without extra padding. This means that you won't have an exact relation between the length,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,167,of the wav and of the mel spectrogram. See the vocoder data loader.,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,170,Skip existing utterances if needed,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,176,Skip utterances that are too short,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,180,Compute the mel spectrogram,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,184,Skip utterances that are too long,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,188,"Write the spectrogram, embed and audio to disk",
Real-Time-Voice-Cloning/synthesizer/preprocess.py,192,Return a tuple describing this training example,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,200,Compute the speaker embedding of the utterance,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,215,Gather the input wave filepath and the target output embed filepath,
Real-Time-Voice-Cloning/synthesizer/preprocess.py,220,"TODO: improve on the multiprocessing, it's terrible. Disk I/O is the bottleneck here.",
Real-Time-Voice-Cloning/synthesizer/preprocess.py,221,Embed the utterances in separate threads,
Real-Time-Voice-Cloning/synthesizer/inference.py,3,You're free to use either one,
Real-Time-Voice-Cloning/synthesizer/inference.py,4,from multiprocessing import Pool   #,
Real-Time-Voice-Cloning/synthesizer/inference.py,33,Prepare the model,
Real-Time-Voice-Cloning/synthesizer/inference.py,34,type: Tacotron2,
Real-Time-Voice-Cloning/synthesizer/inference.py,75,Usual inference mode: load the model on the first request and keep it loaded.,
Real-Time-Voice-Cloning/synthesizer/inference.py,80,Low memory inference mode: load the model upon every request. The model has to be,
Real-Time-Voice-Cloning/synthesizer/inference.py,81,loaded in a separate process to be able to release GPU memory (a simple workaround,
Real-Time-Voice-Cloning/synthesizer/inference.py,82,to tensorflow's intricacies),
Real-Time-Voice-Cloning/synthesizer/inference.py,90,Load the model and forward the inputs,
Real-Time-Voice-Cloning/synthesizer/inference.py,95,Detach the outputs (not doing so will cause the process to hang),
Real-Time-Voice-Cloning/synthesizer/inference.py,98,Close cuda for this process,
Real-Time-Voice-Cloning/synthesizer/utils/plot.py,50,Set common labels,
Real-Time-Voice-Cloning/synthesizer/utils/plot.py,53,target spectrogram subplot,
Real-Time-Voice-Cloning/synthesizer/utils/cleaners.py,17,Regular expression matching whitespace:,
Real-Time-Voice-Cloning/synthesizer/utils/cleaners.py,20,"List of (regular expression, replacement) pairs for abbreviations:",
Real-Time-Voice-Cloning/synthesizer/utils/symbols.py,7,from . import cmudict,
Real-Time-Voice-Cloning/synthesizer/utils/symbols.py,13,"Prepend ""@"" to ARPAbet symbols to ensure uniqueness (some are the same as uppercase letters):",
Real-Time-Voice-Cloning/synthesizer/utils/symbols.py,14,"_arpabet = [""@' + s for s in cmudict.valid_symbols]",
Real-Time-Voice-Cloning/synthesizer/utils/symbols.py,16,Export all symbols:,
Real-Time-Voice-Cloning/synthesizer/utils/symbols.py,17,+ _arpabet,
Real-Time-Voice-Cloning/synthesizer/utils/numbers.py,25,Unexpected format,
Real-Time-Voice-Cloning/synthesizer/utils/text.py,5,Mappings from symbol to numeric ID and vice versa:,
Real-Time-Voice-Cloning/synthesizer/utils/text.py,9,Regular expression matching text enclosed in curly braces:,
Real-Time-Voice-Cloning/synthesizer/utils/text.py,28,Check for curly braces and treat their contents as ARPAbet:,
Real-Time-Voice-Cloning/synthesizer/utils/text.py,38,Append EOS token,
Real-Time-Voice-Cloning/synthesizer/utils/text.py,49,Enclose ARPAbet back in curly braces:,
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,34,Return all 0; we ignore them,
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,39,A sequence is finished when the output probability is > 0.5,
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,42,"Since we are predicting r frames at each step, two modes are",
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,43,then possible:,
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,44,Stop when the model outputs a p > 0.5 for any frame between r frames (Recommended),
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,45,Stop when the model outputs a p > 0.5 for all r frames (Safer),
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,46,Note:,
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,47,"With enough training steps, the model should be able to predict when to stop correctly",
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,48,"and the use of stop_at_any = True would be recommended. If however the model didn""t",
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,49,"learn to stop correctly yet, (stops too soon) one could choose to use the safer option",
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,50,to get a correct synthesis,
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,52,Recommended,
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,54,Safer option,
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,56,"Feed last output frame as next input. outputs is [N, output_dim * r]",
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,64,"inputs is [N, T_in], targets is [N, T_out, D]",
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,76,Feed every r-th target frame as input,
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,79,Maximal sequence length,
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,99,Compute teacher forcing ratio for this global step.,
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,100,"In GTA mode, override teacher forcing scheme to work with full teacher forcing",
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,102,Force GTA model to always feed ground-truth,
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,104,Force eval model to always feed predictions,
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,113,Return all 0; we ignore them,
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,117,synthesis stop (we let the model see paddings as we mask them when computing loss functions),
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,120,Pick previous outputs randomly with respect to teacher forcing ratio,
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,123,Teacher-forcing: return true frame,
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,126,Pass on state,
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,136,,
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,137,Narrow Cosine Decay:,
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,139,Phase 1: tfr = 1,
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,140,We only start learning rate decay after 10k steps,
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,142,"Phase 2: tfr in ]0, 1[",
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,143,decay reach minimal value at step ~280k,
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,145,Phase 3: tfr = 0,
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,146,clip by minimal teacher forcing ratio value (step >~ 280k),
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,147,,
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,148,Compute natural cosine decay,
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,150,tfr = 1 at step 10k,
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,151,tfr = 0 at step ~280k,
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,152,tfr = 0% of init_tfr as final value,
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,155,force teacher forcing ratio to take initial value when global step < start decay step.,
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,30,Initialize encoder layers,
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,35,Pass input sequence through a stack of convolutional layers,
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,38,Extract hidden representation from encoder lstm cells,
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,41,For shape visualization,
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,99,Initialize decoder layers,
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,166,Information bottleneck (essential for learning attention),
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,169,Concat context vector and prenet output to form LSTM cells input (input feeding),
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,172,Unidirectional LSTM layers,
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,176,Compute the attention (context) vector and alignments using,
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,177,the new decoder cell hidden state as query vector,
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,178,and cumulative alignments to extract location features,
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,179,The choice of the new cell hidden state (s_{i}) of the last,
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,180,decoder RNN Cell is based on Luong et Al. (2015):,
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,181,https://arxiv.org/pdf/1508.04025.pdf,
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,189,Concat LSTM outputs and context vector to form projections inputs,
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,192,Compute predicted frames and predicted <stop_token>,
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,196,Save alignment history,
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,199,Prepare next decoder state,
Real-Time-Voice-Cloning/synthesizer/models/attention.py,9,From https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py,
Real-Time-Voice-Cloning/synthesizer/models/attention.py,16,"Reshape from [batch_size, memory_time] to [batch_size, 1, memory_time]",
Real-Time-Voice-Cloning/synthesizer/models/attention.py,18,Context is the inner product of alignments and values along the,
Real-Time-Voice-Cloning/synthesizer/models/attention.py,19,memory time dimension.,
Real-Time-Voice-Cloning/synthesizer/models/attention.py,20,alignments shape is,
Real-Time-Voice-Cloning/synthesizer/models/attention.py,21,"[batch_size, 1, memory_time]",
Real-Time-Voice-Cloning/synthesizer/models/attention.py,22,attention_mechanism.values shape is,
Real-Time-Voice-Cloning/synthesizer/models/attention.py,23,"[batch_size, memory_time, memory_size]",
Real-Time-Voice-Cloning/synthesizer/models/attention.py,24,"the batched matmul is over memory_time, so the output shape is",
Real-Time-Voice-Cloning/synthesizer/models/attention.py,25,"[batch_size, 1, memory_size].",
Real-Time-Voice-Cloning/synthesizer/models/attention.py,26,we then squeeze out the singleton dim.,
Real-Time-Voice-Cloning/synthesizer/models/attention.py,59,Get the number of hidden units from the trailing dimension of keys,
Real-Time-Voice-Cloning/synthesizer/models/attention.py,147,Create normalization function,
Real-Time-Voice-Cloning/synthesizer/models/attention.py,148,Setting it to None defaults in using softmax,
Real-Time-Voice-Cloning/synthesizer/models/attention.py,181,"processed_query shape [batch_size, query_depth] -> [batch_size, attention_dim]",
Real-Time-Voice-Cloning/synthesizer/models/attention.py,183,"-> [batch_size, 1, attention_dim]",
Real-Time-Voice-Cloning/synthesizer/models/attention.py,186,"processed_location_features shape [batch_size, max_time, attention dimension]",
Real-Time-Voice-Cloning/synthesizer/models/attention.py,187,"[batch_size, max_time] -> [batch_size, max_time, 1]",
Real-Time-Voice-Cloning/synthesizer/models/attention.py,189,"location features [batch_size, max_time, filters]",
Real-Time-Voice-Cloning/synthesizer/models/attention.py,191,"Projected location features [batch_size, max_time, attention_dim]",
Real-Time-Voice-Cloning/synthesizer/models/attention.py,194,"energy shape [batch_size, max_time]",
Real-Time-Voice-Cloning/synthesizer/models/attention.py,198,"alignments shape = energy shape = [batch_size, max_time]",
Real-Time-Voice-Cloning/synthesizer/models/attention.py,201,Cumulate alignments,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,43,Convolution bank: concatenate on the last axis to stack channels from all,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,44,convolutions,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,45,The convolution bank uses multiple different kernel sizes to have many insights,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,46,of the input sequence,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,47,This makes one of the strengths of the CBHG block on sequences.,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,54,"Maxpooling (dimension reduction, Using max instead of average helps finding ""Edges""",
Real-Time-Voice-Cloning/synthesizer/models/modules.py,55,in mels),
Real-Time-Voice-Cloning/synthesizer/models/modules.py,62,Two projection layers,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,68,Residual connection,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,71,"Additional projection in case of dimension mismatch (for HighwayNet ""residual""",
Real-Time-Voice-Cloning/synthesizer/models/modules.py,72,connection),
Real-Time-Voice-Cloning/synthesizer/models/modules.py,76,4-layer HighwayNet,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,81,Bidirectional RNN,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,88,Concat forward and backward outputs,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,129,Apply vanilla LSTM,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,143,Apply zoneout,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,145,nn.dropout takes keep_prob (probability to keep activations) not drop_prob (,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,146,probability to mask activations)!,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,216,Create forward LSTM Cell,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,222,Create backward LSTM Cell,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,238,Concat and return forward + backward outputs,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,270,The paper discussed introducing diversity in generation at inference time,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,271,by using a dropout of 0.5 only in prenet layers (in both training and inference).,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,298,Create a set of LSTM layers,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,335,"If activation==None, this returns a simple Linear projection",
Real-Time-Voice-Cloning/synthesizer/models/modules.py,336,else the projection will be passed through an activation function,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,337,"output = tf.layers.dense(inputs, units=self.shape, activation=self.activation,",
Real-Time-Voice-Cloning/synthesizer/models/modules.py,338,"name=""projection_{}"".format(self.scope))",
Real-Time-Voice-Cloning/synthesizer/models/modules.py,369,"During training, don""t use activation as it is integrated inside the",
Real-Time-Voice-Cloning/synthesizer/models/modules.py,370,sigmoid_cross_entropy loss function,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,429,Tf version of remainder = x % multiple,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,431,Tf version of return x if remainder == 0 else x + multiple - remainder,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,453,"[batch_size, time_dimension, 1]",
Real-Time-Voice-Cloning/synthesizer/models/modules.py,454,example:,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,455,"sequence_mask([1, 3, 2], 5) = [[[1., 0., 0., 0., 0.]],",
Real-Time-Voice-Cloning/synthesizer/models/modules.py,456,"[[1., 1., 1., 0., 0.]],",
Real-Time-Voice-Cloning/synthesizer/models/modules.py,457,"[[1., 1., 0., 0., 0.]]]",
Real-Time-Voice-Cloning/synthesizer/models/modules.py,458,Note the maxlen argument that ensures mask shape is compatible with r>1,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,459,This will by default mask the extra paddings caused by r>1,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,463,"[batch_size, time_dimension, channel_dimension(mels)]",
Real-Time-Voice-Cloning/synthesizer/models/modules.py,476,"[batch_size, time_dimension]",
Real-Time-Voice-Cloning/synthesizer/models/modules.py,477,example:,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,478,"sequence_mask([1, 3, 2], 5) = [[1., 0., 0., 0., 0.],",
Real-Time-Voice-Cloning/synthesizer/models/modules.py,479,"[1., 1., 1., 0., 0.],",
Real-Time-Voice-Cloning/synthesizer/models/modules.py,480,"[1., 1., 0., 0., 0.]]",
Real-Time-Voice-Cloning/synthesizer/models/modules.py,481,Note the maxlen argument that ensures mask shape is compatible with r>1,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,482,This will by default mask the extra paddings caused by r>1,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,487,Use a weighted sigmoid cross entropy to measure the <stop_token> loss. Set,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,488,hparams.cross_entropy_pos_weight to 1,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,489,will have the same effect as  vanilla tf.nn.sigmoid_cross_entropy_with_logits.,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,503,"[batch_size, time_dimension, 1]",
Real-Time-Voice-Cloning/synthesizer/models/modules.py,504,example:,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,505,"sequence_mask([1, 3, 2], 5) = [[[1., 0., 0., 0., 0.]],",
Real-Time-Voice-Cloning/synthesizer/models/modules.py,506,"[[1., 1., 1., 0., 0.]],",
Real-Time-Voice-Cloning/synthesizer/models/modules.py,507,"[[1., 1., 0., 0., 0.]]]",
Real-Time-Voice-Cloning/synthesizer/models/modules.py,508,Note the maxlen argument that ensures mask shape is compatible with r>1,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,509,This will by default mask the extra paddings caused by r>1,
Real-Time-Voice-Cloning/synthesizer/models/modules.py,513,"[batch_size, time_dimension, channel_dimension(freq)]",
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,17,x will be a numpy array with the contents of the placeholder below,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,79,SV2TTS,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,84,,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,119,1. Declare GPU Devices,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,130,"GTA is only used for predicting mels to train Wavenet vocoder, so we ommit",
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,131,post processing when doing GTA synthesis,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,134,"Embeddings ==> [batch_size, sequence_length, embedding_dim]",
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,139,"Encoder Cell ==> [batch_size, encoder_steps, encoder_lstm_units]",
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,147,For shape visualization purpose,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,151,SV2TT2,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,153,Append the speaker embedding to the encoder output at each timestep,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,160,,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,163,Decoder Parts,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,164,Attention Decoder Prenet,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,167,Attention Mechanism,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,177,Decoder LSTM Cells,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,182,Frames Projection layer,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,185,<stop_token> projection layer,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,190,"Decoder Cell ==> [batch_size, decoder_steps, num_mels * r] (after decoding)",
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,198,Define the helper for our decoder,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,205,initial decoder state,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,209,Only use max iterations at synthesis time,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,212,Decode,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,220,Reshape outputs to be one output per entry,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,221,"==> [batch_size, non_reduced_decoder_steps (decoder_steps * r), num_mels]",
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,225,Postnet,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,228,"Compute residual using post-net ==> [batch_size, decoder_steps * r,",
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,229,postnet_channels],
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,232,Project residual to same dimension as mel spectrogram,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,233,"==> [batch_size, decoder_steps * r, num_mels]",
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,237,Compute the mel spectrogram,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,241,Add post-processing CBHG. This does a great job at extracting features,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,242,from mels before projection to Linear specs.,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,249,"[batch_size, decoder_steps(mel_frames), cbhg_channels]",
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,252,Linear projection of extracted features to make linear spectrogram,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,256,"[batch_size, decoder_steps(linear_frames), num_freq]",
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,259,Grab alignments from the final decoder state,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,282,self.tower_linear_targets = tower_linear_targets,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,307,1_000_000 is causing syntax problems for some people?! Python please :),
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,338,Compute loss of predictions before postnet,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,342,Compute loss after postnet,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,346,Compute <stop_token> loss (for learning dynamic generation stop),
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,351,SV2TTS extra L1 loss (disabled for now),
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,352,"linear_loss = MaskedLinearLoss(self.tower_mel_targets[i],",
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,353,"self.tower_decoder_output[i],",
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,354,"self.tower_targets_lengths[i],",
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,355,hparams=self._hparams),
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,358,Compute loss of predictions before postnet,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,361,Compute loss after postnet,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,364,Compute <stop_token> loss (for learning dynamic generation stop),
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,369,SV2TTS extra L1 loss,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,373,if hp.predict_linear:,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,374,# Compute linear loss,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,375,# From https://github.com/keithito/tacotron/blob/tacotron2-work-in,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,376,# -progress/models/tacotron.py,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,377,# Prioritize loss for frequencies under 2000 Hz.,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,378,l1 = tf.abs(self.tower_linear_targets[i] - self.tower_linear_outputs[i]),
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,379,n_priority_freq = int(2000 / (hp.sample_rate * 0.5) * hp.num_freq),
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,380,linear_loss = 0.5 * tf.reduce_mean(l1) + 0.5 * tf.reduce_mean(,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,381,"l1[:, :, 0:n_priority_freq])",
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,382,else:,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,383,linear_loss = 0.,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,385,Compute the regularization weight,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,394,Regularize variables,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,395,"Exclude all types of bias, RNN (Bengio et al. On the difficulty of training recurrent neural networks), embeddings and prediction projection layers.",
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,396,"Note that we consider attention mechanism v_a weights as a prediction projection layer and we don""t regularize it. (This gave better stability)",
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,402,Compute final loss term,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,435,1. Declare GPU Devices,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,454,2. Compute Gradient,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,456,Device placement,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,459,agg_loss += self.tower_loss[i],
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,464,3. Average Gradient,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,469,"grads_vars = [(grad1, var), (grad2, var), ...]",
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,473,"Append on a ""tower"" dimension which we will average over below.",
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,475,"Average over the ""tower"" dimension.",
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,484,Just for causion,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,485,https://github.com/Rayhane-mamah/Tacotron-2/issues/11,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,487,__mark 0.5 refer,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,491,"Add dependency on UPDATE_OPS; otherwise batchnorm won""t work correctly. See:",
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,492,https://github.com/tensorflow/tensorflow/issues/1122,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,498,,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,499,Narrow Exponential Decay:,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,501,Phase 1: lr = 1e-3,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,502,We only start learning rate decay after 50k steps,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,504,"Phase 2: lr in ]1e-5, 1e-3[",
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,505,decay reach minimal value at step 310k,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,507,Phase 3: lr = 1e-5,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,508,clip by minimal learning rate value (step > 310k),
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,509,,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,512,Compute natural exponential decay,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,515,lr = 1e-3 at step 50k,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,517,lr = 1e-5 around step 310k,
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,520,clip learning rate by max and min values (initial and final values),
Real-Time-Voice-Cloning/synthesizer/models/custom_decoder.py,62,"To use layer""s compute_output_shape, we need to convert the",
Real-Time-Voice-Cloning/synthesizer/models/custom_decoder.py,63,"RNNCell""s output_size entries into shapes with an unknown",
Real-Time-Voice-Cloning/synthesizer/models/custom_decoder.py,64,"batch size.  We then pass this through the layer""s",
Real-Time-Voice-Cloning/synthesizer/models/custom_decoder.py,65,compute_output_shape and read off all but the first (batch),
Real-Time-Voice-Cloning/synthesizer/models/custom_decoder.py,66,dimensions to get the output size of the rnn with the layer,
Real-Time-Voice-Cloning/synthesizer/models/custom_decoder.py,67,applied to the top.,
Real-Time-Voice-Cloning/synthesizer/models/custom_decoder.py,71,pylint: disable=protected-access,
Real-Time-Voice-Cloning/synthesizer/models/custom_decoder.py,77,Return the cell output and the id,
Real-Time-Voice-Cloning/synthesizer/models/custom_decoder.py,85,Assume the dtype of the cell is the output_size structure,
Real-Time-Voice-Cloning/synthesizer/models/custom_decoder.py,86,"containing the input_state""s first component's dtype.",
Real-Time-Voice-Cloning/synthesizer/models/custom_decoder.py,87,Return that structure and the sample_ids_dtype from the helper.,
Real-Time-Voice-Cloning/synthesizer/models/custom_decoder.py,115,Call outputprojection wrapper cell,
Real-Time-Voice-Cloning/synthesizer/models/custom_decoder.py,118,apply output_layer (if existant),
Real-Time-Voice-Cloning/encoder/model.py,17,Network defition,
Real-Time-Voice-Cloning/encoder/model.py,26,Cosine similarity scaling (with fixed initial parameter values),
Real-Time-Voice-Cloning/encoder/model.py,30,Loss,
Real-Time-Voice-Cloning/encoder/model.py,34,Gradient scale,
Real-Time-Voice-Cloning/encoder/model.py,38,Gradient clipping,
Real-Time-Voice-Cloning/encoder/model.py,51,"Pass the input through the LSTM layers and retrieve all outputs, the final hidden state",
Real-Time-Voice-Cloning/encoder/model.py,52,and the final cell state.,
Real-Time-Voice-Cloning/encoder/model.py,55,We take only the hidden state of the last layer,
Real-Time-Voice-Cloning/encoder/model.py,58,L2-normalize it,
Real-Time-Voice-Cloning/encoder/model.py,74,Inclusive centroids (1 per speaker). Cloning is needed for reverse differentiation,
Real-Time-Voice-Cloning/encoder/model.py,78,Exclusive centroids (1 per utterance),
Real-Time-Voice-Cloning/encoder/model.py,83,Similarity matrix. The cosine similarity of already 2-normed vectors is simply the dot,
Real-Time-Voice-Cloning/encoder/model.py,84,product of these vectors (which is just an element-wise multiplication reduced by a sum).,
Real-Time-Voice-Cloning/encoder/model.py,85,We vectorize the computation for efficiency.,
Real-Time-Voice-Cloning/encoder/model.py,94,Even more vectorized version (slower maybe because of transpose),
Real-Time-Voice-Cloning/encoder/model.py,95,"sim_matrix2 = torch.zeros(speakers_per_batch, speakers_per_batch, utterances_per_speaker",
Real-Time-Voice-Cloning/encoder/model.py,96,).to(self.loss_device),
Real-Time-Voice-Cloning/encoder/model.py,97,"eye = np.eye(speakers_per_batch, dtype=np.int)",
Real-Time-Voice-Cloning/encoder/model.py,98,mask = np.where(1 - eye),
Real-Time-Voice-Cloning/encoder/model.py,99,sim_matrix2[mask] = (embeds[mask[0]] * centroids_incl[mask[1]]).sum(dim=2),
Real-Time-Voice-Cloning/encoder/model.py,100,mask = np.where(eye),
Real-Time-Voice-Cloning/encoder/model.py,101,sim_matrix2[mask] = (embeds * centroids_excl).sum(dim=2),
Real-Time-Voice-Cloning/encoder/model.py,102,"sim_matrix2 = sim_matrix2.transpose(1, 2)",
Real-Time-Voice-Cloning/encoder/model.py,117,Loss,
Real-Time-Voice-Cloning/encoder/model.py,125,EER (not backpropagated),
Real-Time-Voice-Cloning/encoder/model.py,131,Snippet from https://yangcha.github.io/EER-ROC/,
Real-Time-Voice-Cloning/encoder/audio.py,26,Load the wav from disk if needed,
Real-Time-Voice-Cloning/encoder/audio.py,32,Resample the wav if needed,
Real-Time-Voice-Cloning/encoder/audio.py,36,Apply the preprocessing: normalize volume and shorten long silences,
Real-Time-Voice-Cloning/encoder/audio.py,66,Compute the voice detection window size,
Real-Time-Voice-Cloning/encoder/audio.py,69,Trim the end of the audio to have a multiple of the window size,
Real-Time-Voice-Cloning/encoder/audio.py,72,Convert the float waveform to 16-bit mono PCM,
Real-Time-Voice-Cloning/encoder/audio.py,75,Perform voice activation detection,
Real-Time-Voice-Cloning/encoder/audio.py,84,Smooth the voice detection with a moving average,
Real-Time-Voice-Cloning/encoder/audio.py,94,Dilate the voiced regions,
Real-Time-Voice-Cloning/encoder/visualizations.py,6,import webbrowser,
Real-Time-Voice-Cloning/encoder/visualizations.py,29,Tracking data,
Real-Time-Voice-Cloning/encoder/visualizations.py,37,If visdom is disabled TODO: use a better paradigm for that,
Real-Time-Voice-Cloning/encoder/visualizations.py,42,Set the environment name,
Real-Time-Voice-Cloning/encoder/visualizations.py,49,Connect to visdom and open the corresponding window in the browser,
Real-Time-Voice-Cloning/encoder/visualizations.py,55,"webbrowser.open(""http://localhost:8097/env/"" + self.env_name)",
Real-Time-Voice-Cloning/encoder/visualizations.py,57,Create the windows,
Real-Time-Voice-Cloning/encoder/visualizations.py,60,self.lr_win = None,
Real-Time-Voice-Cloning/encoder/visualizations.py,103,Update the tracking data,
Real-Time-Voice-Cloning/encoder/visualizations.py,111,Update the plots every <update_every> steps,
Real-Time-Voice-Cloning/encoder/visualizations.py,150,Reset the tracking,
Real-Time-Voice-Cloning/encoder/params_data.py,2,Mel-filterbank,
Real-Time-Voice-Cloning/encoder/params_data.py,3,In milliseconds,
Real-Time-Voice-Cloning/encoder/params_data.py,4,In milliseconds,
Real-Time-Voice-Cloning/encoder/params_data.py,8,Audio,
Real-Time-Voice-Cloning/encoder/params_data.py,10,Number of spectrogram frames in a partial utterance,
Real-Time-Voice-Cloning/encoder/params_data.py,11,1600 ms,
Real-Time-Voice-Cloning/encoder/params_data.py,12,Number of spectrogram frames at inference,
Real-Time-Voice-Cloning/encoder/params_data.py,13,800 ms,
Real-Time-Voice-Cloning/encoder/params_data.py,16,Voice Activation Detection,
Real-Time-Voice-Cloning/encoder/params_data.py,17,"Window size of the VAD. Must be either 10, 20 or 30 milliseconds.",
Real-Time-Voice-Cloning/encoder/params_data.py,18,This sets the granularity of the VAD. Should not need to be changed.,
Real-Time-Voice-Cloning/encoder/params_data.py,19,In milliseconds,
Real-Time-Voice-Cloning/encoder/params_data.py,20,Number of frames to average together when performing the moving average smoothing.,
Real-Time-Voice-Cloning/encoder/params_data.py,21,"The larger this value, the larger the VAD variations must be to not get smoothed out.",
Real-Time-Voice-Cloning/encoder/params_data.py,23,Maximum number of consecutive silent frames a segment can have.,
Real-Time-Voice-Cloning/encoder/params_data.py,27,Audio volume normalization,
Real-Time-Voice-Cloning/encoder/params_model.py,2,Model parameters,
Real-Time-Voice-Cloning/encoder/params_model.py,8,Training parameters,
Real-Time-Voice-Cloning/encoder/train.py,10,FIXME,
Real-Time-Voice-Cloning/encoder/train.py,12,For correct profiling (cuda operations are async),
Real-Time-Voice-Cloning/encoder/train.py,19,Create a dataset and a dataloader,
Real-Time-Voice-Cloning/encoder/train.py,28,"Setup the device on which to run the forward pass and the loss. These can be different,",
Real-Time-Voice-Cloning/encoder/train.py,29,because the forward pass is faster on the GPU whereas the loss is often (depending on your,
Real-Time-Voice-Cloning/encoder/train.py,30,hyperparameters) faster on the CPU.,
Real-Time-Voice-Cloning/encoder/train.py,32,"FIXME: currently, the gradient is None if loss_device is cuda",
Real-Time-Voice-Cloning/encoder/train.py,35,Create the model and the optimizer,
Real-Time-Voice-Cloning/encoder/train.py,40,Configure file path for the model,
Real-Time-Voice-Cloning/encoder/train.py,44,Load any existing model,
Real-Time-Voice-Cloning/encoder/train.py,59,Initialize the visualization environment,
Real-Time-Voice-Cloning/encoder/train.py,66,Training loop,
Real-Time-Voice-Cloning/encoder/train.py,71,Forward pass,
Real-Time-Voice-Cloning/encoder/train.py,83,Backward pass,
Real-Time-Voice-Cloning/encoder/train.py,91,Update visualizations,
Real-Time-Voice-Cloning/encoder/train.py,92,"learning_rate = optimizer.param_groups[0][""lr""]",
Real-Time-Voice-Cloning/encoder/train.py,95,Draw projections and save them to the backup folder,
Real-Time-Voice-Cloning/encoder/train.py,104,Overwrite the latest version of the model,
Real-Time-Voice-Cloning/encoder/train.py,113,Make a backup,
Real-Time-Voice-Cloning/encoder/preprocess.py,65,Function to preprocess utterances for one speaker,
Real-Time-Voice-Cloning/encoder/preprocess.py,67,Give a name to the speaker that includes its dataset,
Real-Time-Voice-Cloning/encoder/preprocess.py,70,"Create an output directory with that name, as well as a txt file containing a",
Real-Time-Voice-Cloning/encoder/preprocess.py,71,reference to each source file.,
Real-Time-Voice-Cloning/encoder/preprocess.py,76,"There's a possibility that the preprocessing was interrupted earlier, check if",
Real-Time-Voice-Cloning/encoder/preprocess.py,77,there already is a sources file.,
Real-Time-Voice-Cloning/encoder/preprocess.py,87,Gather all audio files for that speaker recursively,
Real-Time-Voice-Cloning/encoder/preprocess.py,90,Check if the target output file already exists,
Real-Time-Voice-Cloning/encoder/preprocess.py,96,Load and preprocess the waveform,
Real-Time-Voice-Cloning/encoder/preprocess.py,101,"Create the mel spectrogram, discard those that are too short",
Real-Time-Voice-Cloning/encoder/preprocess.py,113,Process the utterances for each speaker,
Real-Time-Voice-Cloning/encoder/preprocess.py,123,Initialize the preprocessing,
Real-Time-Voice-Cloning/encoder/preprocess.py,128,Preprocess all speakers,
Real-Time-Voice-Cloning/encoder/preprocess.py,135,Initialize the preprocessing,
Real-Time-Voice-Cloning/encoder/preprocess.py,141,Get the contents of the meta file,
Real-Time-Voice-Cloning/encoder/preprocess.py,145,"Select the ID and the nationality, filter out non-anglophone speakers",
Real-Time-Voice-Cloning/encoder/preprocess.py,152,Get the speaker directories for anglophone speakers only,
Real-Time-Voice-Cloning/encoder/preprocess.py,159,Preprocess all speakers,
Real-Time-Voice-Cloning/encoder/preprocess.py,165,Initialize the preprocessing,
Real-Time-Voice-Cloning/encoder/preprocess.py,171,Get the speaker directories,
Real-Time-Voice-Cloning/encoder/preprocess.py,172,Preprocess all speakers,
Real-Time-Voice-Cloning/encoder/inference.py,3,We want to expose this function from here,
Real-Time-Voice-Cloning/encoder/inference.py,11,type: SpeakerEncoder,
Real-Time-Voice-Cloning/encoder/inference.py,12,type: torch.device,
Real-Time-Voice-Cloning/encoder/inference.py,25,TODO: I think the slow loading of the encoder might have something to do with the device it,
Real-Time-Voice-Cloning/encoder/inference.py,26,was saved on. Worth investigating.,
Real-Time-Voice-Cloning/encoder/inference.py,92,Compute the slices,
Real-Time-Voice-Cloning/encoder/inference.py,101,Evaluate whether extra padding is warranted or not,
Real-Time-Voice-Cloning/encoder/inference.py,130,Process the entire utterance if not using partials,
Real-Time-Voice-Cloning/encoder/inference.py,138,Compute where to split the utterance into partials and pad if necessary,
Real-Time-Voice-Cloning/encoder/inference.py,144,Split the utterance into partials,
Real-Time-Voice-Cloning/encoder/inference.py,149,Compute the utterance embedding from the partial embeddings,
Real-Time-Voice-Cloning/encoder/data_objects/speaker.py,5,Contains the set of utterances of a single speaker,
Real-Time-Voice-Cloning/encoder/data_objects/speaker_verification_dataset.py,8,TODO: improve with a pool of speakers for data efficiency,
Real-Time-Voice-Cloning/encoder/data_objects/speaker_batch.py,10,"Array of shape (n_speakers * n_utterances, n_frames, mel_n), e.g. for 3 speakers with",
Real-Time-Voice-Cloning/encoder/data_objects/speaker_batch.py,11,"4 utterances each of 160 frames of 40 mel coefficients: (12, 160, 40)",
Real-Time-Voice-Cloning/utils/argutils.py,5,In decreasing order,
Real-Time-Voice-Cloning/utils/profiler.py,17,Log the time needed to execute that function,
Real-Time-Voice-Cloning/utils/logmmse.py,1,The MIT License (MIT),
Real-Time-Voice-Cloning/utils/logmmse.py,2,,
Real-Time-Voice-Cloning/utils/logmmse.py,3,Copyright (c) 2015 braindead,
Real-Time-Voice-Cloning/utils/logmmse.py,4,,
Real-Time-Voice-Cloning/utils/logmmse.py,5,"Permission is hereby granted, free of charge, to any person obtaining a copy",
Real-Time-Voice-Cloning/utils/logmmse.py,6,"of this software and associated documentation files (the ""Software""), to deal",
Real-Time-Voice-Cloning/utils/logmmse.py,7,"in the Software without restriction, including without limitation the rights",
Real-Time-Voice-Cloning/utils/logmmse.py,8,"to use, copy, modify, merge, publish, distribute, sublicense, and/or sell",
Real-Time-Voice-Cloning/utils/logmmse.py,9,"copies of the Software, and to permit persons to whom the Software is",
Real-Time-Voice-Cloning/utils/logmmse.py,10,"furnished to do so, subject to the following conditions:",
Real-Time-Voice-Cloning/utils/logmmse.py,11,,
Real-Time-Voice-Cloning/utils/logmmse.py,12,The above copyright notice and this permission notice shall be included in all,
Real-Time-Voice-Cloning/utils/logmmse.py,13,copies or substantial portions of the Software.,
Real-Time-Voice-Cloning/utils/logmmse.py,14,,
Real-Time-Voice-Cloning/utils/logmmse.py,15,"THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR",
Real-Time-Voice-Cloning/utils/logmmse.py,16,"IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,",
Real-Time-Voice-Cloning/utils/logmmse.py,17,FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE,
Real-Time-Voice-Cloning/utils/logmmse.py,18,"AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER",
Real-Time-Voice-Cloning/utils/logmmse.py,19,"LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,",
Real-Time-Voice-Cloning/utils/logmmse.py,20,OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE,
Real-Time-Voice-Cloning/utils/logmmse.py,21,SOFTWARE.,
Real-Time-Voice-Cloning/utils/logmmse.py,22,,
Real-Time-Voice-Cloning/utils/logmmse.py,23,,
Real-Time-Voice-Cloning/utils/logmmse.py,24,This code was extracted from the logmmse package (https://pypi.org/project/logmmse/) and I,
Real-Time-Voice-Cloning/utils/logmmse.py,25,simply modified the interface to meet my needs.,
Real-Time-Voice-Cloning/utils/logmmse.py,136,Alternative VAD algorithm to webrctvad. It has the advantage of not requiring to install that,
Real-Time-Voice-Cloning/utils/logmmse.py,137,darn package and it also works for any sampling rate. Maybe I'll eventually use it instead of,
Real-Time-Voice-Cloning/utils/logmmse.py,138,webrctvad,
Real-Time-Voice-Cloning/utils/logmmse.py,139,"def vad(wav, sampling_rate, eta=0.15, window_size=0):",
Real-Time-Voice-Cloning/utils/logmmse.py,140,"""""""",
Real-Time-Voice-Cloning/utils/logmmse.py,141,TODO: fix doc,
Real-Time-Voice-Cloning/utils/logmmse.py,142,Creates a profile of the noise in a given waveform.,
Real-Time-Voice-Cloning/utils/logmmse.py,143,,
Real-Time-Voice-Cloning/utils/logmmse.py,144,":param wav: a waveform containing noise ONLY, as a numpy array of floats or ints.",
Real-Time-Voice-Cloning/utils/logmmse.py,145,:param sampling_rate: the sampling rate of the audio,
Real-Time-Voice-Cloning/utils/logmmse.py,146,:param window_size: the size of the window the logmmse algorithm operates on. A default value,
Real-Time-Voice-Cloning/utils/logmmse.py,147,will be picked if left as 0.,
Real-Time-Voice-Cloning/utils/logmmse.py,148,:param eta: voice threshold for noise update. While the voice activation detection value is,
Real-Time-Voice-Cloning/utils/logmmse.py,149,"below this threshold, the noise profile will be continuously updated throughout the audio.",
Real-Time-Voice-Cloning/utils/logmmse.py,150,Set to 0 to disable updating the noise profile.,
Real-Time-Voice-Cloning/utils/logmmse.py,151,"""""""",
Real-Time-Voice-Cloning/utils/logmmse.py,152,"wav, dtype = to_float(wav)",
Real-Time-Voice-Cloning/utils/logmmse.py,153,wav += np.finfo(np.float64).eps,
Real-Time-Voice-Cloning/utils/logmmse.py,154,,
Real-Time-Voice-Cloning/utils/logmmse.py,155,if window_size == 0:,
Real-Time-Voice-Cloning/utils/logmmse.py,156,window_size = int(math.floor(0.02 * sampling_rate)),
Real-Time-Voice-Cloning/utils/logmmse.py,157,,
Real-Time-Voice-Cloning/utils/logmmse.py,158,if window_size % 2 == 1:,
Real-Time-Voice-Cloning/utils/logmmse.py,159,window_size = window_size + 1,
Real-Time-Voice-Cloning/utils/logmmse.py,160,,
Real-Time-Voice-Cloning/utils/logmmse.py,161,perc = 50,
Real-Time-Voice-Cloning/utils/logmmse.py,162,len1 = int(math.floor(window_size * perc / 100)),
Real-Time-Voice-Cloning/utils/logmmse.py,163,len2 = int(window_size - len1),
Real-Time-Voice-Cloning/utils/logmmse.py,164,,
Real-Time-Voice-Cloning/utils/logmmse.py,165,win = np.hanning(window_size),
Real-Time-Voice-Cloning/utils/logmmse.py,166,win = win * len2 / np.sum(win),
Real-Time-Voice-Cloning/utils/logmmse.py,167,n_fft = 2 * window_size,
Real-Time-Voice-Cloning/utils/logmmse.py,168,,
Real-Time-Voice-Cloning/utils/logmmse.py,169,wav_mean = np.zeros(n_fft),
Real-Time-Voice-Cloning/utils/logmmse.py,170,n_frames = len(wav) // window_size,
Real-Time-Voice-Cloning/utils/logmmse.py,171,"for j in range(0, window_size * n_frames, window_size):",
Real-Time-Voice-Cloning/utils/logmmse.py,172,"wav_mean += np.absolute(np.fft.fft(win * wav[j:j + window_size], n_fft, axis=0))",
Real-Time-Voice-Cloning/utils/logmmse.py,173,noise_mu2 = (wav_mean / n_frames) ** 2,
Real-Time-Voice-Cloning/utils/logmmse.py,174,,
Real-Time-Voice-Cloning/utils/logmmse.py,175,"wav, dtype = to_float(wav)",
Real-Time-Voice-Cloning/utils/logmmse.py,176,wav += np.finfo(np.float64).eps,
Real-Time-Voice-Cloning/utils/logmmse.py,177,,
Real-Time-Voice-Cloning/utils/logmmse.py,178,nframes = int(math.floor(len(wav) / len2) - math.floor(window_size / len2)),
Real-Time-Voice-Cloning/utils/logmmse.py,179,"vad = np.zeros(nframes * len2, dtype=np.bool)",
Real-Time-Voice-Cloning/utils/logmmse.py,180,,
Real-Time-Voice-Cloning/utils/logmmse.py,181,aa = 0.98,
Real-Time-Voice-Cloning/utils/logmmse.py,182,mu = 0.98,
Real-Time-Voice-Cloning/utils/logmmse.py,183,ksi_min = 10 ** (-25 / 10),
Real-Time-Voice-Cloning/utils/logmmse.py,184,,
Real-Time-Voice-Cloning/utils/logmmse.py,185,xk_prev = np.zeros(len1),
Real-Time-Voice-Cloning/utils/logmmse.py,186,noise_mu2 = noise_mu2,
Real-Time-Voice-Cloning/utils/logmmse.py,187,"for k in range(0, nframes * len2, len2):",
Real-Time-Voice-Cloning/utils/logmmse.py,188,insign = win * wav[k:k + window_size],
Real-Time-Voice-Cloning/utils/logmmse.py,189,,
Real-Time-Voice-Cloning/utils/logmmse.py,190,"spec = np.fft.fft(insign, n_fft, axis=0)",
Real-Time-Voice-Cloning/utils/logmmse.py,191,sig = np.absolute(spec),
Real-Time-Voice-Cloning/utils/logmmse.py,192,sig2 = sig ** 2,
Real-Time-Voice-Cloning/utils/logmmse.py,193,,
Real-Time-Voice-Cloning/utils/logmmse.py,194,"gammak = np.minimum(sig2 / noise_mu2, 40)",
Real-Time-Voice-Cloning/utils/logmmse.py,195,,
Real-Time-Voice-Cloning/utils/logmmse.py,196,if xk_prev.all() == 0:,
Real-Time-Voice-Cloning/utils/logmmse.py,197,"ksi = aa + (1 - aa) * np.maximum(gammak - 1, 0)",
Real-Time-Voice-Cloning/utils/logmmse.py,198,else:,
Real-Time-Voice-Cloning/utils/logmmse.py,199,"ksi = aa * xk_prev / noise_mu2 + (1 - aa) * np.maximum(gammak - 1, 0)",
Real-Time-Voice-Cloning/utils/logmmse.py,200,"ksi = np.maximum(ksi_min, ksi)",
Real-Time-Voice-Cloning/utils/logmmse.py,201,,
Real-Time-Voice-Cloning/utils/logmmse.py,202,log_sigma_k = gammak * ksi / (1 + ksi) - np.log(1 + ksi),
Real-Time-Voice-Cloning/utils/logmmse.py,203,vad_decision = np.sum(log_sigma_k) / window_size,
Real-Time-Voice-Cloning/utils/logmmse.py,204,if vad_decision < eta:,
Real-Time-Voice-Cloning/utils/logmmse.py,205,noise_mu2 = mu * noise_mu2 + (1 - mu) * sig2,
Real-Time-Voice-Cloning/utils/logmmse.py,206,print(vad_decision),
Real-Time-Voice-Cloning/utils/logmmse.py,207,,
Real-Time-Voice-Cloning/utils/logmmse.py,208,a = ksi / (1 + ksi),
Real-Time-Voice-Cloning/utils/logmmse.py,209,vk = a * gammak,
Real-Time-Voice-Cloning/utils/logmmse.py,210,"ei_vk = 0.5 * expn(1, np.maximum(vk, 1e-8))",
Real-Time-Voice-Cloning/utils/logmmse.py,211,hw = a * np.exp(ei_vk),
Real-Time-Voice-Cloning/utils/logmmse.py,212,sig = sig * hw,
Real-Time-Voice-Cloning/utils/logmmse.py,213,xk_prev = sig ** 2,
Real-Time-Voice-Cloning/utils/logmmse.py,214,,
Real-Time-Voice-Cloning/utils/logmmse.py,215,vad[k:k + len2] = vad_decision >= eta,
Real-Time-Voice-Cloning/utils/logmmse.py,216,,
Real-Time-Voice-Cloning/utils/logmmse.py,217,"vad = np.pad(vad, (0, len(wav) - len(vad)), mode=""constant"")",
Real-Time-Voice-Cloning/utils/logmmse.py,218,return vad,
Real-Time-Voice-Cloning/vocoder/distribution.py,8,TF ordering,
Real-Time-Voice-Cloning/vocoder/distribution.py,15,It is adapted from https://github.com/r9y9/wavenet_vocoder/blob/master/wavenet_vocoder/mixture.py,
Real-Time-Voice-Cloning/vocoder/distribution.py,25,(B x T x C),
Real-Time-Voice-Cloning/vocoder/distribution.py,28,"unpack parameters. (B, T, num_mixtures) x 3",
Real-Time-Voice-Cloning/vocoder/distribution.py,33,B x T x 1 -> B x T x num_mixtures,
Real-Time-Voice-Cloning/vocoder/distribution.py,43,log probability for edge case of 0 (before scaling),
Real-Time-Voice-Cloning/vocoder/distribution.py,44,equivalent: torch.log(F.sigmoid(plus_in)),
Real-Time-Voice-Cloning/vocoder/distribution.py,47,log probability for edge case of 255 (before scaling),
Real-Time-Voice-Cloning/vocoder/distribution.py,48,equivalent: (1 - F.sigmoid(min_in)).log(),
Real-Time-Voice-Cloning/vocoder/distribution.py,51,probability for all other cases,
Real-Time-Voice-Cloning/vocoder/distribution.py,55,"log probability in the center of the bin, to be used in extreme cases",
Real-Time-Voice-Cloning/vocoder/distribution.py,56,(not actually used in our code),
Real-Time-Voice-Cloning/vocoder/distribution.py,59,tf equivalent,
Real-Time-Voice-Cloning/vocoder/distribution.py,67,TODO: cdf_delta <= 1e-5 actually can happen. How can we choose the value,
Real-Time-Voice-Cloning/vocoder/distribution.py,68,for num_classes=65536 case? 1e-7? not sure..,
Real-Time-Voice-Cloning/vocoder/distribution.py,101,B x T x C,
Real-Time-Voice-Cloning/vocoder/distribution.py,105,sample mixture indicator from softmax,
Real-Time-Voice-Cloning/vocoder/distribution.py,110,"(B, T) -> (B, T, nr_mix)",
Real-Time-Voice-Cloning/vocoder/distribution.py,112,select logistic parameters,
Real-Time-Voice-Cloning/vocoder/distribution.py,116,sample from logistic & clip to interval,
Real-Time-Voice-Cloning/vocoder/distribution.py,117,we don't actually round to the nearest 8bit value when sampling,
Real-Time-Voice-Cloning/vocoder/distribution.py,127,we perform one hot encore with respect to the last axis,
Real-Time-Voice-Cloning/vocoder/hparams.py,4,Audio settings------------------------------------------------------------------------,
Real-Time-Voice-Cloning/vocoder/hparams.py,5,Match the values of the synthesizer,
Real-Time-Voice-Cloning/vocoder/hparams.py,18,bit depth of signal,
Real-Time-Voice-Cloning/vocoder/hparams.py,19,Recommended to suppress noise if using raw bits in hp.voc_mode,
Real-Time-Voice-Cloning/vocoder/hparams.py,20,below,
Real-Time-Voice-Cloning/vocoder/hparams.py,23,WAVERNN / VOCODER --------------------------------------------------------------------------------,
Real-Time-Voice-Cloning/vocoder/hparams.py,24,either 'RAW' (softmax on raw bits) or 'MOL' (sample from,
Real-Time-Voice-Cloning/vocoder/hparams.py,25,mixture of logistics),
Real-Time-Voice-Cloning/vocoder/hparams.py,26,NB - this needs to correctly factorise hop_length,
Real-Time-Voice-Cloning/vocoder/hparams.py,33,Training,
Real-Time-Voice-Cloning/vocoder/hparams.py,36,number of samples to generate at each checkpoint,
Real-Time-Voice-Cloning/vocoder/hparams.py,37,this will pad the input so that the resnet can 'see' wider,
Real-Time-Voice-Cloning/vocoder/hparams.py,38,than input length,
Real-Time-Voice-Cloning/vocoder/hparams.py,39,must be a multiple of hop_length,
Real-Time-Voice-Cloning/vocoder/hparams.py,41,Generating / Synthesizing,
Real-Time-Voice-Cloning/vocoder/hparams.py,42,very fast (realtime+) single utterance batched generation,
Real-Time-Voice-Cloning/vocoder/hparams.py,43,target number of samples to be generated in each batch entry,
Real-Time-Voice-Cloning/vocoder/hparams.py,44,number of samples for crossfading between batches,
Real-Time-Voice-Cloning/vocoder/vocoder_dataset.py,27,"Load the mel spectrogram and adjust its range to [-1, 1]",
Real-Time-Voice-Cloning/vocoder/vocoder_dataset.py,30,Load the wav,
Real-Time-Voice-Cloning/vocoder/vocoder_dataset.py,36,Fix for missing padding   # TODO: settle on whether this is any useful,
Real-Time-Voice-Cloning/vocoder/vocoder_dataset.py,43,Quantize the wav,
Real-Time-Voice-Cloning/vocoder/train.py,17,Check to make sure the hop length is correctly factorised,
Real-Time-Voice-Cloning/vocoder/train.py,20,Instantiate the model,
Real-Time-Voice-Cloning/vocoder/train.py,37,Initialize the optimizer,
Real-Time-Voice-Cloning/vocoder/train.py,43,Load the weights,
Real-Time-Voice-Cloning/vocoder/train.py,55,Initialize the dataset,
Real-Time-Voice-Cloning/vocoder/train.py,66,Begin the training,
Real-Time-Voice-Cloning/vocoder/train.py,84,Forward pass,
Real-Time-Voice-Cloning/vocoder/train.py,92,Backward pass,
Real-Time-Voice-Cloning/vocoder/inference.py,6,type: WaveRNN,
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,15,The main matmul,
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,18,Output fc layers,
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,24,Input fc layers,
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,28,biases for the gates,
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,33,display num params,
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,39,Main matmul - the projection is split 3 ways,
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,43,Project the prev input,
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,48,Project the prev input and current coarse sample,
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,54,concatenate for the gates,
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,59,Compute all gates for coarse and fine,
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,65,Split the hidden state,
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,68,Compute outputs,
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,77,First split up the biases for the gates,
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,82,Lists for the two output seqs,
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,85,Some initial inputs,
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,89,We'll meed a hidden state,
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,92,Need a clock for display,
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,95,Loop for generation,
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,98,Split into two hidden states,
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,102,Scale and concat previous predictions,
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,107,Project input,
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,112,Project hidden state and split 6 ways,
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,118,Compute the coarse gates,
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,124,Compute the coarse output,
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,131,Project the [prev outputs and predicted coarse sample],
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,138,Compute the fine gates,
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,144,Compute the fine output,
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,151,Put the hidden state back together,
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,154,Display progress,
Real-Time-Voice-Cloning/vocoder/models/fatchord_version.py,204,x = torch.FloatTensor([[sample]]).cuda(),
Real-Time-Voice-Cloning/vocoder/models/fatchord_version.py,235,Fade-out at the end to avoid signal cutting out suddenly,
Real-Time-Voice-Cloning/vocoder/models/fatchord_version.py,259,NB - this is just a quick method i need right now,
Real-Time-Voice-Cloning/vocoder/models/fatchord_version.py,260,"i.e., it won't generalise to other shapes/dims",
Real-Time-Voice-Cloning/vocoder/models/fatchord_version.py,298,Calculate variables needed,
Real-Time-Voice-Cloning/vocoder/models/fatchord_version.py,303,Pad if some time steps poking out,
Real-Time-Voice-Cloning/vocoder/models/fatchord_version.py,311,Get the values for the folded tensor,
Real-Time-Voice-Cloning/vocoder/models/fatchord_version.py,355,Need some silence for the rnn warmup,
Real-Time-Voice-Cloning/vocoder/models/fatchord_version.py,360,Equal power crossfade,
Real-Time-Voice-Cloning/vocoder/models/fatchord_version.py,365,Concat the silence to the fades,
Real-Time-Voice-Cloning/vocoder/models/fatchord_version.py,369,Apply the gain to the overlap samples,
Real-Time-Voice-Cloning/vocoder/models/fatchord_version.py,375,Loop to add up all the samples,
Real-Time-Voice-Cloning/vocoder/models/fatchord_version.py,400,Backwards compatibility,
