file path,line #,comment,satd
transformers/setup.py,48,Remove stale transformers.egg-info directory to avoid https://github.com/pypa/pip/issues/5466,
transformers/setup.py,100,dataclasses for Python versions that don't have it,
transformers/setup.py,102,accessing files from S3 directly,
transformers/setup.py,104,filesystem locks e.g. to prevent parallel downloads,
transformers/setup.py,106,for downloading models over HTTPS,
transformers/setup.py,108,progress bars in model download and training scripts,
transformers/setup.py,110,for OpenAI GPT,
transformers/setup.py,112,for XLNet,
transformers/setup.py,114,for XLM,
transformers/templates/adding_a_new_model/tokenization_xxx.py,1,coding=utf-8,
transformers/templates/adding_a_new_model/tokenization_xxx.py,2,Copyright 2018 XXX Authors.,
transformers/templates/adding_a_new_model/tokenization_xxx.py,3,,
transformers/templates/adding_a_new_model/tokenization_xxx.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/templates/adding_a_new_model/tokenization_xxx.py,5,you may not use this file except in compliance with the License.,
transformers/templates/adding_a_new_model/tokenization_xxx.py,6,You may obtain a copy of the License at,
transformers/templates/adding_a_new_model/tokenization_xxx.py,7,,
transformers/templates/adding_a_new_model/tokenization_xxx.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/templates/adding_a_new_model/tokenization_xxx.py,9,,
transformers/templates/adding_a_new_model/tokenization_xxx.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/templates/adding_a_new_model/tokenization_xxx.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/templates/adding_a_new_model/tokenization_xxx.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/templates/adding_a_new_model/tokenization_xxx.py,13,See the License for the specific language governing permissions and,
transformers/templates/adding_a_new_model/tokenization_xxx.py,14,limitations under the License.,
transformers/templates/adding_a_new_model/tokenization_xxx.py,27,,
transformers/templates/adding_a_new_model/tokenization_xxx.py,28,"In this template, replace all the XXX (various casings) with your model name",
transformers/templates/adding_a_new_model/tokenization_xxx.py,29,,
transformers/templates/adding_a_new_model/tokenization_xxx.py,31,,
transformers/templates/adding_a_new_model/tokenization_xxx.py,32,Mapping from the keyword arguments names of Tokenizer `__init__`,
transformers/templates/adding_a_new_model/tokenization_xxx.py,33,to file names for serializing Tokenizer instances,
transformers/templates/adding_a_new_model/tokenization_xxx.py,34,,
transformers/templates/adding_a_new_model/tokenization_xxx.py,37,,
transformers/templates/adding_a_new_model/tokenization_xxx.py,38,Mapping from the keyword arguments names of Tokenizer `__init__`,
transformers/templates/adding_a_new_model/tokenization_xxx.py,39,to pretrained vocabulary URL for all the model shortcut names.,
transformers/templates/adding_a_new_model/tokenization_xxx.py,40,,
transformers/templates/adding_a_new_model/tokenization_xxx.py,48,,
transformers/templates/adding_a_new_model/tokenization_xxx.py,49,Mapping from model shortcut names to max length of inputs,
transformers/templates/adding_a_new_model/tokenization_xxx.py,50,,
transformers/templates/adding_a_new_model/tokenization_xxx.py,56,,
transformers/templates/adding_a_new_model/tokenization_xxx.py,57,Mapping from model shortcut names to a dictionary of additional,
transformers/templates/adding_a_new_model/tokenization_xxx.py,58,keyword arguments for Tokenizer `__init__`.,
transformers/templates/adding_a_new_model/tokenization_xxx.py,59,To be used for checkpoint specific configurations.,
transformers/templates/adding_a_new_model/tokenization_xxx.py,60,,
transformers/templates/adding_a_new_model/tokenization_xxx.py,120,take into account special tokens,
transformers/templates/adding_a_new_model/tokenization_xxx.py,121,take into account special tokens,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,1,coding=utf-8,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,4,,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,6,you may not use this file except in compliance with the License.,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,7,You may obtain a copy of the License at,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,8,,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,10,,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,11,"Unless required by applicable law or agreed to in writing, software",
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,14,See the License for the specific language governing permissions and,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,15,limitations under the License.,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,18,,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,19,"In this template, replace all the XXX (various casings) with your model name",
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,20,,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,34,,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,35,This dict contrains shortcut names and associated url,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,36,for the pretrained weights provided with the models,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,37,,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,44,,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,45,TF 2.0 Models are constructed using Keras imperative API by sub-classing,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,46,- tf.keras.layers.Layer for the layers and,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,47,- TFPreTrainedModel for the models (itself a sub-class of tf.keras.Model),
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,48,,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,50,,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,51,Here is an example of typical layer in a TF 2.0 model of the library,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,52,The classes are usually identical to the PyTorch ones and prefixed with 'TF'.,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,53,,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,54,Note that class __init__ parameters includes **kwargs (send to 'super').,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,55,This let us have a control on class scope and variable names:,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,56,"More precisely, we set the names of the class attributes (lower level layers) to",
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,57,to the equivalent attributes names in the PyTorch model so we can have equivalent,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,58,class and scope structure between PyTorch and TF 2.0 models and easily load one in the other.,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,59,,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,60,See the conversion methods in modeling_tf_pytorch_utils.py for more details,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,61,,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,84,add attentions if we output them,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,88,,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,89,The full model without a specific pretrained or finetuning head is,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,90,"provided as a tf.keras.layers.Layer usually called ""TFXxxMainLayer""",
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,91,,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,97,Not implemented yet in the library fr TF 2.0 models,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,100,Not implemented yet in the library fr TF 2.0 models,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,105,We allow three types of multi-inputs:,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,106,- traditional keyword arguments in the call method,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,107,- all the arguments provided as a dict in the first positional argument of call,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,108,- all the arguments provided as a list/tuple (ordered) in the first positional argument of call,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,109,The last two options are useful to use the tf.keras fit() method.,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,133,We create a 3D attention mask from a 2D tensor mask.,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,134,"Sizes are [batch_size, 1, 1, to_seq_length]",
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,135,"So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]",
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,136,this attention mask is more simple than the triangular masking of causal attention,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,137,"used in OpenAI GPT, we just need to prepare the broadcast dimension here.",
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,140,Since attention_mask is 1.0 for positions we want to attend and 0.0 for,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,141,"masked positions, this operation will create a tensor which is 0.0 for",
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,142,positions we want to attend and -10000.0 for masked positions.,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,143,"Since we are adding it to the raw scores before the softmax, this is",
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,144,effectively the same as removing these entirely.,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,149,Prepare head mask if needed,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,150,1.0 in head_mask indicate we keep the head,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,151,attention_probs has shape bsz x n_heads x N x N,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,152,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads],
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,153,and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length],
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,158,head_mask = tf.constant([0] * self.num_hidden_layers),
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,160,,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,161,Replace this with your model code,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,165,add hidden_states and attentions if they are here,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,167,"sequence_output, (hidden_states), (attentions)",
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,170,,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,171,TFXxxPreTrainedModel is a sub-class of tf.keras.Model,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,172,which take care of loading and saving pretrained weights,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,173,and various common utilities.,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,174,Here you just need to specify a few (self-explanatory),
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,175,pointers for your model.,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,176,,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,362,Add hidden states and attention if they are here,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,364,"prediction_scores, (hidden_states), (attentions)",
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,417,add hidden states and attention if they are here,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,419,"logits, (hidden_states), (attentions)",
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,472,add hidden states and attention if they are here,
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,474,"scores, (hidden_states), (attentions)",
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,532,"start_logits, end_logits, (hidden_states), (attentions)",
transformers/templates/adding_a_new_model/configuration_xxx.py,1,coding=utf-8,
transformers/templates/adding_a_new_model/configuration_xxx.py,2,"Copyright 2010, XXX authors",
transformers/templates/adding_a_new_model/configuration_xxx.py,3,,
transformers/templates/adding_a_new_model/configuration_xxx.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/templates/adding_a_new_model/configuration_xxx.py,5,you may not use this file except in compliance with the License.,
transformers/templates/adding_a_new_model/configuration_xxx.py,6,You may obtain a copy of the License at,
transformers/templates/adding_a_new_model/configuration_xxx.py,7,,
transformers/templates/adding_a_new_model/configuration_xxx.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/templates/adding_a_new_model/configuration_xxx.py,9,,
transformers/templates/adding_a_new_model/configuration_xxx.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/templates/adding_a_new_model/configuration_xxx.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/templates/adding_a_new_model/configuration_xxx.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/templates/adding_a_new_model/configuration_xxx.py,13,See the License for the specific language governing permissions and,
transformers/templates/adding_a_new_model/configuration_xxx.py,14,limitations under the License.,
transformers/templates/adding_a_new_model/modeling_xxx.py,1,coding=utf-8,
transformers/templates/adding_a_new_model/modeling_xxx.py,2,Copyright 2018 XXX Authors,
transformers/templates/adding_a_new_model/modeling_xxx.py,3,,
transformers/templates/adding_a_new_model/modeling_xxx.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/templates/adding_a_new_model/modeling_xxx.py,5,you may not use this file except in compliance with the License.,
transformers/templates/adding_a_new_model/modeling_xxx.py,6,You may obtain a copy of the License at,
transformers/templates/adding_a_new_model/modeling_xxx.py,7,,
transformers/templates/adding_a_new_model/modeling_xxx.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/templates/adding_a_new_model/modeling_xxx.py,9,,
transformers/templates/adding_a_new_model/modeling_xxx.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/templates/adding_a_new_model/modeling_xxx.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/templates/adding_a_new_model/modeling_xxx.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/templates/adding_a_new_model/modeling_xxx.py,13,See the License for the specific language governing permissions and,
transformers/templates/adding_a_new_model/modeling_xxx.py,14,limitations under the License.,
transformers/templates/adding_a_new_model/modeling_xxx.py,17,,
transformers/templates/adding_a_new_model/modeling_xxx.py,18,"In this template, replace all the XXX (various casings) with your model name",
transformers/templates/adding_a_new_model/modeling_xxx.py,19,,
transformers/templates/adding_a_new_model/modeling_xxx.py,36,,
transformers/templates/adding_a_new_model/modeling_xxx.py,37,This dict contrains shortcut names and associated url,
transformers/templates/adding_a_new_model/modeling_xxx.py,38,for the pretrained weights provided with the models,
transformers/templates/adding_a_new_model/modeling_xxx.py,39,,
transformers/templates/adding_a_new_model/modeling_xxx.py,46,,
transformers/templates/adding_a_new_model/modeling_xxx.py,47,This is a conversion method from TF 1.0 to PyTorch,
transformers/templates/adding_a_new_model/modeling_xxx.py,48,More details: https://medium.com/huggingface/from-tensorflow-to-pytorch-265f40ef2a28,
transformers/templates/adding_a_new_model/modeling_xxx.py,49,,
transformers/templates/adding_a_new_model/modeling_xxx.py,65,Load weights from TF model,
transformers/templates/adding_a_new_model/modeling_xxx.py,77,adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v,
transformers/templates/adding_a_new_model/modeling_xxx.py,78,which are not required for using pretrained model,
transformers/templates/adding_a_new_model/modeling_xxx.py,122,,
transformers/templates/adding_a_new_model/modeling_xxx.py,123,PyTorch Models are constructed by sub-classing,
transformers/templates/adding_a_new_model/modeling_xxx.py,124,- torch.nn.Module for the layers and,
transformers/templates/adding_a_new_model/modeling_xxx.py,125,- PreTrainedModel for the models (itself a sub-class of torch.nn.Module),
transformers/templates/adding_a_new_model/modeling_xxx.py,126,,
transformers/templates/adding_a_new_model/modeling_xxx.py,128,,
transformers/templates/adding_a_new_model/modeling_xxx.py,129,Here is an example of typical layer in a PyTorch model of the library,
transformers/templates/adding_a_new_model/modeling_xxx.py,130,The classes are usually identical to the TF 2.0 ones without the 'TF' prefix.,
transformers/templates/adding_a_new_model/modeling_xxx.py,131,,
transformers/templates/adding_a_new_model/modeling_xxx.py,132,See the conversion methods in modeling_tf_pytorch_utils.py for more details,
transformers/templates/adding_a_new_model/modeling_xxx.py,133,,
transformers/templates/adding_a_new_model/modeling_xxx.py,154,add attentions if we output them,
transformers/templates/adding_a_new_model/modeling_xxx.py,158,,
transformers/templates/adding_a_new_model/modeling_xxx.py,159,PreTrainedModel is a sub-class of torch.nn.Module,
transformers/templates/adding_a_new_model/modeling_xxx.py,160,which take care of loading and saving pretrained weights,
transformers/templates/adding_a_new_model/modeling_xxx.py,161,and various common utilities.,
transformers/templates/adding_a_new_model/modeling_xxx.py,162,,
transformers/templates/adding_a_new_model/modeling_xxx.py,163,Here you just need to specify a few (self-explanatory),
transformers/templates/adding_a_new_model/modeling_xxx.py,164,pointers for your model and the weights initialization,
transformers/templates/adding_a_new_model/modeling_xxx.py,165,method if its not fully covered by PreTrainedModel's default method,
transformers/templates/adding_a_new_model/modeling_xxx.py,166,,
transformers/templates/adding_a_new_model/modeling_xxx.py,190,Slightly different from the TF version which uses truncated_normal for initialization,
transformers/templates/adding_a_new_model/modeling_xxx.py,191,cf https://github.com/pytorch/pytorch/pull/5617,
transformers/templates/adding_a_new_model/modeling_xxx.py,351,We create a 3D attention mask from a 2D tensor mask.,
transformers/templates/adding_a_new_model/modeling_xxx.py,352,"Sizes are [batch_size, 1, 1, to_seq_length]",
transformers/templates/adding_a_new_model/modeling_xxx.py,353,"So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]",
transformers/templates/adding_a_new_model/modeling_xxx.py,354,this attention mask is more simple than the triangular masking of causal attention,
transformers/templates/adding_a_new_model/modeling_xxx.py,355,"used in OpenAI GPT, we just need to prepare the broadcast dimension here.",
transformers/templates/adding_a_new_model/modeling_xxx.py,358,Since attention_mask is 1.0 for positions we want to attend and 0.0 for,
transformers/templates/adding_a_new_model/modeling_xxx.py,359,"masked positions, this operation will create a tensor which is 0.0 for",
transformers/templates/adding_a_new_model/modeling_xxx.py,360,positions we want to attend and -10000.0 for masked positions.,
transformers/templates/adding_a_new_model/modeling_xxx.py,361,"Since we are adding it to the raw scores before the softmax, this is",
transformers/templates/adding_a_new_model/modeling_xxx.py,362,effectively the same as removing these entirely.,
transformers/templates/adding_a_new_model/modeling_xxx.py,363,fp16 compatibility,
transformers/templates/adding_a_new_model/modeling_xxx.py,366,Prepare head mask if needed,
transformers/templates/adding_a_new_model/modeling_xxx.py,367,1.0 in head_mask indicate we keep the head,
transformers/templates/adding_a_new_model/modeling_xxx.py,368,attention_probs has shape bsz x n_heads x N x N,
transformers/templates/adding_a_new_model/modeling_xxx.py,369,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads],
transformers/templates/adding_a_new_model/modeling_xxx.py,370,and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length],
transformers/templates/adding_a_new_model/modeling_xxx.py,378,We can specify head_mask for each layer,
transformers/templates/adding_a_new_model/modeling_xxx.py,381,switch to fload if need + fp16 compatibility,
transformers/templates/adding_a_new_model/modeling_xxx.py,385,,
transformers/templates/adding_a_new_model/modeling_xxx.py,386,Replace this with your model code,
transformers/templates/adding_a_new_model/modeling_xxx.py,392,add hidden_states and attentions if they are here,
transformers/templates/adding_a_new_model/modeling_xxx.py,394,"sequence_output, (hidden_states), (attentions)",
transformers/templates/adding_a_new_model/modeling_xxx.py,465,Add hidden states and attention if they are here,
transformers/templates/adding_a_new_model/modeling_xxx.py,471,"(masked_lm_loss), prediction_scores, (hidden_states), (attentions)",
transformers/templates/adding_a_new_model/modeling_xxx.py,547,add hidden states and attention if they are here,
transformers/templates/adding_a_new_model/modeling_xxx.py,551,We are doing regression,
transformers/templates/adding_a_new_model/modeling_xxx.py,559,"(loss), logits, (hidden_states), (attentions)",
transformers/templates/adding_a_new_model/modeling_xxx.py,633,add hidden states and attention if they are here,
transformers/templates/adding_a_new_model/modeling_xxx.py,636,Only keep active parts of the loss,
transformers/templates/adding_a_new_model/modeling_xxx.py,646,"(loss), scores, (hidden_states), (attentions)",
transformers/templates/adding_a_new_model/modeling_xxx.py,736,"If we are on multi-GPU, split add a dimension",
transformers/templates/adding_a_new_model/modeling_xxx.py,741,"sometimes the start/end positions are outside our model inputs, we ignore these terms",
transformers/templates/adding_a_new_model/modeling_xxx.py,752,"(loss), start_logits, end_logits, (hidden_states), (attentions)",
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,1,coding=utf-8,
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,2,Copyright 2018 The HuggingFace Inc. team.,
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,3,,
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,5,you may not use this file except in compliance with the License.,
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,6,You may obtain a copy of the License at,
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,7,,
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,9,,
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,13,See the License for the specific language governing permissions and,
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,14,limitations under the License.,
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,30,Initialise PyTorch model,
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,35,Load weights from tf checkpoint,
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,38,Save pytorch-model,
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,45,Required parameters,
transformers/templates/adding_a_new_model/tests/test_modeling_xxx.py,1,coding=utf-8,
transformers/templates/adding_a_new_model/tests/test_modeling_xxx.py,2,Copyright 2018 XXX Authors.,
transformers/templates/adding_a_new_model/tests/test_modeling_xxx.py,3,,
transformers/templates/adding_a_new_model/tests/test_modeling_xxx.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/templates/adding_a_new_model/tests/test_modeling_xxx.py,5,you may not use this file except in compliance with the License.,
transformers/templates/adding_a_new_model/tests/test_modeling_xxx.py,6,You may obtain a copy of the License at,
transformers/templates/adding_a_new_model/tests/test_modeling_xxx.py,7,,
transformers/templates/adding_a_new_model/tests/test_modeling_xxx.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/templates/adding_a_new_model/tests/test_modeling_xxx.py,9,,
transformers/templates/adding_a_new_model/tests/test_modeling_xxx.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/templates/adding_a_new_model/tests/test_modeling_xxx.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/templates/adding_a_new_model/tests/test_modeling_xxx.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/templates/adding_a_new_model/tests/test_modeling_xxx.py,13,See the License for the specific language governing permissions and,
transformers/templates/adding_a_new_model/tests/test_modeling_xxx.py,14,limitations under the License.,
transformers/templates/adding_a_new_model/tests/test_tokenization_xxx.py,1,coding=utf-8,
transformers/templates/adding_a_new_model/tests/test_tokenization_xxx.py,2,Copyright 2018 XXX Authors.,
transformers/templates/adding_a_new_model/tests/test_tokenization_xxx.py,3,,
transformers/templates/adding_a_new_model/tests/test_tokenization_xxx.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/templates/adding_a_new_model/tests/test_tokenization_xxx.py,5,you may not use this file except in compliance with the License.,
transformers/templates/adding_a_new_model/tests/test_tokenization_xxx.py,6,You may obtain a copy of the License at,
transformers/templates/adding_a_new_model/tests/test_tokenization_xxx.py,7,,
transformers/templates/adding_a_new_model/tests/test_tokenization_xxx.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/templates/adding_a_new_model/tests/test_tokenization_xxx.py,9,,
transformers/templates/adding_a_new_model/tests/test_tokenization_xxx.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/templates/adding_a_new_model/tests/test_tokenization_xxx.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/templates/adding_a_new_model/tests/test_tokenization_xxx.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/templates/adding_a_new_model/tests/test_tokenization_xxx.py,13,See the License for the specific language governing permissions and,
transformers/templates/adding_a_new_model/tests/test_tokenization_xxx.py,14,limitations under the License.,
transformers/templates/adding_a_new_model/tests/test_modeling_tf_xxx.py,1,coding=utf-8,
transformers/templates/adding_a_new_model/tests/test_modeling_tf_xxx.py,2,Copyright 2018 XXX Authors.,
transformers/templates/adding_a_new_model/tests/test_modeling_tf_xxx.py,3,,
transformers/templates/adding_a_new_model/tests/test_modeling_tf_xxx.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/templates/adding_a_new_model/tests/test_modeling_tf_xxx.py,5,you may not use this file except in compliance with the License.,
transformers/templates/adding_a_new_model/tests/test_modeling_tf_xxx.py,6,You may obtain a copy of the License at,
transformers/templates/adding_a_new_model/tests/test_modeling_tf_xxx.py,7,,
transformers/templates/adding_a_new_model/tests/test_modeling_tf_xxx.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/templates/adding_a_new_model/tests/test_modeling_tf_xxx.py,9,,
transformers/templates/adding_a_new_model/tests/test_modeling_tf_xxx.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/templates/adding_a_new_model/tests/test_modeling_tf_xxx.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/templates/adding_a_new_model/tests/test_modeling_tf_xxx.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/templates/adding_a_new_model/tests/test_modeling_tf_xxx.py,13,See the License for the specific language governing permissions and,
transformers/templates/adding_a_new_model/tests/test_modeling_tf_xxx.py,14,limitations under the License.,
transformers/templates/adding_a_new_example_script/run_xxx.py,1,coding=utf-8,
transformers/templates/adding_a_new_example_script/run_xxx.py,2,Copyright 2018 XXX.  All rights reserved.,
transformers/templates/adding_a_new_example_script/run_xxx.py,3,,
transformers/templates/adding_a_new_example_script/run_xxx.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/templates/adding_a_new_example_script/run_xxx.py,5,you may not use this file except in compliance with the License.,
transformers/templates/adding_a_new_example_script/run_xxx.py,6,You may obtain a copy of the License at,
transformers/templates/adding_a_new_example_script/run_xxx.py,7,,
transformers/templates/adding_a_new_example_script/run_xxx.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/templates/adding_a_new_example_script/run_xxx.py,9,,
transformers/templates/adding_a_new_example_script/run_xxx.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/templates/adding_a_new_example_script/run_xxx.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/templates/adding_a_new_example_script/run_xxx.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/templates/adding_a_new_example_script/run_xxx.py,13,See the License for the specific language governing permissions and,
transformers/templates/adding_a_new_example_script/run_xxx.py,14,limitations under the License.,
transformers/templates/adding_a_new_example_script/run_xxx.py,48,The follwing import is the official SQuAD evaluation script (2.0).,
transformers/templates/adding_a_new_example_script/run_xxx.py,49,You can remove it from the dependencies if you are using this script outside of the library,
transformers/templates/adding_a_new_example_script/run_xxx.py,50,We've added it here for automated tests (see examples/test_examples.py file),
transformers/templates/adding_a_new_example_script/run_xxx.py,96,Prepare optimizer and schedule (linear warmup and decay),
transformers/templates/adding_a_new_example_script/run_xxx.py,116,multi-gpu training (should be after apex fp16 initialization),
transformers/templates/adding_a_new_example_script/run_xxx.py,120,Distributed training (should be after apex fp16 initialization),
transformers/templates/adding_a_new_example_script/run_xxx.py,126,Train!,
transformers/templates/adding_a_new_example_script/run_xxx.py,144,Added here for reproductibility,
transformers/templates/adding_a_new_example_script/run_xxx.py,161,model outputs are always tuple in transformers (see doc),
transformers/templates/adding_a_new_example_script/run_xxx.py,164,mean() to average on multi-gpu parallel (not distributed) training,
transformers/templates/adding_a_new_example_script/run_xxx.py,182,Update learning rate schedule,
transformers/templates/adding_a_new_example_script/run_xxx.py,187,Log metrics,
transformers/templates/adding_a_new_example_script/run_xxx.py,190,Only evaluate when single GPU otherwise metrics may not average well,
transformers/templates/adding_a_new_example_script/run_xxx.py,199,Save model checkpoint,
transformers/templates/adding_a_new_example_script/run_xxx.py,205,Take care of distributed/parallel training,
transformers/templates/adding_a_new_example_script/run_xxx.py,230,Note that DistributedSampler samples randomly,
transformers/templates/adding_a_new_example_script/run_xxx.py,234,Eval!,
transformers/templates/adding_a_new_example_script/run_xxx.py,245,XLM don't use segment_ids,
transformers/templates/adding_a_new_example_script/run_xxx.py,255,XLNet uses a more complex post-processing procedure,
transformers/templates/adding_a_new_example_script/run_xxx.py,270,Compute predictions,
transformers/templates/adding_a_new_example_script/run_xxx.py,279,XLNet uses a more complex post-processing procedure,
transformers/templates/adding_a_new_example_script/run_xxx.py,312,Evaluate with the official SQuAD script,
transformers/templates/adding_a_new_example_script/run_xxx.py,322,"Make sure only the first process in distributed training process the dataset,",
transformers/templates/adding_a_new_example_script/run_xxx.py,323,and the others will use the cache,
transformers/templates/adding_a_new_example_script/run_xxx.py,325,Load data features from cache or dataset file,
transformers/templates/adding_a_new_example_script/run_xxx.py,356,"Make sure only the first process in distributed training process the dataset,",
transformers/templates/adding_a_new_example_script/run_xxx.py,357,and the others will use the cache,
transformers/templates/adding_a_new_example_script/run_xxx.py,359,Convert to Tensors and build dataset,
transformers/templates/adding_a_new_example_script/run_xxx.py,391,Required parameters,
transformers/templates/adding_a_new_example_script/run_xxx.py,424,Other parameters,
transformers/templates/adding_a_new_example_script/run_xxx.py,571,Setup distant debugging if needed,
transformers/templates/adding_a_new_example_script/run_xxx.py,573,Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script,
transformers/templates/adding_a_new_example_script/run_xxx.py,580,"Setup CUDA, GPU & distributed training",
transformers/templates/adding_a_new_example_script/run_xxx.py,584,Initializes the distributed backend which will take care of sychronizing nodes/GPUs,
transformers/templates/adding_a_new_example_script/run_xxx.py,591,Setup logging,
transformers/templates/adding_a_new_example_script/run_xxx.py,606,Set seed,
transformers/templates/adding_a_new_example_script/run_xxx.py,609,Load pretrained model and tokenizer,
transformers/templates/adding_a_new_example_script/run_xxx.py,611,Make sure only the first process in distributed training will,
transformers/templates/adding_a_new_example_script/run_xxx.py,612,download model & vocab,
transformers/templates/adding_a_new_example_script/run_xxx.py,632,Make sure only the first process in distributed training will,
transformers/templates/adding_a_new_example_script/run_xxx.py,633,download model & vocab,
transformers/templates/adding_a_new_example_script/run_xxx.py,639,"Before we do anything with models, we want to ensure that we get fp16 execution of torch.einsum",
transformers/templates/adding_a_new_example_script/run_xxx.py,640,"if args.fp16 is set. Otherwise it'll default to ""promote"" mode, and we'll get fp32 operations.",
transformers/templates/adding_a_new_example_script/run_xxx.py,641,"Note that running `--fp16_opt_level=""O2""` will remove the need for this code, but it is still valid.",
transformers/templates/adding_a_new_example_script/run_xxx.py,650,Training,
transformers/templates/adding_a_new_example_script/run_xxx.py,656,Save the trained model and the tokenizer,
transformers/templates/adding_a_new_example_script/run_xxx.py,658,Create output directory if needed,
transformers/templates/adding_a_new_example_script/run_xxx.py,663,"Save a trained model, configuration and tokenizer using `save_pretrained()`.",
transformers/templates/adding_a_new_example_script/run_xxx.py,664,They can then be reloaded using `from_pretrained()`,
transformers/templates/adding_a_new_example_script/run_xxx.py,667,Take care of distributed/parallel training,
transformers/templates/adding_a_new_example_script/run_xxx.py,671,Good practice: save your training arguments together with the trained model,
transformers/templates/adding_a_new_example_script/run_xxx.py,674,Load a trained model and vocabulary that you have fine-tuned,
transformers/templates/adding_a_new_example_script/run_xxx.py,679,Evaluation - we can ask to evaluate all the checkpoints (sub-directories) in a directory,
transformers/templates/adding_a_new_example_script/run_xxx.py,687,Reduce model loading logs,
transformers/templates/adding_a_new_example_script/run_xxx.py,692,Reload the model,
transformers/templates/adding_a_new_example_script/run_xxx.py,697,Evaluate,
transformers/templates/adding_a_new_example_script/utils_xxx.py,1,coding=utf-8,
transformers/templates/adding_a_new_example_script/utils_xxx.py,2,Copyright 2018 XXX.  All rights reserved.,
transformers/templates/adding_a_new_example_script/utils_xxx.py,3,,
transformers/templates/adding_a_new_example_script/utils_xxx.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/templates/adding_a_new_example_script/utils_xxx.py,5,you may not use this file except in compliance with the License.,
transformers/templates/adding_a_new_example_script/utils_xxx.py,6,You may obtain a copy of the License at,
transformers/templates/adding_a_new_example_script/utils_xxx.py,7,,
transformers/templates/adding_a_new_example_script/utils_xxx.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/templates/adding_a_new_example_script/utils_xxx.py,9,,
transformers/templates/adding_a_new_example_script/utils_xxx.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/templates/adding_a_new_example_script/utils_xxx.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/templates/adding_a_new_example_script/utils_xxx.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/templates/adding_a_new_example_script/utils_xxx.py,13,See the License for the specific language governing permissions and,
transformers/templates/adding_a_new_example_script/utils_xxx.py,14,limitations under the License.,
transformers/templates/adding_a_new_example_script/utils_xxx.py,25,Required by XLNet evaluation method to compute optimal threshold (see write_predictions_extended() method),
transformers/templates/adding_a_new_example_script/utils_xxx.py,158,Only add answers where the text can be exactly recovered from the,
transformers/templates/adding_a_new_example_script/utils_xxx.py,159,document. If this CAN'T happen it's likely due to weird Unicode,
transformers/templates/adding_a_new_example_script/utils_xxx.py,160,stuff so we will just skip the example.,
transformers/templates/adding_a_new_example_script/utils_xxx.py,161,,
transformers/templates/adding_a_new_example_script/utils_xxx.py,162,"Note that this means for training mode, every example is NOT",
transformers/templates/adding_a_new_example_script/utils_xxx.py,163,guaranteed to be preserved.,
transformers/templates/adding_a_new_example_script/utils_xxx.py,207,"cnt_pos, cnt_neg = 0, 0",
transformers/templates/adding_a_new_example_script/utils_xxx.py,208,"max_N, max_M = 1024, 1024",
transformers/templates/adding_a_new_example_script/utils_xxx.py,209,"f = np.zeros((max_N, max_M), dtype=np.float32)",
transformers/templates/adding_a_new_example_script/utils_xxx.py,214,if example_index % 100 == 0:,
transformers/templates/adding_a_new_example_script/utils_xxx.py,215,"logger.info('Converting %s/%s pos %s neg %s', example_index, len(examples), cnt_pos, cnt_neg)",
transformers/templates/adding_a_new_example_script/utils_xxx.py,247,"The -3 accounts for [CLS], [SEP] and [SEP]",
transformers/templates/adding_a_new_example_script/utils_xxx.py,250,We can have documents that are longer than the maximum sequence length.,
transformers/templates/adding_a_new_example_script/utils_xxx.py,251,"To deal with this we do a sliding window approach, where we take chunks",
transformers/templates/adding_a_new_example_script/utils_xxx.py,252,of the up to our max length with a stride of `doc_stride`.,
transformers/templates/adding_a_new_example_script/utils_xxx.py,253,pylint: disable=invalid-name,
transformers/templates/adding_a_new_example_script/utils_xxx.py,271,p_mask: mask with 1 for token than cannot be in the answer (0 for token which can be in an answer),
transformers/templates/adding_a_new_example_script/utils_xxx.py,272,Original TF implem also keep the classification token (set to 0) (not sure why...),
transformers/templates/adding_a_new_example_script/utils_xxx.py,275,CLS token at the beginning,
transformers/templates/adding_a_new_example_script/utils_xxx.py,282,Query,
transformers/templates/adding_a_new_example_script/utils_xxx.py,288,SEP token,
transformers/templates/adding_a_new_example_script/utils_xxx.py,293,Paragraph,
transformers/templates/adding_a_new_example_script/utils_xxx.py,305,SEP token,
transformers/templates/adding_a_new_example_script/utils_xxx.py,310,CLS token at the end,
transformers/templates/adding_a_new_example_script/utils_xxx.py,315,Index of classification token,
transformers/templates/adding_a_new_example_script/utils_xxx.py,319,The mask has 1 for real tokens and 0 for padding tokens. Only real,
transformers/templates/adding_a_new_example_script/utils_xxx.py,320,tokens are attended to.,
transformers/templates/adding_a_new_example_script/utils_xxx.py,323,Zero-pad up to the sequence length.,
transformers/templates/adding_a_new_example_script/utils_xxx.py,338,"For training, if our document chunk does not contain an annotation",
transformers/templates/adding_a_new_example_script/utils_xxx.py,339,"we throw it out, since there is nothing to predict.",
transformers/templates/adding_a_new_example_script/utils_xxx.py,409,The SQuAD annotations are character based. We first project them to,
transformers/templates/adding_a_new_example_script/utils_xxx.py,410,"whitespace-tokenized words. But then after WordPiece tokenization, we can",
transformers/templates/adding_a_new_example_script/utils_xxx.py,411,"often find a ""better match"". For example:",
transformers/templates/adding_a_new_example_script/utils_xxx.py,412,,
transformers/templates/adding_a_new_example_script/utils_xxx.py,413,Question: What year was John Smith born?,
transformers/templates/adding_a_new_example_script/utils_xxx.py,414,Context: The leader was John Smith (1895-1943).,
transformers/templates/adding_a_new_example_script/utils_xxx.py,415,Answer: 1895,
transformers/templates/adding_a_new_example_script/utils_xxx.py,416,,
transformers/templates/adding_a_new_example_script/utils_xxx.py,417,"The original whitespace-tokenized answer will be ""(1895-1943)."". However",
transformers/templates/adding_a_new_example_script/utils_xxx.py,418,"after tokenization, our tokens will be ""( 1895 - 1943 ) ."". So we can match",
transformers/templates/adding_a_new_example_script/utils_xxx.py,419,"the exact answer, 1895.",
transformers/templates/adding_a_new_example_script/utils_xxx.py,420,,
transformers/templates/adding_a_new_example_script/utils_xxx.py,421,"However, this is not always possible. Consider the following:",
transformers/templates/adding_a_new_example_script/utils_xxx.py,422,,
transformers/templates/adding_a_new_example_script/utils_xxx.py,423,Question: What country is the top exporter of electornics?,
transformers/templates/adding_a_new_example_script/utils_xxx.py,424,Context: The Japanese electronics industry is the lagest in the world.,
transformers/templates/adding_a_new_example_script/utils_xxx.py,425,Answer: Japan,
transformers/templates/adding_a_new_example_script/utils_xxx.py,426,,
transformers/templates/adding_a_new_example_script/utils_xxx.py,427,"In this case, the annotator chose ""Japan"" as a character sub-span of",
transformers/templates/adding_a_new_example_script/utils_xxx.py,428,"the word ""Japanese"". Since our WordPiece tokenizer does not split",
transformers/templates/adding_a_new_example_script/utils_xxx.py,429,"""Japanese"", we just use ""Japanese"" as the annotation. This is fairly rare",
transformers/templates/adding_a_new_example_script/utils_xxx.py,430,"in SQuAD, but does happen.",
transformers/templates/adding_a_new_example_script/utils_xxx.py,445,"Because of the sliding window approach taken to scoring documents, a single",
transformers/templates/adding_a_new_example_script/utils_xxx.py,446,token can appear in multiple documents. E.g.,
transformers/templates/adding_a_new_example_script/utils_xxx.py,447,Doc: the man went to the store and bought a gallon of milk,
transformers/templates/adding_a_new_example_script/utils_xxx.py,448,Span A: the man went to the,
transformers/templates/adding_a_new_example_script/utils_xxx.py,449,Span B: to the store and bought,
transformers/templates/adding_a_new_example_script/utils_xxx.py,450,Span C: and bought a gallon of,
transformers/templates/adding_a_new_example_script/utils_xxx.py,451,...,
transformers/templates/adding_a_new_example_script/utils_xxx.py,452,,
transformers/templates/adding_a_new_example_script/utils_xxx.py,453,Now the word 'bought' will have two scores from spans B and C. We only,
transformers/templates/adding_a_new_example_script/utils_xxx.py,454,"want to consider the score with ""maximum context"", which we define as",
transformers/templates/adding_a_new_example_script/utils_xxx.py,455,the *minimum* of its left and right context (the *sum* of left and,
transformers/templates/adding_a_new_example_script/utils_xxx.py,456,"right context will always be the same, of course).",
transformers/templates/adding_a_new_example_script/utils_xxx.py,457,,
transformers/templates/adding_a_new_example_script/utils_xxx.py,458,In the example the maximum context for 'bought' would be span C since,
transformers/templates/adding_a_new_example_script/utils_xxx.py,459,"it has 1 left context and 3 right context, while span B has 4 left context",
transformers/templates/adding_a_new_example_script/utils_xxx.py,460,and 0 right context.,
transformers/templates/adding_a_new_example_script/utils_xxx.py,508,pylint: disable=invalid-name,
transformers/templates/adding_a_new_example_script/utils_xxx.py,520,keep track of the minimum score of null start+end of position 0,
transformers/templates/adding_a_new_example_script/utils_xxx.py,521,large and positive,
transformers/templates/adding_a_new_example_script/utils_xxx.py,522,the paragraph slice with min null score,
transformers/templates/adding_a_new_example_script/utils_xxx.py,523,the start logit at the slice with min null score,
transformers/templates/adding_a_new_example_script/utils_xxx.py,524,the end logit at the slice with min null score,
transformers/templates/adding_a_new_example_script/utils_xxx.py,529,"if we could have irrelevant answers, get the min score of irrelevant",
transformers/templates/adding_a_new_example_script/utils_xxx.py,539,"We could hypothetically create invalid predictions, e.g., predict",
transformers/templates/adding_a_new_example_script/utils_xxx.py,540,that the start of the span is in the question. We throw out all,
transformers/templates/adding_a_new_example_script/utils_xxx.py,541,invalid predictions.,
transformers/templates/adding_a_new_example_script/utils_xxx.py,578,pylint: disable=invalid-name,
transformers/templates/adding_a_new_example_script/utils_xxx.py,588,this is a non-null prediction,
transformers/templates/adding_a_new_example_script/utils_xxx.py,595,De-tokenize WordPieces that have been split off.,
transformers/templates/adding_a_new_example_script/utils_xxx.py,599,Clean whitespace,
transformers/templates/adding_a_new_example_script/utils_xxx.py,614,"if we didn't include the empty option in the n-best, include it",
transformers/templates/adding_a_new_example_script/utils_xxx.py,619,In very rare edge cases we could only have single null prediction.,
transformers/templates/adding_a_new_example_script/utils_xxx.py,620,So we just create a nonce prediction in this case to avoid failure.,
transformers/templates/adding_a_new_example_script/utils_xxx.py,624,In very rare edge cases we could have no valid predictions. So we,
transformers/templates/adding_a_new_example_script/utils_xxx.py,625,just create a nonce prediction in this case to avoid failure.,
transformers/templates/adding_a_new_example_script/utils_xxx.py,655,"predict """" iff the null score - the score of best non-null > threshold",
transformers/templates/adding_a_new_example_script/utils_xxx.py,677,For XLNet (and XLM which uses the same head),
transformers/templates/adding_a_new_example_script/utils_xxx.py,705,pylint: disable=invalid-name,
transformers/templates/adding_a_new_example_script/utils_xxx.py,709,pylint: disable=invalid-name,
transformers/templates/adding_a_new_example_script/utils_xxx.py,714,"logger.info(""Writing nbest to: %s"" % (output_nbest_file))",
transformers/templates/adding_a_new_example_script/utils_xxx.py,732,keep track of the minimum score of null start+end of position 0,
transformers/templates/adding_a_new_example_script/utils_xxx.py,733,large and positive,
transformers/templates/adding_a_new_example_script/utils_xxx.py,740,"if we could have irrelevant answers, get the min score of irrelevant",
transformers/templates/adding_a_new_example_script/utils_xxx.py,753,"We could hypothetically create invalid predictions, e.g., predict",
transformers/templates/adding_a_new_example_script/utils_xxx.py,754,that the start of the span is in the question. We throw out all,
transformers/templates/adding_a_new_example_script/utils_xxx.py,755,invalid predictions.,
transformers/templates/adding_a_new_example_script/utils_xxx.py,790,XLNet un-tokenizer,
transformers/templates/adding_a_new_example_script/utils_xxx.py,791,Let's keep it simple for now and see if we need all this later.,
transformers/templates/adding_a_new_example_script/utils_xxx.py,792,,
transformers/templates/adding_a_new_example_script/utils_xxx.py,793,tok_start_to_orig_index = feature.tok_start_to_orig_index,
transformers/templates/adding_a_new_example_script/utils_xxx.py,794,tok_end_to_orig_index = feature.tok_end_to_orig_index,
transformers/templates/adding_a_new_example_script/utils_xxx.py,795,start_orig_pos = tok_start_to_orig_index[pred.start_index],
transformers/templates/adding_a_new_example_script/utils_xxx.py,796,end_orig_pos = tok_end_to_orig_index[pred.end_index],
transformers/templates/adding_a_new_example_script/utils_xxx.py,797,paragraph_text = example.paragraph_text,
transformers/templates/adding_a_new_example_script/utils_xxx.py,798,final_text = paragraph_text[start_orig_pos: end_orig_pos + 1].strip(),
transformers/templates/adding_a_new_example_script/utils_xxx.py,800,Previously used Bert untokenizer,
transformers/templates/adding_a_new_example_script/utils_xxx.py,807,Clean whitespace,
transformers/templates/adding_a_new_example_script/utils_xxx.py,823,In very rare edge cases we could have no valid predictions. So we,
transformers/templates/adding_a_new_example_script/utils_xxx.py,824,just create a nonce prediction in this case to avoid failure.,
transformers/templates/adding_a_new_example_script/utils_xxx.py,851,note(zhiliny): always predict best_non_null_entry,
transformers/templates/adding_a_new_example_script/utils_xxx.py,852,and the evaluation script will search for the best threshold,
transformers/templates/adding_a_new_example_script/utils_xxx.py,882,"When we created the data, we kept track of the alignment between original",
transformers/templates/adding_a_new_example_script/utils_xxx.py,883,(whitespace tokenized) tokens and our WordPiece tokenized tokens. So,
transformers/templates/adding_a_new_example_script/utils_xxx.py,884,now `orig_text` contains the span of our original text corresponding to the,
transformers/templates/adding_a_new_example_script/utils_xxx.py,885,span that we predicted.,
transformers/templates/adding_a_new_example_script/utils_xxx.py,886,,
transformers/templates/adding_a_new_example_script/utils_xxx.py,887,"However, `orig_text` may contain extra characters that we don't want in",
transformers/templates/adding_a_new_example_script/utils_xxx.py,888,our prediction.,
transformers/templates/adding_a_new_example_script/utils_xxx.py,889,,
transformers/templates/adding_a_new_example_script/utils_xxx.py,890,"For example, let's say:",
transformers/templates/adding_a_new_example_script/utils_xxx.py,891,pred_text = steve smith,
transformers/templates/adding_a_new_example_script/utils_xxx.py,892,orig_text = Steve Smith's,
transformers/templates/adding_a_new_example_script/utils_xxx.py,893,,
transformers/templates/adding_a_new_example_script/utils_xxx.py,894,"We don't want to return `orig_text` because it contains the extra ""'s"".",
transformers/templates/adding_a_new_example_script/utils_xxx.py,895,,
transformers/templates/adding_a_new_example_script/utils_xxx.py,896,We don't want to return `pred_text` because it's already been normalized,
transformers/templates/adding_a_new_example_script/utils_xxx.py,897,(the SQuAD eval script also does punctuation stripping/lower casing but,
transformers/templates/adding_a_new_example_script/utils_xxx.py,898,our tokenizer does additional normalization like stripping accent,
transformers/templates/adding_a_new_example_script/utils_xxx.py,899,characters).,
transformers/templates/adding_a_new_example_script/utils_xxx.py,900,,
transformers/templates/adding_a_new_example_script/utils_xxx.py,901,"What we really want to return is ""Steve Smith"".",
transformers/templates/adding_a_new_example_script/utils_xxx.py,902,,
transformers/templates/adding_a_new_example_script/utils_xxx.py,903,"Therefore, we have to apply a semi-complicated alignment heuristic between",
transformers/templates/adding_a_new_example_script/utils_xxx.py,904,`pred_text` and `orig_text` to get a character-to-character alignment. This,
transformers/templates/adding_a_new_example_script/utils_xxx.py,905,can fail in certain cases in which case we just return `orig_text`.,
transformers/templates/adding_a_new_example_script/utils_xxx.py,918,"We first tokenize `orig_text`, strip whitespace from the result",
transformers/templates/adding_a_new_example_script/utils_xxx.py,919,"and `pred_text`, and check if they are the same length. If they are",
transformers/templates/adding_a_new_example_script/utils_xxx.py,920,"NOT the same length, the heuristic has failed. If they are the same",
transformers/templates/adding_a_new_example_script/utils_xxx.py,921,"length, we assume the characters are one-to-one aligned.",
transformers/templates/adding_a_new_example_script/utils_xxx.py,941,We then project the characters in `pred_text` back to `orig_text` using,
transformers/templates/adding_a_new_example_script/utils_xxx.py,942,the character-to-character alignment.,
transformers/src/,32,pylint: disable=invalid-name,
transformers/src/,40,pylint: disable=invalid-name,
transformers/src/,46,pylint: disable=invalid-name,
transformers/src/,56,pylint: disable=invalid-name,
transformers/src/,62,pylint: disable=invalid-name,
transformers/src/,85,Kept for backward compatibility,
transformers/src/,86,Kept for backward compatibility,
transformers/src/,246,"URL, so get it from the cache (downloading if necessary)",
transformers/src/,257,"File, and it exists.",
transformers/src/,260,"File, but it doesn't exist.",
transformers/src/,263,Something unknown,
transformers/src/,270,Path where we extract compressed archives,
transformers/src/,271,"We avoid '.' in dir name and add ""-extracted"" at the end: ""./model.zip"" => ""./model-zip-extracted/""",
transformers/src/,279,Prevent parallel extractions,
transformers/src/,307,Remove '/' at beginning of path.,
transformers/src/,363,Range not satisfiable,
transformers/src/,376,filter out keep-alive new chunks,
transformers/src/,409,"Get eTag to add to filename, if it exists.",
transformers/src/,418,etag is already None,
transformers/src/,423,get cache path to put the file,
transformers/src/,426,"etag is None = we don't have a connection, or url doesn't exist, or is otherwise inaccessible.",
transformers/src/,427,try to get the last downloaded one,
transformers/src/,440,"If files cannot be found and local_files_only=True,",
transformers/src/,441,the models might've been found if local_files_only=False,
transformers/src/,442,Notify the user about that,
transformers/src/,451,"From now on, etag is not None.",
transformers/src/,455,Prevent parallel downloads of the same file with a lock.,
transformers/src/,476,"Download to temporary file, then copy to cache dir once finished.",
transformers/src/,477,Otherwise you get corrupt cache entries if the download gets interrupted.,
transformers/src/,481,GET file object,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The OpenAI Team Authors and HuggingFace Inc. team.,
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,73,in Attention: n_state=768 (nx=n_embd),
transformers/src/,74,[switch nx => n_state from Block to Attention to keep identical to TF implem],
transformers/src/,102,"q, k, v have shape [batch, heads, sequence, features]",
transformers/src/,105,scale attention_scores,
transformers/src/,108,"w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.",
transformers/src/,115,Apply the attention mask,
transformers/src/,121,Mask heads if we want to,
transformers/src/,140,"(batch, head, seq_length, head_features)",
transformers/src/,159,"a, (attentions)",
transformers/src/,191,"output_attn: a, (attentions)",
transformers/src/,198,"x, (attentions)",
transformers/src/,277,We create a 3D attention mask from a 2D tensor mask.,
transformers/src/,278,"Sizes are [batch_size, 1, 1, to_seq_length]",
transformers/src/,279,"So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]",
transformers/src/,280,this attention mask is more simple than the triangular masking of causal attention,
transformers/src/,281,"used in OpenAI GPT, we just need to prepare the broadcast dimension here.",
transformers/src/,284,Since attention_mask is 1.0 for positions we want to attend and 0.0 for,
transformers/src/,285,"masked positions, this operation will create a tensor which is 0.0 for",
transformers/src/,286,positions we want to attend and -10000.0 for masked positions.,
transformers/src/,287,"Since we are adding it to the raw scores before the softmax, this is",
transformers/src/,288,effectively the same as removing these entirely.,
transformers/src/,295,Prepare head mask if needed,
transformers/src/,296,1.0 in head_mask indicate we keep the head,
transformers/src/,297,attention_probs has shape bsz x n_heads x N x N,
transformers/src/,298,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads],
transformers/src/,299,and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length],
transformers/src/,304,head_mask = tf.constant([0] * self.num_hidden_layers),
transformers/src/,333,Add last hidden state,
transformers/src/,341,let the number of heads free (-1) so we can extract attention even after head pruning,
transformers/src/,345,"last hidden state, (all hidden_states), (attentions)",
transformers/src/,521,"lm_logits, (all hidden_states), (attentions)",
transformers/src/,662,"lm logits, mc logits, (all hidden_states), (attentions)",
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,56,Recomended attributes from https://arxiv.org/abs/1810.03993 (see papers),
transformers/src/,67,Open additional attributes,
transformers/src/,79,"If we save using the predefined names, we can load using `from_pretrained`",
transformers/src/,135,For simplicity we use the same pretrained url than the configuration files,
transformers/src/,136,but with a different suffix (modelcard.json). This suffix is replaced below.,
transformers/src/,151,Load from URL or cache if already cached,
transformers/src/,163,Load model card,
transformers/src/,167,We fall back on creating an empty model card,
transformers/src/,170,Update model card with kwargs if needed,
transformers/src/,1,coding=utf-8,
transformers/src/,2,"Copyright (c) Facebook, Inc. and its affiliates.",
transformers/src/,3,Copyright (c) HuggingFace Inc. team.,
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,248,"We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]",
transformers/src/,249,ourselves in which case we just need to make it broadcastable to all heads.,
transformers/src/,253,"Provided a padding mask of dimensions [batch_size, seq_length]",
transformers/src/,254,"- if the model is a decoder, apply a causal mask in addition to the padding mask",
transformers/src/,255,"- if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]",
transformers/src/,265,Since attention_mask is 1.0 for positions we want to attend and 0.0 for,
transformers/src/,266,"masked positions, this operation will create a tensor which is 0.0 for",
transformers/src/,267,positions we want to attend and -10000.0 for masked positions.,
transformers/src/,268,"Since we are adding it to the raw scores before the softmax, this is",
transformers/src/,269,effectively the same as removing these entirely.,
transformers/src/,270,fp16 compatibility,
transformers/src/,273,If a 2D ou 3D attention mask is provided for the cross-attention,
transformers/src/,274,"we need to make broadcastabe to [batch_size, num_heads, seq_length, seq_length]",
transformers/src/,282,fp16 compatibility,
transformers/src/,285,Prepare head mask if needed,
transformers/src/,286,1.0 in head_mask indicate we keep the head,
transformers/src/,287,attention_probs has shape bsz x n_heads x N x N,
transformers/src/,288,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads],
transformers/src/,289,and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length],
transformers/src/,297,We can specify head_mask for each layer,
transformers/src/,300,switch to fload if need + fp16 compatibility,
transformers/src/,317,add hidden_states and attentions if they are here,
transformers/src/,318,"sequence_output, pooled_output, (hidden_states), (attentions)",
transformers/src/,407,add hidden states and attention if they are here,
transformers/src/,411,We are doing regression,
transformers/src/,419,"(loss), logits, (hidden_states), (attentions)",
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,76,Load weights from TF model,
transformers/src/,88,adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v,
transformers/src/,89,which are not required for using pretrained model,
transformers/src/,153,self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load,
transformers/src/,154,any TensorFlow checkpoint file,
transformers/src/,218,"If this is instantiated as a cross-attention module, the keys",
transformers/src/,219,and values come from an encoder; the attention mask needs to be,
transformers/src/,220,such that the encoder's padding tokens are not attended to.,
transformers/src/,233,"Take the dot product between ""query"" and ""key"" to get the raw attention scores.",
transformers/src/,237,Apply the attention mask is (precomputed for all layers in BertModel forward() function),
transformers/src/,240,Normalize the attention scores to probabilities.,
transformers/src/,243,"This is actually dropping out entire tokens to attend to, which might",
transformers/src/,244,"seem a bit unusual, but is taken from the original Transformer paper.",
transformers/src/,247,Mask heads if we want to,
transformers/src/,286,Convert to set and remove already pruned heads,
transformers/src/,288,Compute how many pruned heads are before the head and move the index accordingly,
transformers/src/,294,Prune linear layers,
transformers/src/,300,Update hyper params and store pruned heads,
transformers/src/,317,add attentions if we output them,
transformers/src/,370,add self attentions if we output attention weights,
transformers/src/,377,add cross attentions if we output attention weights,
transformers/src/,414,Add last layer,
transformers/src/,423,"last-layer hidden state, (all hidden states), (all attentions)",
transformers/src/,433,"We ""pool"" the model by simply taking the hidden state corresponding",
transformers/src/,434,to the first token.,
transformers/src/,463,"The output weights are the same as the input embeddings, but there is",
transformers/src/,464,an output-only bias for each token.,
transformers/src/,469,Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`,
transformers/src/,523,Slightly different from the TF version which uses truncated_normal for initialization,
transformers/src/,524,cf https://github.com/pytorch/pytorch/pull/5617,
transformers/src/,704,"We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]",
transformers/src/,705,ourselves in which case we just need to make it broadcastable to all heads.,
transformers/src/,709,"Provided a padding mask of dimensions [batch_size, seq_length]",
transformers/src/,710,"- if the model is a decoder, apply a causal mask in addition to the padding mask",
transformers/src/,711,"- if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]",
transformers/src/,718,causal and attention masks must have same type with pytorch version < 1.3,
transformers/src/,729,Since attention_mask is 1.0 for positions we want to attend and 0.0 for,
transformers/src/,730,"masked positions, this operation will create a tensor which is 0.0 for",
transformers/src/,731,positions we want to attend and -10000.0 for masked positions.,
transformers/src/,732,"Since we are adding it to the raw scores before the softmax, this is",
transformers/src/,733,effectively the same as removing these entirely.,
transformers/src/,734,fp16 compatibility,
transformers/src/,737,If a 2D ou 3D attention mask is provided for the cross-attention,
transformers/src/,738,"we need to make broadcastabe to [batch_size, num_heads, seq_length, seq_length]",
transformers/src/,758,fp16 compatibility,
transformers/src/,763,Prepare head mask if needed,
transformers/src/,764,1.0 in head_mask indicate we keep the head,
transformers/src/,765,attention_probs has shape bsz x n_heads x N x N,
transformers/src/,766,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads],
transformers/src/,767,and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length],
transformers/src/,775,We can specify head_mask for each layer,
transformers/src/,778,switch to fload if need + fp16 compatibility,
transformers/src/,797,add hidden_states and attentions if they are here,
transformers/src/,798,"sequence_output, pooled_output, (hidden_states), (attentions)",
transformers/src/,893,add hidden states and attention if they are here,
transformers/src/,902,"(loss), prediction_scores, seq_relationship_score, (hidden_states), (attentions)",
transformers/src/,993,Add hidden states and attention if they are here,
transformers/src/,995,"Although this may seem awkward, BertForMaskedLM supports two scenarios:",
transformers/src/,996,"1. If a tensor that contains the indices of masked labels is provided,",
transformers/src/,997,the cross-entropy is the MLM cross-entropy that measures the likelihood,
transformers/src/,998,of predictions for masked words.,
transformers/src/,999,2. If `lm_labels` is provided we are in a causal scenario where we,
transformers/src/,1000,try to predict the next token for each input in the decoder.,
transformers/src/,1002,-100 index = padding token,
transformers/src/,1007,we are doing next-token prediction; shift prediction scores and input ids by one,
transformers/src/,1014,"(masked_lm_loss), (ltr_lm_loss), prediction_scores, (hidden_states), (attentions)",
transformers/src/,1093,add hidden states and attention if they are here,
transformers/src/,1099,"(next_sentence_loss), seq_relationship_score, (hidden_states), (attentions)",
transformers/src/,1184,add hidden states and attention if they are here,
transformers/src/,1188,We are doing regression,
transformers/src/,1196,"(loss), logits, (hidden_states), (attentions)",
transformers/src/,1289,add hidden states and attention if they are here,
transformers/src/,1296,"(loss), reshaped_logits, (hidden_states), (attentions)",
transformers/src/,1379,add hidden states and attention if they are here,
transformers/src/,1382,Only keep active parts of the loss,
transformers/src/,1394,"(loss), scores, (hidden_states), (attentions)",
transformers/src/,1492,"If we are on multi-GPU, split add a dimension",
transformers/src/,1497,"sometimes the start/end positions are outside our model inputs, we ignore these terms",
transformers/src/,1508,"(loss), start_logits, end_logits, (hidden_states), (attentions)",
transformers/src/,1,coding=utf-8,
transformers/src/,2,"Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.",
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,195,Backward compatibility,
transformers/src/,199,Backward compatibility,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The OpenAI Team Authors and HuggingFace Inc. team.,
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,1,coding=utf-8,
transformers/src/,2,"Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.",
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,106,no default special tokens - you can update this value if you add special tokens,
transformers/src/,109,no default special tokens - you can update this value if you add special tokens,
transformers/src/,123,noqa: W605,
transformers/src/,129,"Hack because, honestly this tokenizer was not made to be used",
transformers/src/,130,"in a library like ours, at all.",
transformers/src/,150,noqa: W605,
transformers/src/,291,logger.info('encounter unk {}'.format(sym)),
transformers/src/,292,assert '<eos>' not in sym,
transformers/src/,295,Backward compatibility with pre-trained models,
transformers/src/,320,convert to lower case,
transformers/src/,324,empty delimiter '' will evaluate False,
transformers/src/,330,lm1b,
transformers/src/,338,add spaces before punctuation symbols as should be done in transfo-xl,
transformers/src/,343,searches until the first occurence of a punctuation symbol without surrounding spaces,
transformers/src/,374,Create the correct normalization path,
transformers/src/,377,Include unicode normalization,
transformers/src/,381,Include case normalization,
transformers/src/,385,Strip normalizer at the end,
transformers/src/,391,Setup the splitter,
transformers/src/,475,Work out how cleanly we can divide the dataset into bsz parts.,
transformers/src/,478,Trim off any extra elements that wouldn't cleanly fit (remainders).,
transformers/src/,481,Evenly divide the data across the bsz batches.,
transformers/src/,484,Number of mini-batches,
transformers/src/,538,index iterator,
transformers/src/,541,sentence iterator,
transformers/src/,546,streams for each data in the batch,
transformers/src/,555,data   : [n_retain+bptt x bsz],
transformers/src/,556,target : [bptt x bsz],
transformers/src/,568,number of new tokens to fill in,
transformers/src/,570,first n_retain tokens are retained from last batch,
transformers/src/,593,sent_stream is an iterator,
transformers/src/,626,sent_stream is an iterator,
transformers/src/,643,"redirect to the cache, if necessary",
transformers/src/,663,Instantiate tokenizer.,
transformers/src/,701,the vocab will load from file when build_vocab() is called,
transformers/src/,1,coding=utf-8,
transformers/src/,2,"Copyright 2019-present, the HuggingFace Inc. team, The Google AI Language Team and Facebook, Inc.",
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,44,UTILS AND BUILDING BLOCKS OF THE ARCHITECTURE,
transformers/src/,77,padding_idx=0),
transformers/src/,91,Create and initialize weights. The random normal initializer was chosen,
transformers/src/,92,"arbitrarily, and works well.",
transformers/src/,148,"(bs, max_seq_length, dim)",
transformers/src/,150,"(bs, max_seq_length, dim)",
transformers/src/,151,"(bs, max_seq_length, dim)",
transformers/src/,152,"(bs, max_seq_length, dim)",
transformers/src/,219,"assert dim == self.dim, 'Dimensions do not match: %s input vs %s configured' % (dim, self.dim)",
transformers/src/,220,assert key.size() == value.size(),
transformers/src/,234,"(bs, n_heads, q_length, dim_per_head)",
transformers/src/,235,"(bs, n_heads, k_length, dim_per_head)",
transformers/src/,236,"(bs, n_heads, k_length, dim_per_head)",
transformers/src/,238,"(bs, n_heads, q_length, dim_per_head)",
transformers/src/,239,"(bs, n_heads, q_length, k_length)",
transformers/src/,240,"(bs, n_heads, qlen, klen)",
transformers/src/,241,"scores.masked_fill_(mask, -float('inf'))            # (bs, n_heads, q_length, k_length)",
transformers/src/,244,"(bs, n_heads, qlen, klen)",
transformers/src/,245,"(bs, n_heads, qlen, klen)",
transformers/src/,247,Mask heads if we want to,
transformers/src/,251,"(bs, n_heads, qlen, dim_per_head)",
transformers/src/,252,"(bs, q_length, dim)",
transformers/src/,253,"(bs, q_length, dim)",
transformers/src/,305,"removed: src_enc=None, src_len=None",
transformers/src/,321,Self-Attention,
transformers/src/,324,"(bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)",
transformers/src/,325,To handle these `output_attention` or `output_hidden_states` cases returning tuples,
transformers/src/,326,assert type(sa_output) == tuple,
transformers/src/,328,"(bs, seq_length, dim)",
transformers/src/,330,Feed Forward Network,
transformers/src/,331,"(bs, seq_length, dim)",
transformers/src/,332,"(bs, seq_length, dim)",
transformers/src/,389,Add last layer,
transformers/src/,398,"last-layer hidden state, (all hidden states), (all attentions)",
transformers/src/,406,Embeddings,
transformers/src/,407,Encoder,
transformers/src/,444,"(bs, seq_length)",
transformers/src/,447,Prepare head mask if needed,
transformers/src/,448,1.0 in head_mask indicate we keep the head,
transformers/src/,449,attention_probs has shape bsz x n_heads x N x N,
transformers/src/,450,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads],
transformers/src/,451,and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length],
transformers/src/,457,"(bs, seq_length, dim)",
transformers/src/,460,"last-layer hidden-state, (all hidden_states), (all attentions)",
transformers/src/,463,INTERFACE FOR ENCODER AND TASK SPECIFIC MODEL,
transformers/src/,542,Embeddings,
transformers/src/,582,"The output weights are the same as the input embeddings, but there is",
transformers/src/,583,an output-only bias for each token.,
transformers/src/,650,"(bs, seq_length, dim)",
transformers/src/,651,"(bs, seq_length, dim)",
transformers/src/,652,"(bs, seq_length, dim)",
transformers/src/,653,"(bs, seq_length, dim)",
transformers/src/,657,"logits, (hidden_states), (attentions)",
transformers/src/,714,"(bs, seq_len, dim)",
transformers/src/,715,"(bs, dim)",
transformers/src/,716,"(bs, dim)",
transformers/src/,717,"(bs, dim)",
transformers/src/,718,"(bs, dim)",
transformers/src/,721,"logits, (hidden_states), (attentions)",
transformers/src/,776,add hidden states and attention if they are here,
transformers/src/,778,"scores, (hidden_states), (attentions)",
transformers/src/,831,"(bs, max_query_len, dim)",
transformers/src/,832,"(bs, max_query_len, dim)",
transformers/src/,833,"(bs, max_query_len, 2)",
transformers/src/,839,"start_logits, end_logits, (hidden_states), (attentions)",
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 T5 Authors and HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,30,,
transformers/src/,31,Mapping from the keyword arguments names of Tokenizer `__init__`,
transformers/src/,32,to file names for serializing Tokenizer instances,
transformers/src/,33,,
transformers/src/,36,,
transformers/src/,37,Mapping from the keyword arguments names of Tokenizer `__init__`,
transformers/src/,38,to pretrained vocabulary URL for all the model shortcut names.,
transformers/src/,39,,
transformers/src/,50,,
transformers/src/,51,Mapping from model shortcut names to max length of inputs,
transformers/src/,52,,
transformers/src/,108,Add extra_ids to the special token list,
transformers/src/,123,no default special tokens - you can update this value if you add special tokens,
transformers/src/,126,no default special tokens - you can update this value if you add special tokens,
transformers/src/,1,coding=utf-8,
transformers/src/,2,"Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.",
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,156,No probability for the head cluster,
transformers/src/,171,Add the training-time loss value to the layer using `self.add_loss()`.,
transformers/src/,174,"Log the loss as a metric (we could log arbitrary metrics,",
transformers/src/,175,including different metrics for training and inference.,
transformers/src/,42,Load weights from TF model,
transformers/src/,66,"print(original_name, name)",
transformers/src/,67,adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v,
transformers/src/,68,which are not required for using pretrained model,
transformers/src/,117,self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load,
transformers/src/,118,any TensorFlow checkpoint file,
transformers/src/,168,"We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]",
transformers/src/,169,ourselves in which case we just need to make it broadcastable to all heads.,
transformers/src/,173,"Provided a padding mask of dimensions [batch_size, seq_length]",
transformers/src/,174,"- if the model is a decoder, apply a causal mask in addition to the padding mask",
transformers/src/,175,"- if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]",
transformers/src/,182,causal and attention masks must have same type with pytorch version < 1.3,
transformers/src/,193,Since attention_mask is 1.0 for positions we want to attend and 0.0 for,
transformers/src/,194,"masked positions, this operation will create a tensor which is 0.0 for",
transformers/src/,195,positions we want to attend and -10000.0 for masked positions.,
transformers/src/,196,"Since we are adding it to the raw scores before the softmax, this is",
transformers/src/,197,effectively the same as removing these entirely.,
transformers/src/,198,fp16 compatibility,
transformers/src/,204,Prepare head mask if needed,
transformers/src/,205,1.0 in head_mask indicate we keep the head,
transformers/src/,206,attention_probs has shape bsz x n_heads x N x N,
transformers/src/,207,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads],
transformers/src/,208,and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length],
transformers/src/,217,We can specify head_mask for each layer,
transformers/src/,220,switch to fload if need + fp16 compatibility,
transformers/src/,484,"(loss), scores, (hidden_states), (attentions)",
transformers/src/,569,Masked language modeling softmax layer,
transformers/src/,571,-100 index = padding token,
transformers/src/,577,"(masked_lm_loss), prediction_scores, (hidden_states), (attentions)",
transformers/src/,658,Only keep active parts of the loss,
transformers/src/,671,"(loss), scores, (hidden_states), (attentions)",
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The Open AI Team Authors and The HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,111,no default special tokens - you can update this value if you add special tokens,
transformers/src/,114,no default special tokens - you can update this value if you add special tokens,
transformers/src/,192,Using BERT's BasicTokenizer,
transformers/src/,197,Using SpaCy & ftfy (original tokenization process of OpenAI GPT),
transformers/src/,273,Check for Unicode normalization first (before everything else),
transformers/src/,279,OpenAI normalization is the same as Bert,
transformers/src/,282,Create the normalizer structure,
transformers/src/,1,coding=utf-8,
transformers/src/,2,"Copyright 2019-present CNRS, Facebook Inc. and the HuggingFace Inc. team.",
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,67,six_ensure_text is copied from https://github.com/benjaminp/six,
transformers/src/,1,Copyright 2019 The TensorFlow Authors. All Rights Reserved.,
transformers/src/,2,,
transformers/src/,3,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,4,you may not use this file except in compliance with the License.,
transformers/src/,5,You may obtain a copy of the License at,
transformers/src/,6,,
transformers/src/,7,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,8,,
transformers/src/,9,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,10,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,11,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,12,See the License for the specific language governing permissions and,
transformers/src/,13,limitations under the License.,
transformers/src/,14,==============================================================================,
transformers/src/,36,"Implements polynomial warmup. i.e., if global_step < warmup_steps, the",
transformers/src/,37,learning rate will be `global_step/num_warmup_steps * init_lr`.,
transformers/src/,61,Implements linear decay of the learning rate.,
transformers/src/,180,Inspired from https://github.com/OpenNMT/OpenNMT-tf/blob/master/opennmt/optimizers/utils.py,
transformers/src/,233,"In a replica context, we want to accumulate gradients on each replica",
transformers/src/,234,"without synchronization, so we directly assign the value of the",
transformers/src/,235,current replica.,
transformers/src/,1,coding=utf-8,
transformers/src/,2,"Copyright 2019-present, Facebook, Inc and the HuggingFace Inc. team.",
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,73,assert lengths.max().item() <= slen,
transformers/src/,77,"attention mask is the same as mask, or triangular inferior attention (causal)",
transformers/src/,85,sanity check,
transformers/src/,86,"assert shape_list(mask) == [bs, slen]",
transformers/src/,123,"Input is (bs, qlen, dim)",
transformers/src/,124,"Mask is (bs, klen) (non-causal) or (bs, klen, klen)",
transformers/src/,130,"assert dim == self.dim, 'Dimensions do not match: %s input vs %s configured' % (dim, self.dim)",
transformers/src/,143,"(bs, n_heads, qlen, dim_per_head)",
transformers/src/,145,"(bs, n_heads, qlen, dim_per_head)",
transformers/src/,146,"(bs, n_heads, qlen, dim_per_head)",
transformers/src/,149,"(bs, n_heads, qlen, dim_per_head)",
transformers/src/,150,"(bs, n_heads, qlen, dim_per_head)",
transformers/src/,156,"(bs, n_heads, klen, dim_per_head)",
transformers/src/,157,"(bs, n_heads, klen, dim_per_head)",
transformers/src/,162,"(bs, n_heads, qlen, dim_per_head)",
transformers/src/,163,"(bs, n_heads, qlen, klen)",
transformers/src/,164,"(bs, n_heads, qlen, klen)",
transformers/src/,165,"scores.masked_fill_(mask, -float('inf'))                            # (bs, n_heads, qlen, klen)",
transformers/src/,168,"(bs, n_heads, qlen, klen)",
transformers/src/,169,"(bs, n_heads, qlen, klen)",
transformers/src/,171,Mask heads if we want to,
transformers/src/,175,"(bs, n_heads, qlen, dim_per_head)",
transformers/src/,176,"(bs, qlen, dim)",
transformers/src/,206,"encoder / decoder, output layer",
transformers/src/,211,self.with_output = with_output,
transformers/src/,214,dictionary / languages,
transformers/src/,220,self.dico = dico,
transformers/src/,221,self.id2lang = config.id2lang,
transformers/src/,222,self.lang2id = config.lang2id,
transformers/src/,223,assert len(self.dico) == self.n_words,
transformers/src/,224,assert len(self.id2lang) == len(self.lang2id) == self.n_langs,
transformers/src/,226,model parameters,
transformers/src/,227,512 by default,
transformers/src/,228,2048 by default,
transformers/src/,229,8 by default,
transformers/src/,233,embeddings,
transformers/src/,245,"create_sinusoidal_embeddings(config.max_position_embeddings, self.dim, out=self.position_embeddings.weight)",
transformers/src/,255,padding_idx=self.pad_index),
transformers/src/,258,transformer layers,
transformers/src/,263,if self.is_decoder:,
transformers/src/,264,self.layer_norm15 = [],
transformers/src/,265,self.encoder_attn = [],
transformers/src/,274,if self.is_decoder:,
transformers/src/,275,"self.layer_norm15.append(nn.LayerNorm(self.dim, eps=config.layer_norm_eps))",
transformers/src/,276,"self.encoder_attn.append(MultiHeadAttention(self.n_heads, self.dim, dropout=self.attention_dropout))",
transformers/src/,316,"removed: src_enc=None, src_len=None",
transformers/src/,356,mask = input_ids != self.pad_index,
transformers/src/,358,check inputs,
transformers/src/,359,assert shape_list(lengths)[0] == bs,
transformers/src/,361,assert lengths.max().item() <= slen,
transformers/src/,362,"input_ids = input_ids.transpose(0, 1)  # batch size as dimension 0",
transformers/src/,363,assert (src_enc is None) == (src_len is None),
transformers/src/,364,if src_enc is not None:,
transformers/src/,365,assert self.is_decoder,
transformers/src/,366,assert src_enc.size(0) == bs,
transformers/src/,368,generate masks,
transformers/src/,370,if self.is_decoder and src_enc is not None:,
transformers/src/,371,"src_mask = torch.arange(src_len.max(), dtype=torch.long, device=lengths.device) < src_len[:, None]",
transformers/src/,373,position_ids,
transformers/src/,377,"assert shape_list(position_ids) == [bs, slen]  # (slen, bs)",
transformers/src/,379,"position_ids = position_ids.transpose(0, 1)",
transformers/src/,381,langs,
transformers/src/,383,"assert shape_list(langs) == [bs, slen]  # (slen, bs)",
transformers/src/,385,"langs = langs.transpose(0, 1)",
transformers/src/,387,Prepare head mask if needed,
transformers/src/,388,1.0 in head_mask indicate we keep the head,
transformers/src/,389,attention_probs has shape bsz x n_heads x N x N,
transformers/src/,390,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads],
transformers/src/,391,and head_mask is converted to shape [num_hidden_layers x batch x num_heads x qlen x klen],
transformers/src/,397,do not recompute cached elements,
transformers/src/,407,embeddings,
transformers/src/,420,transformer layers,
transformers/src/,427,self attention,
transformers/src/,436,encoder attention (for decoder only),
transformers/src/,437,if self.is_decoder and src_enc is not None:,
transformers/src/,438,"attn = self.encoder_attn[i](tensor, src_mask, kv=src_enc, cache=cache)",
transformers/src/,439,"attn = F.dropout(attn, p=self.dropout, training=self.training)",
transformers/src/,440,tensor = tensor + attn,
transformers/src/,441,tensor = self.layer_norm15[i](tensor),
transformers/src/,443,FFN,
transformers/src/,448,Add last hidden state,
transformers/src/,452,update cache length,
transformers/src/,456,move back sequence length to dimension 0,
transformers/src/,457,"tensor = tensor.transpose(0, 1)",
transformers/src/,464,"outputs, (hidden_states), (attentions)",
transformers/src/,478,Sometimes XLM has language embeddings so don't forget to build them as well if needed,
transformers/src/,628,self.proj = nn.AdaptiveLogSoftmaxWithLoss(,
transformers/src/,629,"in_features=dim,",
transformers/src/,630,"n_classes=config.n_words,",
transformers/src/,631,"cutoffs=config.asm_cutoffs,",
transformers/src/,632,"div_value=config.asm_div_value,",
transformers/src/,633,"head_bias=True,  # default is False",
transformers/src/,634,),
transformers/src/,637,"The output weights are the same as the input embeddings, but there is an output-only bias for each token.",
transformers/src/,710,Keep new_mems and attention/hidden states if they are here,
transformers/src/,765,Keep new_mems and attention/hidden states if they are here,
transformers/src/,826,"Keep mems, hidden states, attentions if there are in it",
transformers/src/,828,"start_logits, end_logits, (hidden_states), (attentions)",
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,30,Initialise PyTorch model,
transformers/src/,35,Load weights from tf checkpoint,
transformers/src/,38,Save pytorch-model,
transformers/src/,45,Required parameters,
transformers/src/,1,coding=utf-8,
transformers/src/,2,"Copyright 2019-present CNRS, Facebook Inc. and the HuggingFace Inc. team.",
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2020 The HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,1,coding=utf-8,
transformers/src/,2,"Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.",
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,45,Segments (not really needed),
transformers/src/,141,take into account special tokens,
transformers/src/,142,take into account special tokens,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,30,Construct model,
transformers/src/,37,Load weights from numpy,
transformers/src/,40,Save pytorch-model,
transformers/src/,52,Required parameters,
transformers/src/,1,coding=utf-8,
transformers/src/,2,"Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.",
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,203,Backward compatibility,
transformers/src/,207,Backward compatibility,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,115,keyword arguments come in 3 flavors: encoder-specific (prefixed by,
transformers/src/,116,"`encoder_`), decoder-specific (prefixed by `decoder_`) and those",
transformers/src/,117,that apply to the model as a whole.,
transformers/src/,118,We let the specific kwargs override the common ones in case of conflict.,
transformers/src/,141,Load and initialize the encoder and decoder,
transformers/src/,142,The distinction between encoder and decoder at the model level is made,
transformers/src/,143,by the value of the flag `is_decoder` that we need to set correctly.,
transformers/src/,165,"If the root output directory does not exist, create it",
transformers/src/,169,Check whether the output directory is empty or not,
transformers/src/,183,Empty the output directory,
transformers/src/,185,Remove all files into the subdirectory,
transformers/src/,189,Remove the subdirectory itself,
transformers/src/,192,sanity check,
transformers/src/,194,"Create the ""encoder"" directory inside the output directory and save the encoder into it",
transformers/src/,199,"Create the ""encoder"" directory inside the output directory and save the decoder into it",
transformers/src/,225,"Encode if needed (training, first prediction pass)",
transformers/src/,1,coding=utf-8,
transformers/src/,2,"Copyright 2018 Google AI, Google Brain and the HuggingFace Inc. team.",
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,61,Load weights from TF model,
transformers/src/,77,If saved from the TF HUB module,
transformers/src/,80,Renaming and simplifying,
transformers/src/,89,The feed forward layer had an 'intermediate' step which has been abstracted away,
transformers/src/,93,ALBERT attention was split between self and output which have been abstracted away,
transformers/src/,97,The pooler is a linear layer,
transformers/src/,100,The classifier was simplified to predictions from cls/predictions,
transformers/src/,104,Naming was changed to be more explicit,
transformers/src/,109,Classifier,
transformers/src/,113,No ALBERT model currently handles the next sentence prediction task,
transformers/src/,119,Ignore the gradients applied by the LAMB/ADAM optimizers.,
transformers/src/,201,Convert to set and emove already pruned heads,
transformers/src/,203,Compute how many pruned heads are before the head and move the index accordingly,
transformers/src/,209,Prune linear layers,
transformers/src/,215,Update hyper params and store pruned heads,
transformers/src/,229,"Take the dot product between ""query"" and ""key"" to get the raw attention scores.",
transformers/src/,233,Apply the attention mask is (precomputed for all layers in BertModel forward() function),
transformers/src/,236,Normalize the attention scores to probabilities.,
transformers/src/,239,"This is actually dropping out entire tokens to attend to, which might",
transformers/src/,240,"seem a bit unusual, but is taken from the original Transformer paper.",
transformers/src/,243,Mask heads if we want to,
transformers/src/,251,Should find a better way to do this,
transformers/src/,283,add attentions if we output them,
transformers/src/,313,"last-layer hidden state, (layer hidden states), (layer attentions)",
transformers/src/,335,Number of layers in a hidden group,
transformers/src/,338,Index of the hidden group,
transformers/src/,359,"last-layer hidden state, (all hidden states), (all attentions)",
transformers/src/,375,Slightly different from the TF version which uses truncated_normal for initialization,
transformers/src/,376,cf https://github.com/pytorch/pytorch/pull/5617,
transformers/src/,553,fp16 compatibility,
transformers/src/,562,We can specify head_mask for each layer,
transformers/src/,565,switch to fload if need + fp16 compatibility,
transformers/src/,580,add hidden_states and attentions if they are here,
transformers/src/,594,Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`,
transformers/src/,687,Add hidden states and attention if they are here,
transformers/src/,776,add hidden states and attention if they are here,
transformers/src/,780,We are doing regression,
transformers/src/,788,"(loss), logits, (hidden_states), (attentions)",
transformers/src/,871,add hidden states and attention if they are here,
transformers/src/,875,Only keep active parts of the loss,
transformers/src/,885,"(loss), logits, (hidden_states), (attentions)",
transformers/src/,979,"If we are on multi-GPU, split add a dimension",
transformers/src/,984,"sometimes the start/end positions are outside our model inputs, we ignore these terms",
transformers/src/,995,"(loss), start_logits, end_logits, (hidden_states), (attentions)",
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The OpenAI Team Authors and HuggingFace Inc. team.,
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,67,in Attention: n_state=768 (nx=n_embd),
transformers/src/,68,[switch nx => n_state from Block to Attention to keep identical to TF implem],
transformers/src/,96,"q, k, v have shape [batch, heads, sequence, features]",
transformers/src/,99,scale attention_scores,
transformers/src/,102,"w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.",
transformers/src/,109,Apply the attention mask,
transformers/src/,115,Mask heads if we want to,
transformers/src/,134,"(batch, head, seq_length, head_features)",
transformers/src/,149,to cope with keras serialization,
transformers/src/,150,we need to cast `use_cache` to correct bool,
transformers/src/,151,if it is a tensor,
transformers/src/,171,"a, present, (attentions)",
transformers/src/,204,"output_attn: a, present, (attentions)",
transformers/src/,212,"x, present, (attentions)",
transformers/src/,287,"If using past key value states, only the last tokens",
transformers/src/,288,should be given as an input,
transformers/src/,316,We create a 3D attention mask from a 2D tensor mask.,
transformers/src/,317,"Sizes are [batch_size, 1, 1, to_seq_length]",
transformers/src/,318,"So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]",
transformers/src/,319,this attention mask is more simple than the triangular masking of causal attention,
transformers/src/,320,"used in OpenAI GPT, we just need to prepare the broadcast dimension here.",
transformers/src/,323,Since attention_mask is 1.0 for positions we want to attend and 0.0 for,
transformers/src/,324,"masked positions, this operation will create a tensor which is 0.0 for",
transformers/src/,325,positions we want to attend and -10000.0 for masked positions.,
transformers/src/,326,"Since we are adding it to the raw scores before the softmax, this is",
transformers/src/,327,effectively the same as removing these entirely.,
transformers/src/,334,Prepare head mask if needed,
transformers/src/,335,1.0 in head_mask indicate we keep the head,
transformers/src/,336,attention_probs has shape bsz x n_heads x N x N,
transformers/src/,337,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads],
transformers/src/,338,and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length],
transformers/src/,343,head_mask = tf.constant([0] * self.num_hidden_layers),
transformers/src/,378,Add last hidden state,
transformers/src/,389,let the number of heads free (-1) so we can extract attention even after head pruning,
transformers/src/,393,"last hidden state, presents, (all hidden_states), (attentions)",
transformers/src/,542,only last token for inputs_ids if past is defined in kwargs,
transformers/src/,591,"lm_logits, presents, (all hidden_states), (attentions)",
transformers/src/,743,"lm logits, mc logits, presents, (all hidden_states), (attentions)",
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The OpenAI Team Authors and HuggingFace Inc. team.,
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,65,self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load,
transformers/src/,66,any TensorFlow checkpoint file,
transformers/src/,73,Create and initialize weights. The random normal initializer was chosen,
transformers/src/,74,"arbitrarily, and works well.",
transformers/src/,186,"Take the dot product between ""query"" and ""key"" to get the raw attention scores.",
transformers/src/,187,"(batch size, num_heads, seq_len_q, seq_len_k)",
transformers/src/,189,scale attention_scores,
transformers/src/,194,Apply the attention mask is (precomputed for all layers in TFAlbertModel call() function),
transformers/src/,197,Normalize the attention scores to probabilities.,
transformers/src/,200,"This is actually dropping out entire tokens to attend to, which might",
transformers/src/,201,"seem a bit unusual, but is taken from the original Transformer paper.",
transformers/src/,204,Mask heads if we want to,
transformers/src/,213,"(batch_size, seq_len_q, all_head_size)",
transformers/src/,263,"Take the dot product between ""query"" and ""key"" to get the raw attention scores.",
transformers/src/,264,"(batch size, num_heads, seq_len_q, seq_len_k)",
transformers/src/,266,scale attention_scores,
transformers/src/,271,Apply the attention mask is (precomputed for all layers in TFBertModel call() function),
transformers/src/,274,Normalize the attention scores to probabilities.,
transformers/src/,277,"This is actually dropping out entire tokens to attend to, which might",
transformers/src/,278,"seem a bit unusual, but is taken from the original Transformer paper.",
transformers/src/,281,Mask heads if we want to,
transformers/src/,290,"(batch_size, seq_len_q, all_head_size)",
transformers/src/,300,add attentions if we output them,
transformers/src/,338,add attentions if we output them,
transformers/src/,374,"last-layer hidden state, (layer hidden states), (layer attentions)",
transformers/src/,405,Number of layers in a hidden group,
transformers/src/,408,Index of the hidden group,
transformers/src/,433,"last-layer hidden state, (all hidden states), (all attentions)",
transformers/src/,462,"The output weights are the same as the input embeddings, but there is",
transformers/src/,463,an output-only bias for each token.,
transformers/src/,555,We create a 3D attention mask from a 2D tensor mask.,
transformers/src/,556,"Sizes are [batch_size, 1, 1, to_seq_length]",
transformers/src/,557,"So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]",
transformers/src/,558,this attention mask is more simple than the triangular masking of causal attention,
transformers/src/,559,"used in OpenAI GPT, we just need to prepare the broadcast dimension here.",
transformers/src/,562,Since attention_mask is 1.0 for positions we want to attend and 0.0 for,
transformers/src/,563,"masked positions, this operation will create a tensor which is 0.0 for",
transformers/src/,564,positions we want to attend and -10000.0 for masked positions.,
transformers/src/,565,"Since we are adding it to the raw scores before the softmax, this is",
transformers/src/,566,effectively the same as removing these entirely.,
transformers/src/,571,Prepare head mask if needed,
transformers/src/,572,1.0 in head_mask indicate we keep the head,
transformers/src/,573,attention_probs has shape bsz x n_heads x N x N,
transformers/src/,574,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads],
transformers/src/,575,and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length],
transformers/src/,580,head_mask = tf.constant([0] * self.num_hidden_layers),
transformers/src/,588,add hidden_states and attentions if they are here,
transformers/src/,590,"sequence_output, pooled_output, (hidden_states), (attentions)",
transformers/src/,767,Add hidden states and attention if they are here,
transformers/src/,770,"prediction_scores, (hidden_states), (attentions)",
transformers/src/,826,add hidden states and attention if they are here,
transformers/src/,828,"logits, (hidden_states), (attentions)",
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2020 The HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,121,logits,
transformers/src/,122,no classification heads to worry about,
transformers/src/,132,an existing summarization ckpt,
transformers/src/,138,Check results,
transformers/src/,147,Required parameters,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2019 Facebook AI Research and the HuggingFace Inc. team.,
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,121,"^^ We call the grandparent's init, not the parent's.",
transformers/src/,122,take into account special tokens,
transformers/src/,123,take into account special tokens,
transformers/src/,1,coding=utf-8,
transformers/src/,2,"Copyright 2019 Inria, Facebook AI Research and the HuggingFace Inc. team.",
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,1,coding=utf-8,
transformers/src/,2,"Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.",
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,75,layer normalization + positionwise feed-forward,
transformers/src/,82,residual connection,
transformers/src/,85,positionwise feed-forward,
transformers/src/,91,residual connection + layer normalization,
transformers/src/,140,Biases are shared,
transformers/src/,152,Biases are not shared,
transformers/src/,196,qlen x bsz x n_head x d_head,
transformers/src/,197,qlen x bsz x n_head x d_head,
transformers/src/,198,qlen x bsz x n_head x d_head,
transformers/src/,200,qlen x n_head x d_head,
transformers/src/,202,compute attention score,
transformers/src/,203,qlen x bsz x n_head x d_head,
transformers/src/,204,qlen x klen x bsz x n_head,
transformers/src/,207,qlen x klen x bsz x n_head,
transformers/src/,210,[qlen x klen x bsz x n_head],
transformers/src/,214,compute attention probability,
transformers/src/,219,[qlen x klen x bsz x n_head],
transformers/src/,223,Mask heads if we want to,
transformers/src/,227,compute attention vector,
transformers/src/,230,[qlen x bsz x n_head x d_head],
transformers/src/,234,linear projection,
transformers/src/,239,residual connection,
transformers/src/,242,residual connection + layer normalization,
transformers/src/,329,Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint,
transformers/src/,358,Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint,
transformers/src/,421,the default attention,
transformers/src/,443,learnable embeddings and absolute embeddings,
transformers/src/,444,Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint,
transformers/src/,449,default attention,
transformers/src/,451,learnable embeddings and absolute embeddings,
transformers/src/,452,Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint,
transformers/src/,493,does not deal with None,
transformers/src/,497,mems is not None,
transformers/src/,500,There are `mlen + qlen` steps that can be cached into mems,
transformers/src/,501,"For the next step, the last `ext_len` of the `qlen` tokens",
transformers/src/,502,"will be used as the extended context. Hence, we only cache",
transformers/src/,503,the tokens from `mlen + qlen - self.ext_len - self.mem_len`,
transformers/src/,504,to `mlen + qlen - self.ext_len`.,
transformers/src/,532,"the original code for Transformer-XL used shapes [len, bsz] but we want a unified interface in the library",
transformers/src/,533,"so we transpose here from shape [bsz, len] to shape [len, bsz]",
transformers/src/,548,Prepare head mask if needed,
transformers/src/,549,1.0 in head_mask indicate we keep the head,
transformers/src/,550,attention_probs has shape bsz x n_heads x N x N,
transformers/src/,551,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] (a head_mask for each layer),
transformers/src/,552,and head_mask is converted to shape [num_hidden_layers x qlen x klen x bsz x n_head],
transformers/src/,574,::: PyTorch masking code for reference :::,
transformers/src/,575,if self.same_length:,
transformers/src/,576,"all_ones = word_emb.new_ones((qlen, klen), dtype=torch.uint8)",
transformers/src/,577,mask_len = klen - self.mem_len,
transformers/src/,578,if mask_len > 0:,
transformers/src/,579,mask_shift_len = qlen - mask_len,
transformers/src/,580,else:,
transformers/src/,581,mask_shift_len = qlen,
transformers/src/,582,"dec_attn_mask = (torch.triu(all_ones, 1+mlen)",
transformers/src/,583,"+ torch.tril(all_ones, -mask_shift_len))[:, :, None] # -1",
transformers/src/,584,else:,
transformers/src/,585,dec_attn_mask = torch.triu(,
transformers/src/,586,"word_emb.new_ones((qlen, klen), dtype=torch.uint8), diagonal=1+mlen)[:,:,None]",
transformers/src/,590,default,
transformers/src/,606,learnable embeddings and absolute embeddings,
transformers/src/,607,Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint,
transformers/src/,613,"We transpose back here to shape [bsz, len, hidden_dim]",
transformers/src/,616,"Add last layer and transpose to library standard shape [bsz, len, hidden_dim]",
transformers/src/,621,"Transpose to library standard shape [bsz, n_heads, query_seq_len, key_seq_len]",
transformers/src/,624,"last hidden state, new_mems, (all hidden states), (all attentions)",
transformers/src/,742,"The output weights are the same as the input embeddings, but there is",
transformers/src/,743,an output-only bias for each token.,
transformers/src/,853,"logits, new_mems, (all hidden states), (all attentions)",
transformers/src/,858,if past is defined in model kwargs then use it for faster decoding,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The Open AI Team Authors and The HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,48,Define type aliases,
transformers/src/,104,Handle all the truncation and padding stuff,
transformers/src/,520,"Padding side is right by default and over-riden in subclasses. If specified in the kwargs, it is changed.",
transformers/src/,524,Added tokens,
transformers/src/,529,inputs and kwargs for saving and re-loading (see ``from_pretrained`` and ``save_pretrained``),
transformers/src/,604,Get the vocabulary from AWS S3 bucket,
transformers/src/,613,Get the vocabulary from local files,
transformers/src/,635,At this point pretrained_model_name_or_path is either a directory or a model identifier name,
transformers/src/,641,Look for the tokenizer main vocabulary files + the additional tokens files,
transformers/src/,653,"Get files from url, cache, or disk depending on the case",
transformers/src/,703,Prepare tokenizer initialization kwargs,
transformers/src/,704,Did we saved some inputs and kwargs to reload ?,
transformers/src/,715,Update with newly provided kwargs,
transformers/src/,718,Set max length if needed,
transformers/src/,720,"if we're using a pretrained model, ensure the tokenizer",
transformers/src/,721,wont index sequences longer than the number of positional embeddings,
transformers/src/,726,Merge resolved_vocab_files arguments in init_kwargs.,
transformers/src/,739,Instantiate tokenizer.,
transformers/src/,748,Save inputs and kwargs for saving and re-loading with ``save_pretrained``,
transformers/src/,752,update unique_added_tokens_encoder with special tokens for correct tokenization,
transformers/src/,755,Add supplementary tokens.,
transformers/src/,951,convert non-special tokens to lowercase,
transformers/src/,1240,Throw an error if we can pad because there is no padding token,
transformers/src/,1382,Throw an error if we can pad because there is no padding token,
transformers/src/,1422,"Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by",
transformers/src/,1423,"the model. It adds special tokens, truncates sequences if overflowing while taking into account",
transformers/src/,1424,the special tokens and manages a window stride for overflowing tokens,
transformers/src/,1439,Append the non-padded length to the output,
transformers/src/,1450,Do the tensor conversion in batch,
transformers/src/,1560,Handle max sequence length,
transformers/src/,1574,Handle special_tokens,
transformers/src/,1649,Prepare inputs as tensors if asked,
transformers/src/,1804,To avoid mixing byte-level and unicode for byte-level BPT,
transformers/src/,1805,we need to build string separatly for added tokens and byte-level tokens,
transformers/src/,1806,cf. https://github.com/huggingface/transformers/issues/1133,
transformers/src/,1861,take into account special tokens,
transformers/src/,1864,take into account special tokens,
transformers/src/,1958,Prepare inputs as tensors if asked,
transformers/src/,2050,Needed if we have to return a tensor,
transformers/src/,2053,Throw an error if we can pad because there is no padding token,
transformers/src/,2057,Set the truncation and padding strategy and restore the initial configuration,
transformers/src/,2075,Check for the pretokenized path,
transformers/src/,2079,Iterate over each sample (we don't know yet if they are pairs or simple input,
transformers/src/,2089,Convert to tuple for convenience,
transformers/src/,2095,Check if we have pairs,
transformers/src/,2101,"No pair, default to None",
transformers/src/,2105,Something else is invalid,
transformers/src/,2115,Post-process,
transformers/src/,2119,Classical path with strings input,
transformers/src/,2121,Avoid thread overhead if only one example.,
transformers/src/,2137,Convert encoding to dict,
transformers/src/,2151,Sanitize the output to have dict[list] from list[dict],
transformers/src/,2159,elif not return_tensors and len(stack) == 1:,
transformers/src/,2160,stack = stack[0],
transformers/src/,2164,"If returning overflowing tokens, we need to return a mapping",
transformers/src/,2165,from the batch idx to the original sample,
transformers/src/,2193,"Check for pretokenized path (ie [token1, token2, ..., tokenN] -> [id1, id2, ..., idN]",
transformers/src/,2197,Encode through encode_batch with sequence of only one word which will be merged after hand,
transformers/src/,2201,Let's do the same for pairs if provided,
transformers/src/,2203,We prepend empty string before each word so that encoding is aware content is a pair,
transformers/src/,2216,"Post process and if asked to do so, insert special tokens where needed",
transformers/src/,2254,"Return tensor is None, then we can remove the leading batch axis",
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,1,coding=utf-8,
transformers/src/,2,"Copyright 2019-present CNRS, Facebook Inc. and the HuggingFace Inc. team.",
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,114,", dico, is_encoder, with_output):",
transformers/src/,161,"removed: src_enc=None, src_len=None",
transformers/src/,172,mask = input_ids != self.pad_index,
transformers/src/,174,check inputs,
transformers/src/,177,"input_ids = input_ids.transpose(0, 1)  # batch size as dimension 0",
transformers/src/,178,assert (src_enc is None) == (src_len is None),
transformers/src/,179,if src_enc is not None:,
transformers/src/,180,assert self.is_decoder,
transformers/src/,181,assert src_enc.size(0) == bs,
transformers/src/,183,generate masks,
transformers/src/,185,if self.is_decoder and src_enc is not None:,
transformers/src/,186,"src_mask = torch.arange(src_len.max(), dtype=torch.long, device=lengths.device) < src_len[:, None]",
transformers/src/,190,position_ids,
transformers/src/,195,"(slen, bs)",
transformers/src/,196,"position_ids = position_ids.transpose(0, 1)",
transformers/src/,198,langs,
transformers/src/,200,"(slen, bs)",
transformers/src/,201,"langs = langs.transpose(0, 1)",
transformers/src/,203,Prepare head mask if needed,
transformers/src/,204,1.0 in head_mask indicate we keep the head,
transformers/src/,205,attention_probs has shape bsz x n_heads x N x N,
transformers/src/,206,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads],
transformers/src/,207,and head_mask is converted to shape [num_hidden_layers x batch x num_heads x qlen x klen],
transformers/src/,215,We can specify head_mask for each layer,
transformers/src/,218,switch to fload if need + fp16 compatibility,
transformers/src/,222,do not recompute cached elements,
transformers/src/,232,embeddings,
transformers/src/,245,transformer layers,
transformers/src/,249,LayerDrop,
transformers/src/,257,self attention,
transformers/src/,275,encoder attention (for decoder only),
transformers/src/,276,if self.is_decoder and src_enc is not None:,
transformers/src/,277,"attn = self.encoder_attn[i](tensor, src_mask, kv=src_enc, cache=cache)",
transformers/src/,278,"attn = F.dropout(attn, p=self.dropout, training=self.training)",
transformers/src/,279,tensor = tensor + attn,
transformers/src/,280,tensor = self.layer_norm15[i](tensor),
transformers/src/,282,FFN,
transformers/src/,292,Add last hidden state,
transformers/src/,296,update cache length,
transformers/src/,300,move back sequence length to dimension 0,
transformers/src/,301,"tensor = tensor.transpose(0, 1)",
transformers/src/,308,"outputs, (hidden_states), (attentions)",
transformers/src/,1,coding=utf-8,
transformers/src/,2,"Copyright 2020 The Google AI Team, Stanford University and The HuggingFace Inc. team.",
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2020 The Fairseq Authors and The HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,85,encoder_embed_dim and decoder_embed_dim,
transformers/src/,95,"Normal(0, this parameter)",
transformers/src/,97,scale factor will be sqrt(d_model) if True,
transformers/src/,99,"True for mbart, False otherwise",
transformers/src/,100,combo of fairseq's encoder_ and decoder_normalize_before,
transformers/src/,103,3 Types of Dropout,
transformers/src/,108,Classifier stuff,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,1,coding=utf-8,
transformers/src/,2,"Copyright 2018 Mesh TensorFlow authors, T5 Authors and HuggingFace Inc. team.",
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,35,,
transformers/src/,36,This dict contrains shortcut names and associated url,
transformers/src/,37,for the pretrained weights provided with the models,
transformers/src/,38,,
transformers/src/,48,,
transformers/src/,49,This is a conversion method from TF 1.0 to PyTorch,
transformers/src/,50,More details: https://medium.com/huggingface/from-tensorflow-to-pytorch-265f40ef2a28,
transformers/src/,51,,
transformers/src/,67,Load weights from TF model,
transformers/src/,79,adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v,
transformers/src/,80,which are not required for using pretrained model,
transformers/src/,101,elif scope_names[0] == 'scale':,
transformers/src/,102,"pointer = getattr(pointer, 'weight')",
transformers/src/,103,elif scope_names[0] == 'output_bias' or scope_names[0] == 'beta':,
transformers/src/,104,"pointer = getattr(pointer, 'bias')",
transformers/src/,105,elif scope_names[0] == 'squad':,
transformers/src/,106,"pointer = getattr(pointer, 'classifier')",
transformers/src/,131,"logger.info(""Weights not copied to PyTorch model: {}"".format(', '.join(tf_weights.keys())))",
transformers/src/,135,,
transformers/src/,136,PyTorch Models are constructed by sub-classing,
transformers/src/,137,- torch.nn.Module for the layers and,
transformers/src/,138,- PreTrainedModel for the models (it-self a sub-class of torch.nn.Module),
transformers/src/,139,,
transformers/src/,200,Mesh TensorFlow initialization to avoid scaling before softmax,
transformers/src/,220,Prune linear layers,
transformers/src/,225,Update hyper params,
transformers/src/,259,"mtf.to_int32(mtf.less(n, 0)) * num_buckets",
transformers/src/,263,"now n is in the range [0, inf)",
transformers/src/,265,half of the buckets are for exact increments in positions,
transformers/src/,269,The other half of the buckets are for logarithmically bigger bins in positions up to max_distance,
transformers/src/,282,"shape (qlen, klen)",
transformers/src/,284,"shape (qlen, klen)",
transformers/src/,289,"shape (qlen, klen, num_heads)",
transformers/src/,290,"shape (1, num_heads, qlen, klen)",
transformers/src/,307,"Input is (bs, qlen, dim)",
transformers/src/,308,"Mask is (bs, klen) (non-causal) or (bs, klen, klen)",
transformers/src/,309,"past_key_value_state[0] is (bs, n_heads, q_len - 1, dim_per_head)",
transformers/src/,336,"(bs, n_heads, qlen, dim_per_head)",
transformers/src/,339,"(bs, n_heads, qlen, dim_per_head)",
transformers/src/,340,"(bs, n_heads, qlen, dim_per_head)",
transformers/src/,343,"(bs, n_heads, qlen, dim_per_head)",
transformers/src/,344,"(bs, n_heads, qlen, dim_per_head)",
transformers/src/,349,"(bs, n_heads, klen, dim_per_head)",
transformers/src/,350,"(bs, n_heads, klen, dim_per_head)",
transformers/src/,359,q = q / math.sqrt(dim_per_head)                                     # No scaling in T5,
transformers/src/,360,"(bs, n_heads, qlen, klen)",
transformers/src/,367,if key and values are already calculated,
transformers/src/,368,we want only the last query position bias,
transformers/src/,373,"(bs, n_heads, qlen, klen)",
transformers/src/,376,"(bs, n_heads, qlen, klen)",
transformers/src/,377,"(bs, n_heads, qlen, klen)",
transformers/src/,379,Mask heads if we want to,
transformers/src/,383,"(bs, n_heads, qlen, dim_per_head)",
transformers/src/,384,"(bs, qlen, dim)",
transformers/src/,424,add attentions if we output them,
transformers/src/,459,add attentions if we output them,
transformers/src/,508,Keep self-attention outputs and relative position weights,
transformers/src/,511,the actual query length is unknown for cross attention,
transformers/src/,512,if using past key value states. Need to inject it here,
transformers/src/,529,Combine self attn and cross attn key value states,
transformers/src/,533,Keep cross-attention outputs and relative position weights,
transformers/src/,536,Apply Feed Forward layer,
transformers/src/,540,Add attentions if we output them,
transformers/src/,542,"hidden-states, present_key_value_states, (self-attention weights), (self-attention position bias), (cross-attention weights), (cross-attention position bias)",
transformers/src/,568,Used for testing weights initialization,
transformers/src/,572,Mesh TensorFlow embeddings initialization,
transformers/src/,573,See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L1624,
transformers/src/,576,Mesh TensorFlow FF initialization,
transformers/src/,577,See https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/transformer_layers.py#L56,
transformers/src/,578,and https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L89,
transformers/src/,586,Mesh TensorFlow attention initialization to avoid scaling before softmax,
transformers/src/,587,See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/attention.py#L136,
transformers/src/,606,shift inputs to the right,
transformers/src/,612,replace possible -100 values in lm_labels by `pad_token_id`,
transformers/src/,678,required mask seq length can be calculated via length of past,
transformers/src/,679,key value states and seq_length = 1 for the last token,
transformers/src/,690,initialize past_key_value_states with `None` if past does not exist,
transformers/src/,694,"We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]",
transformers/src/,695,ourselves in which case we just need to make it broadcastable to all heads.,
transformers/src/,699,"Provided a padding mask of dimensions [batch_size, mask_seq_length]",
transformers/src/,700,"- if the model is a decoder, apply a causal mask in addition to the padding mask",
transformers/src/,701,"- if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, mask_seq_length, mask_seq_length]",
transformers/src/,712,Since attention_mask is 1.0 for positions we want to attend and 0.0 for,
transformers/src/,713,"masked positions, this operation will create a tensor which is 0.0 for",
transformers/src/,714,positions we want to attend and -1e9 for masked positions.,
transformers/src/,715,"Since we are adding it to the raw scores before the softmax, this is",
transformers/src/,716,effectively the same as removing these entirely.,
transformers/src/,718,"T5 has a mask that can compare sequence ids, we can simulate this here with this transposition",
transformers/src/,719,Cf. https://github.com/tensorflow/mesh/blob/8d2465e9bc93129b913b5ccc6a59aa97abd96ec6/mesh_tensorflow/transformer/transformer_layers.py#L270,
transformers/src/,720,"extended_attention_mask = (extended_attention_mask == extended_attention_mask.transpose(-1, -2))",
transformers/src/,722,fp16 compatibility,
transformers/src/,726,If a 2D ou 3D attention mask is provided for the cross-attention,
transformers/src/,727,"we need to make broadcastabe to [batch_size, num_heads, mask_seq_length, mask_seq_length]",
transformers/src/,733,"T5 has a mask that can compare sequence ids, we can simulate this here with this transposition",
transformers/src/,734,Cf. https://github.com/tensorflow/mesh/blob/8d2465e9bc93129b913b5ccc6a59aa97abd96ec6/mesh_tensorflow/transformer/transformer_layers.py#L270,
transformers/src/,735,"encoder_extended_attention_mask = (encoder_extended_attention_mask == encoder_extended_attention_mask.transpose(-1, -2))",
transformers/src/,739,fp16 compatibility,
transformers/src/,744,Prepare head mask if needed,
transformers/src/,745,1.0 in head_mask indicate we keep the head,
transformers/src/,746,attention_probs has shape bsz x n_heads x N x N,
transformers/src/,747,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads],
transformers/src/,748,and head_mask is converted to shape [num_hidden_layers x batch x num_heads x mask_seq_length x mask_seq_length],
transformers/src/,756,We can specify head_mask for each layer,
transformers/src/,759,switch to fload if need + fp16 compatibility,
transformers/src/,786,layer_outputs is a tuple with:,
transformers/src/,787,"hidden-states, key-value-states, (self-attention weights), (self-attention position bias), (cross-attention weights), (cross-attention position bias)",
transformers/src/,790,We share the position biases between the layers - the first layer store them,
transformers/src/,791,"layer_outputs = hidden-states, key-value-states (self-attention weights), (self-attention position bias), (cross-attention weights), (cross-attention position bias)",
transformers/src/,795,append next layer key value states,
transformers/src/,799,We keep only self-attention weights for now,
transformers/src/,804,Add last layer,
transformers/src/,816,"last-layer hidden state, (presents,) (all hidden states), (all attentions)",
transformers/src/,976,"Encode if needed (training, first prediction pass)",
transformers/src/,984,"If decoding with past key value states, only the last tokens",
transformers/src/,985,should be given as an input,
transformers/src/,992,Decode,
transformers/src/,1105,"Encode if needed (training, first prediction pass)",
transformers/src/,1107,Convert encoder inputs in embeddings if needed,
transformers/src/,1115,get decoder inputs from shifting lm labels to the right,
transformers/src/,1118,"If decoding with past key value states, only the last tokens",
transformers/src/,1119,should be given as an input,
transformers/src/,1127,Decode,
transformers/src/,1139,insert decoder past at right place,
transformers/src/,1140,to speed up decoding,
transformers/src/,1146,Rescale output before projecting on vocab,
transformers/src/,1147,See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586,
transformers/src/,1151,Add hidden states and attention if they are here,
transformers/src/,1155,TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L666,
transformers/src/,1163,first step,
transformers/src/,1178,if decoder past is not included in output,
transformers/src/,1179,speedy decoding is disabled and no need to reorder,
transformers/src/,1188,get the correct batch idx from layer past batch dim,
transformers/src/,1189,batch dim of `past` is at 2nd position,
transformers/src/,1192,need to set correct `past` for each of the four key / value states,
transformers/src/,1,coding=utf-8,
transformers/src/,2,"Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.",
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License,
transformers/src/,45,Load with,
transformers/src/,46,"`tokenizer = AutoTokenizer.from_pretrained(""username/pretrained_model"")`",
transformers/src/,132,take into account special tokens,
transformers/src/,133,take into account special tokens,
transformers/src/,137,HACK: These tokens were added by fairseq but don't seem to be actually used when duplicated in the actual,
transformers/src/,138,sentencepiece vocabulary (this is the case for <s> and </s>,
transformers/src/,244,Convert sentence piece unk token to fairseq unk token index,
transformers/src/,1,coding=utf-8,
transformers/src/,2,"Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.",
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,128,"x = torch.index_select(x, 1, torch.arange(klen, device=x.device, dtype=torch.long))",
transformers/src/,137,content based attention score,
transformers/src/,140,position based attention score,
transformers/src/,144,segment based attention score,
transformers/src/,151,merge attention scores and perform masking,
transformers/src/,154,attn_score = attn_score * (1 - attn_mask) - 1e30 * attn_mask,
transformers/src/,160,attention probability,
transformers/src/,165,Mask heads if we want to,
transformers/src/,169,attention output,
transformers/src/,179,post-attention projection (back to `d_model`),
transformers/src/,196,Two-stream attention with relative positional encoding.,
transformers/src/,197,content based attention score,
transformers/src/,203,content-based key head,
transformers/src/,206,content-based value head,
transformers/src/,209,position-based key head,
transformers/src/,212,h-stream,
transformers/src/,213,content-stream query head,
transformers/src/,216,core attention ops,
transformers/src/,224,post processing,
transformers/src/,227,g-stream,
transformers/src/,228,query-stream query head,
transformers/src/,231,core attention ops,
transformers/src/,250,post processing,
transformers/src/,257,Multi-head attention with relative positional encoding,
transformers/src/,263,content heads,
transformers/src/,268,positional heads,
transformers/src/,271,core attention ops,
transformers/src/,279,post processing,
transformers/src/,331,Add again attentions if there are there,
transformers/src/,339,"The output weights are the same as the input embeddings, but there is",
transformers/src/,340,an output-only bias for each token.,
transformers/src/,454,"beg, end = klen - 1, -qlen",
transformers/src/,457,"beg, end = klen - 1, -1",
transformers/src/,475,"With bi_data, the batch size should be divisible by 2.",
transformers/src/,535,"the original code for XLNet uses shapes [len, bsz] with the batch dimension at the end",
transformers/src/,536,but we want a unified interface in the library with the batch size on the first dimension,
transformers/src/,537,so we move here the first dimension (batch) to the end,
transformers/src/,561,Attention mask,
transformers/src/,562,causal attention mask,
transformers/src/,571,data mask: input mask & perm mask,
transformers/src/,588,all mems can be attended to,
transformers/src/,606,Word embeddings and prepare h & g hidden states,
transformers/src/,614,else:  # We removed the inp_q input which was same as target mapping,
transformers/src/,615,"inp_q_ext = inp_q[:, :, None]",
transformers/src/,616,word_emb_q = inp_q_ext * self.mask_emb + (1 - inp_q_ext) * word_emb_k,
transformers/src/,621,Segment embedding,
transformers/src/,623,Convert `token_type_ids` to one-hot `seg_mat`,
transformers/src/,627,`1` indicates not in the same segment [qlen x klen x bsz],
transformers/src/,633,Positional encoding,
transformers/src/,637,Prepare head mask if needed,
transformers/src/,638,1.0 in head_mask indicate we keep the head,
transformers/src/,639,attention_probs has shape bsz x n_heads x N x N,
transformers/src/,640,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] (a head_mask for each layer),
transformers/src/,641,and head_mask is converted to shape [num_hidden_layers x qlen x klen x bsz x n_head],
transformers/src/,650,switch to fload if need + fp16 compatibility,
transformers/src/,661,cache new mems,
transformers/src/,675,Add last hidden state,
transformers/src/,681,"Prepare outputs, we transpose back here to shape [bsz, len, hidden_dim] (cf. beginning of forward() method)",
transformers/src/,697,"outputs, (new_mems), (hidden_states), (attentions)",
transformers/src/,856,Add dummy token at the end (no attention on this one),
transformers/src/,862,Build permutation mask so that previous tokens don't see last token,
transformers/src/,868,We'll only predict the last token,
transformers/src/,880,if past is defined in model kwargs then use it for faster decoding,
transformers/src/,933,"Keep mems, hidden states, attentions if there are in it",
transformers/src/,935,"return logits, (mems), (hidden states), (attentions)",
transformers/src/,997,"Keep mems, hidden states, attentions if there are in it",
transformers/src/,999,"return logits, (mems), (hidden states), (attentions)",
transformers/src/,1056,"Keep mems, hidden states, attentions if there are in it",
transformers/src/,1058,"return logits, (mems), (hidden states), (attentions)",
transformers/src/,1124,"Keep mems, hidden states, attentions if there are in it",
transformers/src/,1126,"start_logits, end_logits, (mems), (hidden_states), (attentions)",
transformers/src/,1129,"@add_start_docstrings(""""""XLNet Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear layers on top of",
transformers/src/,1130,"the hidden-states output to compute `span start logits` and `span end logits`). """""",",
transformers/src/,1131,"XLNET_START_DOCSTRING, XLNET_INPUTS_DOCSTRING)",
transformers/src/,1132,class TFXLNetForQuestionAnswering(TFXLNetPreTrainedModel):,
transformers/src/,1133,"r""""""",
transformers/src/,1134,Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:,
transformers/src/,1135,"**start_top_log_probs**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)",
transformers/src/,1136,"``tf.Tensor`` of shape ``(batch_size, config.start_n_top)``",
transformers/src/,1137,Log probabilities for the top config.start_n_top start token possibilities (beam-search).,
transformers/src/,1138,"**start_top_index**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)",
transformers/src/,1139,"``tf.Tensor`` of shape ``(batch_size, config.start_n_top)``",
transformers/src/,1140,Indices for the top config.start_n_top start token possibilities (beam-search).,
transformers/src/,1141,"**end_top_log_probs**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)",
transformers/src/,1142,"``tf.Tensor`` of shape ``(batch_size, config.start_n_top * config.end_n_top)``",
transformers/src/,1143,Log probabilities for the top ``config.start_n_top * config.end_n_top`` end token possibilities (beam-search).,
transformers/src/,1144,"**end_top_index**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)",
transformers/src/,1145,"``tf.Tensor`` of shape ``(batch_size, config.start_n_top * config.end_n_top)``",
transformers/src/,1146,Indices for the top ``config.start_n_top * config.end_n_top`` end token possibilities (beam-search).,
transformers/src/,1147,"**cls_logits**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)",
transformers/src/,1148,"``tf.Tensor`` of shape ``(batch_size,)``",
transformers/src/,1149,Log probabilities for the ``is_impossible`` label of the answers.,
transformers/src/,1150,**mems**:,
transformers/src/,1151,list of ``tf.Tensor`` (one for each layer):,
transformers/src/,1152,that contains pre-computed hidden-states (key and values in the attention blocks) as computed by the model,
transformers/src/,1153,if config.mem_len > 0 else tuple of None. Can be used to speed up sequential decoding and attend to longer context.,
transformers/src/,1154,See details in the docstring of the `mems` input above.,
transformers/src/,1155,"**hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)",
transformers/src/,1156,list of ``tf.Tensor`` (one for the output of each layer + the output of the embeddings),
transformers/src/,1157,"of shape ``(batch_size, sequence_length, hidden_size)``:",
transformers/src/,1158,Hidden-states of the model at the output of each layer plus the initial embedding outputs.,
transformers/src/,1159,"**attentions**: (`optional`, returned when ``config.output_attentions=True``)",
transformers/src/,1160,"list of ``tf.Tensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:",
transformers/src/,1161,"Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.",
transformers/src/,1163,Examples::,
transformers/src/,1165,# For example purposes. Not runnable.,
transformers/src/,1166,tokenizer = XLMTokenizer.from_pretrained('xlm-mlm-en-2048'),
transformers/src/,1167,model = XLMForQuestionAnswering.from_pretrained('xlnet-large-cased'),
transformers/src/,1168,"input_ids = tf.constant(tokenizer.encode(""Hello, my dog is cute"", add_special_tokens=True))[None, :]  # Batch size 1",
transformers/src/,1169,start_positions = tf.constant([1]),
transformers/src/,1170,end_positions = tf.constant([3]),
transformers/src/,1171,"outputs = model(input_ids, start_positions=start_positions, end_positions=end_positions)",
transformers/src/,1172,"loss, start_scores, end_scores = outputs[:2]",
transformers/src/,1174,"""""""",
transformers/src/,1175,"def __init__(self, config, *inputs, **kwargs):",
transformers/src/,1176,"super().__init__(config, *inputs, **kwargs)",
transformers/src/,1177,self.start_n_top = config.start_n_top,
transformers/src/,1178,self.end_n_top = config.end_n_top,
transformers/src/,1180,"self.transformer = TFXLNetMainLayer(config, name='transformer')",
transformers/src/,1181,"self.start_logits = TFPoolerStartLogits(config, name='start_logits')",
transformers/src/,1182,"self.end_logits = TFPoolerEndLogits(config, name='end_logits')",
transformers/src/,1183,"self.answer_class = TFPoolerAnswerClass(config, name='answer_class')",
transformers/src/,1185,"def call(self, inputs, training=False):",
transformers/src/,1186,"transformer_outputs = self.transformer(inputs, training=training)",
transformers/src/,1187,hidden_states = transformer_outputs[0],
transformers/src/,1188,"start_logits = self.start_logits(hidden_states, p_mask=p_mask)",
transformers/src/,1190,"outputs = transformer_outputs[1:]  # Keep mems, hidden states, attentions if there are in it",
transformers/src/,1192,if start_positions is not None and end_positions is not None:,
transformers/src/,1193,"# If we are on multi-GPU, let's remove the dimension added by batch splitting",
transformers/src/,1194,"for x in (start_positions, end_positions, cls_index, is_impossible):",
transformers/src/,1195,if x is not None and x.dim() > 1:,
transformers/src/,1196,x.squeeze_(-1),
transformers/src/,1198,"# during training, compute the end logits based on the ground truth of the start position",
transformers/src/,1199,"end_logits = self.end_logits(hidden_states, start_positions=start_positions, p_mask=p_mask)",
transformers/src/,1201,loss_fct = CrossEntropyLoss(),
transformers/src/,1202,"start_loss = loss_fct(start_logits, start_positions)",
transformers/src/,1203,"end_loss = loss_fct(end_logits, end_positions)",
transformers/src/,1204,total_loss = (start_loss + end_loss) / 2,
transformers/src/,1206,if cls_index is not None and is_impossible is not None:,
transformers/src/,1207,# Predict answerability from the representation of CLS and START,
transformers/src/,1208,"cls_logits = self.answer_class(hidden_states, start_positions=start_positions, cls_index=cls_index)",
transformers/src/,1209,loss_fct_cls = nn.BCEWithLogitsLoss(),
transformers/src/,1210,"cls_loss = loss_fct_cls(cls_logits, is_impossible)",
transformers/src/,1212,# note(zhiliny): by default multiply the loss by 0.5 so that the scale is comparable to start_loss and end_loss,
transformers/src/,1213,total_loss += cls_loss * 0.5,
transformers/src/,1215,"outputs = (total_loss,) + outputs",
transformers/src/,1217,else:,
transformers/src/,1218,"# during inference, compute the end logits based on beam search",
transformers/src/,1219,"bsz, slen, hsz = hidden_states.size()",
transformers/src/,1220,"start_log_probs = F.softmax(start_logits, dim=-1) # shape (bsz, slen)",
transformers/src/,1222,"start_top_log_probs, start_top_index = torch.topk(start_log_probs, self.start_n_top, dim=-1) # shape (bsz, start_n_top)",
transformers/src/,1223,"start_top_index_exp = start_top_index.unsqueeze(-1).expand(-1, -1, hsz) # shape (bsz, start_n_top, hsz)",
transformers/src/,1224,"start_states = torch.gather(hidden_states, -2, start_top_index_exp) # shape (bsz, start_n_top, hsz)",
transformers/src/,1225,"start_states = start_states.unsqueeze(1).expand(-1, slen, -1, -1) # shape (bsz, slen, start_n_top, hsz)",
transformers/src/,1227,"hidden_states_expanded = hidden_states.unsqueeze(2).expand_as(start_states) # shape (bsz, slen, start_n_top, hsz)",
transformers/src/,1228,p_mask = p_mask.unsqueeze(-1) if p_mask is not None else None,
transformers/src/,1229,"end_logits = self.end_logits(hidden_states_expanded, start_states=start_states, p_mask=p_mask)",
transformers/src/,1230,"end_log_probs = F.softmax(end_logits, dim=1) # shape (bsz, slen, start_n_top)",
transformers/src/,1232,"end_top_log_probs, end_top_index = torch.topk(end_log_probs, self.end_n_top, dim=1) # shape (bsz, end_n_top, start_n_top)",
transformers/src/,1233,"end_top_log_probs = end_top_log_probs.view(-1, self.start_n_top * self.end_n_top)",
transformers/src/,1234,"end_top_index = end_top_index.view(-1, self.start_n_top * self.end_n_top)",
transformers/src/,1236,"start_states = torch.einsum(""blh,bl->bh"", hidden_states, start_log_probs)  # get the representation of START as weighted sum of hidden states",
transformers/src/,1237,"cls_logits = self.answer_class(hidden_states, start_states=start_states, cls_index=cls_index)  # Shape (batch size,): one single `cls_logits` for each sample",
transformers/src/,1239,"outputs = (start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits) + outputs",
transformers/src/,1241,"# return start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits",
transformers/src/,1242,"# or (if labels are provided) (total_loss,)",
transformers/src/,1243,return outputs,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,1,coding=utf-8,
transformers/src/,2,"Copyright 2018 Google AI, Google Brain and the HuggingFace Inc. team.",
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,140,take into account special tokens,
transformers/src/,141,take into account special tokens,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The Open AI Team Authors and The HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,153,take into account special tokens,
transformers/src/,154,take into account special tokens,
transformers/src/,288,As we override the post_processor post super.__init__ the computed num_added_tokens is wrong in super().,
transformers/src/,289,We need to recompute max_len according to the newly register post_processor to get real values.,
transformers/src/,292,take into account special tokens,
transformers/src/,295,take into account special tokens,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2019 Facebook AI Research and the HuggingFace Inc. team.,
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,49,self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load,
transformers/src/,50,any TensorFlow checkpoint file,
transformers/src/,57,Create and initialize weights. The random normal initializer was chosen,
transformers/src/,58,"arbitrarily, and works well.",
transformers/src/,170,We create a 3D attention mask from a 2D tensor mask.,
transformers/src/,171,"Sizes are [batch_size, 1, 1, to_seq_length]",
transformers/src/,172,"So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]",
transformers/src/,173,this attention mask is more simple than the triangular masking of causal attention,
transformers/src/,174,"used in OpenAI GPT, we just need to prepare the broadcast dimension here.",
transformers/src/,177,Since attention_mask is 1.0 for positions we want to attend and 0.0 for,
transformers/src/,178,"masked positions, this operation will create a tensor which is 0.0 for",
transformers/src/,179,positions we want to attend and -10000.0 for masked positions.,
transformers/src/,180,"Since we are adding it to the raw scores before the softmax, this is",
transformers/src/,181,effectively the same as removing these entirely.,
transformers/src/,455,"(loss), scores, (hidden_states), (attentions)",
transformers/src/,550,"(masked_lm_loss), prediction_scores, (hidden_states), (attentions)",
transformers/src/,616,"(loss), scores, (hidden_states), (attentions)",
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,59,Create the position ids from the input token ids. Any padded tokens remain padded.,
transformers/src/,237,Add hidden states and attention if they are here,
transformers/src/,244,"(masked_lm_loss), prediction_scores, (hidden_states), (attentions)",
transformers/src/,258,Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`,
transformers/src/,266,project back to size of vocabulary with bias,
transformers/src/,352,We are doing regression,
transformers/src/,360,"(loss), logits, (hidden_states), (attentions)",
transformers/src/,452,add hidden states and attention if they are here,
transformers/src/,459,"(loss), reshaped_logits, (hidden_states), (attentions)",
transformers/src/,544,add hidden states and attention if they are here,
transformers/src/,548,Only keep active parts of the loss,
transformers/src/,560,"(loss), scores, (hidden_states), (attentions)",
transformers/src/,573,take <s> token (equiv. to [CLS]),
transformers/src/,681,"If we are on multi-GPU, split add a dimension",
transformers/src/,686,"sometimes the start/end positions are outside our model inputs, we ignore these terms",
transformers/src/,697,"(loss), start_logits, end_logits, (hidden_states), (attentions)",
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,116,self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load,
transformers/src/,117,any TensorFlow checkpoint file,
transformers/src/,124,Create and initialize weights. The random normal initializer was chosen,
transformers/src/,125,"arbitrarily, and works well.",
transformers/src/,239,"Take the dot product between ""query"" and ""key"" to get the raw attention scores.",
transformers/src/,242,"(batch size, num_heads, seq_len_q, seq_len_k)",
transformers/src/,243,scale attention_scores,
transformers/src/,247,Apply the attention mask is (precomputed for all layers in TFBertModel call() function),
transformers/src/,250,Normalize the attention scores to probabilities.,
transformers/src/,253,"This is actually dropping out entire tokens to attend to, which might",
transformers/src/,254,"seem a bit unusual, but is taken from the original Transformer paper.",
transformers/src/,257,Mask heads if we want to,
transformers/src/,266,"(batch_size, seq_len_q, all_head_size)",
transformers/src/,304,add attentions if we output them,
transformers/src/,357,add attentions if we output them,
transformers/src/,383,Add last layer,
transformers/src/,392,"outputs, (hidden states), (attentions)",
transformers/src/,406,"We ""pool"" the model by simply taking the hidden state corresponding",
transformers/src/,407,to the first token.,
transformers/src/,438,"The output weights are the same as the input embeddings, but there is",
transformers/src/,439,an output-only bias for each token.,
transformers/src/,543,We create a 3D attention mask from a 2D tensor mask.,
transformers/src/,544,"Sizes are [batch_size, 1, 1, to_seq_length]",
transformers/src/,545,"So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]",
transformers/src/,546,this attention mask is more simple than the triangular masking of causal attention,
transformers/src/,547,"used in OpenAI GPT, we just need to prepare the broadcast dimension here.",
transformers/src/,550,Since attention_mask is 1.0 for positions we want to attend and 0.0 for,
transformers/src/,551,"masked positions, this operation will create a tensor which is 0.0 for",
transformers/src/,552,positions we want to attend and -10000.0 for masked positions.,
transformers/src/,553,"Since we are adding it to the raw scores before the softmax, this is",
transformers/src/,554,effectively the same as removing these entirely.,
transformers/src/,559,Prepare head mask if needed,
transformers/src/,560,1.0 in head_mask indicate we keep the head,
transformers/src/,561,attention_probs has shape bsz x n_heads x N x N,
transformers/src/,562,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads],
transformers/src/,563,and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length],
transformers/src/,568,head_mask = tf.constant([0] * self.num_hidden_layers),
transformers/src/,578,add hidden_states and attentions if they are here,
transformers/src/,579,"sequence_output, pooled_output, (hidden_states), (attentions)",
transformers/src/,769,add hidden states and attention if they are here,
transformers/src/,771,"prediction_scores, seq_relationship_score, (hidden_states), (attentions)",
transformers/src/,820,Add hidden states and attention if they are here,
transformers/src/,822,"prediction_scores, (hidden_states), (attentions)",
transformers/src/,870,add hidden states and attention if they are here,
transformers/src/,872,"seq_relationship_score, (hidden_states), (attentions)",
transformers/src/,928,add hidden states and attention if they are here,
transformers/src/,930,"logits, (hidden_states), (attentions)",
transformers/src/,1047,add hidden states and attention if they are here,
transformers/src/,1049,"reshaped_logits, (hidden_states), (attentions)",
transformers/src/,1105,add hidden states and attention if they are here,
transformers/src/,1107,"scores, (hidden_states), (attentions)",
transformers/src/,1168,"start_logits, end_logits, (hidden_states), (attentions)",
transformers/src/,1,coding=utf-8,
transformers/src/,2,"Copyright 2019-present, Facebook, Inc and the HuggingFace Inc. team.",
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,70,"attention mask is the same as mask, or triangular inferior attention (causal)",
transformers/src/,77,sanity check,
transformers/src/,114,Prune linear layers,
transformers/src/,119,Update hyper params,
transformers/src/,128,"Input is (bs, qlen, dim)",
transformers/src/,129,"Mask is (bs, klen) (non-causal) or (bs, klen, klen)",
transformers/src/,135,"assert dim == self.dim, 'Dimensions do not match: %s input vs %s configured' % (dim, self.dim)",
transformers/src/,148,"(bs, n_heads, qlen, dim_per_head)",
transformers/src/,150,"(bs, n_heads, qlen, dim_per_head)",
transformers/src/,151,"(bs, n_heads, qlen, dim_per_head)",
transformers/src/,154,"(bs, n_heads, qlen, dim_per_head)",
transformers/src/,155,"(bs, n_heads, qlen, dim_per_head)",
transformers/src/,161,"(bs, n_heads, klen, dim_per_head)",
transformers/src/,162,"(bs, n_heads, klen, dim_per_head)",
transformers/src/,167,"(bs, n_heads, qlen, dim_per_head)",
transformers/src/,168,"(bs, n_heads, qlen, klen)",
transformers/src/,169,"(bs, n_heads, qlen, klen)",
transformers/src/,170,"(bs, n_heads, qlen, klen)",
transformers/src/,172,"(bs, n_heads, qlen, klen)",
transformers/src/,173,"(bs, n_heads, qlen, klen)",
transformers/src/,175,Mask heads if we want to,
transformers/src/,179,"(bs, n_heads, qlen, dim_per_head)",
transformers/src/,180,"(bs, qlen, dim)",
transformers/src/,314,", dico, is_encoder, with_output):",
transformers/src/,319,"encoder / decoder, output layer",
transformers/src/,324,self.with_output = with_output,
transformers/src/,327,dictionary / languages,
transformers/src/,333,self.dico = dico,
transformers/src/,334,self.id2lang = config.id2lang,
transformers/src/,335,self.lang2id = config.lang2id,
transformers/src/,336,assert len(self.dico) == self.n_words,
transformers/src/,337,assert len(self.id2lang) == len(self.lang2id) == self.n_langs,
transformers/src/,339,model parameters,
transformers/src/,340,512 by default,
transformers/src/,341,2048 by default,
transformers/src/,342,8 by default,
transformers/src/,348,embeddings,
transformers/src/,357,transformer layers,
transformers/src/,362,if self.is_decoder:,
transformers/src/,363,self.layer_norm15 = nn.ModuleList(),
transformers/src/,364,self.encoder_attn = nn.ModuleList(),
transformers/src/,369,if self.is_decoder:,
transformers/src/,370,"self.layer_norm15.append(nn.LayerNorm(self.dim, eps=config.layer_norm_eps))",
transformers/src/,371,"self.encoder_attn.append(MultiHeadAttention(self.n_heads, self.dim, dropout=self.attention_dropout))",
transformers/src/,450,mask = input_ids != self.pad_index,
transformers/src/,452,check inputs,
transformers/src/,455,"input_ids = input_ids.transpose(0, 1)  # batch size as dimension 0",
transformers/src/,456,assert (src_enc is None) == (src_len is None),
transformers/src/,457,if src_enc is not None:,
transformers/src/,458,assert self.is_decoder,
transformers/src/,459,assert src_enc.size(0) == bs,
transformers/src/,461,generate masks,
transformers/src/,463,if self.is_decoder and src_enc is not None:,
transformers/src/,464,"src_mask = torch.arange(src_len.max(), dtype=torch.long, device=lengths.device) < src_len[:, None]",
transformers/src/,468,position_ids,
transformers/src/,473,"(slen, bs)",
transformers/src/,474,"position_ids = position_ids.transpose(0, 1)",
transformers/src/,476,langs,
transformers/src/,478,"(slen, bs)",
transformers/src/,479,"langs = langs.transpose(0, 1)",
transformers/src/,481,Prepare head mask if needed,
transformers/src/,482,1.0 in head_mask indicate we keep the head,
transformers/src/,483,attention_probs has shape bsz x n_heads x N x N,
transformers/src/,484,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads],
transformers/src/,485,and head_mask is converted to shape [num_hidden_layers x batch x num_heads x qlen x klen],
transformers/src/,493,We can specify head_mask for each layer,
transformers/src/,496,switch to fload if need + fp16 compatibility,
transformers/src/,500,do not recompute cached elements,
transformers/src/,510,embeddings,
transformers/src/,523,transformer layers,
transformers/src/,530,self attention,
transformers/src/,539,encoder attention (for decoder only),
transformers/src/,540,if self.is_decoder and src_enc is not None:,
transformers/src/,541,"attn = self.encoder_attn[i](tensor, src_mask, kv=src_enc, cache=cache)",
transformers/src/,542,"attn = F.dropout(attn, p=self.dropout, training=self.training)",
transformers/src/,543,tensor = tensor + attn,
transformers/src/,544,tensor = self.layer_norm15[i](tensor),
transformers/src/,546,FFN,
transformers/src/,551,Add last hidden state,
transformers/src/,555,update cache length,
transformers/src/,559,move back sequence length to dimension 0,
transformers/src/,560,"tensor = tensor.transpose(0, 1)",
transformers/src/,567,"outputs, (hidden_states), (attentions)",
transformers/src/,590,default is False,
transformers/src/,708,Keep new_mems and attention/hidden states if they are here,
transformers/src/,795,Keep new_mems and attention/hidden states if they are here,
transformers/src/,799,We are doing regression,
transformers/src/,907,"If we are on multi-GPU, split add a dimension",
transformers/src/,912,"sometimes the start/end positions are outside our model inputs, we ignore these terms",
transformers/src/,923,Keep new_mems and attention/hidden states if they are here,
transformers/src/,1040,Keep new_mems and attention/hidden states if they are here,
transformers/src/,1122,add hidden states and attention if they are here,
transformers/src/,1125,Only keep active parts of the loss,
transformers/src/,1137,"(loss), scores, (hidden_states), (attentions)",
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,185,take into account special tokens,
transformers/src/,186,take into account special tokens,
transformers/src/,383,"This was added on November 1st, 2018 for the multilingual and Chinese",
transformers/src/,384,"models. This is also applied to the English models now, but it doesn't",
transformers/src/,385,matter since the English models were not trained on any Chinese data,
transformers/src/,386,and generally don't have any Chinese data in them (there are Chinese,
transformers/src/,387,characters in the vocabulary because Wikipedia does have some Chinese,
transformers/src/,388,words in the English Wikipedia.).,
transformers/src/,450,"This defines a ""chinese character"" as anything in the CJK Unicode block:",
transformers/src/,451,https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block),
transformers/src/,452,,
transformers/src/,453,"Note that the CJK Unicode block is NOT all Japanese and Korean characters,",
transformers/src/,454,"despite its name. The modern Korean Hangul alphabet is a different block,",
transformers/src/,455,as is Japanese Hiragana and Katakana. Those alphabets are used to write,
transformers/src/,456,"space-separated words, so they are not treated specially and handled",
transformers/src/,457,like the all of the other languages.,
transformers/src/,460,,
transformers/src/,461,,
transformers/src/,462,,
transformers/src/,463,,
transformers/src/,464,,
transformers/src/,466,,
transformers/src/,467,,
transformers/src/,548,"\t, \n, and \r are technically contorl characters but we treat them",
transformers/src/,549,as whitespace since they are generally considered as such.,
transformers/src/,560,These are technically control characters but we count them as whitespace,
transformers/src/,561,characters.,
transformers/src/,573,We treat all non-letter/number ASCII as punctuation.,
transformers/src/,574,"Characters such as ""^"", ""$"", and ""`"" are not in the Unicode",
transformers/src/,575,"Punctuation class but we treat them as punctuation anyways, for",
transformers/src/,576,consistency.,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2020 The Facebook AI Research Team Authors and The HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,25,vocab and merges same as roberta,
transformers/src/,34,merges and vocab same as Roberta,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,54,Initialise PyTorch model,
transformers/src/,69,Load weights from tf checkpoint,
transformers/src/,72,Save pytorch-model,
transformers/src/,84,Required parameters,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,39,We do this to be able to load python 2 datasets pickles,
transformers/src/,40,See e.g. https://stackoverflow.com/questions/2121874/python-pickling-after-changing-a-modules-directory/2121918#2121918,
transformers/src/,51,Convert a pre-processed corpus (see original TensorFlow repo),
transformers/src/,54,Save vocabulary and dataset cache as Dictionaries (should be better than pickles for the long-term),
transformers/src/,67,Convert a pre-trained TensorFlow model,
transformers/src/,72,Initialise PyTorch model,
transformers/src/,81,Save pytorch-model,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,30,Initialise PyTorch model,
transformers/src/,35,Load weights from tf checkpoint,
transformers/src/,38,Save pytorch-model,
transformers/src/,45,Required parameters,
transformers/src/,1,coding=utf-8,
transformers/src/,2,"Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.",
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,51,We will load also the output bias,
transformers/src/,54,We will load also the sequence summary,
transformers/src/,65,Now load the rest of the transformer,
transformers/src/,68,Embeddings and output,
transformers/src/,76,Transformer blocks,
transformers/src/,97,Relative positioning biases,
transformers/src/,136,Load weights from TF model,
transformers/src/,144,Build TF to PyTorch weights loading map,
transformers/src/,153,adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v,
transformers/src/,154,which are not required for using pretrained model,
transformers/src/,159,Here we will split the TF weights,
transformers/src/,233,"x = x[:, 0:klen, :, :]",
transformers/src/,245,Note: the tensor-slice form was faster in my testing than torch.index_select,
transformers/src/,246,"However, tracing doesn't like the nature of the slice, and if klen changes",
transformers/src/,247,"during the run then it'll fail, whereas index_select will be fine.",
transformers/src/,249,"x = x[:, :, :, :klen]",
transformers/src/,256,content based attention score,
transformers/src/,259,position based attention score,
transformers/src/,263,segment based attention score,
transformers/src/,270,merge attention scores and perform masking,
transformers/src/,273,attn_score = attn_score * (1 - attn_mask) - 1e30 * attn_mask,
transformers/src/,279,attention probability,
transformers/src/,283,Mask heads if we want to,
transformers/src/,287,attention output,
transformers/src/,297,post-attention projection (back to `d_model`),
transformers/src/,309,Two-stream attention with relative positional encoding.,
transformers/src/,310,content based attention score,
transformers/src/,316,content-based key head,
transformers/src/,319,content-based value head,
transformers/src/,322,position-based key head,
transformers/src/,325,h-stream,
transformers/src/,326,content-stream query head,
transformers/src/,329,core attention ops,
transformers/src/,337,post processing,
transformers/src/,340,g-stream,
transformers/src/,341,query-stream query head,
transformers/src/,344,core attention ops,
transformers/src/,363,post processing,
transformers/src/,370,Multi-head attention with relative positional encoding,
transformers/src/,376,content heads,
transformers/src/,381,positional heads,
transformers/src/,384,core attention ops,
transformers/src/,392,post processing,
transformers/src/,452,Add again attentions if there are there,
transformers/src/,470,Slightly different from the TF version which uses truncated_normal for initialization,
transformers/src/,471,cf https://github.com/pytorch/pytorch/pull/5617,
transformers/src/,630,cache hidden states into memory.,
transformers/src/,653,create relative positional encoding.,
transformers/src/,658,"beg, end = klen - 1, -qlen",
transformers/src/,661,"beg, end = klen - 1, -1",
transformers/src/,741,"the original code for XLNet uses shapes [len, bsz] with the batch dimension at the end",
transformers/src/,742,but we want a unified interface in the library with the batch size on the first dimension,
transformers/src/,743,so we move here the first dimension (batch) to the end,
transformers/src/,767,Attention mask,
transformers/src/,768,causal attention mask,
transformers/src/,777,data mask: input mask & perm mask,
transformers/src/,792,all mems can be attended to,
transformers/src/,812,Word embeddings and prepare h & g hidden states,
transformers/src/,820,else:  # We removed the inp_q input which was same as target mapping,
transformers/src/,821,"inp_q_ext = inp_q[:, :, None]",
transformers/src/,822,word_emb_q = inp_q_ext * self.mask_emb + (1 - inp_q_ext) * word_emb_k,
transformers/src/,827,Segment embedding,
transformers/src/,829,Convert `token_type_ids` to one-hot `seg_mat`,
transformers/src/,836,`1` indicates not in the same segment [qlen x klen x bsz],
transformers/src/,842,Positional encoding,
transformers/src/,846,Prepare head mask if needed,
transformers/src/,847,1.0 in head_mask indicate we keep the head,
transformers/src/,848,attention_probs has shape bsz x n_heads x N x N,
transformers/src/,849,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] (a head_mask for each layer),
transformers/src/,850,and head_mask is converted to shape [num_hidden_layers x qlen x klen x bsz x n_head],
transformers/src/,859,switch to fload if need + fp16 compatibility,
transformers/src/,871,cache new mems,
transformers/src/,891,Add last hidden state,
transformers/src/,897,"Prepare outputs, we transpose back here to shape [bsz, len, hidden_dim] (cf. beginning of forward() method)",
transformers/src/,911,"when target_mapping is provided, there are 2-tuple of attentions",
transformers/src/,919,"outputs, (new_mems), (hidden_states), (attentions)",
transformers/src/,942,Add dummy token at the end (no attention on this one),
transformers/src/,948,Build permutation mask so that previous tokens don't see last token,
transformers/src/,955,We'll only predict the last token,
transformers/src/,968,if past is defined in model kwargs then use it for faster decoding,
transformers/src/,1067,"Keep mems, hidden states, attentions if there are in it",
transformers/src/,1070,Flatten the tokens,
transformers/src/,1075,"return (loss), logits, (mems), (hidden states), (attentions)",
transformers/src/,1169,"Keep mems, hidden states, attentions if there are in it",
transformers/src/,1173,We are doing regression,
transformers/src/,1181,"return (loss), logits, (mems), (hidden states), (attentions)",
transformers/src/,1275,"Keep mems, hidden states, attentions if there are in it",
transformers/src/,1278,Only keep active parts of the loss,
transformers/src/,1290,"return (loss), logits, (mems), (hidden states), (attentions)",
transformers/src/,1396,"Keep mems, hidden states, attentions if there are in it",
transformers/src/,1403,"return (loss), logits, (mems), (hidden states), (attentions)",
transformers/src/,1510,"If we are on multi-GPU, split add a dimension",
transformers/src/,1515,"sometimes the start/end positions are outside our model inputs, we ignore these terms",
transformers/src/,1526,"(loss), start_logits, end_logits, (mems), (hidden_states), (attentions)",
transformers/src/,1643,"Keep mems, hidden states, attentions if there are in it",
transformers/src/,1646,"If we are on multi-GPU, let's remove the dimension added by batch splitting",
transformers/src/,1651,"during training, compute the end logits based on the ground truth of the start position",
transformers/src/,1660,Predict answerability from the representation of CLS and START,
transformers/src/,1665,note(zhiliny): by default multiply the loss by 0.5 so that the scale is comparable to start_loss and end_loss,
transformers/src/,1671,"during inference, compute the end logits based on beam search",
transformers/src/,1673,"shape (bsz, slen)",
transformers/src/,1677,"shape (bsz, start_n_top)",
transformers/src/,1678,"shape (bsz, start_n_top, hsz)",
transformers/src/,1679,"shape (bsz, start_n_top, hsz)",
transformers/src/,1680,"shape (bsz, slen, start_n_top, hsz)",
transformers/src/,1684,"shape (bsz, slen, start_n_top, hsz)",
transformers/src/,1687,"shape (bsz, slen, start_n_top)",
transformers/src/,1691,"shape (bsz, end_n_top, start_n_top)",
transformers/src/,1697,get the representation of START as weighted sum of hidden states,
transformers/src/,1700,"Shape (batch size,): one single `cls_logits` for each sample",
transformers/src/,1704,"return start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits",
transformers/src/,1705,"or (if labels are provided) (total_loss,)",
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2019 The Open AI Team Authors and The HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,518,https://github.com/rsennrich/wmt16-scripts/blob/master/preprocess/normalise-romanian.py,
transformers/src/,521,https://github.com/rsennrich/wmt16-scripts/blob/master/preprocess/remove-diacritics.py,
transformers/src/,522,s-comma,
transformers/src/,523,t-comma,
transformers/src/,632,take into account special tokens,
transformers/src/,633,take into account special tokens,
transformers/src/,635,cache of sm.MosesPunctNormalizer instance,
transformers/src/,637,cache of sm.MosesTokenizer instance,
transformers/src/,640,"True for current supported model (v1.2.0), False for XLM-17 & 100",
transformers/src/,795,"TODO: make sure we are using `xlm-mlm-enro-1024`, since XLM-100 doesn't have this step",
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,351,Initialise TF model,
transformers/src/,360,Load weights from tf checkpoint,
transformers/src/,365,Load PyTorch checkpoint in tf2 model:,
transformers/src/,369,build the network,
transformers/src/,385,Save pytorch-model,
transformers/src/,469,Required parameters,
transformers/src/,513,if args.pytorch_checkpoint_path is not None:,
transformers/src/,514,"convert_pt_checkpoint_to_tf(args.model_type.lower(),",
transformers/src/,515,"args.pytorch_checkpoint_path,",
transformers/src/,516,"args.config_file if args.config_file is not None else args.pytorch_checkpoint_path,",
transformers/src/,517,"args.tf_dump_path,",
transformers/src/,518,"compare_with_pt_model=args.compare_with_pt_model,",
transformers/src/,519,use_cached_models=args.use_cached_models),
transformers/src/,520,else:,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,78,"normal layer construction, call with unchanged args (config is already in there)",
transformers/src/,81,"Keras deserialization, convert dict to config",
transformers/src/,147,Save config in model,
transformers/src/,172,Overwrite for models with output embeddings,
transformers/src/,188,if new_num_tokens is None:,
transformers/src/,189,return old_embeddings,
transformers/src/,191,"old_num_tokens, old_embedding_dim = old_embeddings.weight.size()",
transformers/src/,192,if old_num_tokens == new_num_tokens:,
transformers/src/,193,return old_embeddings,
transformers/src/,195,# Build new embeddings,
transformers/src/,196,"new_embeddings = nn.Embedding(new_num_tokens, old_embedding_dim)",
transformers/src/,197,new_embeddings.to(old_embeddings.weight.device),
transformers/src/,199,# initialize all new embeddings (in particular added tokens),
transformers/src/,200,self._init_weights(new_embeddings),
transformers/src/,202,# Copy token embeddings from the previous weights,
transformers/src/,203,"num_tokens_to_copy = min(old_num_tokens, new_num_tokens)",
transformers/src/,204,"new_embeddings.weight.data[:num_tokens_to_copy, :] = old_embeddings.weight.data[:num_tokens_to_copy, :]",
transformers/src/,206,return new_embeddings,
transformers/src/,240,Save configuration file,
transformers/src/,243,"If we save using the predefined names, we can load using `from_pretrained`",
transformers/src/,323,Load config if we don't provide a configuration,
transformers/src/,338,Load model,
transformers/src/,344,Load from a TF 2.0 checkpoint,
transformers/src/,347,Load from a PyTorch checkpoint,
transformers/src/,364,"redirect to the cache, if necessary",
transformers/src/,394,Instantiate model.,
transformers/src/,398,Load from a PyTorch checkpoint,
transformers/src/,401,build the network with dummy inputs,
transformers/src/,404,'by_name' allow us to do transfer learning by skipping/adding layers,
transformers/src/,405,see https://github.com/tensorflow/tensorflow/blob/00fad90125b18b80fe054de1055770cfb8fe4ba3/tensorflow/python/keras/engine/network.py#L1339-L1357,
transformers/src/,414,Make sure restore ops are run,
transformers/src/,416,Check if the models are the same to output loading informations,
transformers/src/,598,We cannot generate if the model does not have a LM head,
transformers/src/,631,overriden by the input batch_size,
transformers/src/,671,not allow to duplicate outputs when greedy decoding,
transformers/src/,674,no_beam_search greedy generation conditions,
transformers/src/,680,beam_search greedy generation conditions,
transformers/src/,685,create attention mask if necessary,
transformers/src/,686,TODO (PVP): this should later be handled by the forward fn() in each model in the future see PR 3140,
transformers/src/,698,current position and vocab size,
transformers/src/,702,set effective batch size and effective batch multiplier according to do_sample,
transformers/src/,720,get encoder and store encoder outputs,
transformers/src/,725,Expand input ids if num_beams > 1 or num_return_sequences > 1,
transformers/src/,736,"shape: (batch_size * num_return_sequences * num_beams, cur_len)",
transformers/src/,739,"shape: (batch_size * num_return_sequences * num_beams, cur_len)",
transformers/src/,743,create empty decoder_input_ids,
transformers/src/,751,expand batch_idx to assign correct encoder output for expanded input_ids (due to num_beams > 1 and num_return_sequences > 1),
transformers/src/,756,expand encoder_outputs,
transformers/src/,843,length of generated sentences / unfinished sentences,
transformers/src/,847,"defined for encoder-decoder models, None for decoder-only models",
transformers/src/,856,"if model has past, then set the past variable to speed up decoding",
transformers/src/,860,repetition penalty from CTRL paper (https://arxiv.org/abs/1909.05858),
transformers/src/,868,calculate a list of banned tokens to prevent repetitively generating the same ngrams,
transformers/src/,869,from fairseq: https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345,
transformers/src/,871,create banned_tokens boolean mask,
transformers/src/,883,calculate a list of banned tokens according to bad words,
transformers/src/,896,set eos token prob to zero if min_length is not reached,
transformers/src/,898,create eos_token_id boolean mask,
transformers/src/,909,Temperature (higher temperature => more likely to sample low probability tokens),
transformers/src/,912,Top-p/top-k filtering,
transformers/src/,914,Sample,
transformers/src/,919,Greedy decoding,
transformers/src/,922,update generations and finished sentences,
transformers/src/,924,pad finished sentences if eos_token_id exist,
transformers/src/,933,"if sentence is unfinished and the token to add is eos, sent_lengths is filled with current length",
transformers/src/,942,unfinished_sents is set to zero if eos in sentence,
transformers/src/,945,"stop when there is a </s> in each sentence, or if we exceed the maximul length",
transformers/src/,949,extend attention_mask for new generated input if only decoder,
transformers/src/,957,"if there are different sentences lengths in the batch, some batches have to be padded",
transformers/src/,962,finished sents are filled with pad_token,
transformers/src/,965,create length masks for tf.where operation,
transformers/src/,1009,generated hypotheses,
transformers/src/,1015,for greedy decoding it is made sure that only tokens of the first beam are considered to avoid sampling the exact same tokens three times,
transformers/src/,1025,cache compute states,
transformers/src/,1028,done sentences,
transformers/src/,1035,"(batch_size * num_beams, cur_len, vocab_size)",
transformers/src/,1036,"(batch_size * num_beams, vocab_size)",
transformers/src/,1038,"if model has past, then set the past variable to speed up decoding",
transformers/src/,1042,repetition penalty (from CTRL paper https://arxiv.org/abs/1909.05858),
transformers/src/,1049,Temperature (higher temperature => more likely to sample low probability tokens),
transformers/src/,1053,calculate log softmax score,
transformers/src/,1054,"(batch_size * num_beams, vocab_size)",
transformers/src/,1056,set eos token prob to zero if min_length is not reached,
transformers/src/,1058,create eos_token_id boolean mask,
transformers/src/,1069,calculate a list of banned tokens to prevent repetitively generating the same ngrams,
transformers/src/,1070,from fairseq: https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345,
transformers/src/,1075,create banned_tokens boolean mask,
transformers/src/,1087,calculate a list of banned tokens according to bad words,
transformers/src/,1105,"(batch_size * num_beams, vocab_size)",
transformers/src/,1107,Top-p/top-k filtering,
transformers/src/,1110,"(batch_size * num_beams, vocab_size)",
transformers/src/,1111,Sample 2 next tokens for each beam (so we have some spare tokens and match output of greedy beam search),
transformers/src/,1116,"(batch_size, 2 * num_beams)",
transformers/src/,1117,Compute next scores,
transformers/src/,1118,"(batch_size, 2 * num_beams)",
transformers/src/,1120,sort the sampled vector to make sure that the first num_beams samples are the best,
transformers/src/,1122,"(batch_size, num_beams * 2)",
transformers/src/,1123,"(batch_size, num_beams * 2)",
transformers/src/,1125,Add the log prob of the new beams to the log prob of the beginning of the sequence (sum of logs == log of the product),
transformers/src/,1128,"(batch_size * num_beams, vocab_size)",
transformers/src/,1130,re-organize to group the beam together (we are keeping top hypothesis accross beams),
transformers/src/,1133,"(batch_size, num_beams * vocab_size)",
transformers/src/,1139,next batch beam content,
transformers/src/,1142,for each sentence,
transformers/src/,1145,if we are done with this sentence,
transformers/src/,1153,pad the batch,
transformers/src/,1156,next sentence beam content,
transformers/src/,1159,next tokens for this sentence,
transformers/src/,1163,get beam and token IDs,
transformers/src/,1168,add to generated hypotheses if end of sentence or last iteration,
transformers/src/,1170,"if beam_token does not belong to top num_beams tokens, it should not be added",
transformers/src/,1178,add next predicted token if it is not eos_token,
transformers/src/,1181,the beam for next step is full,
transformers/src/,1185,Check if were done so that we can save a pad step if all(done),
transformers/src/,1190,update next beam content,
transformers/src/,1195,stop when we are done with each sentence,
transformers/src/,1199,sanity check / prepare next batch,
transformers/src/,1205,re-order batch,
transformers/src/,1208,re-order internal states,
transformers/src/,1212,extend attention_mask for new generated input if only decoder,
transformers/src/,1218,update current length,
transformers/src/,1221,finalize all open beam hypotheses and end to generated hypotheses,
transformers/src/,1223,Add all open beam hypothesis to generated_hyps,
transformers/src/,1226,test that beam scores match previously calculated scores if not eos and batch_idx not done,
transformers/src/,1236,need to add best num_beams hypotheses to generated hyps,
transformers/src/,1243,depending on whether greedy generation is wanted or not define different output_batch_size and output_num_return_sequences_per_batch,
transformers/src/,1247,select the best hypotheses,
transformers/src/,1251,retrieve best hypotheses,
transformers/src/,1264,shorter batches are filled with pad_token,
transformers/src/,1270,fill with hypothesis and eos_token_id if necessary,
transformers/src/,1273,if sent_length is max_len do not pad,
transformers/src/,1277,else pad to sent_max_len,
transformers/src/,1282,finish sentence with EOS token,
transformers/src/,1289,add to list,
transformers/src/,1294,none of the hypotheses have an eos_token,
transformers/src/,1304,get the correct batch idx from layer past batch dim,
transformers/src/,1305,batch dim of `past` and `mems` is at 2nd position,
transformers/src/,1308,check that shape matches,
transformers/src/,1316,create logit penalties for already seen input_ids,
transformers/src/,1322,if previous logit score is < 0 then multiply repetition penalty else divide,
transformers/src/,1330,"Copied from fairseq for no_repeat_ngram in beam_search""""""",
transformers/src/,1332,return no banned tokens if we haven't generated no_repeat_ngram_size tokens yet,
transformers/src/,1343,"Before decoding the next token, prevent decoding of ngrams that have already appeared",
transformers/src/,1357,if bad word tokens is just one token always ban it,
transformers/src/,1360,if bad word tokens are longer then prev input_ids they can't be equal,
transformers/src/,1364,if tokens match,
transformers/src/,1378,if tokens do not match continue,
transformers/src/,1401,Safety check,
transformers/src/,1402,Remove all tokens with a probability less than the last token of the top-k,
transformers/src/,1410,"expects logits to be of dim (batch_size, vocab_size)",
transformers/src/,1414,Remove tokens with cumulative probability above the threshold (token with 0 are kept),
transformers/src/,1418,Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below),
transformers/src/,1427,Shift the indices to the right to keep also the first token above the threshold,
transformers/src/,1432,scatter sorted tensors to original indexing,
transformers/src/,1440,broadcast batch dim to shape,
transformers/src/,1442,transform batch_indices to pair_indices,
transformers/src/,1444,scatter values to pair indices,
transformers/src/,1449,create value_tensor since tensor value assignment is not possible in TF,
transformers/src/,1459,ignoring bos_token,
transformers/src/,1613,We should use a standard multi-head attention module with absolute positional embedding for that.,
transformers/src/,1614,Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276,
transformers/src/,1615,We can probably just use the multi-head attention module of PyTorch >=1.1.0,
transformers/src/,1665,"e.g. [batch, num choices, seq length, hidden dims]",
transformers/src/,1669,"A tensor full of shape [batch] or [batch, num choices] full of sequence length",
transformers/src/,1673,else:,
transformers/src/,1674,"cls_index = cls_index[..., tf.newaxis]",
transformers/src/,1675,"cls_index = cls_index.expand((-1,) * (cls_index.dim()-1) + (hidden_states.size(-1),))",
transformers/src/,1676,"shape of cls_index: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states",
transformers/src/,1680,"shape of output: (batch, num choices, hidden_size)",
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,194,Fallback: use pattern matching on the string.,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The T5 authors and HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,30,Initialise PyTorch model,
transformers/src/,35,Load weights from tf checkpoint,
transformers/src/,38,Save pytorch-model,
transformers/src/,45,Required parameters,
transformers/src/,1,coding=utf-8,
transformers/src/,2,"Copyright 2018 The Google AI Language Team Authors, Facebook AI Research authors and The HuggingFace Inc. team.",
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,46,Older PyTorch compatibility,
transformers/src/,156,Save config in model,
transformers/src/,199,Overwrite for models with output embeddings,
transformers/src/,242,get the base model if needed,
transformers/src/,247,Update base model and current model config,
transformers/src/,251,Tie weights again if needed,
transformers/src/,283,Build new embeddings,
transformers/src/,287,initialize all new embeddings (in particular added tokens),
transformers/src/,290,Copy token embeddings from the previous weights,
transformers/src/,298,Initialize weights,
transformers/src/,301,Prune heads if needed,
transformers/src/,305,Tie weights if needed,
transformers/src/,316,save new sets of pruned heads as union of previously stored pruned heads and newly pruned heads,
transformers/src/,319,Unfortunately we have to store it as list for JSON,
transformers/src/,334,Only save the model itself if we are using distributed training,
transformers/src/,337,Attach architecture to the config,
transformers/src/,340,"If we save using the predefined names, we can load using `from_pretrained`",
transformers/src/,347,Save configuration file,
transformers/src/,349,xm.save takes care of saving only from master,
transformers/src/,438,Load config if we don't provide a configuration,
transformers/src/,455,Load model,
transformers/src/,461,Load from a TF 1.0 checkpoint,
transformers/src/,464,Load from a TF 2.0 checkpoint,
transformers/src/,467,Load from a PyTorch checkpoint,
transformers/src/,490,"redirect to the cache, if necessary",
transformers/src/,523,Instantiate model.,
transformers/src/,541,Load from a TensorFlow 1.X checkpoint - provided by original authors,
transformers/src/,542,Remove the '.index',
transformers/src/,544,Load from our TensorFlow 2.0 checkpoints,
transformers/src/,556,Convert old format to new format if needed from a PyTorch state_dict,
transformers/src/,571,copy state_dict so _load_from_state_dict can modify it,
transformers/src/,577,PyTorch's `_load_from_state_dict` does not copy parameters in a module's descendants,
transformers/src/,578,so we need to apply the function recursively.,
transformers/src/,588,Make sure we are able to load base models as well as derived models (with heads),
transformers/src/,628,make sure token embedding weights are still tied if needed,
transformers/src/,630,Set model in evaluation mode to desactivate DropOut modules by default,
transformers/src/,667,if score < 0 then repetition penalty has to multiplied to reduce the previous token probability,
transformers/src/,816,We cannot generate if the model does not have a LM head,
transformers/src/,849,overriden by the input batch_size,
transformers/src/,894,not allow to duplicate outputs when greedy decoding,
transformers/src/,897,no_beam_search greedy generation conditions,
transformers/src/,903,beam_search greedy generation conditions,
transformers/src/,908,create attention mask if necessary,
transformers/src/,909,TODO (PVP): this should later be handled by the forward fn() in each model in the future see PR 3140,
transformers/src/,915,set pad_token_id to eos_token_id if not set. Important that this is done after,
transformers/src/,916,attention_mask is created,
transformers/src/,923,current position and vocab size,
transformers/src/,926,set effective batch size and effective batch multiplier according to do_sample,
transformers/src/,944,get encoder and store encoder outputs,
transformers/src/,949,Expand input ids if num_beams > 1 or num_return_sequences > 1,
transformers/src/,959,"shape: (batch_size * num_return_sequences * num_beams, cur_len)",
transformers/src/,962,"shape: (batch_size * num_return_sequences * num_beams, cur_len)",
transformers/src/,965,create empty decoder_input_ids,
transformers/src/,978,expand batch_idx to assign correct encoder output for expanded input_ids (due to num_beams > 1 and num_return_sequences > 1),
transformers/src/,986,expand encoder_outputs,
transformers/src/,1070,length of generated sentences / unfinished sentences,
transformers/src/,1074,"defined for encoder-decoder models, None for decoder-only models",
transformers/src/,1084,"if model has past, then set the past variable to speed up decoding",
transformers/src/,1088,repetition penalty from CTRL paper (https://arxiv.org/abs/1909.05858),
transformers/src/,1093,calculate a list of banned tokens to prevent repetitively generating the same ngrams,
transformers/src/,1094,from fairseq: https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345,
transformers/src/,1100,calculate a list of banned tokens according to bad words,
transformers/src/,1106,set eos token prob to zero if min_length is not reached,
transformers/src/,1111,Temperature (higher temperature => more likely to sample low probability tokens),
transformers/src/,1114,Top-p/top-k filtering,
transformers/src/,1116,Sample,
transformers/src/,1120,Greedy decoding,
transformers/src/,1123,update generations and finished sentences,
transformers/src/,1125,pad finished sentences if eos_token_id exist,
transformers/src/,1134,"if sentence is unfinished and the token to add is eos, sent_lengths is filled with current length",
transformers/src/,1137,unfinished_sents is set to zero if eos in sentence,
transformers/src/,1140,"stop when there is a </s> in each sentence, or if we exceed the maximul length",
transformers/src/,1144,extend attention_mask for new generated input if only decoder,
transformers/src/,1152,"if there are different sentences lengths in the batch, some batches have to be padded",
transformers/src/,1155,finished sents are filled with pad_token,
transformers/src/,1195,generated hypotheses,
transformers/src/,1201,scores for each sentence in the beam,
transformers/src/,1204,for greedy decoding it is made sure that only tokens of the first beam are considered to avoid sampling the exact same tokens three times,
transformers/src/,1207,"shape (batch_size * num_beams,)",
transformers/src/,1209,cache compute states,
transformers/src/,1210,"defined for encoder-decoder models, None for decoder-only models",
transformers/src/,1212,done sentences,
transformers/src/,1219,"(batch_size * num_beams, cur_len, vocab_size)",
transformers/src/,1220,"(batch_size * num_beams, vocab_size)",
transformers/src/,1222,"if model has past, then set the past variable to speed up decoding",
transformers/src/,1226,repetition penalty (from CTRL paper https://arxiv.org/abs/1909.05858),
transformers/src/,1235,"(batch_size * num_beams, vocab_size)",
transformers/src/,1237,TODO (PVP) still a bit hacky here - there might be a better solutino,
transformers/src/,1240,set eos token prob to zero if min_length is not reached,
transformers/src/,1245,calculate a list of banned tokens to prevent repetitively generating the same ngrams,
transformers/src/,1247,from fairseq: https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345,
transformers/src/,1255,calculate a list of banned tokens according to bad words,
transformers/src/,1266,"(batch_size * num_beams, vocab_size)",
transformers/src/,1267,Top-p/top-k filtering,
transformers/src/,1270,"(batch_size * num_beams, vocab_size)",
transformers/src/,1271,re-organize to group the beam together to sample from all beam_idxs,
transformers/src/,1274,"(batch_size, num_beams * vocab_size)",
transformers/src/,1276,Sample 2 next tokens for each beam (so we have some spare tokens and match output of greedy beam search),
transformers/src/,1278,"(batch_size, num_beams * 2)",
transformers/src/,1279,Compute next scores,
transformers/src/,1280,"(batch_size, num_beams * 2)",
transformers/src/,1281,sort the sampled vector to make sure that the first num_beams samples are the best,
transformers/src/,1283,"(batch_size, num_beams * 2)",
transformers/src/,1286,"(batch_size * num_beams, vocab_size)",
transformers/src/,1288,re-organize to group the beam together (we are keeping top hypothesis accross beams),
transformers/src/,1291,"(batch_size, num_beams * vocab_size)",
transformers/src/,1297,next batch beam content,
transformers/src/,1300,for each sentence,
transformers/src/,1303,if we are done with this sentence,
transformers/src/,1311,pad the batch,
transformers/src/,1314,next sentence beam content,
transformers/src/,1317,next tokens for this sentence,
transformers/src/,1321,get beam and token IDs,
transformers/src/,1326,add to generated hypotheses if end of sentence or last iteration,
transformers/src/,1328,"if beam_token does not belong to top num_beams tokens, it should not be added",
transformers/src/,1336,add next predicted token if it is not eos_token,
transformers/src/,1339,the beam for next step is full,
transformers/src/,1343,Check if were done so that we can save a pad step if all(done),
transformers/src/,1348,update next beam content,
transformers/src/,1353,stop when we are done with each sentence,
transformers/src/,1357,sanity check / prepare next batch,
transformers/src/,1363,re-order batch,
transformers/src/,1366,re-order internal states,
transformers/src/,1370,extend attention_mask for new generated input if only decoder,
transformers/src/,1376,update current length,
transformers/src/,1379,finalize all open beam hypotheses and end to generated hypotheses,
transformers/src/,1384,test that beam scores match previously calculated scores if not eos and batch_idx not done,
transformers/src/,1394,need to add best num_beams hypotheses to generated hyps,
transformers/src/,1401,depending on whether greedy generation is wanted or not define different output_batch_size and output_num_return_sequences_per_batch,
transformers/src/,1405,select the best hypotheses,
transformers/src/,1409,retrieve best hypotheses,
transformers/src/,1418,shorter batches are filled with pad_token,
transformers/src/,1424,fill with hypothesis and eos_token_id if necessary,
transformers/src/,1430,none of the hypotheses have an eos_token,
transformers/src/,1436,force one of token_ids to be generated by setting prob of all other tokens to 0.,
transformers/src/,1454,"Copied from fairseq for no_repeat_ngram in beam_search""""""",
transformers/src/,1456,return no banned tokens if we haven't generated no_repeat_ngram_size tokens yet,
transformers/src/,1467,"Before decoding the next token, prevent decoding of ngrams that have already appeared",
transformers/src/,1481,if bad word tokens is just one token always ban it,
transformers/src/,1484,if bad word tokens are longer then prev input_ids they can't be equal,
transformers/src/,1488,if tokens match,
transformers/src/,1502,if tokens do not match continue,
transformers/src/,1523,Safety check,
transformers/src/,1524,Remove all tokens with a probability less than the last token of the top-k,
transformers/src/,1532,Remove tokens with cumulative probability above the threshold (token with 0 are kept),
transformers/src/,1535,Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below),
transformers/src/,1537,Shift the indices to the right to keep also the first token above the threshold,
transformers/src/,1541,scatter sorted tensors to original indexing,
transformers/src/,1552,ignoring bos_token,
transformers/src/,1669,"shape (bsz, 1, hsz)",
transformers/src/,1670,"shape (bsz, 1, hsz)",
transformers/src/,1671,"shape (bsz, slen, hsz)",
transformers/src/,1718,"shape (bsz, 1, hsz)",
transformers/src/,1719,"shape (bsz, hsz)",
transformers/src/,1722,"shape (bsz, 1, hsz)",
transformers/src/,1723,"shape (bsz, hsz)",
transformers/src/,1725,"shape (bsz, hsz)",
transformers/src/,1792,"If we are on multi-GPU, let's remove the dimension added by batch splitting",
transformers/src/,1797,"during training, compute the end logits based on the ground truth of the start position",
transformers/src/,1806,Predict answerability from the representation of CLS and START,
transformers/src/,1811,note(zhiliny): by default multiply the loss by 0.5 so that the scale is comparable to start_loss and end_loss,
transformers/src/,1817,"during inference, compute the end logits based on beam search",
transformers/src/,1819,"shape (bsz, slen)",
transformers/src/,1823,"shape (bsz, start_n_top)",
transformers/src/,1824,"shape (bsz, start_n_top, hsz)",
transformers/src/,1825,"shape (bsz, start_n_top, hsz)",
transformers/src/,1826,"shape (bsz, slen, start_n_top, hsz)",
transformers/src/,1830,"shape (bsz, slen, start_n_top, hsz)",
transformers/src/,1833,"shape (bsz, slen, start_n_top)",
transformers/src/,1837,"shape (bsz, end_n_top, start_n_top)",
transformers/src/,1846,"return start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits",
transformers/src/,1847,"or (if labels are provided) (total_loss,)",
transformers/src/,1872,We should use a standard multi-head attention module with absolute positional embedding for that.,
transformers/src/,1873,Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276,
transformers/src/,1874,We can probably just use the multi-head attention module of PyTorch >=1.1.0,
transformers/src/,1888,type: typing.Callable,
transformers/src/,1917,"shape of cls_index: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states",
transformers/src/,1918,"shape (bsz, XX, hidden_size)",
transformers/src/,1938,The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.,
transformers/src/,1,coding=utf-8,
transformers/src/,2,"Copyright 2019-present, the HuggingFace Inc. team, The Google AI Language Team and Facebook, Inc.",
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,50,UTILS AND BUILDING BLOCKS OF THE ARCHITECTURE,
transformers/src/,87,(max_seq_length),
transformers/src/,88,"(bs, max_seq_length)",
transformers/src/,90,"(bs, max_seq_length, dim)",
transformers/src/,91,"(bs, max_seq_length, dim)",
transformers/src/,93,"(bs, max_seq_length, dim)",
transformers/src/,94,"(bs, max_seq_length, dim)",
transformers/src/,95,"(bs, max_seq_length, dim)",
transformers/src/,128,Prune linear layers,
transformers/src/,133,Update hyper params,
transformers/src/,156,"assert dim == self.dim, 'Dimensions do not match: %s input vs %s configured' % (dim, self.dim)",
transformers/src/,157,assert key.size() == value.size(),
transformers/src/,171,"(bs, n_heads, q_length, dim_per_head)",
transformers/src/,172,"(bs, n_heads, k_length, dim_per_head)",
transformers/src/,173,"(bs, n_heads, k_length, dim_per_head)",
transformers/src/,175,"(bs, n_heads, q_length, dim_per_head)",
transformers/src/,176,"(bs, n_heads, q_length, k_length)",
transformers/src/,177,"(bs, n_heads, q_length, k_length)",
transformers/src/,178,"(bs, n_heads, q_length, k_length)",
transformers/src/,180,"(bs, n_heads, q_length, k_length)",
transformers/src/,181,"(bs, n_heads, q_length, k_length)",
transformers/src/,183,Mask heads if we want to,
transformers/src/,187,"(bs, n_heads, q_length, dim_per_head)",
transformers/src/,188,"(bs, q_length, dim)",
transformers/src/,189,"(bs, q_length, dim)",
transformers/src/,244,Self-Attention,
transformers/src/,247,"(bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)",
transformers/src/,248,To handle these `output_attention` or `output_hidden_states` cases returning tuples,
transformers/src/,251,"(bs, seq_length, dim)",
transformers/src/,253,Feed Forward Network,
transformers/src/,254,"(bs, seq_length, dim)",
transformers/src/,255,"(bs, seq_length, dim)",
transformers/src/,311,Add last layer,
transformers/src/,320,"last-layer hidden state, (all hidden states), (all attentions)",
transformers/src/,323,INTERFACE FOR ENCODER AND TASK SPECIFIC MODEL,
transformers/src/,396,Embeddings,
transformers/src/,397,Encoder,
transformers/src/,460,"(bs, seq_length)",
transformers/src/,462,Prepare head mask if needed,
transformers/src/,463,1.0 in head_mask indicate we keep the head,
transformers/src/,464,attention_probs has shape bsz x n_heads x N x N,
transformers/src/,465,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads],
transformers/src/,466,and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length],
transformers/src/,474,We can specify head_mask for each layer,
transformers/src/,477,switch to fload if need + fp16 compatibility,
transformers/src/,482,"(bs, seq_length, dim)",
transformers/src/,487,"last-layer hidden-state, (all hidden_states), (all attentions)",
transformers/src/,553,"(bs, seq_length, dim)",
transformers/src/,554,"(bs, seq_length, dim)",
transformers/src/,555,"(bs, seq_length, dim)",
transformers/src/,556,"(bs, seq_length, dim)",
transformers/src/,557,"(bs, seq_length, vocab_size)",
transformers/src/,566,"(mlm_loss), prediction_logits, (all hidden_states), (all attentions)",
transformers/src/,629,"(bs, seq_len, dim)",
transformers/src/,630,"(bs, dim)",
transformers/src/,631,"(bs, dim)",
transformers/src/,632,"(bs, dim)",
transformers/src/,633,"(bs, dim)",
transformers/src/,634,"(bs, dim)",
transformers/src/,646,"(loss), logits, (hidden_states), (attentions)",
transformers/src/,722,"(bs, max_query_len, dim)",
transformers/src/,724,"(bs, max_query_len, dim)",
transformers/src/,725,"(bs, max_query_len, 2)",
transformers/src/,727,"(bs, max_query_len)",
transformers/src/,728,"(bs, max_query_len)",
transformers/src/,732,"If we are on multi-GPU, split add a dimension",
transformers/src/,737,"sometimes the start/end positions are outside our model inputs, we ignore these terms",
transformers/src/,748,"(loss), start_logits, end_logits, (hidden_states), (attentions)",
transformers/src/,815,add hidden states and attention if they are here,
transformers/src/,818,Only keep active parts of the loss,
transformers/src/,830,"(loss), scores, (hidden_states), (attentions)",
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,74,Both framework are available but the user supplied a model class instance.,
transformers/src/,75,Try to guess which framework to use from the model classname,
transformers/src/,84,framework = 'tf' if is_tf_available() else 'pt',
transformers/src/,247,Split for multi-columns,
transformers/src/,252,Dictionary to map arguments,
transformers/src/,257,No dictionary to map arguments,
transformers/src/,362,Special handling,
transformers/src/,366,Update config with task specific parameters,
transformers/src/,438,PR #1548 (CLI) There is an issue with attention_mask,
transformers/src/,439,if 'xlnet' in model_type or 'xlm' in model_type:,
transformers/src/,440,"args += ['cls_index', 'p_mask']",
transformers/src/,451,Parse arguments,
transformers/src/,461,Filter out features not available on specific models,
transformers/src/,462,inputs = self.inputs_for_model(inputs),
transformers/src/,479,Encode for forward,
transformers/src/,482,TODO trace model,
transformers/src/,687,Filter padding out:,
transformers/src/,691,Append,
transformers/src/,769,Manage correct placement of the tensors,
transformers/src/,779,Forward,
transformers/src/,803,Append,
transformers/src/,823,"Position args, handling is sensibly the same as X and data, so forwarding to avoid duplicating",
transformers/src/,830,Generic compatibility with sklearn and Keras,
transformers/src/,831,Batched data,
transformers/src/,838,Copy to avoid overriding arguments,
transformers/src/,855,Tabular input,
transformers/src/,970,Set defaults values,
transformers/src/,983,Convert inputs to features,
transformers/src/,1000,Manage tensor allocation on correct device,
transformers/src/,1008,Retrieve the score for the context tokens only (removing question tokens),
transformers/src/,1015,Normalize logits and spans to retrieve the answer,
transformers/src/,1019,Mask padding and question,
transformers/src/,1025,TODO : What happens if not possible,
transformers/src/,1026,Mask CLS,
transformers/src/,1032,Convert the answer (tokens) back to the original text,
transformers/src/,1065,Ensure we have batch axis,
transformers/src/,1072,"Compute the score of each tuple(start, end) to be the real answer",
transformers/src/,1075,Remove candidate with end < start and end - start > max_answer_len,
transformers/src/,1078,Inspired by Chen & al. (https://github.com/facebookresearch/DrQA),
transformers/src/,1110,Append words if they are in the span,
transformers/src/,1120,Stop if we went over the end of the answer,
transformers/src/,1124,Append the subtokenization length to the running index,
transformers/src/,1128,Join text with spaces,
transformers/src/,1385,Register all the supported task here,
transformers/src/,1560,Retrieve the task,
transformers/src/,1569,Use default model/config/tokenizer for the task if no model is provided,
transformers/src/,1574,Try to infer tokenizer from model or config name (if provided as str),
transformers/src/,1581,Impossible to guest what is the right tokenizer here,
transformers/src/,1588,Try to infer modelcard from model or config name (if provided as str),
transformers/src/,1594,Instantiate tokenizer if needed,
transformers/src/,1597,"For tuple we have (tokenizer name, {kwargs})",
transformers/src/,1602,Instantiate config if needed,
transformers/src/,1606,Instantiate modelcard if needed,
transformers/src/,1610,Instantiate model if needed,
transformers/src/,1612,Handle transparent TF/PT model conversion,
transformers/src/,43,"field.metadata is not used at all by Data Classes,",
transformers/src/,44,it is provided as a third-party extension mechanism.,
transformers/src/,108,additional namespace.,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,140,State initialization,
transformers/src/,143,Exponential moving average of gradient values,
transformers/src/,145,Exponential moving average of squared gradient values,
transformers/src/,153,Decay the first and second moment running average coefficient,
transformers/src/,154,In-place operations to update the averages at the same time,
transformers/src/,160,No bias correction for Bert,
transformers/src/,167,Just adding the square of the weights to the loss function is *not*,
transformers/src/,168,"the correct way of using L2 regularization/weight decay with Adam,",
transformers/src/,169,since that will interact with the m and v parameters in strange ways.,
transformers/src/,170,,
transformers/src/,171,Instead we want to decay the weights in a manner that doesn't interact,
transformers/src/,172,with the m/v parameters. This is equivalent to adding the square,
transformers/src/,173,of the weights to the loss with plain (non-momentum) SGD.,
transformers/src/,174,Add weight decay at the end (fixed version),
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The Open AI Team Authors and The HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,152,no default special tokens - you can update this value if you add special tokens,
transformers/src/,155,no default special tokens - you can update this value if you add special tokens,
transformers/src/,160,how to handle errors in decoding,
transformers/src/,169,Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions,
transformers/src/,227,"Maps all our bytes to unicode strings, avoiding controle tokens of the BPE (spaces in our case)",
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,30,Construct model,
transformers/src/,37,Load weights from numpy,
transformers/src/,40,Save pytorch-model,
transformers/src/,52,Required parameters,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,1,coding=utf-8,
transformers/src/,2,"Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.",
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License,
transformers/src/,131,take into account special tokens,
transformers/src/,132,take into account special tokens,
transformers/src/,147,"Original fairseq vocab and spm vocab must be ""aligned"":",
transformers/src/,148,Vocab    |    0    |    1    |   2    |    3    |  4  |  5  |  6  |   7   |   8   |  9,
transformers/src/,149,-------- | ------- | ------- | ------ | ------- | --- | --- | --- | ----- | ----- | ----,
transformers/src/,150,"fairseq  | '<s>'   | '<pad>' | '</s>' | '<unk>' | ',' | '.' | '▁' | 's'   | '▁de' | '-'",
transformers/src/,151,"spm      | '<unk>' | '<s>'   | '</s>' | ','     | '.' | '▁' | 's' | '▁de' | '-'   | '▁a'",
transformers/src/,153,Mimic fairseq token-to-id alignment for the first 4 token,
transformers/src/,156,"The first ""real"" token "","" has position 4 in the original fairseq vocab and position 3 in the spm vocab",
transformers/src/,265,Add the <mask> token,
transformers/src/,281,Need to return unknown token if the SP model returned 0,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,30,Initialise PyTorch model,
transformers/src/,41,Load weights from tf checkpoint,
transformers/src/,46,Save pytorch-model,
transformers/src/,53,Required parameters,
transformers/src/,1,coding=utf-8,
transformers/src/,2,"Copyright 2010, The T5 Authors and HuggingFace Inc.",
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The OpenAI Team Authors and HuggingFace Inc. team.,
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,55,"type: Dict[str, str]",
transformers/src/,56,type: str,
transformers/src/,59,Attributes with defaults,
transformers/src/,62,Not used by all models,
transformers/src/,63,Only used by PyTorch models,
transformers/src/,67,Is decoder is used in encoder-decoder models to differentiate encoder from decoder,
transformers/src/,71,Parameters for sequence generation,
transformers/src/,86,Fine-tuning task arguments,
transformers/src/,95,Tokenizer arguments TODO: eventually tokenizer and models should share the same config,
transformers/src/,102,task specific arguments,
transformers/src/,105,TPU arguments,
transformers/src/,108,Additional attributes without default values,
transformers/src/,141,"If we save using the predefined names, we can load using `from_pretrained`",
transformers/src/,243,Load from URL or cache if already cached,
transformers/src/,252,Load config dict,
transformers/src/,313,Update config with kwargs if needed,
transformers/src/,1,coding=utf-8,
transformers/src/,2,"Copyright 2019-present, Facebook, Inc and the HuggingFace Inc. team.",
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,133,"removed: src_enc=None, src_len=None",
transformers/src/,173,mask = input_ids != self.pad_index,
transformers/src/,175,check inputs,
transformers/src/,176,assert shape_list(lengths)[0] == bs,
transformers/src/,178,assert lengths.max().item() <= slen,
transformers/src/,179,"input_ids = input_ids.transpose(0, 1)  # batch size as dimension 0",
transformers/src/,180,assert (src_enc is None) == (src_len is None),
transformers/src/,181,if src_enc is not None:,
transformers/src/,182,assert self.is_decoder,
transformers/src/,183,assert src_enc.size(0) == bs,
transformers/src/,185,generate masks,
transformers/src/,187,if self.is_decoder and src_enc is not None:,
transformers/src/,188,"src_mask = torch.arange(src_len.max(), dtype=torch.long, device=lengths.device) < src_len[:, None]",
transformers/src/,190,position_ids,
transformers/src/,194,"assert shape_list(position_ids) == [bs, slen]  # (slen, bs)",
transformers/src/,196,"position_ids = position_ids.transpose(0, 1)",
transformers/src/,198,langs,
transformers/src/,200,"assert shape_list(langs) == [bs, slen]  # (slen, bs)",
transformers/src/,202,"langs = langs.transpose(0, 1)",
transformers/src/,204,Prepare head mask if needed,
transformers/src/,205,1.0 in head_mask indicate we keep the head,
transformers/src/,206,attention_probs has shape bsz x n_heads x N x N,
transformers/src/,207,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads],
transformers/src/,208,and head_mask is converted to shape [num_hidden_layers x batch x num_heads x qlen x klen],
transformers/src/,214,do not recompute cached elements,
transformers/src/,224,embeddings,
transformers/src/,237,transformer layers,
transformers/src/,241,LayerDrop,
transformers/src/,249,self attention,
transformers/src/,269,encoder attention (for decoder only),
transformers/src/,270,if self.is_decoder and src_enc is not None:,
transformers/src/,271,"attn = self.encoder_attn[i](tensor, src_mask, kv=src_enc, cache=cache)",
transformers/src/,272,"attn = F.dropout(attn, p=self.dropout, training=self.training)",
transformers/src/,273,tensor = tensor + attn,
transformers/src/,274,tensor = self.layer_norm15[i](tensor),
transformers/src/,276,FFN,
transformers/src/,286,Add last hidden state,
transformers/src/,290,update cache length,
transformers/src/,294,move back sequence length to dimension 0,
transformers/src/,295,"tensor = tensor.transpose(0, 1)",
transformers/src/,302,"outputs, (hidden_states), (attentions)",
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,1,flake8: noqa,
transformers/src/,2,"There's no way to ignore ""F401 '...' imported but unused"" warnings in this",
transformers/src/,3,"module, but to preserve other warnings. So, don't check this module at all.",
transformers/src/,7,Work around to update TensorFlow's absl.logging threshold which alters the,
transformers/src/,8,default Python logging output behavior when present.,
transformers/src/,9,see: https://github.com/abseil/abseil-py/issues/99,
transformers/src/,10,and: https://github.com/tensorflow/tensorflow/issues/26691#issuecomment-500369493,
transformers/src/,22,Benchmarking,
transformers/src/,50,Configurations,
transformers/src/,75,Files and general utilities,
transformers/src/,93,Model Cards,
transformers/src/,96,TF 2.0 <=> PyTorch conversion utilities,
transformers/src/,107,Pipelines,
transformers/src/,140,Tokenizers,
transformers/src/,148,pylint: disable=invalid-name,
transformers/src/,155,Modeling,
transformers/src/,313,Optimization,
transformers/src/,324,TensorFlow,
transformers/src/,488,Optimization,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 Salesforce and HuggingFace Inc. team.,
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 T5 Authors and The HuggingFace Inc. team.,
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,41,,
transformers/src/,42,TF 2.0 Models are constructed using Keras imperative API by sub-classing,
transformers/src/,43,- tf.keras.layers.Layer for the layers and,
transformers/src/,44,- TFPreTrainedModel for the models (it-self a sub-class of tf.keras.Model),
transformers/src/,45,,
transformers/src/,113,Mesh TensorFlow initialization to avoid scaling before softmax,
transformers/src/,162,"now n is in the range [0, inf)",
transformers/src/,179,"shape (qlen, klen)",
transformers/src/,183,"shape (qlen, klen, num_heads)",
transformers/src/,184,"shape (1, num_heads, qlen, klen)",
transformers/src/,193,"Input is (bs, qlen, dim)",
transformers/src/,194,"Mask is (bs, klen) (non-causal) or (bs, klen, klen)",
transformers/src/,209,"(bs, n_heads, qlen, dim_per_head)",
transformers/src/,211,"(bs, n_heads, qlen, dim_per_head)",
transformers/src/,212,"(bs, n_heads, qlen, dim_per_head)",
transformers/src/,215,"(bs, n_heads, qlen, dim_per_head)",
transformers/src/,216,"(bs, n_heads, qlen, dim_per_head)",
transformers/src/,222,"(bs, n_heads, klen, dim_per_head)",
transformers/src/,223,"(bs, n_heads, klen, dim_per_head)",
transformers/src/,228,q = q / math.sqrt(dim_per_head)                                     # No scaling in T5,
transformers/src/,229,"scores = tf.matmul(q, k, transpose_b=True)                            # (bs, n_heads, qlen, klen)",
transformers/src/,230,"(bs, n_heads, qlen, klen)",
transformers/src/,238,"mask = (mask == 0).expand_as(scores)                              # (bs, n_heads, qlen, klen)",
transformers/src/,239,"scores.masked_fill_(mask, -float('inf'))                          # (bs, n_heads, qlen, klen)",
transformers/src/,242,"(bs, n_heads, qlen, klen)",
transformers/src/,243,"(bs, n_heads, qlen, klen)",
transformers/src/,245,Mask heads if we want to,
transformers/src/,249,"(bs, n_heads, qlen, dim_per_head)",
transformers/src/,250,"(bs, qlen, dim)",
transformers/src/,280,add attentions if we output them,
transformers/src/,302,add attentions if we output them,
transformers/src/,360,add attentions if we output them,
transformers/src/,361,"hidden-states, (self-attention weights), (self-attention position bias), (cross-attention weights), (cross-attention position bias)",
transformers/src/,379,"if an abs scope name is given to the embedding variable, call variable from absolute scope",
transformers/src/,388,"if an abs scope name is given to the embedding variable, call variable from absolute scope",
transformers/src/,394,,
transformers/src/,395,The full model without a specific pretrained or finetuning head is,
transformers/src/,396,"provided as a tf.keras.layers.Layer usually called ""TFT5MainLayer""",
transformers/src/,397,,
transformers/src/,427,Not implemented yet in the library fr TF 2.0 models,
transformers/src/,430,Not implemented yet in the library fr TF 2.0 models,
transformers/src/,465,"We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]",
transformers/src/,466,ourselves in which case we just need to make it broadcastable to all heads.,
transformers/src/,472,"Provided a padding mask of dimensions [batch_size, seq_length]",
transformers/src/,473,"- if the model is a decoder, apply a causal mask in addition to the padding mask",
transformers/src/,474,"- if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]",
transformers/src/,485,Since attention_mask is 1.0 for positions we want to attend and 0.0 for,
transformers/src/,486,"masked positions, this operation will create a tensor which is 0.0 for",
transformers/src/,487,positions we want to attend and -10000.0 for masked positions.,
transformers/src/,488,"Since we are adding it to the raw scores before the softmax, this is",
transformers/src/,489,effectively the same as removing these entirely.,
transformers/src/,491,"T5 has a mask that can compare sequence ids, we can simulate this here with this transposistion",
transformers/src/,492,Cf. https://github.com/tensorflow/mesh/blob/8d2465e9bc93129b913b5ccc6a59aa97abd96ec6/mesh_tensorflow/transformer/transformer_layers.py#L270,
transformers/src/,493,"extended_attention_mask = tf.math.equal(extended_attention_mask,",
transformers/src/,494,"tf.transpose(extended_attention_mask, perm=(-1, -2)))",
transformers/src/,499,If a 2D ou 3D attention mask is provided for the cross-attention,
transformers/src/,500,"we need to make broadcastabe to [batch_size, num_heads, seq_length, seq_length]",
transformers/src/,508,"T5 has a mask that can compare sequence ids, we can simulate this here with this transposistion",
transformers/src/,509,Cf. https://github.com/tensorflow/mesh/blob/8d2465e9bc93129b913b5ccc6a59aa97abd96ec6/mesh_tensorflow/transformer/transformer_layers.py#L270,
transformers/src/,510,"encoder_extended_attention_mask = tf.math.equal(encoder_extended_attention_mask,",
transformers/src/,511,"tf.transpose(encoder_extended_attention_mask, perm=(-1, -2)))",
transformers/src/,517,Prepare head mask if needed,
transformers/src/,518,1.0 in head_mask indicate we keep the head,
transformers/src/,519,attention_probs has shape bsz x n_heads x N x N,
transformers/src/,520,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads],
transformers/src/,521,and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length],
transformers/src/,526,head_mask = tf.constant([0] * self.num_hidden_layers),
transformers/src/,550,We share the position biases between the layers - the first layer store them,
transformers/src/,551,"layer_outputs = hidden-states, (self-attention weights), (self-attention position bias), (cross-attention weights), (cross-attention position bias)",
transformers/src/,562,Add last layer,
transformers/src/,571,"last-layer hidden state, (all hidden states), (all attentions)",
transformers/src/,574,,
transformers/src/,575,TFT5PreTrainedModel is a sub-class of tf.keras.Model,
transformers/src/,576,which take care of loading and saving pretrained weights,
transformers/src/,577,and various common utilities.,
transformers/src/,578,Here you just need to specify a few (self-explanatory),
transformers/src/,579,pointers for your model.,
transformers/src/,580,,
transformers/src/,689,retrieve correct absolute scope for embed token wrapper,
transformers/src/,744,retrieve arguments,
transformers/src/,754,"Encode if needed (training, first prediction pass)",
transformers/src/,762,Decode,
transformers/src/,783,retrieve correct absolute scope for embed token wrapper,
transformers/src/,847,retrieve arguments,
transformers/src/,857,"Encode if needed (training, first prediction pass)",
transformers/src/,859,Convert encoder inputs in embeddings if needed,
transformers/src/,866,Decode,
transformers/src/,886,first step,
transformers/src/,893,"inputs don't have to be defined, but still need to be passed to make Keras.layer.__call__ happy",
transformers/src/,894,input_ids are the decoder_input_ids,
transformers/src/,900,past does not have to be re-ordered for T5.,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 Salesforce and HuggingFace Inc. team.,
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,42,create the sinusoidal pattern for the positional encoding,
transformers/src/,57,calculate attention,
transformers/src/,68,Apply the attention mask,
transformers/src/,73,Mask heads if we want to,
transformers/src/,188,Slightly different from the TF version which uses truncated_normal for initialization,
transformers/src/,189,cf https://github.com/pytorch/pytorch/pull/5617,
transformers/src/,347,"If using past key value states, only the last tokens",
transformers/src/,348,should be given as an input,
transformers/src/,379,Attention mask.,
transformers/src/,383,We create a 3D attention mask from a 2D tensor mask.,
transformers/src/,384,"Sizes are [batch_size, 1, 1, to_seq_length]",
transformers/src/,385,"So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]",
transformers/src/,386,this attention mask is more simple than the triangular masking of causal attention,
transformers/src/,387,"used in OpenAI GPT, we just need to prepare the broadcast dimension here.",
transformers/src/,390,Since attention_mask is 1.0 for positions we want to attend and 0.0 for,
transformers/src/,391,"masked positions, this operation will create a tensor which is 0.0 for",
transformers/src/,392,positions we want to attend and -10000.0 for masked positions.,
transformers/src/,393,"Since we are adding it to the raw scores before the softmax, this is",
transformers/src/,394,effectively the same as removing these entirely.,
transformers/src/,395,fp16 compatibility,
transformers/src/,398,Prepare head mask if needed,
transformers/src/,399,1.0 in head_mask indicate we keep the head,
transformers/src/,400,attention_probs has shape bsz x n_heads x N x N,
transformers/src/,401,head_mask has shape n_layer x batch x n_heads x N x N,
transformers/src/,409,We can specify head_mask for each layer,
transformers/src/,412,switch to fload if need + fp16 compatibility,
transformers/src/,426,inputs_embeds = embedded.unsqueeze(0) if len(input_ids.shape)<2 else embedded,
transformers/src/,471,let the number of heads free (-1) so we can extract attention even after head pruning,
transformers/src/,495,only last token for inputs_ids if past is defined in kwargs,
transformers/src/,574,Shift so that tokens < n predict n,
transformers/src/,577,Flatten the tokens,
transformers/src/,582,"(loss), lm_logits, presents, (all hidden_states), (attentions)",
transformers/src/,23,pylint: disable=invalid-name,
transformers/src/,187,Filter events,
transformers/src/,194,Filter modules,
transformers/src/,199,Filter whitelist of modules to trace,
transformers/src/,206,Filter blacklist of modules not to trace,
transformers/src/,213,"Record current tracing state (file, location in file...)",
transformers/src/,221,Record current memory state (rss memory) and compute difference with previous memory state,
transformers/src/,229,Clear GPU caches,
transformers/src/,233,See https://github.com/tensorflow/tensorflow/issues/20218#issuecomment-416771802,
transformers/src/,235,Sum used memory for all GPUs,
transformers/src/,316,order by the total CPU + GPU memory increase,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The OpenAI Team Authors and HuggingFace Inc. team.,
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,61,This was used when we had a single embedding matrix for positions and tokens,
transformers/src/,62,"init_params[0] = np.concatenate([init_params[1], init_params[0]], 0)",
transformers/src/,63,del init_params[1],
transformers/src/,77,Pop position and token embedding arrays,
transformers/src/,81,"names[1:n_transfer], init_params[1:n_transfer]):",
transformers/src/,82,"skip ""model/""",
transformers/src/,124,in Attention: n_state=768 (nx=n_embd),
transformers/src/,125,[switch nx => n_state from Block to Attention to keep identical to TF implem],
transformers/src/,151,Prune conv1d layers,
transformers/src/,154,Update hyper params,
transformers/src/,163,w = w * self.bias + -1e9 * (1 - self.bias)  # TF implem method: mask_attn_weights,
transformers/src/,164,"XD: self.b may be larger than w, so we need to crop it",
transformers/src/,169,Apply the attention mask,
transformers/src/,175,Mask heads if we want to,
transformers/src/,187,in Tensorflow implem: fct merge_states,
transformers/src/,191,in Tensorflow implem: fct split_states,
transformers/src/,212,"a, (attentions)",
transformers/src/,216,in MLP: n_state=3072 (4 * n_embd),
transformers/src/,265,Slightly different from the TF version which uses truncated_normal for initialization,
transformers/src/,266,cf https://github.com/pytorch/pytorch/pull/5617,
transformers/src/,405,Code is different from when we had a single embedding matrice from position and token embeddings,
transformers/src/,410,Attention mask.,
transformers/src/,412,We create a 3D attention mask from a 2D tensor mask.,
transformers/src/,413,"Sizes are [batch_size, 1, 1, to_seq_length]",
transformers/src/,414,"So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]",
transformers/src/,415,this attention mask is more simple than the triangular masking of causal attention,
transformers/src/,416,"used in OpenAI GPT, we just need to prepare the broadcast dimension here.",
transformers/src/,419,Since attention_mask is 1.0 for positions we want to attend and 0.0 for,
transformers/src/,420,"masked positions, this operation will create a tensor which is 0.0 for",
transformers/src/,421,positions we want to attend and -10000.0 for masked positions.,
transformers/src/,422,"Since we are adding it to the raw scores before the softmax, this is",
transformers/src/,423,effectively the same as removing these entirely.,
transformers/src/,424,fp16 compatibility,
transformers/src/,427,Prepare head mask if needed,
transformers/src/,428,1.0 in head_mask indicate we keep the head,
transformers/src/,429,attention_probs has shape bsz x n_heads x N x N,
transformers/src/,430,head_mask has shape n_layer x batch x n_heads x N x N,
transformers/src/,438,We can specify head_mask for each layer,
transformers/src/,441,switch to fload if need + fp16 compatibility,
transformers/src/,469,Add last layer,
transformers/src/,478,"last hidden state, (all hidden states), (all attentions)",
transformers/src/,563,Shift so that tokens < n predict n,
transformers/src/,566,Flatten the tokens,
transformers/src/,571,"(loss), lm_logits, (all hidden states), (all attentions)",
transformers/src/,693,"(lm loss), (mc loss), lm logits, mc logits, (all hidden_states), (attentions)",
transformers/src/,1,coding=utf-8,
transformers/src/,2,"Copyright 2019-present, the HuggingFace Inc. team.",
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,45,mime-type to send to S3.,
transformers/src/,55,S3 object key,
transformers/src/,59,filename relative to config.json,
transformers/src/,76,id of model,
transformers/src/,77,S3 object key of config.json,
transformers/src/,81,list of files that constitute the model,
transformers/src/,152,streaming upload:,
transformers/src/,153,https://2.python-requests.org/en/master/user/advanced/#streaming-uploads,
transformers/src/,154,,
transformers/src/,155,"Even though we presign with the correct content-type,",
transformers/src/,156,the client still has to specify it when uploading the file.,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2020 The Facebook AI Research Team Authors and The HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,151,"Helper Functions, mostly for making masks",
transformers/src/,174,Helper Modules,
transformers/src/,257,mbart has one extra layer_norm,
transformers/src/,278,check attention mask and invert,
transformers/src/,288,B x T x C -> T x B x C,
transformers/src/,295,add LayerDrop (see https://arxiv.org/abs/1909.11556 for description),
transformers/src/,297,skip the layer,
transformers/src/,310,T x B x C -> B x T x C,
transformers/src/,357,Self Attention,
transformers/src/,362,adds keys to layer state,
transformers/src/,372,Cross attention,
transformers/src/,381,mutates layer state,
transformers/src/,388,Fully Connected,
transformers/src/,403,"just self_attn weights for now, following t5, layer_state = cache for decoding",
transformers/src/,430,type: List[DecoderLayer],
transformers/src/,463,check attention mask and invert,
transformers/src/,467,embed positions,
transformers/src/,472,happens after we embed them,
transformers/src/,480,"Convert to Bart output format: (seq_len, BS, model_dim) -> (BS, seq_len, model_dim)",
transformers/src/,484,decoder layers,
transformers/src/,489,add LayerDrop (see https://arxiv.org/abs/1909.11556 for description),
transformers/src/,510,last layer of mbart,
transformers/src/,515,"Convert to standard output format: (seq_len, BS, model_dim) -> (BS, seq_len, model_dim)",
transformers/src/,543,otherwise self_attention,
transformers/src/,573,type: bool,
transformers/src/,577,get here for encoder decoder cause of static_kv,
transformers/src/,578,"reuse k,v and encoder_padding_mask",
transformers/src/,581,previous time steps are cached - no need to recompute key and value if they are static,
transformers/src/,608,Update cache,
transformers/src/,624,This is part of a workaround to get around fork/join parallelism not supporting Optional types.,
transformers/src/,629,don't attend to padding symbols,
transformers/src/,649,"saved states are stored with shape (bsz, num_heads, seq_len, head_dim)",
transformers/src/,669,type: Optional[Tensor],
transformers/src/,683,"saved key padding masks have shape (bsz, seq_len)",
transformers/src/,706,This can trivially be shared with RobertaClassificationHead,
transformers/src/,736,if padding_idx is specified then offset the embedding ids by,
transformers/src/,737,this index and adjust num_embeddings appropriately,
transformers/src/,739,WHY?,
transformers/src/,744,the position is our current step in the decoded sequence,
transformers/src/,773,Public API,
transformers/src/,801,type: Tuple,
transformers/src/,807,make masks if user doesn't supply,
transformers/src/,823,"decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)",
transformers/src/,833,Attention and hidden_states will be [] or None if they aren't needed,
transformers/src/,834,type: tuple,
transformers/src/,836,type: tuple,
transformers/src/,848,make it on the fly,
transformers/src/,927,Add hidden states and attention if they are here,
transformers/src/,930,TODO(SS): do we need to ignore pad tokens in lm_labels?,
transformers/src/,939,"first step, decoder_cached_states are empty",
transformers/src/,945,encoder_outputs is defined. input_ids not needed,
transformers/src/,950,change this to avoid caching (presumably for debugging),
transformers/src/,965,get the correct batch idx from decoder layer's batch dim for cross and self-attn,
transformers/src/,981,make it on the fly,
transformers/src/,1051,last hidden state,
transformers/src/,1057,Prepend logits,
transformers/src/,1058,Add hidden states and attention if they are here,
transformers/src/,1059,"prepend loss to output,",
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,76,Create the position ids from the input token ids. Any padded tokens remain padded.,
transformers/src/,240,"The output weights are the same as the input embeddings, but there is",
transformers/src/,241,an output-only bias for each token.,
transformers/src/,253,project back to size of vocabulary with bias,
transformers/src/,305,Add hidden states and attention if they are here,
transformers/src/,307,"prediction_scores, (hidden_states), (attentions)",
transformers/src/,327,take <s> token (equiv. to [CLS]),
transformers/src/,386,"logits, (hidden_states), (attentions)",
transformers/src/,442,add hidden states and attention if they are here,
transformers/src/,444,"scores, (hidden_states), (attentions)",
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,33,Load checkpoint,
transformers/src/,38,We have the base model one level deeper than the original XLM repository,
transformers/src/,52,Save pytorch-model,
transformers/src/,71,Required parameters,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 Salesforce and The HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,145,no default special tokens - you can update this value if you add special tokens,
transformers/src/,148,no default special tokens - you can update this value if you add special tokens,
transformers/src/,269,"def decode(self, token_ids, skip_special_tokens=False, clean_up_tokenization_spaces=True):",
transformers/src/,270,"filtered_tokens = ' '.join(self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens))",
transformers/src/,271,"tokens_generated_so_far = re.sub('(@@ )', '', string=filtered_tokens)",
transformers/src/,272,"tokens_generated_so_far = re.sub('(@@ ?$)', '', string=tokens_generated_so_far)",
transformers/src/,273,return ''.join(tokens_generated_so_far),
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 Salesforce and HuggingFace Inc. team.,
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,41,create the sinusoidal pattern for the positional encoding,
transformers/src/,47,"pos_encoding = tf.cast(np.concatenate([sines, cosines], axis=-1)[np.newaxis, ...], dtype=tf.float32)",
transformers/src/,53,calculate attention,
transformers/src/,63,Apply the attention mask,
transformers/src/,68,Mask heads if we want to,
transformers/src/,113,to cope with keras serialization,
transformers/src/,114,we need to cast `use_cache` to correct bool,
transformers/src/,115,if it is a tensor,
transformers/src/,263,"If using past key value states, only the last tokens",
transformers/src/,264,should be given as an input,
transformers/src/,292,Attention mask.,
transformers/src/,294,We create a 3D attention mask from a 2D tensor mask.,
transformers/src/,295,"Sizes are [batch_size, 1, 1, to_seq_length]",
transformers/src/,296,"So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]",
transformers/src/,297,this attention mask is more simple than the triangular masking of causal attention,
transformers/src/,298,"used in OpenAI GPT, we just need to prepare the broadcast dimension here.",
transformers/src/,301,Since attention_mask is 1.0 for positions we want to attend and 0.0 for,
transformers/src/,302,"masked positions, this operation will create a tensor which is 0.0 for",
transformers/src/,303,positions we want to attend and -10000.0 for masked positions.,
transformers/src/,304,"Since we are adding it to the raw scores before the softmax, this is",
transformers/src/,305,effectively the same as removing these entirely.,
transformers/src/,312,Prepare head mask if needed,
transformers/src/,313,1.0 in head_mask indicate we keep the head,
transformers/src/,314,attention_probs has shape bsz x n_heads x N x N,
transformers/src/,315,head_mask has shape n_layer x batch x n_heads x N x N,
transformers/src/,369,let the number of heads free (-1) so we can extract attention even after head pruning,
transformers/src/,518,"The output weights are the same as the input embeddings, but there is",
transformers/src/,519,an output-only bias for each token.,
transformers/src/,548,only last token for inputs_ids if past is defined in kwargs,
transformers/src/,597,"lm_logits, presents, (all hidden_states), (attentions)",
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The OpenAI Team Authors and HuggingFace Inc. team.,
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,58,Load weights from TF model,
transformers/src/,69,"skip ""model/""",
transformers/src/,104,in Attention: n_state=768 (nx=n_embd),
transformers/src/,105,[switch nx => n_state from Block to Attention to keep identical to TF implem],
transformers/src/,125,Convert to set and emove already pruned heads,
transformers/src/,127,Compute how many pruned heads are before the head and move the index accordingly,
transformers/src/,134,Prune conv1d layers,
transformers/src/,138,Update hyper params,
transformers/src/,152,Apply the attention mask,
transformers/src/,158,Mask heads if we want to,
transformers/src/,170,in Tensorflow implem: fct merge_states,
transformers/src/,174,in Tensorflow implem: fct split_states,
transformers/src/,176,"(batch, head, head_features, seq_length)",
transformers/src/,178,"(batch, head, seq_length, head_features)",
transformers/src/,187,transpose back cf below,
transformers/src/,192,transpose to have same shapes for stacking,
transformers/src/,204,"a, present, (attentions)",
transformers/src/,208,in MLP: n_state=3072 (4 * n_embd),
transformers/src/,239,"output_attn: a, present, (attentions)",
transformers/src/,246,"x, present, (attentions)",
transformers/src/,266,Slightly different from the TF version which uses truncated_normal for initialization,
transformers/src/,267,cf https://github.com/pytorch/pytorch/pull/5617,
transformers/src/,414,"If using past key value states, only the last tokens",
transformers/src/,415,should be given as an input,
transformers/src/,451,Attention mask.,
transformers/src/,455,We create a 3D attention mask from a 2D tensor mask.,
transformers/src/,456,"Sizes are [batch_size, 1, 1, to_seq_length]",
transformers/src/,457,"So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]",
transformers/src/,458,this attention mask is more simple than the triangular masking of causal attention,
transformers/src/,459,"used in OpenAI GPT, we just need to prepare the broadcast dimension here.",
transformers/src/,462,Since attention_mask is 1.0 for positions we want to attend and 0.0 for,
transformers/src/,463,"masked positions, this operation will create a tensor which is 0.0 for",
transformers/src/,464,positions we want to attend and -10000.0 for masked positions.,
transformers/src/,465,"Since we are adding it to the raw scores before the softmax, this is",
transformers/src/,466,effectively the same as removing these entirely.,
transformers/src/,467,fp16 compatibility,
transformers/src/,470,Prepare head mask if needed,
transformers/src/,471,1.0 in head_mask indicate we keep the head,
transformers/src/,472,attention_probs has shape bsz x n_heads x N x N,
transformers/src/,473,head_mask has shape n_layer x batch x n_heads x N x N,
transformers/src/,481,We can specify head_mask for each layer,
transformers/src/,484,switch to fload if need + fp16 compatibility,
transformers/src/,525,Add last hidden state,
transformers/src/,535,let the number of heads free (-1) so we can extract attention even after head pruning,
transformers/src/,539,"last hidden state, (presents), (all hidden_states), (attentions)",
transformers/src/,559,only last token for inputs_ids if past is defined in kwargs,
transformers/src/,636,Shift so that tokens < n predict n,
transformers/src/,639,Flatten the tokens,
transformers/src/,644,"(loss), lm_logits, presents, (all hidden_states), (attentions)",
transformers/src/,775,"(lm loss), (mc loss), lm logits, mc logits, presents, (all hidden_states), (attentions)",
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The HuggingFace Inc. team.,
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,49,disable dropout,
transformers/src/,59,PyTorch default used in fairseq,
transformers/src/,68,Now let's copy all the weights.,
transformers/src/,69,Embeddings,
transformers/src/,74,just zero them out b/c RoBERTa doesn't use them.,
transformers/src/,79,Encoder: start of layer,
transformers/src/,83,self attention,
transformers/src/,99,self-attention output,
transformers/src/,107,intermediate,
transformers/src/,113,output,
transformers/src/,120,end of layer,
transformers/src/,128,LM Head,
transformers/src/,136,Let's check that we get the same results.,
transformers/src/,137,batch of size 1,
transformers/src/,146,~ 1e-7,
transformers/src/,159,Required parameters,
transformers/src/,1,coding=utf-8,
transformers/src/,2,"Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.",
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,26,CUDA_MAJOR = int(torch.version.cuda.split('.')[0]),
transformers/src/,27,CUDA_MINOR = int(torch.version.cuda.split('.')[1]),
transformers/src/,76,if CUDA_MAJOR <= 9 and CUDA_MINOR <= 1:,
transformers/src/,79,else:,
transformers/src/,80,"logit = torch.einsum('bd,de,ev->bv', (hidden, proj, weight.t()))",
transformers/src/,81,if bias is not None:,
transformers/src/,82,logit = logit + bias,
transformers/src/,102,Shift so that tokens < n predict n,
transformers/src/,119,construct weights and biases,
transformers/src/,175,No probability for the head cluster,
transformers/src/,210,construct weights and biases,
transformers/src/,1,coding=utf-8,
transformers/src/,2,"Copyright (c) Facebook, Inc. and its affiliates.",
transformers/src/,3,Copyright (c) HuggingFace Inc. team.,
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,1,coding=utf-8,
transformers/src/,2,"Copyright 2019-present, Facebook, Inc and the HuggingFace Inc. team.",
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,240,For backward compatibility,
transformers/src/,244,For backward compatibility,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,40,device ids,
transformers/src/,43,'$1___$2' is replaced by $2 (can be used to duplicate or remove layers in TF2.0 vs PyTorch),
transformers/src/,46,'_._' is replaced by a level separation (can be used to convert TF2.0 lists in PyTorch nn.ModulesList),
transformers/src/,47,Remove empty levels at the end,
transformers/src/,48,Convert from TF2.0 '/' separators to PyTorch '.' separators,
transformers/src/,49,Remove level zero,
transformers/src/,51,When should we transpose the weights,
transformers/src/,54,Convert standard TF2.0 names in PyTorch names,
transformers/src/,60,Remove prefix if needed,
transformers/src/,68,,
transformers/src/,69,PyTorch => TF 2.0,
transformers/src/,70,,
transformers/src/,77,noqa: F401,
transformers/src/,78,noqa: F401,
transformers/src/,111,noqa: F401,
transformers/src/,112,noqa: F401,
transformers/src/,125,Make sure model is built,
transformers/src/,127,Adapt state dict - TODO remove this and update the AWS weights files instead,
transformers/src/,128,Convert old format to new format if needed from a PyTorch state_dict,
transformers/src/,143,Make sure we are able to load PyTorch base models as well as derived models (with heads),
transformers/src/,144,"TF models always have a prefix, some of PyTorch models (base ones) don't",
transformers/src/,159,Find associated numpy array in pytorch model state dict,
transformers/src/,183,"logger.warning(""Initialize TF weight {}"".format(symbolic_weight.name))",
transformers/src/,191,Make sure restore ops are run,
transformers/src/,200,,
transformers/src/,201,TF 2.0 => PyTorch,
transformers/src/,202,,
transformers/src/,211,noqa: F401,
transformers/src/,212,noqa: F401,
transformers/src/,224,Instantiate and load the associated TF 2.0 model,
transformers/src/,225,"Add ""TF"" at the beggining",
transformers/src/,233,Make sure model is built,
transformers/src/,252,noqa: F401,
transformers/src/,253,noqa: F401,
transformers/src/,264,Make sure we are able to load PyTorch base models as well as derived models (with heads),
transformers/src/,265,"TF models always have a prefix, some of PyTorch models (base ones) don't",
transformers/src/,270,Build a map from potential PyTorch weight names to TF 2.0 Variables,
transformers/src/,282,Handle PyTorch shared weight ()not duplicated in TF 2.0,
transformers/src/,287,Find associated numpy array in pytorch model state dict,
transformers/src/,311,"logger.warning(""Initialize PyTorch weight {}"".format(pt_weight_name))",
transformers/src/,1,coding=utf-8,
transformers/src/,2,"Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.",
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,48,We are loading in a TransfoXLLMHeadModel => we will load also the Adaptive Softmax,
transformers/src/,63,I don't think this is implemented in the TF code,
transformers/src/,67,Now load the rest of the transformer,
transformers/src/,70,Embeddings,
transformers/src/,75,Transformer blocks,
transformers/src/,94,Relative positioning biases,
transformers/src/,120,Build TF to PyTorch weights loading map,
transformers/src/,123,Load weights from TF model,
transformers/src/,134,adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v,
transformers/src/,135,which are not required for using pretrained model,
transformers/src/,139,Here we will split the TF weights,
transformers/src/,207,layer normalization + positionwise feed-forward,
transformers/src/,210,residual connection,
transformers/src/,213,positionwise feed-forward,
transformers/src/,216,residual connection + layer normalization,
transformers/src/,259,Biases are not shared,
transformers/src/,304,qlen x bsz x n_head x d_head,
transformers/src/,305,qlen x bsz x n_head x d_head,
transformers/src/,306,qlen x bsz x n_head x d_head,
transformers/src/,308,qlen x n_head x d_head,
transformers/src/,310,compute attention score,
transformers/src/,311,qlen x bsz x n_head x d_head,
transformers/src/,312,qlen x klen x bsz x n_head,
transformers/src/,315,qlen x klen x bsz x n_head,
transformers/src/,318,[qlen x klen x bsz x n_head],
transformers/src/,322,compute attention probability,
transformers/src/,324,Switch to bool,
transformers/src/,338,[qlen x klen x bsz x n_head],
transformers/src/,342,Mask heads if we want to,
transformers/src/,346,compute attention vector,
transformers/src/,349,[qlen x bsz x n_head x d_head],
transformers/src/,352,linear projection,
transformers/src/,357,residual connection,
transformers/src/,360,residual connection + layer normalization,
transformers/src/,585,the default attention,
transformers/src/,605,learnable embeddings and absolute embeddings are not used in our pretrained checkpoints,
transformers/src/,606,Removed them to avoid maintaining dead code,
transformers/src/,611,default attention,
transformers/src/,613,learnable embeddings and absolute embeddings,
transformers/src/,614,Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint,
transformers/src/,649,does not deal with None,
transformers/src/,653,mems is not None,
transformers/src/,656,There are `mlen + qlen` steps that can be cached into mems,
transformers/src/,657,"For the next step, the last `ext_len` of the `qlen` tokens",
transformers/src/,658,"will be used as the extended context. Hence, we only cache",
transformers/src/,659,the tokens from `mlen + qlen - self.ext_len - self.mem_len`,
transformers/src/,660,to `mlen + qlen - self.ext_len`.,
transformers/src/,707,"the original code for Transformer-XL used shapes [len, bsz] but we want a unified interface in the library",
transformers/src/,708,"so we transpose here from shape [bsz, len] to shape [len, bsz]",
transformers/src/,723,Prepare head mask if needed,
transformers/src/,724,1.0 in head_mask indicate we keep the head,
transformers/src/,725,attention_probs has shape bsz x n_heads x N x N,
transformers/src/,726,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] (a head_mask for each layer),
transformers/src/,727,and head_mask is converted to shape [num_hidden_layers x qlen x klen x bsz x n_head],
transformers/src/,736,switch to fload if need + fp16 compatibility,
transformers/src/,754,-1,
transformers/src/,762,default,
transformers/src/,780,learnable embeddings and absolute embeddings,
transformers/src/,781,Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint,
transformers/src/,787,"We transpose back here to shape [bsz, len, hidden_dim]",
transformers/src/,790,"Add last layer and transpose to library standard shape [bsz, len, hidden_dim]",
transformers/src/,795,"Transpose to library standard shape [bsz, n_heads, query_seq_len, key_seq_len]",
transformers/src/,799,"last hidden state, new_mems, (all hidden states), (all attentions)",
transformers/src/,915,"(loss), logits or None if labels is not None (speed up adaptive softmax), new_mems, (all hidden states), (all attentions)",
transformers/src/,928,if past is defined in model kwargs then use it for faster decoding,
transformers/src/,1,coding=utf-8,
transformers/src/,2,"Copyright 2019-present, the HuggingFace Inc. team, The Google AI Language Team and Facebook, Inc.",
transformers/src/,3,,
transformers/src/,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,5,you may not use this file except in compliance with the License.,
transformers/src/,6,You may obtain a copy of the License at,
transformers/src/,7,,
transformers/src/,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,9,,
transformers/src/,10,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,13,See the License for the specific language governing permissions and,
transformers/src/,14,limitations under the License.,
transformers/src/,25,s3,
transformers/src/,35,upload,
transformers/src/,81,"probably invalid credentials, display error message.",
transformers/src/,169,"(filepath, filename)",
transformers/src/,205,Check we don't have empty string,
transformers/src/,210,Forward through the model,
transformers/src/,8,pylint: disable=invalid-name,
transformers/src/,91,Saving data,
transformers/src/,35,deprecated in v2.1,
transformers/src/,38,"returns list of devices, convert to bool",
transformers/src/,13,TF training parameters,
transformers/src/,143,Save trained pipeline,
transformers/src/,1,flake8: noqa,
transformers/src/,2,"There's no way to ignore ""F401 '...' imported but unused"" warnings in this",
transformers/src/,3,"module, but to preserve other warnings. So, don't check this module at all.",
transformers/src/,60,"If either is no-answer, then F1 is 1 if they agree, 0 otherwise",
transformers/src/,82,"For unanswerable questions, only correct answer is empty string",
transformers/src/,245,"When we created the data, we kept track of the alignment between original",
transformers/src/,246,(whitespace tokenized) tokens and our WordPiece tokenized tokens. So,
transformers/src/,247,now `orig_text` contains the span of our original text corresponding to the,
transformers/src/,248,span that we predicted.,
transformers/src/,249,,
transformers/src/,250,"However, `orig_text` may contain extra characters that we don't want in",
transformers/src/,251,our prediction.,
transformers/src/,252,,
transformers/src/,253,"For example, let's say:",
transformers/src/,254,pred_text = steve smith,
transformers/src/,255,orig_text = Steve Smith's,
transformers/src/,256,,
transformers/src/,257,"We don't want to return `orig_text` because it contains the extra ""'s"".",
transformers/src/,258,,
transformers/src/,259,We don't want to return `pred_text` because it's already been normalized,
transformers/src/,260,(the SQuAD eval script also does punctuation stripping/lower casing but,
transformers/src/,261,our tokenizer does additional normalization like stripping accent,
transformers/src/,262,characters).,
transformers/src/,263,,
transformers/src/,264,"What we really want to return is ""Steve Smith"".",
transformers/src/,265,,
transformers/src/,266,"Therefore, we have to apply a semi-complicated alignment heuristic between",
transformers/src/,267,`pred_text` and `orig_text` to get a character-to-character alignment. This,
transformers/src/,268,can fail in certain cases in which case we just return `orig_text`.,
transformers/src/,281,"We first tokenize `orig_text`, strip whitespace from the result",
transformers/src/,282,"and `pred_text`, and check if they are the same length. If they are",
transformers/src/,283,"NOT the same length, the heuristic has failed. If they are the same",
transformers/src/,284,"length, we assume the characters are one-to-one aligned.",
transformers/src/,304,We then project the characters in `pred_text` back to `orig_text` using,
transformers/src/,305,the character-to-character alignment.,
transformers/src/,398,pylint: disable=invalid-name,
transformers/src/,410,keep track of the minimum score of null start+end of position 0,
transformers/src/,411,large and positive,
transformers/src/,412,the paragraph slice with min null score,
transformers/src/,413,the start logit at the slice with min null score,
transformers/src/,414,the end logit at the slice with min null score,
transformers/src/,419,"if we could have irrelevant answers, get the min score of irrelevant",
transformers/src/,429,"We could hypothetically create invalid predictions, e.g., predict",
transformers/src/,430,that the start of the span is in the question. We throw out all,
transformers/src/,431,invalid predictions.,
transformers/src/,468,pylint: disable=invalid-name,
transformers/src/,478,this is a non-null prediction,
transformers/src/,486,"tok_text = "" "".join(tok_tokens)",
transformers/src/,487,,
transformers/src/,488,# De-tokenize WordPieces that have been split off.,
transformers/src/,489,"tok_text = tok_text.replace("" ##"", """")",
transformers/src/,490,"tok_text = tok_text.replace(""##"", """")",
transformers/src/,492,Clean whitespace,
transformers/src/,507,"if we didn't include the empty option in the n-best, include it",
transformers/src/,512,In very rare edge cases we could only have single null prediction.,
transformers/src/,513,So we just create a nonce prediction in this case to avoid failure.,
transformers/src/,517,In very rare edge cases we could have no valid predictions. So we,
transformers/src/,518,just create a nonce prediction in this case to avoid failure.,
transformers/src/,548,"predict """" iff the null score - the score of best non-null > threshold",
transformers/src/,590,pylint: disable=invalid-name,
transformers/src/,594,pylint: disable=invalid-name,
transformers/src/,599,"logger.info(""Writing nbest to: %s"" % (output_nbest_file))",
transformers/src/,617,keep track of the minimum score of null start+end of position 0,
transformers/src/,618,large and positive,
transformers/src/,625,"if we could have irrelevant answers, get the min score of irrelevant",
transformers/src/,638,"We could hypothetically create invalid predictions, e.g., predict",
transformers/src/,639,that the start of the span is in the question. We throw out all,
transformers/src/,640,invalid predictions.,
transformers/src/,675,XLNet un-tokenizer,
transformers/src/,676,Let's keep it simple for now and see if we need all this later.,
transformers/src/,677,,
transformers/src/,678,tok_start_to_orig_index = feature.tok_start_to_orig_index,
transformers/src/,679,tok_end_to_orig_index = feature.tok_end_to_orig_index,
transformers/src/,680,start_orig_pos = tok_start_to_orig_index[pred.start_index],
transformers/src/,681,end_orig_pos = tok_end_to_orig_index[pred.end_index],
transformers/src/,682,paragraph_text = example.paragraph_text,
transformers/src/,683,final_text = paragraph_text[start_orig_pos: end_orig_pos + 1].strip(),
transformers/src/,685,Previously used Bert untokenizer,
transformers/src/,692,Clean whitespace,
transformers/src/,713,In very rare edge cases we could have no valid predictions. So we,
transformers/src/,714,just create a nonce prediction in this case to avoid failure.,
transformers/src/,741,note(zhiliny): always predict best_non_null_entry,
transformers/src/,742,and the evaluation script will search for the best threshold,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,214,Update examples,
transformers/src/,220,Update labels,
transformers/src/,279,The mask has 1 for real tokens and 0 for padding tokens. Only real,
transformers/src/,280,tokens are attended to.,
transformers/src/,283,Zero-pad up to the sequence length.,
transformers/src/,60,if len(doc_spans) == 1:,
transformers/src/,61,return True,
transformers/src/,89,Get start and end position,
transformers/src/,93,"If the answer cannot be found in the text, then skip this example.",
transformers/src/,194,Identify the position of the CLS token,
transformers/src/,197,p_mask: mask with 1 for token than cannot be in the answer (0 for token which can be in an answer),
transformers/src/,198,Original TF implem also keep the classification token (set to 0) (not sure why...),
transformers/src/,204,Limit positive values to one,
transformers/src/,209,Set the CLS index to '0',
transformers/src/,216,"For training, if our document chunk does not contain an annotation",
transformers/src/,217,"we throw it out, since there is nothing to predict.",
transformers/src/,245,Can not set unique_id and example_index here. They will be set after multiple processing.,
transformers/src/,302,Defining helper methods,
transformers/src/,338,Convert to Tensors and build dataset,
transformers/src/,617,Split on whitespace so that different tokens may be attributed to their original position.,
transformers/src/,632,Start and end positions only has a value during evaluation.,
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/src/,1,flake8: noqa,
transformers/src/,2,"There's no way to ignore ""F401 '...' imported but unused"" warnings in this",
transformers/src/,3,"module, but to preserve other warnings. So, don't check this module at all.",
transformers/src/,1,coding=utf-8,
transformers/src/,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/src/,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/src/,4,,
transformers/src/,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/src/,6,you may not use this file except in compliance with the License.,
transformers/src/,7,You may obtain a copy of the License at,
transformers/src/,8,,
transformers/src/,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/src/,10,,
transformers/src/,11,"Unless required by applicable law or agreed to in writing, software",
transformers/src/,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/src/,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/src/,14,See the License for the specific language governing permissions and,
transformers/src/,15,limitations under the License.,
transformers/tests/test_tokenization_bert.py,1,coding=utf-8,
transformers/tests/test_tokenization_bert.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_tokenization_bert.py,3,,
transformers/tests/test_tokenization_bert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_tokenization_bert.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_tokenization_bert.py,6,You may obtain a copy of the License at,
transformers/tests/test_tokenization_bert.py,7,,
transformers/tests/test_tokenization_bert.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_tokenization_bert.py,9,,
transformers/tests/test_tokenization_bert.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_tokenization_bert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_tokenization_bert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_tokenization_bert.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_tokenization_bert.py,14,limitations under the License.,
transformers/tests/test_modeling_tf_distilbert.py,1,coding=utf-8,
transformers/tests/test_modeling_tf_distilbert.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_modeling_tf_distilbert.py,3,,
transformers/tests/test_modeling_tf_distilbert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_modeling_tf_distilbert.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_modeling_tf_distilbert.py,6,You may obtain a copy of the License at,
transformers/tests/test_modeling_tf_distilbert.py,7,,
transformers/tests/test_modeling_tf_distilbert.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_modeling_tf_distilbert.py,9,,
transformers/tests/test_modeling_tf_distilbert.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_modeling_tf_distilbert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_modeling_tf_distilbert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_modeling_tf_distilbert.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_modeling_tf_distilbert.py,14,limitations under the License.,
transformers/tests/test_modeling_tf_distilbert.py,219,@slow,
transformers/tests/test_modeling_tf_distilbert.py,220,def test_model_from_pretrained(self):,
transformers/tests/test_modeling_tf_distilbert.py,221,for model_name in list(DISTILBERT_PRETRAINED_MODEL_ARCHIVE_MAP.keys())[:1]:,
transformers/tests/test_modeling_tf_distilbert.py,222,"model = DistilBertModesss.from_pretrained(model_name, cache_dir=CACHE_DIR)",
transformers/tests/test_modeling_tf_distilbert.py,223,self.assertIsNotNone(model),
transformers/tests/test_modeling_flaubert.py,1,coding=utf-8,
transformers/tests/test_modeling_flaubert.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_modeling_flaubert.py,3,,
transformers/tests/test_modeling_flaubert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_modeling_flaubert.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_modeling_flaubert.py,6,You may obtain a copy of the License at,
transformers/tests/test_modeling_flaubert.py,7,,
transformers/tests/test_modeling_flaubert.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_modeling_flaubert.py,9,,
transformers/tests/test_modeling_flaubert.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_modeling_flaubert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_modeling_flaubert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_modeling_flaubert.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_modeling_flaubert.py,14,limitations under the License.,
transformers/tests/test_modeling_flaubert.py,123,small variation of seq_length,
transformers/tests/test_doc_samples.py,1,coding=utf-8,
transformers/tests/test_doc_samples.py,2,"Copyright 2019-present, the HuggingFace Inc. team.",
transformers/tests/test_doc_samples.py,3,,
transformers/tests/test_doc_samples.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_doc_samples.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_doc_samples.py,6,You may obtain a copy of the License at,
transformers/tests/test_doc_samples.py,7,,
transformers/tests/test_doc_samples.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_doc_samples.py,9,,
transformers/tests/test_doc_samples.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_doc_samples.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_doc_samples.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_doc_samples.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_doc_samples.py,14,limitations under the License.,
transformers/tests/test_doc_samples.py,32,"Check if the indentation is 0 for the example, so that we don't exit as soon as there's a line return.",
transformers/tests/test_doc_samples.py,35,If we're back to the example indentation or if it's the end of the docstring.,
transformers/tests/test_doc_samples.py,37,Exit the example mode and add the example to the examples list,
transformers/tests/test_doc_samples.py,43,"If line is not empty, add it to the current example",
transformers/tests/test_doc_samples.py,47,Detect the example from '::' or 'example::',
transformers/tests/test_doc_samples.py,54,"elif ""::"" in line.lower() and len(line.strip()) == 2:",
transformers/tests/test_doc_samples.py,55,example_mode = True,
transformers/tests/test_doc_samples.py,56,"example_indentation = line.lower().find(""::"")",
transformers/tests/test_doc_samples.py,80,Open all files,
transformers/tests/test_doc_samples.py,83,Retrieve examples,
transformers/tests/test_doc_samples.py,90,Some examples are the continuation of others.,
transformers/tests/test_doc_samples.py,95,"If they contain this line, then they're a continuation of the previous script",
transformers/tests/test_doc_samples.py,98,"If not, create a new example and increment the index",
transformers/tests/test_doc_samples.py,105,Execute sub tests with every example.,
transformers/tests/test_optimization.py,1,coding=utf-8,
transformers/tests/test_optimization.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_optimization.py,3,,
transformers/tests/test_optimization.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_optimization.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_optimization.py,6,You may obtain a copy of the License at,
transformers/tests/test_optimization.py,7,,
transformers/tests/test_optimization.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_optimization.py,9,,
transformers/tests/test_optimization.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_optimization.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_optimization.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_optimization.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_optimization.py,14,limitations under the License.,
transformers/tests/test_optimization.py,73,"No warmup, constant schedule, no gradient clipping",
transformers/tests/test_optimization.py,79,No zero_grad() function on simple tensors. we do it ourselves.,
transformers/tests/test_modeling_distilbert.py,1,coding=utf-8,
transformers/tests/test_modeling_distilbert.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_modeling_distilbert.py,3,,
transformers/tests/test_modeling_distilbert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_modeling_distilbert.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_modeling_distilbert.py,6,You may obtain a copy of the License at,
transformers/tests/test_modeling_distilbert.py,7,,
transformers/tests/test_modeling_distilbert.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_modeling_distilbert.py,9,,
transformers/tests/test_modeling_distilbert.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_modeling_distilbert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_modeling_distilbert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_modeling_distilbert.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_modeling_distilbert.py,14,limitations under the License.,
transformers/tests/test_modeling_distilbert.py,248,@slow,
transformers/tests/test_modeling_distilbert.py,249,def test_model_from_pretrained(self):,
transformers/tests/test_modeling_distilbert.py,250,for model_name in list(DISTILBERT_PRETRAINED_MODEL_ARCHIVE_MAP.keys())[:1]:,
transformers/tests/test_modeling_distilbert.py,251,"model = DistilBertModel.from_pretrained(model_name, cache_dir=CACHE_DIR)",
transformers/tests/test_modeling_distilbert.py,252,self.assertIsNotNone(model),
transformers/tests/test_modeling_ctrl.py,1,coding=utf-8,
transformers/tests/test_modeling_ctrl.py,2,Copyright 2018 Salesforce and HuggingFace Inc. team.,
transformers/tests/test_modeling_ctrl.py,3,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_modeling_ctrl.py,4,you may not use this file except in compliance with the License.,
transformers/tests/test_modeling_ctrl.py,5,You may obtain a copy of the License at,
transformers/tests/test_modeling_ctrl.py,6,,
transformers/tests/test_modeling_ctrl.py,7,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_modeling_ctrl.py,8,,
transformers/tests/test_modeling_ctrl.py,9,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_modeling_ctrl.py,10,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_modeling_ctrl.py,11,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_modeling_ctrl.py,12,See the License for the specific language governing permissions and,
transformers/tests/test_modeling_ctrl.py,13,limitations under the License.,
transformers/tests/test_modeling_ctrl.py,119,"intermediate_size=self.intermediate_size,",
transformers/tests/test_modeling_ctrl.py,120,"hidden_act=self.hidden_act,",
transformers/tests/test_modeling_ctrl.py,121,"hidden_dropout_prob=self.hidden_dropout_prob,",
transformers/tests/test_modeling_ctrl.py,122,"attention_probs_dropout_prob=self.attention_probs_dropout_prob,",
transformers/tests/test_modeling_ctrl.py,125,"type_vocab_size=self.type_vocab_size,",
transformers/tests/test_modeling_ctrl.py,126,initializer_range=self.initializer_range,
transformers/tests/test_modeling_ctrl.py,224,Legal the president is,
transformers/tests/test_modeling_ctrl.py,246,Legal the president is a good guy and I don't want to lose my job. \n \n I have a,
transformers/tests/test_modeling_xlnet.py,1,coding=utf-8,
transformers/tests/test_modeling_xlnet.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_modeling_xlnet.py,3,,
transformers/tests/test_modeling_xlnet.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_modeling_xlnet.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_modeling_xlnet.py,6,You may obtain a copy of the License at,
transformers/tests/test_modeling_xlnet.py,7,,
transformers/tests/test_modeling_xlnet.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_modeling_xlnet.py,9,,
transformers/tests/test_modeling_xlnet.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_modeling_xlnet.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_modeling_xlnet.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_modeling_xlnet.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_modeling_xlnet.py,14,limitations under the License.,
transformers/tests/test_modeling_xlnet.py,57,TODO (PVP): Check other models whether language generation is also applicable,
transformers/tests/test_modeling_xlnet.py,92,self.key_len = seq_length + mem_len,
transformers/tests/test_modeling_xlnet.py,124,Previous tokens don't see last token,
transformers/tests/test_modeling_xlnet.py,128,predict last token,
transformers/tests/test_modeling_xlnet.py,689,"In 1991, the remains of Russian Tsar Nicholas II and his family",
transformers/tests/test_modeling_xlnet.py,690,(except for Alexei and Maria) are discovered.,
transformers/tests/test_modeling_xlnet.py,691,"The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the",
transformers/tests/test_modeling_xlnet.py,692,"remainder of the story. 1883 Western Siberia,",
transformers/tests/test_modeling_xlnet.py,693,a young Grigori Rasputin is asked by his father and a group of men to perform magic.,
transformers/tests/test_modeling_xlnet.py,694,Rasputin has a vision and denounces one of the men as a horse thief. Although his,
transformers/tests/test_modeling_xlnet.py,695,"father initially slaps him for making such an accusation, Rasputin watches as the",
transformers/tests/test_modeling_xlnet.py,696,"man is chased outside and beaten. Twenty years later, Rasputin sees a vision of",
transformers/tests/test_modeling_xlnet.py,697,"the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,",
transformers/tests/test_modeling_xlnet.py,698,"with people, even a bishop, begging for his blessing. """"""",
transformers/tests/test_modeling_xlnet.py,902,"In 1991, the remains of Russian Tsar Nicholas II and his family (except for Alexei and Maria)",
transformers/tests/test_modeling_xlnet.py,903,"are discovered. The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich,",
transformers/tests/test_modeling_xlnet.py,904,"narrates the remainder of the story. 1883 Western Siberia, a young Grigori Rasputin",
transformers/tests/test_modeling_xlnet.py,905,is asked by his father and a group of men to perform magic. Rasputin has a vision and,
transformers/tests/test_modeling_xlnet.py,906,denounces one of the men as a horse thief. Although his father initially slaps,
transformers/tests/test_modeling_xlnet.py,907,"him for making such an accusation, Rasputin watches as the man is chased outside and beaten.",
transformers/tests/test_modeling_xlnet.py,908,"Twenty years later, Rasputin sees a vision of the Virgin Mary, prompting him to become a priest.",
transformers/tests/test_modeling_xlnet.py,909,"Rasputin quickly becomes famous, with people, even a bishop, begging for his blessing.",
transformers/tests/test_modeling_xlnet.py,910,"<sep><cls>, Rasputin is asked to perform magic.",
transformers/tests/test_modeling_xlnet.py,911,"He is not able to perform magic, and his father and",
transformers/tests/test_modeling_xlnet.py,912,the men are forced to leave the monastery. Rasputin is forced to return to,
transformers/tests/test_pipelines.py,45,"('xlnet-base-cased', 'xlnet-base-cased', None), # Disabled for now as it crash for TF2",
transformers/tests/test_pipelines.py,50,"('xlnet-base-cased', 'xlnet-base-cased', None), # Disabled for now as it crash for TF2",
transformers/tests/test_pipelines.py,382,Test that pipelines can be correctly loaded without any argument,
transformers/tests/test_pipelines.py,390,Test that pipelines can be correctly loaded without any argument,
transformers/tests/test_modeling_tf_gpt2.py,1,coding=utf-8,
transformers/tests/test_modeling_tf_gpt2.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_modeling_tf_gpt2.py,3,,
transformers/tests/test_modeling_tf_gpt2.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_modeling_tf_gpt2.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_modeling_tf_gpt2.py,6,You may obtain a copy of the License at,
transformers/tests/test_modeling_tf_gpt2.py,7,,
transformers/tests/test_modeling_tf_gpt2.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_modeling_tf_gpt2.py,9,,
transformers/tests/test_modeling_tf_gpt2.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_modeling_tf_gpt2.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_modeling_tf_gpt2.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_modeling_tf_gpt2.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_modeling_tf_gpt2.py,14,limitations under the License.,
transformers/tests/test_modeling_tf_gpt2.py,124,"intermediate_size=self.intermediate_size,",
transformers/tests/test_modeling_tf_gpt2.py,125,"hidden_act=self.hidden_act,",
transformers/tests/test_modeling_tf_gpt2.py,126,"hidden_dropout_prob=self.hidden_dropout_prob,",
transformers/tests/test_modeling_tf_gpt2.py,127,"attention_probs_dropout_prob=self.attention_probs_dropout_prob,",
transformers/tests/test_modeling_tf_gpt2.py,130,"type_vocab_size=self.type_vocab_size,",
transformers/tests/test_modeling_tf_gpt2.py,131,initializer_range=self.initializer_range,
transformers/tests/test_modeling_tf_gpt2.py,159,None is the input for 'past',
transformers/tests/test_modeling_tf_gpt2.py,174,first forward pass,
transformers/tests/test_modeling_tf_gpt2.py,177,create hypothetical next token and extent to next_input_ids,
transformers/tests/test_modeling_tf_gpt2.py,181,append to next input_ids and token_type_ids,
transformers/tests/test_modeling_tf_gpt2.py,188,select random slice,
transformers/tests/test_modeling_tf_gpt2.py,193,test that outputs are equal for slice,
transformers/tests/test_modeling_tf_gpt2.py,201,create attention mask,
transformers/tests/test_modeling_tf_gpt2.py,207,first forward pass,
transformers/tests/test_modeling_tf_gpt2.py,210,create hypothetical next token and extent to next_input_ids,
transformers/tests/test_modeling_tf_gpt2.py,213,change a random masked slice from input_ids,
transformers/tests/test_modeling_tf_gpt2.py,222,append to next input_ids and attn_mask,
transformers/tests/test_modeling_tf_gpt2.py,226,get two different outputs,
transformers/tests/test_modeling_tf_gpt2.py,230,select random slice,
transformers/tests/test_modeling_tf_gpt2.py,235,test that outputs are equal for slice,
transformers/tests/test_modeling_tf_gpt2.py,335,The dog,
transformers/tests/test_modeling_tf_gpt2.py,357,The dog was found in a field near the intersection of West and West Streets.\n\nThe dog,
transformers/tests/test_modeling_tf_gpt2.py,364,The president,
transformers/tests/test_modeling_tf_gpt2.py,386,"The president of the United States, and the president of the United Kingdom, have been in the White",
transformers/tests/test_modeling_tf_xlnet.py,1,coding=utf-8,
transformers/tests/test_modeling_tf_xlnet.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_modeling_tf_xlnet.py,3,,
transformers/tests/test_modeling_tf_xlnet.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_modeling_tf_xlnet.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_modeling_tf_xlnet.py,6,You may obtain a copy of the License at,
transformers/tests/test_modeling_tf_xlnet.py,7,,
transformers/tests/test_modeling_tf_xlnet.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_modeling_tf_xlnet.py,9,,
transformers/tests/test_modeling_tf_xlnet.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_modeling_tf_xlnet.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_modeling_tf_xlnet.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_modeling_tf_xlnet.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_modeling_tf_xlnet.py,14,limitations under the License.,
transformers/tests/test_modeling_tf_xlnet.py,56,TODO (PVP): Check other models whether language generation is also applicable,
transformers/tests/test_modeling_tf_xlnet.py,91,self.key_len = seq_length + mem_len,
transformers/tests/test_modeling_tf_xlnet.py,123,"perm_mask[:, :, -1] = 1.0  # Previous tokens don't see last token",
transformers/tests/test_modeling_tf_xlnet.py,127,"target_mapping[:, 0, -1] = 1.0  # predict last token",
transformers/tests/test_modeling_tf_xlnet.py,347,'token_type_ids': token_type_ids,
transformers/tests/test_modeling_tf_xlnet.py,590,"In 1991, the remains of Russian Tsar Nicholas II and his family",
transformers/tests/test_modeling_tf_xlnet.py,591,(except for Alexei and Maria) are discovered.,
transformers/tests/test_modeling_tf_xlnet.py,592,"The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the",
transformers/tests/test_modeling_tf_xlnet.py,593,"remainder of the story. 1883 Western Siberia,",
transformers/tests/test_modeling_tf_xlnet.py,594,a young Grigori Rasputin is asked by his father and a group of men to perform magic.,
transformers/tests/test_modeling_tf_xlnet.py,595,Rasputin has a vision and denounces one of the men as a horse thief. Although his,
transformers/tests/test_modeling_tf_xlnet.py,596,"father initially slaps him for making such an accusation, Rasputin watches as the",
transformers/tests/test_modeling_tf_xlnet.py,597,"man is chased outside and beaten. Twenty years later, Rasputin sees a vision of",
transformers/tests/test_modeling_tf_xlnet.py,598,"the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,",
transformers/tests/test_modeling_tf_xlnet.py,599,"with people, even a bishop, begging for his blessing. """"""",
transformers/tests/test_modeling_tf_xlnet.py,803,"In 1991, the remains of Russian Tsar Nicholas II and his family (except for Alexei and Maria)",
transformers/tests/test_modeling_tf_xlnet.py,804,"are discovered. The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich,",
transformers/tests/test_modeling_tf_xlnet.py,805,"narrates the remainder of the story. 1883 Western Siberia, a young Grigori Rasputin",
transformers/tests/test_modeling_tf_xlnet.py,806,is asked by his father and a group of men to perform magic. Rasputin has a vision and,
transformers/tests/test_modeling_tf_xlnet.py,807,denounces one of the men as a horse thief. Although his father initially slaps,
transformers/tests/test_modeling_tf_xlnet.py,808,"him for making such an accusation, Rasputin watches as the man is chased outside and beaten.",
transformers/tests/test_modeling_tf_xlnet.py,809,"Twenty years later, Rasputin sees a vision of the Virgin Mary, prompting him to become a priest.",
transformers/tests/test_modeling_tf_xlnet.py,810,"Rasputin quickly becomes famous, with people, even a bishop, begging for his blessing.",
transformers/tests/test_modeling_tf_xlnet.py,811,"<sep><cls>, Rasputin is asked to perform magic.",
transformers/tests/test_modeling_tf_xlnet.py,812,"He is not able to perform magic, and his father and",
transformers/tests/test_modeling_tf_xlnet.py,813,the men are forced to leave the monastery. Rasputin is forced to return to,
transformers/tests/test_hf_api.py,1,coding=utf-8,
transformers/tests/test_hf_api.py,2,"Copyright 2019-present, the HuggingFace Inc. team.",
transformers/tests/test_hf_api.py,3,,
transformers/tests/test_hf_api.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_hf_api.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_hf_api.py,6,You may obtain a copy of the License at,
transformers/tests/test_hf_api.py,7,,
transformers/tests/test_hf_api.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_hf_api.py,9,,
transformers/tests/test_hf_api.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_hf_api.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_hf_api.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_hf_api.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_hf_api.py,14,limitations under the License.,
transformers/tests/test_hf_api.py,35,space is intentional,
transformers/tests/test_hf_api.py,128,"^^ not an error, we test that the",
transformers/tests/test_hf_api.py,129,second call does not fail.,
transformers/tests/test_modeling_tf_transfo_xl.py,1,coding=utf-8,
transformers/tests/test_modeling_tf_transfo_xl.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_modeling_tf_transfo_xl.py,3,,
transformers/tests/test_modeling_tf_transfo_xl.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_modeling_tf_transfo_xl.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_modeling_tf_transfo_xl.py,6,You may obtain a copy of the License at,
transformers/tests/test_modeling_tf_transfo_xl.py,7,,
transformers/tests/test_modeling_tf_transfo_xl.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_modeling_tf_transfo_xl.py,9,,
transformers/tests/test_modeling_tf_transfo_xl.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_modeling_tf_transfo_xl.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_modeling_tf_transfo_xl.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_modeling_tf_transfo_xl.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_modeling_tf_transfo_xl.py,14,limitations under the License.,
transformers/tests/test_modeling_tf_transfo_xl.py,41,TODO: add this test when TFTransfoXLLMHead has a linear output layer implemented,
transformers/tests/test_modeling_tf_transfo_xl.py,369,"In 1991 , the remains of Russian Tsar Nicholas II and his family",
transformers/tests/test_modeling_tf_transfo_xl.py,370,( except for Alexei and Maria ) are discovered .,
transformers/tests/test_modeling_tf_transfo_xl.py,371,"The voice of Nicholas's young son , Tsarevich Alexei Nikolaevich , narrates the",
transformers/tests/test_modeling_tf_transfo_xl.py,372,"remainder of the story . 1883 Western Siberia ,",
transformers/tests/test_modeling_tf_transfo_xl.py,373,a young Grigori Rasputin is asked by his father and a group of men to perform magic .,
transformers/tests/test_modeling_tf_transfo_xl.py,374,Rasputin has a vision and denounces one of the men as a horse thief . Although his,
transformers/tests/test_modeling_tf_transfo_xl.py,375,"father initially slaps him for making such an accusation , Rasputin watches as the",
transformers/tests/test_modeling_tf_transfo_xl.py,376,"man is chased outside and beaten . Twenty years later , Rasputin sees a vision of",
transformers/tests/test_modeling_tf_transfo_xl.py,377,"the Virgin Mary , prompting him to become a priest . Rasputin quickly becomes famous ,",
transformers/tests/test_modeling_tf_transfo_xl.py,378,"with people , even a bishop , begging for his blessing . <eod> </s> <eos>",
transformers/tests/test_modeling_tf_transfo_xl.py,559,"In 1991, the remains of Russian Tsar Nicholas II and his family (",
transformers/tests/test_modeling_tf_transfo_xl.py,560,"except for Alexei and Maria ) are discovered. The voice of young son,",
transformers/tests/test_modeling_tf_transfo_xl.py,561,"Tsarevich Alexei Nikolaevich, narrates the remainder of the story.",
transformers/tests/test_modeling_tf_transfo_xl.py,562,"1883 Western Siberia, a young Grigori Rasputin is asked by his father",
transformers/tests/test_modeling_tf_transfo_xl.py,563,and a group of men to perform magic. Rasputin has a vision and,
transformers/tests/test_modeling_tf_transfo_xl.py,564,denounces one of the men as a horse thief. Although his father initially,
transformers/tests/test_modeling_tf_transfo_xl.py,565,"slaps him for making such an accusation, Rasputin watches as the man",
transformers/tests/test_modeling_tf_transfo_xl.py,566,"is chased outside and beaten. Twenty years later, Rasputin sees a vision",
transformers/tests/test_modeling_tf_transfo_xl.py,567,"of the Virgin Mary, prompting him to become a priest.",
transformers/tests/test_modeling_tf_transfo_xl.py,568,"Rasputin quickly becomes famous, with people, even a bishop, begging for",
transformers/tests/test_modeling_tf_transfo_xl.py,569,"his blessing. <unk> <unk> <eos> In the 1990s, the remains of Russian Tsar",
transformers/tests/test_modeling_tf_transfo_xl.py,570,"Nicholas II and his family were discovered. The voice of <unk> young son,",
transformers/tests/test_modeling_tf_transfo_xl.py,571,"Tsarevich Alexei Nikolaevich, narrates the remainder of the story.<eos>",
transformers/tests/test_tokenization_distilbert.py,1,coding=utf-8,
transformers/tests/test_tokenization_distilbert.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_tokenization_distilbert.py,3,,
transformers/tests/test_tokenization_distilbert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_tokenization_distilbert.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_tokenization_distilbert.py,6,You may obtain a copy of the License at,
transformers/tests/test_tokenization_distilbert.py,7,,
transformers/tests/test_tokenization_distilbert.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_tokenization_distilbert.py,9,,
transformers/tests/test_tokenization_distilbert.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_tokenization_distilbert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_tokenization_distilbert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_tokenization_distilbert.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_tokenization_distilbert.py,14,limitations under the License.,
transformers/tests/test_tokenization_common.py,1,coding=utf-8,
transformers/tests/test_tokenization_common.py,2,Copyright 2019 HuggingFace Inc.,
transformers/tests/test_tokenization_common.py,3,,
transformers/tests/test_tokenization_common.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_tokenization_common.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_tokenization_common.py,6,You may obtain a copy of the License at,
transformers/tests/test_tokenization_common.py,7,,
transformers/tests/test_tokenization_common.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_tokenization_common.py,9,,
transformers/tests/test_tokenization_common.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_tokenization_common.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_tokenization_common.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_tokenization_common.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_tokenization_common.py,14,limitations under the License.,
transformers/tests/test_tokenization_common.py,71,"Switch from batch_encode_plus format:   {'input_ids': [[...], [...]], ...}",
transformers/tests/test_tokenization_common.py,72,"to the concatenated encode_plus format: [{'input_ids': [...], ...}, {'input_ids': [...], ...}]",
transformers/tests/test_tokenization_common.py,101,safety check on max_len default value so we are sure the test works,
transformers/tests/test_tokenization_common.py,105,Now let's start the test,
transformers/tests/test_tokenization_common.py,149,toks before adding new_toks,
transformers/tests/test_tokenization_common.py,159,toks0 should be longer,
transformers/tests/test_tokenization_common.py,162,Check that none of the special tokens are lowercased,
transformers/tests/test_tokenization_common.py,177,Length should still be the same,
transformers/tests/test_tokenization_common.py,179,But at least the first non-special tokens should differ,
transformers/tests/test_tokenization_common.py,307,Method is implemented (e.g. not GPT-2),
transformers/tests/test_tokenization_common.py,403,Add tokens so that masked token isn't split,
transformers/tests/test_tokenization_common.py,409,Test first masked sequence,
transformers/tests/test_tokenization_common.py,416,Test second masked sequence,
transformers/tests/test_tokenization_common.py,429,Testing single inputs,
transformers/tests/test_tokenization_common.py,444,Testing inputs pairs,
transformers/tests/test_tokenization_common.py,460,Testing with already existing special tokens,
transformers/tests/test_tokenization_common.py,480,check correct behaviour if no pad_token_id exists and add it eventually,
transformers/tests/test_tokenization_common.py,485,RIGHT PADDING - Check that it correctly pads when a maximum length is specified along with the padding flag set to True,
transformers/tests/test_tokenization_common.py,494,LEFT PADDING - Check that it correctly pads when a maximum length is specified along with the padding flag set to True,
transformers/tests/test_tokenization_common.py,503,RIGHT & LEFT PADDING - Check that nothing is done when a maximum length is not specified,
transformers/tests/test_tokenization_common.py,525,check correct behaviour if no pad_token_id exists and add it eventually,
transformers/tests/test_tokenization_common.py,537,Test right padding,
transformers/tests/test_tokenization_common.py,555,Test left padding,
transformers/tests/test_tokenization_common.py,588,This tests that tokenizers don't impact others. Unfortunately the case where it fails is when,
transformers/tests/test_tokenization_common.py,589,"we're loading an S3 configuration from a pre-trained identifier, and we have no way of testing those today.",
transformers/tests/test_tokenization_common.py,618,Tests that all encoded values have the correct size,
transformers/tests/test_tokenization_common.py,634,check correct behaviour if no pad_token_id exists and add it eventually,
transformers/tests/test_tokenization_common.py,649,Test that padded sequences are equivalent between batch_encode_plus and encode_plus,
transformers/tests/test_tokenization_common.py,651,Right padding tests,
transformers/tests/test_tokenization_common.py,661,check correct behaviour if no pad_token_id exists and add it eventually,
transformers/tests/test_tokenization_common.py,672,Left padding tests,
transformers/tests/test_tokenization_common.py,684,check correct behaviour if no pad_token_id exists and add it eventually,
transformers/tests/test_tokenization_common.py,705,A Tensor cannot be build by sequences which are not the same size,
transformers/tests/test_tokenization_common.py,729,"if tokenizer does not have pad_token_id, an error should be thrown",
transformers/tests/test_tokenization_common.py,737,add pad_token_id to pass subsequent tests,
transformers/tests/test_tokenization_common.py,759,Make sure the model contains at least the full vocabulary size in its embedding matrix,
transformers/tests/test_tokenization_common.py,763,Build sequence,
transformers/tests/test_tokenization_common.py,768,This should not fail,
transformers/tests/test_tokenization_common.py,776,This should not fail,
transformers/tests/test_tokenization_common.py,799,Make sure the model contains at least the full vocabulary size in its embedding matrix,
transformers/tests/test_tokenization_common.py,802,Build sequence,
transformers/tests/test_tokenization_common.py,808,This should not fail,
transformers/tests/test_tokenization_common.py,816,This should not fail,
transformers/tests/test_tokenization_roberta.py,1,coding=utf-8,
transformers/tests/test_tokenization_roberta.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_tokenization_roberta.py,3,,
transformers/tests/test_tokenization_roberta.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_tokenization_roberta.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_tokenization_roberta.py,6,You may obtain a copy of the License at,
transformers/tests/test_tokenization_roberta.py,7,,
transformers/tests/test_tokenization_roberta.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_tokenization_roberta.py,9,,
transformers/tests/test_tokenization_roberta.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_tokenization_roberta.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_tokenization_roberta.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_tokenization_roberta.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_tokenization_roberta.py,14,limitations under the License.,
transformers/tests/test_tokenization_roberta.py,33,Adapted from Sennrich et al. 2015 and https://github.com/rsennrich/subword-nmt,
transformers/tests/test_tokenization_roberta.py,120,Testing encoder arguments,
transformers/tests/test_tokenization_roberta.py,134,Testing spaces after special tokenss,
transformers/tests/test_modeling_tf_common.py,1,coding=utf-8,
transformers/tests/test_modeling_tf_common.py,2,Copyright 2019 HuggingFace Inc.,
transformers/tests/test_modeling_tf_common.py,3,,
transformers/tests/test_modeling_tf_common.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_modeling_tf_common.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_modeling_tf_common.py,6,You may obtain a copy of the License at,
transformers/tests/test_modeling_tf_common.py,7,,
transformers/tests/test_modeling_tf_common.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_modeling_tf_common.py,9,,
transformers/tests/test_modeling_tf_common.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_modeling_tf_common.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_modeling_tf_common.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_modeling_tf_common.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_modeling_tf_common.py,14,limitations under the License.,
transformers/tests/test_modeling_tf_common.py,38,Restrict TensorFlow to only allocate x GB of memory on the GPUs,
transformers/tests/test_modeling_tf_common.py,46,Virtual devices must be set before GPUs have been initialized,
transformers/tests/test_modeling_tf_common.py,71,"config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()",
transformers/tests/test_modeling_tf_common.py,73,configs_no_init = _config_zero_init(config),
transformers/tests/test_modeling_tf_common.py,74,for model_class in self.all_model_classes:,
transformers/tests/test_modeling_tf_common.py,75,model = model_class(config=configs_no_init),
transformers/tests/test_modeling_tf_common.py,76,"for name, param in model.named_parameters():",
transformers/tests/test_modeling_tf_common.py,77,if param.requires_grad:,
transformers/tests/test_modeling_tf_common.py,78,"self.assertIn(param.data.mean().item(), [0.0, 1.0],",
transformers/tests/test_modeling_tf_common.py,79,"msg=""Parameter {} of model {} seems not properly initialized"".format(name, model_class))",
transformers/tests/test_modeling_tf_common.py,128,Make sure we don't have nans,
transformers/tests/test_modeling_tf_common.py,147,"Skip the ""TF"" at the beggining",
transformers/tests/test_modeling_tf_common.py,155,Check we can load pt model in tf and vice-versa with model => model functions,
transformers/tests/test_modeling_tf_common.py,160,Check predictions on first output (logits/hidden-states) are close enought given low-level computational differences,
transformers/tests/test_modeling_tf_common.py,165,"need to rename encoder-decoder ""inputs"" for PyTorch",
transformers/tests/test_modeling_tf_common.py,184,Debug info (remove when fixed),
transformers/tests/test_modeling_tf_common.py,193,Check we can load pt model in tf and vice-versa with checkpoint => model functions,
transformers/tests/test_modeling_tf_common.py,203,Check predictions on first output (logits/hidden-states) are close enought given low-level computational differences,
transformers/tests/test_modeling_tf_common.py,208,"need to rename encoder-decoder ""inputs"" for PyTorch",
transformers/tests/test_modeling_tf_common.py,243,Prepare our model,
transformers/tests/test_modeling_tf_common.py,246,Let's load it from the disk to be sure we can use pretrained weights,
transformers/tests/test_modeling_tf_common.py,248,build the model,
transformers/tests/test_modeling_tf_common.py,255,Add a dense layer on top to test intetgration with other keras modules,
transformers/tests/test_modeling_tf_common.py,258,Compile extended model,
transformers/tests/test_modeling_tf_common.py,324,Check attention is always last and order is fine,
transformers/tests/test_modeling_tf_common.py,382,"^^ In our TF models, the input_embeddings can take slightly different forms,",
transformers/tests/test_modeling_tf_common.py,383,so we try a few of them.,
transformers/tests/test_modeling_tf_common.py,384,We used to fall back to just synthetically creating a dummy tensor of ones:,
transformers/tests/test_modeling_tf_common.py,427,iterate over all generative models,
transformers/tests/test_modeling_tf_common.py,432,if bos token id is not defined mobel needs input_ids,
transformers/tests/test_modeling_tf_common.py,435,num_return_sequences = 1,
transformers/tests/test_modeling_tf_common.py,438,num_return_sequences = 1,
transformers/tests/test_modeling_tf_common.py,442,generating multiple sequences when no beam search generation,
transformers/tests/test_modeling_tf_common.py,443,is not allowed as it would always generate the same sequences,
transformers/tests/test_modeling_tf_common.py,446,"num_return_sequences > 1, sample",
transformers/tests/test_modeling_tf_common.py,449,check bad words tokens language generation,
transformers/tests/test_modeling_tf_common.py,450,create list of 1-seq bad token and list of 2-seq of bad tokens,
transformers/tests/test_modeling_tf_common.py,455,only count generated tokens,
transformers/tests/test_modeling_tf_common.py,467,"if bos token id is not defined mobel needs input_ids, num_return_sequences = 1",
transformers/tests/test_modeling_tf_common.py,470,num_return_sequences = 1,
transformers/tests/test_modeling_tf_common.py,474,generating more sequences than having beams leads is not possible,
transformers/tests/test_modeling_tf_common.py,477,"num_return_sequences > 1, sample",
transformers/tests/test_modeling_tf_common.py,479,"num_return_sequences > 1, greedy",
transformers/tests/test_modeling_tf_common.py,482,check bad words tokens language generation,
transformers/tests/test_modeling_tf_common.py,483,create list of 1-seq bad token and list of 2-seq of bad tokens,
transformers/tests/test_modeling_tf_common.py,488,only count generated tokens,
transformers/tests/test_modeling_tf_common.py,493,special tokens cannot be bad tokens,
transformers/tests/test_modeling_tf_common.py,502,create random bad tokens that are not special tokens,
transformers/tests/test_modeling_tf_common.py,516,for all bad word tokens,
transformers/tests/test_modeling_tf_common.py,518,for all slices in batch,
transformers/tests/test_modeling_tf_common.py,520,for all word idx,
transformers/tests/test_modeling_tf_common.py,522,if tokens match,
transformers/tests/test_modeling_tf_common.py,549,tests whether the top_k_top_p_filtering function behaves as expected,
transformers/tests/test_modeling_tf_common.py,554,3rd highest value; idx. 0,
transformers/tests/test_modeling_tf_common.py,563,5th highest value; idx. 9,
transformers/tests/test_modeling_tf_common.py,564,2nd highest value; idx. 10,
transformers/tests/test_modeling_tf_common.py,579,4th highest value; idx. 25,
transformers/tests/test_modeling_tf_common.py,580,1st highest value; idx. 26,
transformers/tests/test_modeling_tf_common.py,584,cummulative prob of 5 highest values <= 0.6,
transformers/tests/test_modeling_tf_common.py,599,4th highest value; idx. 13,
transformers/tests/test_modeling_tf_common.py,603,2nd highest value; idx. 17,
transformers/tests/test_modeling_tf_common.py,604,5th highest value; idx. 18,
transformers/tests/test_modeling_tf_common.py,606,3rd highest value; idx. 20,
transformers/tests/test_modeling_tf_common.py,613,1st highest value; idx. 27,
transformers/tests/test_modeling_tf_common.py,616,cummulative prob of 5 highest values <= 0.6,
transformers/tests/test_modeling_tf_common.py,623,expected non filtered idx as noted above,
transformers/tests/test_modeling_tf_common.py,628,expected non filtered values as noted above,
transformers/tests/test_tokenization_utils.py,1,coding=utf-8,
transformers/tests/test_tokenization_utils.py,2,Copyright 2018 HuggingFace Inc..,
transformers/tests/test_tokenization_utils.py,3,,
transformers/tests/test_tokenization_utils.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_tokenization_utils.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_tokenization_utils.py,6,You may obtain a copy of the License at,
transformers/tests/test_tokenization_utils.py,7,,
transformers/tests/test_tokenization_utils.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_tokenization_utils.py,9,,
transformers/tests/test_tokenization_utils.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_tokenization_utils.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_tokenization_utils.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_tokenization_utils.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_tokenization_utils.py,14,limitations under the License.,
transformers/tests/test_modeling_xlm_roberta.py,1,coding=utf-8,
transformers/tests/test_modeling_xlm_roberta.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_modeling_xlm_roberta.py,3,,
transformers/tests/test_modeling_xlm_roberta.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_modeling_xlm_roberta.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_modeling_xlm_roberta.py,6,You may obtain a copy of the License at,
transformers/tests/test_modeling_xlm_roberta.py,7,,
transformers/tests/test_modeling_xlm_roberta.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_modeling_xlm_roberta.py,9,,
transformers/tests/test_modeling_xlm_roberta.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_modeling_xlm_roberta.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_modeling_xlm_roberta.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_modeling_xlm_roberta.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_modeling_xlm_roberta.py,14,limitations under the License.,
transformers/tests/test_modeling_xlm_roberta.py,34,The dog is cute and lives in the garden house,
transformers/tests/test_modeling_xlm_roberta.py,36,"batch_size, sequence_length, embedding_vector_dim",
transformers/tests/test_modeling_xlm_roberta.py,40,"xlmr = torch.hub.load('pytorch/fairseq', 'xlmr.base')",
transformers/tests/test_modeling_xlm_roberta.py,41,xlmr.eval(),
transformers/tests/test_modeling_xlm_roberta.py,42,"expected_output_values_last_dim = xlmr.extract_features(input_ids[0])[:, :, -1]",
transformers/tests/test_modeling_xlm_roberta.py,46,compare the actual values for a slice of last dim,
transformers/tests/test_modeling_xlm_roberta.py,53,The dog is cute and lives in the garden house,
transformers/tests/test_modeling_xlm_roberta.py,55,"batch_size, sequence_length, embedding_vector_dim",
transformers/tests/test_modeling_xlm_roberta.py,59,"xlmr = torch.hub.load('pytorch/fairseq', 'xlmr.large')",
transformers/tests/test_modeling_xlm_roberta.py,60,xlmr.eval(),
transformers/tests/test_modeling_xlm_roberta.py,61,"expected_output_values_last_dim = xlmr.extract_features(input_ids[0])[:, :, -1]",
transformers/tests/test_modeling_xlm_roberta.py,65,compare the actual values for a slice of last dim,
transformers/tests/test_tokenization_gpt2.py,1,coding=utf-8,
transformers/tests/test_tokenization_gpt2.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_tokenization_gpt2.py,3,,
transformers/tests/test_tokenization_gpt2.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_tokenization_gpt2.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_tokenization_gpt2.py,6,You may obtain a copy of the License at,
transformers/tests/test_tokenization_gpt2.py,7,,
transformers/tests/test_tokenization_gpt2.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_tokenization_gpt2.py,9,,
transformers/tests/test_tokenization_gpt2.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_tokenization_gpt2.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_tokenization_gpt2.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_tokenization_gpt2.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_tokenization_gpt2.py,14,limitations under the License.,
transformers/tests/test_tokenization_gpt2.py,34,Adapted from Sennrich et al. 2015 and https://github.com/rsennrich/subword-nmt,
transformers/tests/test_tokenization_gpt2.py,101,Testing tokenization,
transformers/tests/test_tokenization_gpt2.py,106,Testing conversion to ids without special tokens,
transformers/tests/test_tokenization_gpt2.py,111,Testing conversion to ids with special tokens,
transformers/tests/test_tokenization_gpt2.py,117,Testing the unknown token,
transformers/tests/test_tokenization_fast.py,49,Tokenizer.filter makes it possible to filter which Tokenizer to case based on all the,
transformers/tests/test_tokenization_fast.py,50,"information available in Tokenizer (name, rust class, python class, vocab key name)",
transformers/tests/test_tokenization_fast.py,62,Check is_fast is set correctly,
transformers/tests/test_tokenization_fast.py,66,Check that Rust and Python align,
transformers/tests/test_tokenization_fast.py,73,TODO: enable for v3.0.0,
transformers/tests/test_tokenization_fast.py,74,"self.assert_empty_output_no_special_tokens(tokenizer_r, tokenizer_p)",
transformers/tests/test_tokenization_fast.py,77,Ensure None raise an error,
transformers/tests/test_tokenization_fast.py,88,Ensure basic input match,
transformers/tests/test_tokenization_fast.py,101,Ensure truncation match,
transformers/tests/test_tokenization_fast.py,108,Ensure truncation with stride match,
transformers/tests/test_tokenization_fast.py,116,Check we have the same number of added_tokens for both pair and non-pair inputs.,
transformers/tests/test_tokenization_fast.py,121,Check we have the correct max_length for both pair and non-pair inputs.,
transformers/tests/test_tokenization_fast.py,126,Assert the set of special tokens match.,
transformers/tests/test_tokenization_fast.py,152,No pair,
transformers/tests/test_tokenization_fast.py,159,Assert there is the same number of tokens and offsets,
transformers/tests/test_tokenization_fast.py,162,Assert there is online added_tokens special_tokens,
transformers/tests/test_tokenization_fast.py,165,Pairs,
transformers/tests/test_tokenization_fast.py,172,Assert there is the same number of tokens and offsets,
transformers/tests/test_tokenization_fast.py,175,Assert there is online added_tokens special_tokens,
transformers/tests/test_tokenization_fast.py,201,Mono sample,
transformers/tests/test_tokenization_fast.py,214,Multi sample,
transformers/tests/test_tokenization_fast.py,228,Input string,
transformers/tests/test_tokenization_fast.py,232,Generate output,
transformers/tests/test_tokenization_fast.py,237,Generate pair output,
transformers/tests/test_tokenization_fast.py,242,Input tokens id,
transformers/tests/test_tokenization_fast.py,246,Generate output,
transformers/tests/test_tokenization_fast.py,251,Generate pair output,
transformers/tests/test_tokenization_fast.py,259,Ensure we match max_length,
transformers/tests/test_tokenization_fast.py,262,Ensure the number of padded tokens is the same,
transformers/tests/test_tokenization_fast.py,278,Simple input,
transformers/tests/test_tokenization_fast.py,283,Pair input,
transformers/tests/test_tokenization_fast.py,292,Simple input,
transformers/tests/test_tokenization_fast.py,298,Pair input,
transformers/tests/test_tokenization_fast.py,308,Simple input,
transformers/tests/test_tokenization_fast.py,309,TODO: Re-enable this test when batch_encode_plus with padding correctly handles padding,
transformers/tests/test_tokenization_fast.py,318,Pair input,
transformers/tests/test_tokenization_fast.py,319,TODO: Re-enable this test when batch_encode_plus with padding correctly handles padding,
transformers/tests/test_tokenization_fast.py,339,Checks it save with the same files,
transformers/tests/test_tokenization_fast.py,342,Checks everything loads correctly in the same way,
transformers/tests/test_tokenization_fast.py,345,Check special tokens are set accordingly on Rust and Python,
transformers/tests/test_tokenization_fast.py,348,"self.assertEqual(getattr(tokenizer_rp, key), getattr(tokenizer_pp, key))",
transformers/tests/test_tokenization_fast.py,349,"self.assertEqual(getattr(tokenizer_rp, key + ""_id""), getattr(tokenizer_pp, key + ""_id""))",
transformers/tests/test_tokenization_fast.py,372,pair_num_special_tokens_to_add = tokenizer_r.num_special_tokens_to_add(pair=True),
transformers/tests/test_tokenization_fast.py,375,tokenize(),
transformers/tests/test_tokenization_fast.py,380,encode(),
transformers/tests/test_tokenization_fast.py,385,encode_plus(),
transformers/tests/test_tokenization_fast.py,393,# batch_encode_plus,
transformers/tests/test_tokenization_fast.py,433,BERT normalizes this away,
transformers/tests/test_tokenization_fast.py,434,Append MASK here after lower-casing,
transformers/tests/test_tokenization_fast.py,442,Check if the tokenizer is uncased,
transformers/tests/test_tokenization_fast.py,446,Append the special tokens,
transformers/tests/test_tokenization_fast.py,452,"self.assertEqual([e[0] for e in expected_results], tokens[""offset_mapping""])",
transformers/tests/test_tokenization_fast.py,465,Rust correctly handles the space before the mask while python doesnt,
transformers/tests/test_tokenization_fast.py,469,token_type_ids should put 0 everywhere,
transformers/tests/test_tokenization_fast.py,472,"attention_mask should put 1 everywhere, so sum over length should be 1",
transformers/tests/test_tokenization_fast.py,478,Rust should have 'Ġ' before <mask> which should be left as an entire token,
transformers/tests/test_tokenization_fast.py,490,Simple input,
transformers/tests/test_tokenization_fast.py,499,Simple input tests,
transformers/tests/test_tokenization_fast.py,502,Simple input,
transformers/tests/test_tokenization_fast.py,505,Simple input,
transformers/tests/test_tokenization_fast.py,508,Pair input,
transformers/tests/test_tokenization_fast.py,511,Pair input,
transformers/tests/test_tokenization_fast.py,514,Pair input,
transformers/tests/test_tokenization_albert.py,1,coding=utf-8,
transformers/tests/test_tokenization_albert.py,2,Copyright 2019 Hugging Face inc.,
transformers/tests/test_tokenization_albert.py,3,,
transformers/tests/test_tokenization_albert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_tokenization_albert.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_tokenization_albert.py,6,You may obtain a copy of the License at,
transformers/tests/test_tokenization_albert.py,7,,
transformers/tests/test_tokenization_albert.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_tokenization_albert.py,9,,
transformers/tests/test_tokenization_albert.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_tokenization_albert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_tokenization_albert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_tokenization_albert.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_tokenization_albert.py,14,limitations under the License.,
transformers/tests/test_tokenization_albert.py,35,We have a SentencePiece fixture for testing,
transformers/tests/test_modeling_electra.py,1,coding=utf-8,
transformers/tests/test_modeling_electra.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_modeling_electra.py,3,,
transformers/tests/test_modeling_electra.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_modeling_electra.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_modeling_electra.py,6,You may obtain a copy of the License at,
transformers/tests/test_modeling_electra.py,7,,
transformers/tests/test_modeling_electra.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_modeling_electra.py,9,,
transformers/tests/test_modeling_electra.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_modeling_electra.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_modeling_electra.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_modeling_electra.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_modeling_electra.py,14,limitations under the License.,
transformers/tests/test_tokenization_bert_japanese.py,1,coding=utf-8,
transformers/tests/test_tokenization_bert_japanese.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_tokenization_bert_japanese.py,3,,
transformers/tests/test_tokenization_bert_japanese.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_tokenization_bert_japanese.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_tokenization_bert_japanese.py,6,You may obtain a copy of the License at,
transformers/tests/test_tokenization_bert_japanese.py,7,,
transformers/tests/test_tokenization_bert_japanese.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_tokenization_bert_japanese.py,9,,
transformers/tests/test_tokenization_bert_japanese.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_tokenization_bert_japanese.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_tokenization_bert_japanese.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_tokenization_bert_japanese.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_tokenization_bert_japanese.py,14,limitations under the License.,
transformers/tests/test_tokenization_bert_japanese.py,100,"if dict doesn't exist in the system, previous code raises this error.",
transformers/tests/test_tokenization_bert_japanese.py,142,"2 is for ""[CLS]"", 3 is for ""[SEP]""",
transformers/tests/test_tokenization_bert_japanese.py,203,"2 is for ""[CLS]"", 3 is for ""[SEP]""",
transformers/tests/test_modeling_transfo_xl.py,1,coding=utf-8,
transformers/tests/test_modeling_transfo_xl.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_modeling_transfo_xl.py,3,,
transformers/tests/test_modeling_transfo_xl.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_modeling_transfo_xl.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_modeling_transfo_xl.py,6,You may obtain a copy of the License at,
transformers/tests/test_modeling_transfo_xl.py,7,,
transformers/tests/test_modeling_transfo_xl.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_modeling_transfo_xl.py,9,,
transformers/tests/test_modeling_transfo_xl.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_modeling_transfo_xl.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_modeling_transfo_xl.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_modeling_transfo_xl.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_modeling_transfo_xl.py,14,limitations under the License.,
transformers/tests/test_modeling_transfo_xl.py,370,"In 1991 , the remains of Russian Tsar Nicholas II and his family",
transformers/tests/test_modeling_transfo_xl.py,371,( except for Alexei and Maria ) are discovered .,
transformers/tests/test_modeling_transfo_xl.py,372,"The voice of Nicholas's young son , Tsarevich Alexei Nikolaevich , narrates the",
transformers/tests/test_modeling_transfo_xl.py,373,"remainder of the story . 1883 Western Siberia ,",
transformers/tests/test_modeling_transfo_xl.py,374,a young Grigori Rasputin is asked by his father and a group of men to perform magic .,
transformers/tests/test_modeling_transfo_xl.py,375,Rasputin has a vision and denounces one of the men as a horse thief . Although his,
transformers/tests/test_modeling_transfo_xl.py,376,"father initially slaps him for making such an accusation , Rasputin watches as the",
transformers/tests/test_modeling_transfo_xl.py,377,"man is chased outside and beaten . Twenty years later , Rasputin sees a vision of",
transformers/tests/test_modeling_transfo_xl.py,378,"the Virgin Mary , prompting him to become a priest . Rasputin quickly becomes famous ,",
transformers/tests/test_modeling_transfo_xl.py,380,"with people , even a bishop , begging for his blessing . <eod> </s> <eos>",
transformers/tests/test_modeling_transfo_xl.py,561,"In 1991, the remains of Russian Tsar Nicholas II and his family (",
transformers/tests/test_modeling_transfo_xl.py,562,"except for Alexei and Maria ) are discovered. The voice of young son,",
transformers/tests/test_modeling_transfo_xl.py,563,"Tsarevich Alexei Nikolaevich, narrates the remainder of the story.",
transformers/tests/test_modeling_transfo_xl.py,564,"1883 Western Siberia, a young Grigori Rasputin is asked by his father",
transformers/tests/test_modeling_transfo_xl.py,565,and a group of men to perform magic. Rasputin has a vision and,
transformers/tests/test_modeling_transfo_xl.py,566,denounces one of the men as a horse thief. Although his father initially,
transformers/tests/test_modeling_transfo_xl.py,567,"slaps him for making such an accusation, Rasputin watches as the man",
transformers/tests/test_modeling_transfo_xl.py,568,"is chased outside and beaten. Twenty years later, Rasputin sees a vision",
transformers/tests/test_modeling_transfo_xl.py,569,"of the Virgin Mary, prompting him to become a priest.",
transformers/tests/test_modeling_transfo_xl.py,570,"Rasputin quickly becomes famous, with people, even a bishop, begging for",
transformers/tests/test_modeling_transfo_xl.py,571,"his blessing. <unk> <unk> <eos> In the 1990s, the remains of Russian Tsar",
transformers/tests/test_modeling_transfo_xl.py,572,"Nicholas II and his family were discovered. The voice of <unk> young son,",
transformers/tests/test_modeling_transfo_xl.py,573,"Tsarevich Alexei Nikolaevich, narrates the remainder of the story.<eos>",
transformers/tests/utils.py,13,"Used to test Auto{Config, Model, Tokenizer} model_type detection.",
transformers/tests/utils.py,20,"KEY isn't set, default to `default`.",
transformers/tests/utils.py,23,"KEY is set, convert it to True or False.",
transformers/tests/utils.py,27,"More values are supported, but let's keep the message simple.",
transformers/tests/utils.py,101,Set the USE_CUDA environment variable to select a GPU.,
transformers/tests/test_modeling_roberta.py,1,coding=utf-8,
transformers/tests/test_modeling_roberta.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_modeling_roberta.py,3,,
transformers/tests/test_modeling_roberta.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_modeling_roberta.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_modeling_roberta.py,6,You may obtain a copy of the License at,
transformers/tests/test_modeling_roberta.py,7,,
transformers/tests/test_modeling_roberta.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_modeling_roberta.py,9,,
transformers/tests/test_modeling_roberta.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_modeling_roberta.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_modeling_roberta.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_modeling_roberta.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_modeling_roberta.py,14,limitations under the License.,
transformers/tests/test_modeling_roberta.py,331,compare the actual values for a slice.,
transformers/tests/test_modeling_roberta.py,336,"roberta = torch.hub.load('pytorch/fairseq', 'roberta.base')",
transformers/tests/test_modeling_roberta.py,337,roberta.eval(),
transformers/tests/test_modeling_roberta.py,338,"expected_slice = roberta.model.forward(input_ids)[0][:, :3, :3].detach()",
transformers/tests/test_modeling_roberta.py,348,compare the actual values for a slice.,
transformers/tests/test_modeling_roberta.py,353,"roberta = torch.hub.load('pytorch/fairseq', 'roberta.base')",
transformers/tests/test_modeling_roberta.py,354,roberta.eval(),
transformers/tests/test_modeling_roberta.py,355,"expected_slice = roberta.extract_features(input_ids)[:, :3, :3].detach()",
transformers/tests/test_modeling_roberta.py,369,"roberta = torch.hub.load('pytorch/fairseq', 'roberta.large.mnli')",
transformers/tests/test_modeling_roberta.py,370,roberta.eval(),
transformers/tests/test_modeling_roberta.py,371,"expected_tensor = roberta.predict(""mnli"", input_ids, return_logits=True).detach()",
transformers/tests/test_modeling_tf_camembert.py,1,coding=utf-8,
transformers/tests/test_modeling_tf_camembert.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_modeling_tf_camembert.py,3,,
transformers/tests/test_modeling_tf_camembert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_modeling_tf_camembert.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_modeling_tf_camembert.py,6,You may obtain a copy of the License at,
transformers/tests/test_modeling_tf_camembert.py,7,,
transformers/tests/test_modeling_tf_camembert.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_modeling_tf_camembert.py,9,,
transformers/tests/test_modeling_tf_camembert.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_modeling_tf_camembert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_modeling_tf_camembert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_modeling_tf_camembert.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_modeling_tf_camembert.py,14,limitations under the License.,
transformers/tests/test_modeling_tf_camembert.py,37,"J'aime le camembert !""",
transformers/tests/test_modeling_tf_camembert.py,42,compare the actual values for a slice.,
transformers/tests/test_modeling_tf_camembert.py,46,"camembert = torch.hub.load('pytorch/fairseq', 'camembert.v0')",
transformers/tests/test_modeling_tf_camembert.py,47,camembert.eval(),
transformers/tests/test_modeling_tf_camembert.py,48,"expected_slice = roberta.model.forward(input_ids)[0][:, :3, :3].detach()",
transformers/tests/test_modeling_tf_openai_gpt.py,1,coding=utf-8,
transformers/tests/test_modeling_tf_openai_gpt.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_modeling_tf_openai_gpt.py,3,,
transformers/tests/test_modeling_tf_openai_gpt.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_modeling_tf_openai_gpt.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_modeling_tf_openai_gpt.py,6,You may obtain a copy of the License at,
transformers/tests/test_modeling_tf_openai_gpt.py,7,,
transformers/tests/test_modeling_tf_openai_gpt.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_modeling_tf_openai_gpt.py,9,,
transformers/tests/test_modeling_tf_openai_gpt.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_modeling_tf_openai_gpt.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_modeling_tf_openai_gpt.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_modeling_tf_openai_gpt.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_modeling_tf_openai_gpt.py,14,limitations under the License.,
transformers/tests/test_modeling_tf_openai_gpt.py,44,TODO (PVP): Add Double HeadsModel when generate() function is changed accordingly,
transformers/tests/test_modeling_tf_openai_gpt.py,125,"intermediate_size=self.intermediate_size,",
transformers/tests/test_modeling_tf_openai_gpt.py,126,"hidden_act=self.hidden_act,",
transformers/tests/test_modeling_tf_openai_gpt.py,127,"hidden_dropout_prob=self.hidden_dropout_prob,",
transformers/tests/test_modeling_tf_openai_gpt.py,128,"attention_probs_dropout_prob=self.attention_probs_dropout_prob,",
transformers/tests/test_modeling_tf_openai_gpt.py,131,"type_vocab_size=self.type_vocab_size,",
transformers/tests/test_modeling_tf_openai_gpt.py,132,initializer_range=self.initializer_range,
transformers/tests/test_modeling_tf_openai_gpt.py,247,the president is,
transformers/tests/test_modeling_tf_openai_gpt.py,269,"the president is a very good man. "" \n "" i\'m sure he is, "" said the",
transformers/tests/test_model_card.py,1,coding=utf-8,
transformers/tests/test_model_card.py,2,Copyright 2019 HuggingFace Inc.,
transformers/tests/test_model_card.py,3,,
transformers/tests/test_model_card.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_model_card.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_model_card.py,6,You may obtain a copy of the License at,
transformers/tests/test_model_card.py,7,,
transformers/tests/test_model_card.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_model_card.py,9,,
transformers/tests/test_model_card.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_model_card.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_model_card.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_model_card.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_model_card.py,14,limitations under the License.,
transformers/tests/test_modeling_bart.py,1,coding=utf-8,
transformers/tests/test_modeling_bart.py,2,Copyright 2020 Huggingface,
transformers/tests/test_modeling_bart.py,3,,
transformers/tests/test_modeling_bart.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_modeling_bart.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_modeling_bart.py,6,You may obtain a copy of the License at,
transformers/tests/test_modeling_bart.py,7,,
transformers/tests/test_modeling_bart.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_modeling_bart.py,9,,
transformers/tests/test_modeling_bart.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_modeling_bart.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_modeling_bart.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_modeling_bart.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_modeling_bart.py,14,limitations under the License.,
transformers/tests/test_modeling_bart.py,74,Eos Token,
transformers/tests/test_modeling_bart.py,114,TODO(SS): fix the below in a separate PR,
transformers/tests/test_modeling_bart.py,118,This requires inputs_dict['input_ids'],
transformers/tests/test_modeling_bart.py,119,because BartForConditionalGeneration and BartModel now have identical state_dict,
transformers/tests/test_modeling_bart.py,129,"(config, input_ids, token_type_ids, input_mask, *unused) = \",
transformers/tests/test_modeling_bart.py,134,test init,
transformers/tests/test_modeling_bart.py,161,no hidden states or attentions,
transformers/tests/test_modeling_bart.py,165,some tokens were masked,
transformers/tests/test_modeling_bart.py,168,Test different encoder attention masks,
transformers/tests/test_modeling_bart.py,190,same vocab size,
transformers/tests/test_modeling_bart.py,191,same tokenizer,
transformers/tests/test_modeling_bart.py,248,"example_english_phrase = "" UN Chief Says There Is No Military Solution in Syria""",
transformers/tests/test_modeling_bart.py,249,"inputs: dict = tokenizer.batch_encode_plus([example_english_phrase], return_tensors=""pt"",)",
transformers/tests/test_modeling_bart.py,254,250004,
transformers/tests/test_modeling_bart.py,281,"TODO(SS): should be  [8274, ..., 2, 250020]",
transformers/tests/test_modeling_bart.py,317,note padding,
transformers/tests/test_modeling_bart.py,417,"TODO(SS): uneven length batches, empty inputs",
transformers/tests/test_modeling_bart.py,431,need leading spaces for equality,
transformers/tests/test_modeling_bart.py,474,"never attend to the final token, because its pad",
transformers/tests/test_modeling_bart.py,542,eval called in from_pre,
transformers/tests/test_modeling_bart.py,544,Test that model hasn't changed,
transformers/tests/test_modeling_bart.py,552,Test that padding does not change results,
transformers/tests/test_modeling_bart.py,563,Forces 1.6GB download from S3 for each model,
transformers/tests/test_modeling_bart.py,606,@noqa,
transformers/tests/test_modeling_bart.py,612,The below article tests that we don't add any hypotheses outside of the top n_beams,
transformers/tests/test_modeling_bart.py,651,"TODO(SS): run fairseq again with num_beams=2, min_len=20.",
transformers/tests/test_modeling_bart.py,652,TODO(SS): add test case that hits max_length,
transformers/tests/test_tokenization_t5.py,1,coding=utf-8,
transformers/tests/test_tokenization_t5.py,2,Copyright 2018 Google T5 Authors and HuggingFace Inc. team.,
transformers/tests/test_tokenization_t5.py,3,,
transformers/tests/test_tokenization_t5.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_tokenization_t5.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_tokenization_t5.py,6,You may obtain a copy of the License at,
transformers/tests/test_tokenization_t5.py,7,,
transformers/tests/test_tokenization_t5.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_tokenization_t5.py,9,,
transformers/tests/test_tokenization_t5.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_tokenization_t5.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_tokenization_t5.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_tokenization_t5.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_tokenization_t5.py,14,limitations under the License.,
transformers/tests/test_tokenization_t5.py,36,We have a SentencePiece fixture for testing,
transformers/tests/test_modeling_common.py,1,coding=utf-8,
transformers/tests/test_modeling_common.py,2,Copyright 2019 HuggingFace Inc.,
transformers/tests/test_modeling_common.py,3,,
transformers/tests/test_modeling_common.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_modeling_common.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_modeling_common.py,6,You may obtain a copy of the License at,
transformers/tests/test_modeling_common.py,7,,
transformers/tests/test_modeling_common.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_modeling_common.py,9,,
transformers/tests/test_modeling_common.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_modeling_common.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_modeling_common.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_modeling_common.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_modeling_common.py,14,limitations under the License.,
transformers/tests/test_modeling_common.py,83,Make sure we don't have nans,
transformers/tests/test_modeling_common.py,150,loss will come first,
transformers/tests/test_modeling_common.py,151,compute loss,
transformers/tests/test_modeling_common.py,163,Check attention is always last and order is fine,
transformers/tests/test_modeling_common.py,203,To be sure we have no Nan,
transformers/tests/test_modeling_common.py,209,Let's keep only input_ids,
transformers/tests/test_modeling_common.py,258,To be sure we have no Nan,
transformers/tests/test_modeling_common.py,264,Prepare head_mask,
transformers/tests/test_modeling_common.py,265,Set require_grad after having prepared the tensor to avoid error (leaf variable has been moved into the graph interior),
transformers/tests/test_modeling_common.py,277,Test that we can get a gradient back for importance score computation,
transformers/tests/test_modeling_common.py,285,Remove Nan,
transformers/tests/test_modeling_common.py,289,Check we don't have more than 25% nans (arbitrary),
transformers/tests/test_modeling_common.py,292,remove them (the test is less complete),
transformers/tests/test_modeling_common.py,488,Retrieve the embeddings and clone theme,
transformers/tests/test_modeling_common.py,492,Check that resizing the token embeddings with a larger vocab size increases the model's vocab size,
transformers/tests/test_modeling_common.py,495,Check that it actually resizes the embeddings matrix,
transformers/tests/test_modeling_common.py,497,Check that the model can still do a forward pass successfully (every parameter should be resized),
transformers/tests/test_modeling_common.py,500,Check that resizing the token embeddings with a smaller vocab size decreases the model's vocab size,
transformers/tests/test_modeling_common.py,503,Check that it actually resizes the embeddings matrix,
transformers/tests/test_modeling_common.py,506,Check that the model can still do a forward pass successfully (every parameter should be resized),
transformers/tests/test_modeling_common.py,507,Input ids should be clamped to the maximum size of the vocabulary,
transformers/tests/test_modeling_common.py,511,Check that adding and removing tokens has not modified the first part of the embedding matrix.,
transformers/tests/test_modeling_common.py,572,Check that the embedding layer and decoding layer are the same in size and in value,
transformers/tests/test_modeling_common.py,574,"self.assertTrue(check_same_values(embeddings, decoding))",
transformers/tests/test_modeling_common.py,576,"# Check that after modification, they remain the same.",
transformers/tests/test_modeling_common.py,577,embeddings.weight.data.div_(2),
transformers/tests/test_modeling_common.py,578,# Check that the embedding layer and decoding layer are the same in size and in value,
transformers/tests/test_modeling_common.py,579,"self.assertTrue(embeddings.weight.shape, decoding.weight.shape)",
transformers/tests/test_modeling_common.py,580,"self.assertTrue(check_same_values(embeddings, decoding))",
transformers/tests/test_modeling_common.py,582,"# Check that after modification, they remain the same.",
transformers/tests/test_modeling_common.py,583,decoding.weight.data.div_(4),
transformers/tests/test_modeling_common.py,584,# Check that the embedding layer and decoding layer are the same in size and in value,
transformers/tests/test_modeling_common.py,585,"self.assertTrue(embeddings.weight.shape, decoding.weight.shape)",
transformers/tests/test_modeling_common.py,586,"self.assertTrue(check_same_values(embeddings, decoding))",
transformers/tests/test_modeling_common.py,588,Check that after resize they remain tied.,
transformers/tests/test_modeling_common.py,594,decoding.weight.data.mul_(20),
transformers/tests/test_modeling_common.py,595,# Check that the embedding layer and decoding layer are the same in size and in value,
transformers/tests/test_modeling_common.py,596,"self.assertTrue(model.transformer.wte.weight.shape, model.lm_head.weight.shape)",
transformers/tests/test_modeling_common.py,597,"self.assertTrue(check_same_values(model.transformer.wte, model.lm_head))",
transformers/tests/test_modeling_common.py,630,iterate over all generative models,
transformers/tests/test_modeling_common.py,635,if bos token id is not defined mobel needs input_ids,
transformers/tests/test_modeling_common.py,638,num_return_sequences = 1,
transformers/tests/test_modeling_common.py,641,num_return_sequences = 1,
transformers/tests/test_modeling_common.py,645,generating multiple sequences when no beam search generation,
transformers/tests/test_modeling_common.py,646,is not allowed as it would always generate the same sequences,
transformers/tests/test_modeling_common.py,649,"num_return_sequences > 1, sample",
transformers/tests/test_modeling_common.py,652,check bad words tokens language generation,
transformers/tests/test_modeling_common.py,653,create list of 1-seq bad token and list of 2-seq of bad tokens,
transformers/tests/test_modeling_common.py,658,only count generated tokens,
transformers/tests/test_modeling_common.py,670,"if bos token id is not defined mobel needs input_ids, num_return_sequences = 1",
transformers/tests/test_modeling_common.py,673,num_return_sequences = 1,
transformers/tests/test_modeling_common.py,677,generating more sequences than having beams leads is not possible,
transformers/tests/test_modeling_common.py,680,"num_return_sequences > 1, sample",
transformers/tests/test_modeling_common.py,682,"num_return_sequences > 1, greedy",
transformers/tests/test_modeling_common.py,685,check bad words tokens language generation,
transformers/tests/test_modeling_common.py,686,create list of 1-seq bad token and list of 2-seq of bad tokens,
transformers/tests/test_modeling_common.py,691,only count generated tokens,
transformers/tests/test_modeling_common.py,696,special tokens cannot be bad tokens,
transformers/tests/test_modeling_common.py,705,create random bad tokens that are not special tokens,
transformers/tests/test_modeling_common.py,719,for all bad word tokens,
transformers/tests/test_modeling_common.py,721,for all slices in batch,
transformers/tests/test_modeling_common.py,723,for all word idx,
transformers/tests/test_modeling_common.py,725,if tokens match,
transformers/tests/test_modeling_common.py,735,Creates a random int32 tensor of the shape within the vocab size,
transformers/tests/test_modeling_common.py,793,tests whether the top_k_top_p function behaves as expected,
transformers/tests/test_modeling_common.py,798,3rd highest value; idx. 0,
transformers/tests/test_modeling_common.py,807,5th highest value; idx. 9,
transformers/tests/test_modeling_common.py,808,2nd highest value; idx. 10,
transformers/tests/test_modeling_common.py,823,4th highest value; idx. 25,
transformers/tests/test_modeling_common.py,824,1st highest value; idx. 26,
transformers/tests/test_modeling_common.py,828,cummulative prob of 5 highest values <= 0.6,
transformers/tests/test_modeling_common.py,843,4th highest value; idx. 13,
transformers/tests/test_modeling_common.py,847,2nd highest value; idx. 17,
transformers/tests/test_modeling_common.py,848,5th highest value; idx. 18,
transformers/tests/test_modeling_common.py,850,3rd highest value; idx. 20,
transformers/tests/test_modeling_common.py,857,1st highest value; idx. 27,
transformers/tests/test_modeling_common.py,860,cummulative prob of 5 highest values <= 0.6,
transformers/tests/test_modeling_common.py,870,expected non filtered idx as noted above,
transformers/tests/test_modeling_common.py,884,expected non filtered values as noted above,
transformers/tests/test_configuration_auto.py,1,coding=utf-8,
transformers/tests/test_configuration_auto.py,2,"Copyright 2019-present, the HuggingFace Inc. team.",
transformers/tests/test_configuration_auto.py,3,,
transformers/tests/test_configuration_auto.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_configuration_auto.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_configuration_auto.py,6,You may obtain a copy of the License at,
transformers/tests/test_configuration_auto.py,7,,
transformers/tests/test_configuration_auto.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_configuration_auto.py,9,,
transformers/tests/test_configuration_auto.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_configuration_auto.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_configuration_auto.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_configuration_auto.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_configuration_auto.py,14,limitations under the License.,
transformers/tests/test_configuration_auto.py,51,no key string should be included in a later key string (typical failure case),
transformers/tests/test_modeling_tf_albert.py,1,coding=utf-8,
transformers/tests/test_modeling_tf_albert.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_modeling_tf_albert.py,3,,
transformers/tests/test_modeling_tf_albert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_modeling_tf_albert.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_modeling_tf_albert.py,6,You may obtain a copy of the License at,
transformers/tests/test_modeling_tf_albert.py,7,,
transformers/tests/test_modeling_tf_albert.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_modeling_tf_albert.py,9,,
transformers/tests/test_modeling_tf_albert.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_modeling_tf_albert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_modeling_tf_albert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_modeling_tf_albert.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_modeling_tf_albert.py,14,limitations under the License.,
transformers/tests/test_modeling_tf_albert.py,132,"inputs = {'input_ids': input_ids,",
transformers/tests/test_modeling_tf_albert.py,133,"'attention_mask': input_mask,",
transformers/tests/test_modeling_tf_albert.py,134,'token_type_ids': token_type_ids},
transformers/tests/test_modeling_tf_albert.py,135,"sequence_output, pooled_output = model(**inputs)",
transformers/tests/test_modeling_tf_electra.py,1,coding=utf-8,
transformers/tests/test_modeling_tf_electra.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_modeling_tf_electra.py,3,,
transformers/tests/test_modeling_tf_electra.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_modeling_tf_electra.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_modeling_tf_electra.py,6,You may obtain a copy of the License at,
transformers/tests/test_modeling_tf_electra.py,7,,
transformers/tests/test_modeling_tf_electra.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_modeling_tf_electra.py,9,,
transformers/tests/test_modeling_tf_electra.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_modeling_tf_electra.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_modeling_tf_electra.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_modeling_tf_electra.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_modeling_tf_electra.py,14,limitations under the License.,
transformers/tests/test_modeling_tf_electra.py,224,for model_name in list(TF_ELECTRA_PRETRAINED_MODEL_ARCHIVE_MAP.keys())[:1]:,
transformers/tests/test_modeling_t5.py,1,coding=utf-8,
transformers/tests/test_modeling_t5.py,2,Copyright 2018 Google T5 Authors and HuggingFace Inc. team.,
transformers/tests/test_modeling_t5.py,3,,
transformers/tests/test_modeling_t5.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_modeling_t5.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_modeling_t5.py,6,You may obtain a copy of the License at,
transformers/tests/test_modeling_t5.py,7,,
transformers/tests/test_modeling_t5.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_modeling_t5.py,9,,
transformers/tests/test_modeling_t5.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_modeling_t5.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_modeling_t5.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_modeling_t5.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_modeling_t5.py,14,limitations under the License.,
transformers/tests/test_modeling_t5.py,138,make sure that lm_labels are correctly padded from the right,
transformers/tests/test_modeling_t5.py,141,add casaul pad token mask,
transformers/tests/test_modeling_t5.py,147,first item,
transformers/tests/test_modeling_t5.py,151,items before diagonal,
transformers/tests/test_modeling_t5.py,155,pad items after diagonal,
transformers/tests/test_modeling_t5.py,161,all items after square,
transformers/tests/test_modeling_t5.py,192,decoder_past[0] should correspond to encoder output,
transformers/tests/test_modeling_t5.py,194,There should be `num_layers` key value embeddings stored in decoder_past[1],
transformers/tests/test_modeling_t5.py,196,"There should be a self attn key, a self attn value, a cross attn key and a cross attn value stored in each decoder_past[1] tuple",
transformers/tests/test_modeling_t5.py,229,first forward pass,
transformers/tests/test_modeling_t5.py,232,create hypothetical next token and extent to next_input_ids,
transformers/tests/test_modeling_t5.py,235,append to next input_ids and,
transformers/tests/test_modeling_t5.py,241,select random slice,
transformers/tests/test_modeling_t5.py,246,test that outputs are equal for slice,
transformers/tests/test_modeling_t5.py,256,create attention mask,
transformers/tests/test_modeling_t5.py,262,first forward pass,
transformers/tests/test_modeling_t5.py,265,create hypothetical next token and extent to next_input_ids,
transformers/tests/test_modeling_t5.py,268,change a random masked slice from input_ids,
transformers/tests/test_modeling_t5.py,273,append to next input_ids and attn_mask,
transformers/tests/test_modeling_t5.py,279,get two different outputs,
transformers/tests/test_modeling_t5.py,285,select random slice,
transformers/tests/test_modeling_t5.py,290,test that outputs are equal for slice,
transformers/tests/test_modeling_t5.py,373,@noqa,
transformers/tests/test_tokenization_ctrl.py,1,coding=utf-8,
transformers/tests/test_tokenization_ctrl.py,2,Copyright 2018 Salesforce and HuggingFace Inc. team.,
transformers/tests/test_tokenization_ctrl.py,3,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_tokenization_ctrl.py,4,you may not use this file except in compliance with the License.,
transformers/tests/test_tokenization_ctrl.py,5,You may obtain a copy of the License at,
transformers/tests/test_tokenization_ctrl.py,6,,
transformers/tests/test_tokenization_ctrl.py,7,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_tokenization_ctrl.py,8,,
transformers/tests/test_tokenization_ctrl.py,9,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_tokenization_ctrl.py,10,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_tokenization_ctrl.py,11,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_tokenization_ctrl.py,12,See the License for the specific language governing permissions and,
transformers/tests/test_tokenization_ctrl.py,13,limitations under the License.,
transformers/tests/test_tokenization_ctrl.py,32,Adapted from Sennrich et al. 2015 and https://github.com/rsennrich/subword-nmt,
transformers/tests/test_modeling_tf_ctrl.py,1,coding=utf-8,
transformers/tests/test_modeling_tf_ctrl.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_modeling_tf_ctrl.py,3,,
transformers/tests/test_modeling_tf_ctrl.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_modeling_tf_ctrl.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_modeling_tf_ctrl.py,6,You may obtain a copy of the License at,
transformers/tests/test_modeling_tf_ctrl.py,7,,
transformers/tests/test_modeling_tf_ctrl.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_modeling_tf_ctrl.py,9,,
transformers/tests/test_modeling_tf_ctrl.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_modeling_tf_ctrl.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_modeling_tf_ctrl.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_modeling_tf_ctrl.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_modeling_tf_ctrl.py,14,limitations under the License.,
transformers/tests/test_modeling_tf_ctrl.py,116,"intermediate_size=self.intermediate_size,",
transformers/tests/test_modeling_tf_ctrl.py,117,"hidden_act=self.hidden_act,",
transformers/tests/test_modeling_tf_ctrl.py,118,"hidden_dropout_prob=self.hidden_dropout_prob,",
transformers/tests/test_modeling_tf_ctrl.py,119,"attention_probs_dropout_prob=self.attention_probs_dropout_prob,",
transformers/tests/test_modeling_tf_ctrl.py,122,"type_vocab_size=self.type_vocab_size,",
transformers/tests/test_modeling_tf_ctrl.py,123,initializer_range=self.initializer_range,
transformers/tests/test_modeling_tf_ctrl.py,145,None is the input for 'past',
transformers/tests/test_modeling_tf_ctrl.py,212,Legal the president is,
transformers/tests/test_modeling_tf_ctrl.py,234,Legal the president is a good guy and I don't want to lose my job. \n \n I have a,
transformers/tests/test_modeling_openai.py,1,coding=utf-8,
transformers/tests/test_modeling_openai.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_modeling_openai.py,3,,
transformers/tests/test_modeling_openai.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_modeling_openai.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_modeling_openai.py,6,You may obtain a copy of the License at,
transformers/tests/test_modeling_openai.py,7,,
transformers/tests/test_modeling_openai.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_modeling_openai.py,9,,
transformers/tests/test_modeling_openai.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_modeling_openai.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_modeling_openai.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_modeling_openai.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_modeling_openai.py,14,limitations under the License.,
transformers/tests/test_modeling_openai.py,45,TODO (PVP): Add Double HeadsModel when generate() function is changed accordingly,
transformers/tests/test_modeling_openai.py,114,"intermediate_size=self.intermediate_size,",
transformers/tests/test_modeling_openai.py,115,"hidden_act=self.hidden_act,",
transformers/tests/test_modeling_openai.py,116,"hidden_dropout_prob=self.hidden_dropout_prob,",
transformers/tests/test_modeling_openai.py,117,"attention_probs_dropout_prob=self.attention_probs_dropout_prob,",
transformers/tests/test_modeling_openai.py,120,"type_vocab_size=self.type_vocab_size,",
transformers/tests/test_modeling_openai.py,121,initializer_range=self.initializer_range,
transformers/tests/test_modeling_openai.py,230,the president is,
transformers/tests/test_modeling_openai.py,252,"the president is a very good man. "" \n "" i\'m sure he is, "" said the",
transformers/tests/test_modeling_camembert.py,1,coding=utf-8,
transformers/tests/test_modeling_camembert.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_modeling_camembert.py,3,,
transformers/tests/test_modeling_camembert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_modeling_camembert.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_modeling_camembert.py,6,You may obtain a copy of the License at,
transformers/tests/test_modeling_camembert.py,7,,
transformers/tests/test_modeling_camembert.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_modeling_camembert.py,9,,
transformers/tests/test_modeling_camembert.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_modeling_camembert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_modeling_camembert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_modeling_camembert.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_modeling_camembert.py,14,limitations under the License.,
transformers/tests/test_modeling_camembert.py,36,J'aime le camembert !,
transformers/tests/test_modeling_camembert.py,40,compare the actual values for a slice.,
transformers/tests/test_modeling_camembert.py,46,"camembert = torch.hub.load('pytorch/fairseq', 'camembert.v0')",
transformers/tests/test_modeling_camembert.py,47,camembert.eval(),
transformers/tests/test_modeling_camembert.py,48,"expected_slice = roberta.model.forward(input_ids)[0][:, :3, :3].detach()",
transformers/tests/test_configuration_common.py,1,coding=utf-8,
transformers/tests/test_configuration_common.py,2,Copyright 2019 HuggingFace Inc.,
transformers/tests/test_configuration_common.py,3,,
transformers/tests/test_configuration_common.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_configuration_common.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_configuration_common.py,6,You may obtain a copy of the License at,
transformers/tests/test_configuration_common.py,7,,
transformers/tests/test_configuration_common.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_configuration_common.py,9,,
transformers/tests/test_configuration_common.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_configuration_common.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_configuration_common.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_configuration_common.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_configuration_common.py,14,limitations under the License.,
transformers/tests/test_tokenization_xlm.py,1,coding=utf-8,
transformers/tests/test_tokenization_xlm.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_tokenization_xlm.py,3,,
transformers/tests/test_tokenization_xlm.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_tokenization_xlm.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_tokenization_xlm.py,6,You may obtain a copy of the License at,
transformers/tests/test_tokenization_xlm.py,7,,
transformers/tests/test_tokenization_xlm.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_tokenization_xlm.py,9,,
transformers/tests/test_tokenization_xlm.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_tokenization_xlm.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_tokenization_xlm.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_tokenization_xlm.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_tokenization_xlm.py,14,limitations under the License.,
transformers/tests/test_tokenization_xlm.py,34,Adapted from Sennrich et al. 2015 and https://github.com/rsennrich/subword-nmt,
transformers/tests/test_modeling_gpt2.py,1,coding=utf-8,
transformers/tests/test_modeling_gpt2.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_modeling_gpt2.py,3,,
transformers/tests/test_modeling_gpt2.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_modeling_gpt2.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_modeling_gpt2.py,6,You may obtain a copy of the License at,
transformers/tests/test_modeling_gpt2.py,7,,
transformers/tests/test_modeling_gpt2.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_modeling_gpt2.py,9,,
transformers/tests/test_modeling_gpt2.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_modeling_gpt2.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_modeling_gpt2.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_modeling_gpt2.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_modeling_gpt2.py,14,limitations under the License.,
transformers/tests/test_modeling_gpt2.py,43,TODO (PVP): Add Double HeadsModel when generate() function is changed accordingly,
transformers/tests/test_modeling_gpt2.py,126,"intermediate_size=self.intermediate_size,",
transformers/tests/test_modeling_gpt2.py,127,"hidden_act=self.hidden_act,",
transformers/tests/test_modeling_gpt2.py,128,"hidden_dropout_prob=self.hidden_dropout_prob,",
transformers/tests/test_modeling_gpt2.py,129,"attention_probs_dropout_prob=self.attention_probs_dropout_prob,",
transformers/tests/test_modeling_gpt2.py,132,"type_vocab_size=self.type_vocab_size,",
transformers/tests/test_modeling_gpt2.py,133,initializer_range=self.initializer_range,
transformers/tests/test_modeling_gpt2.py,178,first forward pass,
transformers/tests/test_modeling_gpt2.py,181,create hypothetical next token and extent to next_input_ids,
transformers/tests/test_modeling_gpt2.py,185,append to next input_ids and token_type_ids,
transformers/tests/test_modeling_gpt2.py,192,select random slice,
transformers/tests/test_modeling_gpt2.py,197,test that outputs are equal for slice,
transformers/tests/test_modeling_gpt2.py,207,create attention mask,
transformers/tests/test_modeling_gpt2.py,212,first forward pass,
transformers/tests/test_modeling_gpt2.py,215,create hypothetical next token and extent to next_input_ids,
transformers/tests/test_modeling_gpt2.py,218,change a random masked slice from input_ids,
transformers/tests/test_modeling_gpt2.py,223,append to next input_ids and attn_mask,
transformers/tests/test_modeling_gpt2.py,229,get two different outputs,
transformers/tests/test_modeling_gpt2.py,233,select random slice,
transformers/tests/test_modeling_gpt2.py,238,test that outputs are equal for slice,
transformers/tests/test_modeling_gpt2.py,346,The dog,
transformers/tests/test_modeling_gpt2.py,368,The dog was found in a field near the intersection of West and West Streets.\n\nThe dog,
transformers/tests/test_modeling_gpt2.py,375,The president,
transformers/tests/test_modeling_gpt2.py,397,"The president of the United States, and the president of the United Kingdom, have been in the White",
transformers/tests/test_modeling_xlm.py,1,coding=utf-8,
transformers/tests/test_modeling_xlm.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_modeling_xlm.py,3,,
transformers/tests/test_modeling_xlm.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_modeling_xlm.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_modeling_xlm.py,6,You may obtain a copy of the License at,
transformers/tests/test_modeling_xlm.py,7,,
transformers/tests/test_modeling_xlm.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_modeling_xlm.py,9,,
transformers/tests/test_modeling_xlm.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_modeling_xlm.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_modeling_xlm.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_modeling_xlm.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_modeling_xlm.py,14,limitations under the License.,
transformers/tests/test_modeling_xlm.py,56,TODO (PVP): Check other models whether language generation is also applicable,
transformers/tests/test_modeling_xlm.py,130,small variation of seq_length,
transformers/tests/test_modeling_xlm.py,437,the president,
transformers/tests/test_modeling_xlm.py,459,the president the president the president the president the president the president the president the president the president the president,
transformers/tests/test_modeling_xlm.py,460,TODO(PVP): this and other input_ids I tried for generation give pretty bad results. Not sure why. Model might just not be made for auto-regressive inference,
transformers/tests/test_modeling_bert.py,1,coding=utf-8,
transformers/tests/test_modeling_bert.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_modeling_bert.py,3,,
transformers/tests/test_modeling_bert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_modeling_bert.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_modeling_bert.py,6,You may obtain a copy of the License at,
transformers/tests/test_modeling_bert.py,7,,
transformers/tests/test_modeling_bert.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_modeling_bert.py,9,,
transformers/tests/test_modeling_bert.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_modeling_bert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_modeling_bert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_modeling_bert.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_modeling_bert.py,14,limitations under the License.,
transformers/tests/test_modeling_bert.py,442,This regression test was failing with PyTorch < 1.3,
transformers/tests/test_tokenization_xlm_roberta.py,1,coding=utf-8,
transformers/tests/test_tokenization_xlm_roberta.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_tokenization_xlm_roberta.py,3,,
transformers/tests/test_tokenization_xlm_roberta.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_tokenization_xlm_roberta.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_tokenization_xlm_roberta.py,6,You may obtain a copy of the License at,
transformers/tests/test_tokenization_xlm_roberta.py,7,,
transformers/tests/test_tokenization_xlm_roberta.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_tokenization_xlm_roberta.py,9,,
transformers/tests/test_tokenization_xlm_roberta.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_tokenization_xlm_roberta.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_tokenization_xlm_roberta.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_tokenization_xlm_roberta.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_tokenization_xlm_roberta.py,14,limitations under the License.,
transformers/tests/test_tokenization_xlm_roberta.py,36,We have a SentencePiece fixture for testing,
transformers/tests/test_tokenization_xlm_roberta.py,92,^ unk: 2 + 1 = 3                  unk: 2 + 1 = 3 ^,
transformers/tests/test_tokenization_xlm_roberta.py,130,"xlmr = torch.hub.load('pytorch/fairseq', 'xlmr.base')  # xlmr.large has same tokenizer",
transformers/tests/test_tokenization_xlm_roberta.py,131,xlmr.eval(),
transformers/tests/test_tokenization_xlm_roberta.py,132,xlmr.encode(symbols),
transformers/tests/test_tokenization_xlm_roberta.py,191,"4426, # What fairseq tokenizes from ""<unk>"": ""_<""",
transformers/tests/test_tokenization_xlm_roberta.py,192,"3678, # What fairseq tokenizes from ""<unk>"": ""unk""",
transformers/tests/test_tokenization_xlm_roberta.py,193,"2740, # What fairseq tokenizes from ""<unk>"": "">""",
transformers/tests/test_tokenization_xlm_roberta.py,194,"What we tokenize from ""<unk>"": ""<unk>""",
transformers/tests/test_tokenization_xlm_roberta.py,195,Residue from the tokenization: an extra sentencepiece underline,
transformers/tests/test_tokenization_xlm_roberta.py,208,"xlmr = torch.hub.load('pytorch/fairseq', 'xlmr.base')  # xlmr.large has same tokenizer",
transformers/tests/test_tokenization_xlm_roberta.py,209,xlmr.eval(),
transformers/tests/test_tokenization_xlm_roberta.py,210,xlmr.encode(symbols),
transformers/tests/test_modeling_tf_xlm.py,1,coding=utf-8,
transformers/tests/test_modeling_tf_xlm.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_modeling_tf_xlm.py,3,,
transformers/tests/test_modeling_tf_xlm.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_modeling_tf_xlm.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_modeling_tf_xlm.py,6,You may obtain a copy of the License at,
transformers/tests/test_modeling_tf_xlm.py,7,,
transformers/tests/test_modeling_tf_xlm.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_modeling_tf_xlm.py,9,,
transformers/tests/test_modeling_tf_xlm.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_modeling_tf_xlm.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_modeling_tf_xlm.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_modeling_tf_xlm.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_modeling_tf_xlm.py,14,limitations under the License.,
transformers/tests/test_modeling_tf_xlm.py,48,TODO (PVP): Check other models whether language generation is also applicable,
transformers/tests/test_modeling_tf_xlm.py,122,small variation of seq_length,
transformers/tests/test_modeling_tf_xlm.py,320,the president,
transformers/tests/test_modeling_tf_xlm.py,342,the president the president the president the president the president the president the president the president the president the president,
transformers/tests/test_modeling_tf_xlm.py,343,TODO(PVP): this and other input_ids I tried for generation give pretty bad results. Not sure why. Model might just not be made for auto-regressive inference,
transformers/tests/test_tokenization_xlnet.py,1,coding=utf-8,
transformers/tests/test_tokenization_xlnet.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_tokenization_xlnet.py,3,,
transformers/tests/test_tokenization_xlnet.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_tokenization_xlnet.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_tokenization_xlnet.py,6,You may obtain a copy of the License at,
transformers/tests/test_tokenization_xlnet.py,7,,
transformers/tests/test_tokenization_xlnet.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_tokenization_xlnet.py,9,,
transformers/tests/test_tokenization_xlnet.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_tokenization_xlnet.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_tokenization_xlnet.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_tokenization_xlnet.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_tokenization_xlnet.py,14,limitations under the License.,
transformers/tests/test_tokenization_xlnet.py,36,We have a SentencePiece fixture for testing,
transformers/tests/test_modeling_albert.py,1,coding=utf-8,
transformers/tests/test_modeling_albert.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_modeling_albert.py,3,,
transformers/tests/test_modeling_albert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_modeling_albert.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_modeling_albert.py,6,You may obtain a copy of the License at,
transformers/tests/test_modeling_albert.py,7,,
transformers/tests/test_modeling_albert.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_modeling_albert.py,9,,
transformers/tests/test_modeling_albert.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_modeling_albert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_modeling_albert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_modeling_albert.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_modeling_albert.py,14,limitations under the License.,
transformers/tests/test_tokenization_auto.py,1,coding=utf-8,
transformers/tests/test_tokenization_auto.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_tokenization_auto.py,3,,
transformers/tests/test_tokenization_auto.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_tokenization_auto.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_tokenization_auto.py,6,You may obtain a copy of the License at,
transformers/tests/test_tokenization_auto.py,7,,
transformers/tests/test_tokenization_auto.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_tokenization_auto.py,9,,
transformers/tests/test_tokenization_auto.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_tokenization_auto.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_tokenization_auto.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_tokenization_auto.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_tokenization_auto.py,14,limitations under the License.,
transformers/tests/test_tokenization_auto.py,33,noqa: F401,
transformers/tests/test_tokenization_auto.py,37,@slow,
transformers/tests/test_tokenization_auto.py,84,"Test that the children are placed before the parents in the mappings, as the `instanceof` will be triggered",
transformers/tests/test_tokenization_auto.py,85,by the parents and will return the wrong configuration type when using auto models,
transformers/tests/test_tokenization_auto.py,99,Check for Fast tokenizer implementation if provided,
transformers/tests/test_modeling_tf_roberta.py,1,coding=utf-8,
transformers/tests/test_modeling_tf_roberta.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_modeling_tf_roberta.py,3,,
transformers/tests/test_modeling_tf_roberta.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_modeling_tf_roberta.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_modeling_tf_roberta.py,6,You may obtain a copy of the License at,
transformers/tests/test_modeling_tf_roberta.py,7,,
transformers/tests/test_modeling_tf_roberta.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_modeling_tf_roberta.py,9,,
transformers/tests/test_modeling_tf_roberta.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_modeling_tf_roberta.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_modeling_tf_roberta.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_modeling_tf_roberta.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_modeling_tf_roberta.py,14,limitations under the License.,
transformers/tests/test_modeling_tf_roberta.py,223,compare the actual values for a slice.,
transformers/tests/test_modeling_tf_roberta.py,235,compare the actual values for a slice.,
transformers/tests/test_tokenization_openai.py,1,coding=utf-8,
transformers/tests/test_tokenization_openai.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_tokenization_openai.py,3,,
transformers/tests/test_tokenization_openai.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_tokenization_openai.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_tokenization_openai.py,6,You may obtain a copy of the License at,
transformers/tests/test_tokenization_openai.py,7,,
transformers/tests/test_tokenization_openai.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_tokenization_openai.py,9,,
transformers/tests/test_tokenization_openai.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_tokenization_openai.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_tokenization_openai.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_tokenization_openai.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_tokenization_openai.py,14,limitations under the License.,
transformers/tests/test_tokenization_openai.py,33,Adapted from Sennrich et al. 2015 and https://github.com/rsennrich/subword-nmt,
transformers/tests/test_tokenization_transfo_xl.py,1,coding=utf-8,
transformers/tests/test_tokenization_transfo_xl.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_tokenization_transfo_xl.py,3,,
transformers/tests/test_tokenization_transfo_xl.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_tokenization_transfo_xl.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_tokenization_transfo_xl.py,6,You may obtain a copy of the License at,
transformers/tests/test_tokenization_transfo_xl.py,7,,
transformers/tests/test_tokenization_transfo_xl.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_tokenization_transfo_xl.py,9,,
transformers/tests/test_tokenization_transfo_xl.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_tokenization_transfo_xl.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_tokenization_transfo_xl.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_tokenization_transfo_xl.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_tokenization_transfo_xl.py,14,limitations under the License.,
transformers/tests/test_modeling_tf_bert.py,1,coding=utf-8,
transformers/tests/test_modeling_tf_bert.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_modeling_tf_bert.py,3,,
transformers/tests/test_modeling_tf_bert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_modeling_tf_bert.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_modeling_tf_bert.py,6,You may obtain a copy of the License at,
transformers/tests/test_modeling_tf_bert.py,7,,
transformers/tests/test_modeling_tf_bert.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_modeling_tf_bert.py,9,,
transformers/tests/test_modeling_tf_bert.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_modeling_tf_bert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_modeling_tf_bert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_modeling_tf_bert.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_modeling_tf_bert.py,14,limitations under the License.,
transformers/tests/test_modeling_tf_bert.py,314,for model_name in list(TF_BERT_PRETRAINED_MODEL_ARCHIVE_MAP.keys())[:1]:,
transformers/tests/test_modeling_auto.py,1,coding=utf-8,
transformers/tests/test_modeling_auto.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_modeling_auto.py,3,,
transformers/tests/test_modeling_auto.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_modeling_auto.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_modeling_auto.py,6,You may obtain a copy of the License at,
transformers/tests/test_modeling_auto.py,7,,
transformers/tests/test_modeling_auto.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_modeling_auto.py,9,,
transformers/tests/test_modeling_auto.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_modeling_auto.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_modeling_auto.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_modeling_auto.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_modeling_auto.py,14,limitations under the License.,
transformers/tests/test_modeling_auto.py,155,"Test that the children are placed before the parents in the mappings, as the `instanceof` will be triggered",
transformers/tests/test_modeling_auto.py,156,by the parents and will return the wrong configuration type when using auto models,
transformers/tests/test_modeling_tf_t5.py,1,coding=utf-8,
transformers/tests/test_modeling_tf_t5.py,2,Copyright 2018 Google T5 Authors and HuggingFace Inc. team.,
transformers/tests/test_modeling_tf_t5.py,3,,
transformers/tests/test_modeling_tf_t5.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_modeling_tf_t5.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_modeling_tf_t5.py,6,You may obtain a copy of the License at,
transformers/tests/test_modeling_tf_t5.py,7,,
transformers/tests/test_modeling_tf_t5.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_modeling_tf_t5.py,9,,
transformers/tests/test_modeling_tf_t5.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_modeling_tf_t5.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_modeling_tf_t5.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_modeling_tf_t5.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_modeling_tf_t5.py,14,limitations under the License.,
transformers/tests/test_modeling_tf_t5.py,187,@noqa,
transformers/tests/test_modeling_tf_auto.py,1,coding=utf-8,
transformers/tests/test_modeling_tf_auto.py,2,Copyright 2018 The Google AI Language Team Authors.,
transformers/tests/test_modeling_tf_auto.py,3,,
transformers/tests/test_modeling_tf_auto.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/tests/test_modeling_tf_auto.py,5,you may not use this file except in compliance with the License.,
transformers/tests/test_modeling_tf_auto.py,6,You may obtain a copy of the License at,
transformers/tests/test_modeling_tf_auto.py,7,,
transformers/tests/test_modeling_tf_auto.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/tests/test_modeling_tf_auto.py,9,,
transformers/tests/test_modeling_tf_auto.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/tests/test_modeling_tf_auto.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/tests/test_modeling_tf_auto.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/tests/test_modeling_tf_auto.py,13,See the License for the specific language governing permissions and,
transformers/tests/test_modeling_tf_auto.py,14,limitations under the License.,
transformers/tests/test_modeling_tf_auto.py,52,for model_name in list(TF_BERT_PRETRAINED_MODEL_ARCHIVE_MAP.keys())[:1]:,
transformers/tests/test_modeling_tf_auto.py,69,for model_name in list(TF_BERT_PRETRAINED_MODEL_ARCHIVE_MAP.keys())[:1]:,
transformers/tests/test_modeling_tf_auto.py,82,for model_name in list(TF_BERT_PRETRAINED_MODEL_ARCHIVE_MAP.keys())[:1]:,
transformers/tests/test_modeling_tf_auto.py,95,for model_name in list(TF_BERT_PRETRAINED_MODEL_ARCHIVE_MAP.keys())[:1]:,
transformers/tests/test_modeling_tf_auto.py,108,for model_name in list(TF_BERT_PRETRAINED_MODEL_ARCHIVE_MAP.keys())[:1]:,
transformers/examples/run_bertology.py,1,!/usr/bin/env python3,
transformers/examples/run_bertology.py,2,Copyright 2018 CMU and The HuggingFace Inc. team.,
transformers/examples/run_bertology.py,3,,
transformers/examples/run_bertology.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/run_bertology.py,5,you may not use this file except in compliance with the License.,
transformers/examples/run_bertology.py,6,You may obtain a copy of the License at,
transformers/examples/run_bertology.py,7,,
transformers/examples/run_bertology.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/run_bertology.py,9,,
transformers/examples/run_bertology.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/run_bertology.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/run_bertology.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/run_bertology.py,13,See the License for the specific language governing permissions and,
transformers/examples/run_bertology.py,14,limitations under the License.,
transformers/examples/run_bertology.py,66,Prepare our tensors,
transformers/examples/run_bertology.py,82,Do a forward pass (not with torch.no_grad() since we need gradients for importance score - see below),
transformers/examples/run_bertology.py,90,"Loss and logits are the first, attention the last",
transformers/examples/run_bertology.py,91,Backpropagate to populate the gradients in the head mask,
transformers/examples/run_bertology.py,101,Also store our logits/labels if we want to compute metrics afterwards,
transformers/examples/run_bertology.py,111,Normalize,
transformers/examples/run_bertology.py,114,Layerwise importance normalization,
transformers/examples/run_bertology.py,123,Print/save matrices,
transformers/examples/run_bertology.py,156,save current head mask,
transformers/examples/run_bertology.py,157,heads from least important to most - keep only not-masked heads,
transformers/examples/run_bertology.py,164,mask heads,
transformers/examples/run_bertology.py,172,Compute metric and head importance again,
transformers/examples/run_bertology.py,196,Try pruning and test time speedup,
transformers/examples/run_bertology.py,197,Pruning is like masking but we actually remove the masked weights,
transformers/examples/run_bertology.py,232,Required parameters,
transformers/examples/run_bertology.py,262,Other parameters,
transformers/examples/run_bertology.py,331,Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script,
transformers/examples/run_bertology.py,338,Setup devices and distributed training,
transformers/examples/run_bertology.py,346,Initializes the distributed backend,
transformers/examples/run_bertology.py,348,Setup logging,
transformers/examples/run_bertology.py,352,Set seeds,
transformers/examples/run_bertology.py,355,Prepare GLUE task,
transformers/examples/run_bertology.py,364,Load pretrained model and tokenizer,
transformers/examples/run_bertology.py,366,Make sure only the first process in distributed training will download model & vocab,
transformers/examples/run_bertology.py,371,take the first match in model types,
transformers/examples/run_bertology.py,393,Make sure only the first process in distributed training will download model & vocab,
transformers/examples/run_bertology.py,395,Distributed and parallel training,
transformers/examples/run_bertology.py,404,Print/save training arguments,
transformers/examples/run_bertology.py,408,Prepare dataset for the GLUE task,
transformers/examples/run_bertology.py,415,Compute head entropy and importance score,
transformers/examples/run_bertology.py,418,Try head masking (set heads to zero until the score goes under a threshole),
transformers/examples/run_bertology.py,419,and head pruning (remove masked heads and see the effect on the network),
transformers/examples/run_squad.py,1,coding=utf-8,
transformers/examples/run_squad.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/examples/run_squad.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/examples/run_squad.py,4,,
transformers/examples/run_squad.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/run_squad.py,6,you may not use this file except in compliance with the License.,
transformers/examples/run_squad.py,7,You may obtain a copy of the License at,
transformers/examples/run_squad.py,8,,
transformers/examples/run_squad.py,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/run_squad.py,10,,
transformers/examples/run_squad.py,11,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/run_squad.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/run_squad.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/run_squad.py,14,See the License for the specific language governing permissions and,
transformers/examples/run_squad.py,15,limitations under the License.,
transformers/examples/run_squad.py,91,Prepare optimizer and schedule (linear warmup and decay),
transformers/examples/run_squad.py,105,Check if saved optimizer or scheduler states exist,
transformers/examples/run_squad.py,109,Load in optimizer and scheduler states,
transformers/examples/run_squad.py,121,multi-gpu training (should be after apex fp16 initialization),
transformers/examples/run_squad.py,125,Distributed training (should be after apex fp16 initialization),
transformers/examples/run_squad.py,131,Train!,
transformers/examples/run_squad.py,148,Check if continuing training from a checkpoint,
transformers/examples/run_squad.py,151,set global_step to gobal_step of last saved checkpoint from model path,
transformers/examples/run_squad.py,169,Added here for reproductibility,
transformers/examples/run_squad.py,176,Skip past any already trained steps if resuming training,
transformers/examples/run_squad.py,205,model outputs are always tuple in transformers (see doc),
transformers/examples/run_squad.py,209,mean() to average on multi-gpu parallel (not distributed) training,
transformers/examples/run_squad.py,227,Update learning rate schedule,
transformers/examples/run_squad.py,231,Log metrics,
transformers/examples/run_squad.py,233,Only evaluate when single GPU otherwise metrics may not average well,
transformers/examples/run_squad.py,242,Save model checkpoint,
transformers/examples/run_squad.py,247,Take care of distributed/parallel training,
transformers/examples/run_squad.py,280,Note that DistributedSampler samples randomly,
transformers/examples/run_squad.py,284,multi-gpu evaluate,
transformers/examples/run_squad.py,288,Eval!,
transformers/examples/run_squad.py,312,XLNet and XLM use more arguments for their predictions,
transformers/examples/run_squad.py,315,for lang_id-sensitive xlm models,
transformers/examples/run_squad.py,329,"Some models (XLNet, XLM) use 5 arguments for their predictions, while the other ""simpler""",
transformers/examples/run_squad.py,330,models only use two.,
transformers/examples/run_squad.py,356,Compute predictions,
transformers/examples/run_squad.py,365,XLNet and XLM use a more complex post-processing procedure,
transformers/examples/run_squad.py,402,Compute the F1 and exact scores.,
transformers/examples/run_squad.py,409,"Make sure only the first process in distributed training process the dataset, and the others will use the cache",
transformers/examples/run_squad.py,412,Load data features from cache or dataset file,
transformers/examples/run_squad.py,423,Init features and dataset from cache if it exists,
transformers/examples/run_squad.py,469,"Make sure only the first process in distributed training process the dataset, and the others will use the cache",
transformers/examples/run_squad.py,480,Required parameters,
transformers/examples/run_squad.py,503,Other parameters,
transformers/examples/run_squad.py,686,Setup distant debugging if needed,
transformers/examples/run_squad.py,688,Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script,
transformers/examples/run_squad.py,695,"Setup CUDA, GPU & distributed training",
transformers/examples/run_squad.py,699,Initializes the distributed backend which will take care of sychronizing nodes/GPUs,
transformers/examples/run_squad.py,706,Setup logging,
transformers/examples/run_squad.py,721,Set seed,
transformers/examples/run_squad.py,724,Load pretrained model and tokenizer,
transformers/examples/run_squad.py,726,Make sure only the first process in distributed training will download model & vocab,
transformers/examples/run_squad.py,747,Make sure only the first process in distributed training will download model & vocab,
transformers/examples/run_squad.py,754,"Before we do anything with models, we want to ensure that we get fp16 execution of torch.einsum if args.fp16 is set.",
transformers/examples/run_squad.py,755,"Otherwise it'll default to ""promote"" mode, and we'll get fp32 operations. Note that running `--fp16_opt_level=""O2""` will",
transformers/examples/run_squad.py,756,"remove the need for this code, but it is still valid.",
transformers/examples/run_squad.py,765,Training,
transformers/examples/run_squad.py,771,Save the trained model and the tokenizer,
transformers/examples/run_squad.py,773,Create output directory if needed,
transformers/examples/run_squad.py,778,"Save a trained model, configuration and tokenizer using `save_pretrained()`.",
transformers/examples/run_squad.py,779,They can then be reloaded using `from_pretrained()`,
transformers/examples/run_squad.py,780,Take care of distributed/parallel training,
transformers/examples/run_squad.py,785,Good practice: save your training arguments together with the trained model,
transformers/examples/run_squad.py,788,Load a trained model and vocabulary that you have fine-tuned,
transformers/examples/run_squad.py,789,", force_download=True)",
transformers/examples/run_squad.py,793,Evaluation - we can ask to evaluate all the checkpoints (sub-directories) in a directory,
transformers/examples/run_squad.py,804,Reduce model loading logs,
transformers/examples/run_squad.py,812,Reload the model,
transformers/examples/run_squad.py,814,", force_download=True)",
transformers/examples/run_squad.py,817,Evaluate,
transformers/examples/run_multiple_choice.py,1,coding=utf-8,
transformers/examples/run_multiple_choice.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/examples/run_multiple_choice.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/examples/run_multiple_choice.py,4,,
transformers/examples/run_multiple_choice.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/run_multiple_choice.py,6,you may not use this file except in compliance with the License.,
transformers/examples/run_multiple_choice.py,7,You may obtain a copy of the License at,
transformers/examples/run_multiple_choice.py,8,,
transformers/examples/run_multiple_choice.py,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/run_multiple_choice.py,10,,
transformers/examples/run_multiple_choice.py,11,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/run_multiple_choice.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/run_multiple_choice.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/run_multiple_choice.py,14,See the License for the specific language governing permissions and,
transformers/examples/run_multiple_choice.py,15,limitations under the License.,
transformers/examples/run_multiple_choice.py,98,Prepare optimizer and schedule (linear warmup and decay),
transformers/examples/run_multiple_choice.py,118,multi-gpu training (should be after apex fp16 initialization),
transformers/examples/run_multiple_choice.py,122,Distributed training (should be after apex fp16 initialization),
transformers/examples/run_multiple_choice.py,128,Train!,
transformers/examples/run_multiple_choice.py,148,Added here for reproductibility,
transformers/examples/run_multiple_choice.py,159,XLM don't use segment_ids,
transformers/examples/run_multiple_choice.py,163,model outputs are always tuple in transformers (see doc),
transformers/examples/run_multiple_choice.py,166,mean() to average on multi-gpu parallel training,
transformers/examples/run_multiple_choice.py,182,Update learning rate schedule,
transformers/examples/run_multiple_choice.py,187,Log metrics,
transformers/examples/run_multiple_choice.py,190,Only evaluate when single GPU otherwise metrics may not average well,
transformers/examples/run_multiple_choice.py,217,Save model checkpoint,
transformers/examples/run_multiple_choice.py,223,Take care of distributed/parallel training,
transformers/examples/run_multiple_choice.py,254,Note that DistributedSampler samples randomly,
transformers/examples/run_multiple_choice.py,258,multi-gpu evaluate,
transformers/examples/run_multiple_choice.py,262,Eval!,
transformers/examples/run_multiple_choice.py,280,XLM don't use segment_ids,
transformers/examples/run_multiple_choice.py,325,"Make sure only the first process in distributed training process the dataset, and the others will use the cache",
transformers/examples/run_multiple_choice.py,328,Load data features from cache or dataset file,
transformers/examples/run_multiple_choice.py,363,pad on the left for xlnet,
transformers/examples/run_multiple_choice.py,372,"Make sure only the first process in distributed training process the dataset, and the others will use the cache",
transformers/examples/run_multiple_choice.py,374,Convert to Tensors and build dataset,
transformers/examples/run_multiple_choice.py,387,Required parameters,
transformers/examples/run_multiple_choice.py,424,Other parameters,
transformers/examples/run_multiple_choice.py,527,Setup distant debugging if needed,
transformers/examples/run_multiple_choice.py,529,Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script,
transformers/examples/run_multiple_choice.py,536,"Setup CUDA, GPU & distributed training",
transformers/examples/run_multiple_choice.py,540,Initializes the distributed backend which will take care of sychronizing nodes/GPUs,
transformers/examples/run_multiple_choice.py,547,Setup logging,
transformers/examples/run_multiple_choice.py,562,Set seed,
transformers/examples/run_multiple_choice.py,565,Prepare GLUE task,
transformers/examples/run_multiple_choice.py,573,Load pretrained model and tokenizer,
transformers/examples/run_multiple_choice.py,575,Make sure only the first process in distributed training will download model & vocab,
transformers/examples/run_multiple_choice.py,598,Make sure only the first process in distributed training will download model & vocab,
transformers/examples/run_multiple_choice.py,605,Training,
transformers/examples/run_multiple_choice.py,611,"Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()",
transformers/examples/run_multiple_choice.py,613,Create output directory if needed,
transformers/examples/run_multiple_choice.py,618,"Save a trained model, configuration and tokenizer using `save_pretrained()`.",
transformers/examples/run_multiple_choice.py,619,They can then be reloaded using `from_pretrained()`,
transformers/examples/run_multiple_choice.py,622,Take care of distributed/parallel training,
transformers/examples/run_multiple_choice.py,626,Good practice: save your training arguments together with the trained model,
transformers/examples/run_multiple_choice.py,629,Load a trained model and vocabulary that you have fine-tuned,
transformers/examples/run_multiple_choice.py,634,Evaluation,
transformers/examples/run_multiple_choice.py,644,Reduce logging,
transformers/examples/run_multiple_choice.py,660,if args.eval_all_checkpoints: # can not use this to do test!!,
transformers/examples/run_multiple_choice.py,661,"checkpoints = list(os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + '/**/' + WEIGHTS_NAME, recursive=True)))",
transformers/examples/run_multiple_choice.py,662,"logging.getLogger(""transformers.modeling_utils"").setLevel(logging.WARN)  # Reduce logging",
transformers/examples/test_examples.py,1,coding=utf-8,
transformers/examples/test_examples.py,2,Copyright 2018 HuggingFace Inc..,
transformers/examples/test_examples.py,3,,
transformers/examples/test_examples.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/test_examples.py,5,you may not use this file except in compliance with the License.,
transformers/examples/test_examples.py,6,You may obtain a copy of the License at,
transformers/examples/test_examples.py,7,,
transformers/examples/test_examples.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/test_examples.py,9,,
transformers/examples/test_examples.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/test_examples.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/test_examples.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/test_examples.py,13,See the License for the specific language governing permissions and,
transformers/examples/test_examples.py,14,limitations under the License.,
transformers/examples/benchmarks.py,1,coding=utf-8,
transformers/examples/benchmarks.py,2,Copyright 2018 The HuggingFace Inc. team.,
transformers/examples/benchmarks.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/examples/benchmarks.py,4,,
transformers/examples/benchmarks.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/benchmarks.py,6,you may not use this file except in compliance with the License.,
transformers/examples/benchmarks.py,7,You may obtain a copy of the License at,
transformers/examples/benchmarks.py,8,,
transformers/examples/benchmarks.py,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/benchmarks.py,10,,
transformers/examples/benchmarks.py,11,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/benchmarks.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/benchmarks.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/benchmarks.py,14,See the License for the specific language governing permissions and,
transformers/examples/benchmarks.py,15,limitations under the License.,
transformers/examples/benchmarks.py,18,If checking the tensors placement,
transformers/examples/benchmarks.py,19,tf.debugging.set_log_device_placement(True),
transformers/examples/benchmarks.py,466,model.add_memory_hooks()  # Forward method tracing (only for PyTorch models),
transformers/examples/benchmarks.py,468,Line by line memory tracing (all code in the module `transformers`) works for all models/arbitrary code,
transformers/examples/benchmarks.py,531,To make sure that the model is traced + that the tensors are on the appropriate device,
transformers/examples/benchmarks.py,535,Line by line memory tracing (all code in the module `transformers`) works for all models/arbitrary code,
transformers/examples/run_generation.py,1,!/usr/bin/env python3,
transformers/examples/run_generation.py,2,coding=utf-8,
transformers/examples/run_generation.py,3,"Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.",
transformers/examples/run_generation.py,4,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/examples/run_generation.py,5,,
transformers/examples/run_generation.py,6,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/run_generation.py,7,you may not use this file except in compliance with the License.,
transformers/examples/run_generation.py,8,You may obtain a copy of the License at,
transformers/examples/run_generation.py,9,,
transformers/examples/run_generation.py,10,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/run_generation.py,11,,
transformers/examples/run_generation.py,12,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/run_generation.py,13,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/run_generation.py,14,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/run_generation.py,15,See the License for the specific language governing permissions and,
transformers/examples/run_generation.py,16,limitations under the License.,
transformers/examples/run_generation.py,48,Hardcoded max length to avoid infinite loop,
transformers/examples/run_generation.py,59,Padding text to help Transformer-XL and XLNet with short prompts as proposed by Aman Rusia,
transformers/examples/run_generation.py,60,in https://github.com/rusiaaman/XLNet-gen#methodology,
transformers/examples/run_generation.py,61,and https://medium.com/@amanrusia/xlnet-speaks-comparison-to-gpt-2-ea1a4e9ba39e,
transformers/examples/run_generation.py,81,,
transformers/examples/run_generation.py,82,Functions to prepare models' input,
transformers/examples/run_generation.py,83,,
transformers/examples/run_generation.py,97,"kwargs = {""language"": None, ""mask_token_id"": None}",
transformers/examples/run_generation.py,99,Set the language,
transformers/examples/run_generation.py,111,"kwargs[""language""] = tokenizer.lang2id[language]",
transformers/examples/run_generation.py,113,TODO fix mask_token_id setup when configurations will be synchronized between models and tokenizers,
transformers/examples/run_generation.py,114,XLM masked-language modeling (MLM) models need masked token,
transformers/examples/run_generation.py,115,"is_xlm_mlm = ""mlm"" in args.model_name_or_path",
transformers/examples/run_generation.py,116,if is_xlm_mlm:,
transformers/examples/run_generation.py,117,"kwargs[""mask_token_id""] = tokenizer.mask_token_id",
transformers/examples/run_generation.py,144,No generation bigger than model size,
transformers/examples/run_generation.py,146,avoid infinite loop,
transformers/examples/run_generation.py,196,Initialize the model and tokenizer,
transformers/examples/run_generation.py,212,Different models need different input formatting and/or extra arguments,
transformers/examples/run_generation.py,235,Remove the batch dimension when returning multiple sequences,
transformers/examples/run_generation.py,245,Decode text,
transformers/examples/run_generation.py,248,Remove all text after the stop token,
transformers/examples/run_generation.py,251,Add the prompt at the beginning of the sequence. Remove the excess text that was used for pre-processing,
transformers/examples/run_glue.py,1,coding=utf-8,
transformers/examples/run_glue.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/examples/run_glue.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/examples/run_glue.py,4,,
transformers/examples/run_glue.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/run_glue.py,6,you may not use this file except in compliance with the License.,
transformers/examples/run_glue.py,7,You may obtain a copy of the License at,
transformers/examples/run_glue.py,8,,
transformers/examples/run_glue.py,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/run_glue.py,10,,
transformers/examples/run_glue.py,11,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/run_glue.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/run_glue.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/run_glue.py,14,See the License for the specific language governing permissions and,
transformers/examples/run_glue.py,15,limitations under the License.,
transformers/examples/run_glue.py,88,Prepare optimizer and schedule (linear warmup and decay),
transformers/examples/run_glue.py,103,Check if saved optimizer or scheduler states exist,
transformers/examples/run_glue.py,107,Load in optimizer and scheduler states,
transformers/examples/run_glue.py,118,multi-gpu training (should be after apex fp16 initialization),
transformers/examples/run_glue.py,122,Distributed training (should be after apex fp16 initialization),
transformers/examples/run_glue.py,128,Train!,
transformers/examples/run_glue.py,145,Check if continuing training from a checkpoint,
transformers/examples/run_glue.py,147,set global_step to global_step of last saved checkpoint from model path,
transformers/examples/run_glue.py,165,Added here for reproductibility,
transformers/examples/run_glue.py,170,Skip past any already trained steps if resuming training,
transformers/examples/run_glue.py,181,"XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use segment_ids",
transformers/examples/run_glue.py,183,model outputs are always tuple in transformers (see doc),
transformers/examples/run_glue.py,186,mean() to average on multi-gpu parallel training,
transformers/examples/run_glue.py,198,last step in epoch but step is always smaller than gradient_accumulation_steps,
transformers/examples/run_glue.py,208,Update learning rate schedule,
transformers/examples/run_glue.py,216,Only evaluate when single GPU otherwise metrics may not average well,
transformers/examples/run_glue.py,233,Save model checkpoint,
transformers/examples/run_glue.py,239,Take care of distributed/parallel training,
transformers/examples/run_glue.py,264,"Loop to handle MNLI double evaluation (matched, mis-matched)",
transformers/examples/run_glue.py,276,Note that DistributedSampler samples randomly,
transformers/examples/run_glue.py,280,multi-gpu eval,
transformers/examples/run_glue.py,284,Eval!,
transformers/examples/run_glue.py,301,"XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use segment_ids",
transformers/examples/run_glue.py,334,"Make sure only the first process in distributed training process the dataset, and the others will use the cache",
transformers/examples/run_glue.py,338,Load data features from cache or dataset file,
transformers/examples/run_glue.py,355,HACK(label indices are swapped in RoBERTa pretrained model),
transformers/examples/run_glue.py,368,"Make sure only the first process in distributed training process the dataset, and the others will use the cache",
transformers/examples/run_glue.py,370,Convert to Tensors and build dataset,
transformers/examples/run_glue.py,428,"For now, let's merge all the sets of args into one,",
transformers/examples/run_glue.py,429,"but soon, we'll keep distinct sets of args, with a cleaner separation of concerns.",
transformers/examples/run_glue.py,442,"Setup CUDA, GPU & distributed training",
transformers/examples/run_glue.py,446,Initializes the distributed backend which will take care of sychronizing nodes/GPUs,
transformers/examples/run_glue.py,453,Setup logging,
transformers/examples/run_glue.py,468,Set seed,
transformers/examples/run_glue.py,471,Prepare GLUE task,
transformers/examples/run_glue.py,480,Load pretrained model and tokenizer,
transformers/examples/run_glue.py,482,Make sure only the first process in distributed training will download model & vocab,
transformers/examples/run_glue.py,502,Make sure only the first process in distributed training will download model & vocab,
transformers/examples/run_glue.py,508,Training,
transformers/examples/run_glue.py,514,"Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()",
transformers/examples/run_glue.py,516,Create output directory if needed,
transformers/examples/run_glue.py,521,"Save a trained model, configuration and tokenizer using `save_pretrained()`.",
transformers/examples/run_glue.py,522,They can then be reloaded using `from_pretrained()`,
transformers/examples/run_glue.py,525,Take care of distributed/parallel training,
transformers/examples/run_glue.py,529,Good practice: save your training arguments together with the trained model,
transformers/examples/run_glue.py,532,Load a trained model and vocabulary that you have fine-tuned,
transformers/examples/run_glue.py,537,Evaluation,
transformers/examples/run_glue.py,546,Reduce logging,
transformers/examples/run_tpu_glue.py,1,coding=utf-8,
transformers/examples/run_tpu_glue.py,2,Copyright 2019 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/examples/run_tpu_glue.py,3,"Copyright (c) 2019, NVIDIA CORPORATION.  All rights reserved.",
transformers/examples/run_tpu_glue.py,4,,
transformers/examples/run_tpu_glue.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/run_tpu_glue.py,6,you may not use this file except in compliance with the License.,
transformers/examples/run_tpu_glue.py,7,You may obtain a copy of the License at,
transformers/examples/run_tpu_glue.py,8,,
transformers/examples/run_tpu_glue.py,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/run_tpu_glue.py,10,,
transformers/examples/run_tpu_glue.py,11,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/run_tpu_glue.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/run_tpu_glue.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/run_tpu_glue.py,14,See the License for the specific language governing permissions and,
transformers/examples/run_tpu_glue.py,15,limitations under the License.,
transformers/examples/run_tpu_glue.py,63,Only tensorboardX supports writing directly to gs://,
transformers/examples/run_tpu_glue.py,103,Only master writes to Tensorboard,
transformers/examples/run_tpu_glue.py,115,Prepare optimizer and schedule (linear warmup and decay),
transformers/examples/run_tpu_glue.py,129,Train!,
transformers/examples/run_tpu_glue.py,145,Added here for reproductibility (even between python 2 and 3),
transformers/examples/run_tpu_glue.py,147,tpu-comment: Get TPU parallel loader which sends data to TPU in background.,
transformers/examples/run_tpu_glue.py,152,Save model checkpoint.,
transformers/examples/run_tpu_glue.py,162,Barrier to wait for saving checkpoint.,
transformers/examples/run_tpu_glue.py,164,model.save_pretrained needs to be called by all ordinals,
transformers/examples/run_tpu_glue.py,170,"XLM, DistilBERT and RoBERTa don't use segment_ids",
transformers/examples/run_tpu_glue.py,173,model outputs are always tuple in transformers (see doc),
transformers/examples/run_tpu_glue.py,184,Update learning rate schedule,
transformers/examples/run_tpu_glue.py,189,Log metrics.,
transformers/examples/run_tpu_glue.py,200,tpu-comment: All values must be in CPU and not on TPU device,
transformers/examples/run_tpu_glue.py,210,"tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)",
transformers/examples/run_tpu_glue.py,224,Only master writes to Tensorboard,
transformers/examples/run_tpu_glue.py,227,"Loop to handle MNLI double evaluation (matched, mis-matched)",
transformers/examples/run_tpu_glue.py,242,Eval!,
transformers/examples/run_tpu_glue.py,256,"XLM, DistilBERT and RoBERTa don't use segment_ids",
transformers/examples/run_tpu_glue.py,270,tpu-comment: Get all predictions and labels from all worker shards of eval dataset,
transformers/examples/run_tpu_glue.py,293,"tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)",
transformers/examples/run_tpu_glue.py,318,Load data features from cache or dataset file,
transformers/examples/run_tpu_glue.py,326,HACK(label indices are swapped in RoBERTa pretrained model),
transformers/examples/run_tpu_glue.py,340,Convert to Tensors and build dataset,
transformers/examples/run_tpu_glue.py,366,tpu-comment: Get TPU/XLA Device,
transformers/examples/run_tpu_glue.py,369,Setup logging,
transformers/examples/run_tpu_glue.py,377,Disable all non-master loggers below CRITICAL.,
transformers/examples/run_tpu_glue.py,382,Set seed to have same initialization,
transformers/examples/run_tpu_glue.py,385,Prepare GLUE task,
transformers/examples/run_tpu_glue.py,397,Make sure only the first process in distributed training will download model & vocab,
transformers/examples/run_tpu_glue.py,399,Load pretrained model and tokenizer,
transformers/examples/run_tpu_glue.py,424,Send model to TPU/XLA device.,
transformers/examples/run_tpu_glue.py,430,Train the model.,
transformers/examples/run_tpu_glue.py,436,Save trained model.,
transformers/examples/run_tpu_glue.py,437,"Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()",
transformers/examples/run_tpu_glue.py,439,Create output directory if needed,
transformers/examples/run_tpu_glue.py,444,"Save a trained model, configuration and tokenizer using `save_pretrained()`.",
transformers/examples/run_tpu_glue.py,445,They can then be reloaded using `from_pretrained()`,
transformers/examples/run_tpu_glue.py,447,Good practice: save your training arguments together with the trained.,
transformers/examples/run_tpu_glue.py,451,model.save_pretrained needs to be called by all ordinals,
transformers/examples/run_tpu_glue.py,454,Load a trained model and vocabulary that you have fine-tuned,
transformers/examples/run_tpu_glue.py,459,Evaluation,
transformers/examples/run_tpu_glue.py,468,Reduce logging,
transformers/examples/run_tpu_glue.py,486,Required parameters,
transformers/examples/run_tpu_glue.py,523,TPU Parameters,
transformers/examples/run_tpu_glue.py,527,Other parameters,
transformers/examples/run_xnli.py,1,coding=utf-8,
transformers/examples/run_xnli.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/examples/run_xnli.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/examples/run_xnli.py,4,,
transformers/examples/run_xnli.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/run_xnli.py,6,you may not use this file except in compliance with the License.,
transformers/examples/run_xnli.py,7,You may obtain a copy of the License at,
transformers/examples/run_xnli.py,8,,
transformers/examples/run_xnli.py,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/run_xnli.py,10,,
transformers/examples/run_xnli.py,11,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/run_xnli.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/run_xnli.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/run_xnli.py,14,See the License for the specific language governing permissions and,
transformers/examples/run_xnli.py,15,limitations under the License.,
transformers/examples/run_xnli.py,94,Prepare optimizer and schedule (linear warmup and decay),
transformers/examples/run_xnli.py,108,Check if saved optimizer or scheduler states exist,
transformers/examples/run_xnli.py,112,Load in optimizer and scheduler states,
transformers/examples/run_xnli.py,123,multi-gpu training (should be after apex fp16 initialization),
transformers/examples/run_xnli.py,127,Distributed training (should be after apex fp16 initialization),
transformers/examples/run_xnli.py,133,Train!,
transformers/examples/run_xnli.py,150,Check if continuing training from a checkpoint,
transformers/examples/run_xnli.py,152,set global_step to gobal_step of last saved checkpoint from model path,
transformers/examples/run_xnli.py,167,Added here for reproductibility,
transformers/examples/run_xnli.py,171,Skip past any already trained steps if resuming training,
transformers/examples/run_xnli.py,182,XLM and DistilBERT don't use segment_ids,
transformers/examples/run_xnli.py,184,model outputs are always tuple in transformers (see doc),
transformers/examples/run_xnli.py,187,mean() to average on multi-gpu parallel training,
transformers/examples/run_xnli.py,205,Update learning rate schedule,
transformers/examples/run_xnli.py,210,Log metrics,
transformers/examples/run_xnli.py,213,Only evaluate when single GPU otherwise metrics may not average well,
transformers/examples/run_xnli.py,222,Save model checkpoint,
transformers/examples/run_xnli.py,228,Take care of distributed/parallel training,
transformers/examples/run_xnli.py,264,Note that DistributedSampler samples randomly,
transformers/examples/run_xnli.py,268,multi-gpu eval,
transformers/examples/run_xnli.py,272,Eval!,
transformers/examples/run_xnli.py,289,XLM and DistilBERT don't use segment_ids,
transformers/examples/run_xnli.py,322,"Make sure only the first process in distributed training process the dataset, and the others will use the cache",
transformers/examples/run_xnli.py,326,Load data features from cache or dataset file,
transformers/examples/run_xnli.py,354,"Make sure only the first process in distributed training process the dataset, and the others will use the cache",
transformers/examples/run_xnli.py,356,Convert to Tensors and build dataset,
transformers/examples/run_xnli.py,372,Required parameters,
transformers/examples/run_xnli.py,412,Other parameters,
transformers/examples/run_xnli.py,514,Setup distant debugging if needed,
transformers/examples/run_xnli.py,516,Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script,
transformers/examples/run_xnli.py,523,"Setup CUDA, GPU & distributed training",
transformers/examples/run_xnli.py,527,Initializes the distributed backend which will take care of sychronizing nodes/GPUs,
transformers/examples/run_xnli.py,534,Setup logging,
transformers/examples/run_xnli.py,549,Set seed,
transformers/examples/run_xnli.py,552,Prepare XNLI task,
transformers/examples/run_xnli.py,561,Load pretrained model and tokenizer,
transformers/examples/run_xnli.py,563,Make sure only the first process in distributed training will download model & vocab,
transformers/examples/run_xnli.py,586,Make sure only the first process in distributed training will download model & vocab,
transformers/examples/run_xnli.py,592,Training,
transformers/examples/run_xnli.py,598,"Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()",
transformers/examples/run_xnli.py,600,Create output directory if needed,
transformers/examples/run_xnli.py,605,"Save a trained model, configuration and tokenizer using `save_pretrained()`.",
transformers/examples/run_xnli.py,606,They can then be reloaded using `from_pretrained()`,
transformers/examples/run_xnli.py,609,Take care of distributed/parallel training,
transformers/examples/run_xnli.py,613,Good practice: save your training arguments together with the trained model,
transformers/examples/run_xnli.py,616,Load a trained model and vocabulary that you have fine-tuned,
transformers/examples/run_xnli.py,621,Evaluation,
transformers/examples/run_xnli.py,630,Reduce logging,
transformers/examples/transformer_base.py,199,Log results,
transformers/examples/transformer_base.py,210,Log and save results to file,
transformers/examples/transformer_base.py,260,init model,
transformers/examples/transformer_base.py,263,Setup distant debugging if needed,
transformers/examples/transformer_base.py,265,Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script,
transformers/examples/utils_multiple_choice.py,1,coding=utf-8,
transformers/examples/utils_multiple_choice.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/examples/utils_multiple_choice.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/examples/utils_multiple_choice.py,4,,
transformers/examples/utils_multiple_choice.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/utils_multiple_choice.py,6,you may not use this file except in compliance with the License.,
transformers/examples/utils_multiple_choice.py,7,You may obtain a copy of the License at,
transformers/examples/utils_multiple_choice.py,8,,
transformers/examples/utils_multiple_choice.py,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/utils_multiple_choice.py,10,,
transformers/examples/utils_multiple_choice.py,11,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/utils_multiple_choice.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/utils_multiple_choice.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/utils_multiple_choice.py,14,See the License for the specific language governing permissions and,
transformers/examples/utils_multiple_choice.py,15,limitations under the License.,
transformers/examples/utils_multiple_choice.py,144,this is not efficient but convenient,
transformers/examples/utils_multiple_choice.py,190,"in the swag dataset, the",
transformers/examples/utils_multiple_choice.py,191,common beginning of each,
transformers/examples/utils_multiple_choice.py,192,"choice is stored in ""sent2"".",
transformers/examples/utils_multiple_choice.py,197,we skip the line with the column names,
transformers/examples/utils_multiple_choice.py,232,There are two types of labels. They should be normalized,
transformers/examples/utils_multiple_choice.py,247,we deleted example which has more than or less than four choices,
transformers/examples/utils_multiple_choice.py,318,this is for cloze question,
transformers/examples/utils_multiple_choice.py,335,The mask has 1 for real tokens and 0 for padding tokens. Only real,
transformers/examples/utils_multiple_choice.py,336,tokens are attended to.,
transformers/examples/utils_multiple_choice.py,339,Zero-pad up to the sequence length.,
transformers/examples/run_language_modeling.py,1,coding=utf-8,
transformers/examples/run_language_modeling.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/examples/run_language_modeling.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/examples/run_language_modeling.py,4,,
transformers/examples/run_language_modeling.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/run_language_modeling.py,6,you may not use this file except in compliance with the License.,
transformers/examples/run_language_modeling.py,7,You may obtain a copy of the License at,
transformers/examples/run_language_modeling.py,8,,
transformers/examples/run_language_modeling.py,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/run_language_modeling.py,10,,
transformers/examples/run_language_modeling.py,11,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/run_language_modeling.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/run_language_modeling.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/run_language_modeling.py,14,See the License for the specific language governing permissions and,
transformers/examples/run_language_modeling.py,15,limitations under the License.,
transformers/examples/run_language_modeling.py,90,Truncate in block of block_size,
transformers/examples/run_language_modeling.py,92,Note that we are loosing the last truncated example here for the sake of simplicity (no padding),
transformers/examples/run_language_modeling.py,93,"If your dataset is small, first you should loook for a bigger one :-) and second you",
transformers/examples/run_language_modeling.py,94,can change this behavior by adding (model specific) padding.,
transformers/examples/run_language_modeling.py,110,"Here, we do not cache the features, operating under the assumption",
transformers/examples/run_language_modeling.py,111,that we will soon use fast multithreaded tokenizers from the,
transformers/examples/run_language_modeling.py,112,`tokenizers` repo everywhere =),
transformers/examples/run_language_modeling.py,167,Check if we should delete older checkpoint(s),
transformers/examples/run_language_modeling.py,188,We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa),
transformers/examples/run_language_modeling.py,198,We only compute loss on masked tokens,
transformers/examples/run_language_modeling.py,200,"80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])",
transformers/examples/run_language_modeling.py,204,"10% of the time, we replace masked input tokens with random word",
transformers/examples/run_language_modeling.py,209,The rest of the time (10% of the time) we keep the masked input tokens unchanged,
transformers/examples/run_language_modeling.py,236,Take care of distributed/parallel training,
transformers/examples/run_language_modeling.py,239,Prepare optimizer and schedule (linear warmup and decay),
transformers/examples/run_language_modeling.py,253,Check if saved optimizer or scheduler states exist,
transformers/examples/run_language_modeling.py,259,Load in optimizer and scheduler states,
transformers/examples/run_language_modeling.py,270,multi-gpu training (should be after apex fp16 initialization),
transformers/examples/run_language_modeling.py,274,Distributed training (should be after apex fp16 initialization),
transformers/examples/run_language_modeling.py,280,Train!,
transformers/examples/run_language_modeling.py,297,Check if continuing training from a checkpoint,
transformers/examples/run_language_modeling.py,300,set global_step to gobal_step of last saved checkpoint from model path,
transformers/examples/run_language_modeling.py,319,Added here for reproducibility,
transformers/examples/run_language_modeling.py,328,Skip past any already trained steps if resuming training,
transformers/examples/run_language_modeling.py,338,model outputs are always tuple in transformers (see doc),
transformers/examples/run_language_modeling.py,341,mean() to average on multi-gpu parallel training,
transformers/examples/run_language_modeling.py,358,Update learning rate schedule,
transformers/examples/run_language_modeling.py,363,Log metrics,
transformers/examples/run_language_modeling.py,366,Only evaluate when single GPU otherwise metrics may not average well,
transformers/examples/run_language_modeling.py,376,Save model checkpoint,
transformers/examples/run_language_modeling.py,381,Take care of distributed/parallel training,
transformers/examples/run_language_modeling.py,408,"Loop to handle MNLI double evaluation (matched, mis-matched)",
transformers/examples/run_language_modeling.py,417,Note that DistributedSampler samples randomly,
transformers/examples/run_language_modeling.py,429,multi-gpu evaluate,
transformers/examples/run_language_modeling.py,433,Eval!,
transformers/examples/run_language_modeling.py,470,Required parameters,
transformers/examples/run_language_modeling.py,484,Other parameters,
transformers/examples/run_language_modeling.py,639,Setup distant debugging if needed,
transformers/examples/run_language_modeling.py,641,Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script,
transformers/examples/run_language_modeling.py,648,"Setup CUDA, GPU & distributed training",
transformers/examples/run_language_modeling.py,652,Initializes the distributed backend which will take care of sychronizing nodes/GPUs,
transformers/examples/run_language_modeling.py,659,Setup logging,
transformers/examples/run_language_modeling.py,674,Set seed,
transformers/examples/run_language_modeling.py,677,Load pretrained model and tokenizer,
transformers/examples/run_language_modeling.py,679,Barrier to make sure only the first process in distributed training download model & vocab,
transformers/examples/run_language_modeling.py,686,"When we release a pip version exposing CONFIG_MAPPING,",
transformers/examples/run_language_modeling.py,687,we can do `config = CONFIG_MAPPING[args.model_type]()`.,
transformers/examples/run_language_modeling.py,705,Our input block size will be the max possible for the model,
transformers/examples/run_language_modeling.py,723,End of barrier to make sure only the first process in distributed training download model & vocab,
transformers/examples/run_language_modeling.py,727,Training,
transformers/examples/run_language_modeling.py,730,"Barrier to make sure only the first process in distributed training process the dataset, and the others will use the cache",
transformers/examples/run_language_modeling.py,740,"Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()",
transformers/examples/run_language_modeling.py,742,Create output directory if needed,
transformers/examples/run_language_modeling.py,747,"Save a trained model, configuration and tokenizer using `save_pretrained()`.",
transformers/examples/run_language_modeling.py,748,They can then be reloaded using `from_pretrained()`,
transformers/examples/run_language_modeling.py,751,Take care of distributed/parallel training,
transformers/examples/run_language_modeling.py,755,Good practice: save your training arguments together with the trained model,
transformers/examples/run_language_modeling.py,758,Load a trained model and vocabulary that you have fine-tuned,
transformers/examples/run_language_modeling.py,763,Evaluation,
transformers/examples/run_language_modeling.py,771,Reduce logging,
transformers/examples/run_tf_glue.py,16,script parameters,
transformers/examples/run_tf_glue.py,38,"Load tokenizer and model from pretrained model/vocabulary. Specify the number of labels to classify (2+: classification, 1: regression)",
transformers/examples/run_tf_glue.py,43,Load dataset via TensorFlow Datasets,
transformers/examples/run_tf_glue.py,47,MNLI expects either validation_matched or validation_mismatched,
transformers/examples/run_tf_glue.py,50,Prepare dataset for GLUE as a tf.data.Dataset instance,
transformers/examples/run_tf_glue.py,53,MNLI expects either validation_matched or validation_mismatched,
transformers/examples/run_tf_glue.py,58,"Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule",
transformers/examples/run_tf_glue.py,61,loss scaling is currently required when using mixed precision,
transformers/examples/run_tf_glue.py,73,Train and evaluate using tf.keras.Model.fit(),
transformers/examples/run_tf_glue.py,85,Save TF2 model,
transformers/examples/run_tf_glue.py,90,Load the TensorFlow model in PyTorch for inspection,
transformers/examples/run_tf_glue.py,91,"This is to demo the interoperability between the two frameworks, you don't have to",
transformers/examples/run_tf_glue.py,92,do this in real life (you can run the inference on the TF model).,
transformers/examples/run_tf_glue.py,95,"Quickly test a few predictions - MRPC is a paraphrasing task, let's see if our model learned the task",
transformers/examples/mm-imdb/run_mmimdb.py,1,coding=utf-8,
transformers/examples/mm-imdb/run_mmimdb.py,2,"Copyright (c) Facebook, Inc. and its affiliates.",
transformers/examples/mm-imdb/run_mmimdb.py,3,Copyright (c) HuggingFace Inc. team.,
transformers/examples/mm-imdb/run_mmimdb.py,4,,
transformers/examples/mm-imdb/run_mmimdb.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/mm-imdb/run_mmimdb.py,6,you may not use this file except in compliance with the License.,
transformers/examples/mm-imdb/run_mmimdb.py,7,You may obtain a copy of the License at,
transformers/examples/mm-imdb/run_mmimdb.py,8,,
transformers/examples/mm-imdb/run_mmimdb.py,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/mm-imdb/run_mmimdb.py,10,,
transformers/examples/mm-imdb/run_mmimdb.py,11,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/mm-imdb/run_mmimdb.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/mm-imdb/run_mmimdb.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/mm-imdb/run_mmimdb.py,14,See the License for the specific language governing permissions and,
transformers/examples/mm-imdb/run_mmimdb.py,15,limitations under the License.,
transformers/examples/mm-imdb/run_mmimdb.py,117,Prepare optimizer and schedule (linear warmup and decay),
transformers/examples/mm-imdb/run_mmimdb.py,138,multi-gpu training (should be after apex fp16 initialization),
transformers/examples/mm-imdb/run_mmimdb.py,142,Distributed training (should be after apex fp16 initialization),
transformers/examples/mm-imdb/run_mmimdb.py,148,Train!,
transformers/examples/mm-imdb/run_mmimdb.py,167,Added here for reproductibility,
transformers/examples/mm-imdb/run_mmimdb.py,182,model outputs are always tuple in transformers (see doc),
transformers/examples/mm-imdb/run_mmimdb.py,186,mean() to average on multi-gpu parallel training,
transformers/examples/mm-imdb/run_mmimdb.py,204,Update learning rate schedule,
transformers/examples/mm-imdb/run_mmimdb.py,212,Only evaluate when single GPU otherwise metrics may not average well,
transformers/examples/mm-imdb/run_mmimdb.py,229,Save model checkpoint,
transformers/examples/mm-imdb/run_mmimdb.py,235,Take care of distributed/parallel training,
transformers/examples/mm-imdb/run_mmimdb.py,266,"Loop to handle MNLI double evaluation (matched, mis-matched)",
transformers/examples/mm-imdb/run_mmimdb.py,274,Note that DistributedSampler samples randomly,
transformers/examples/mm-imdb/run_mmimdb.py,280,multi-gpu eval,
transformers/examples/mm-imdb/run_mmimdb.py,284,Eval!,
transformers/examples/mm-imdb/run_mmimdb.py,307,model outputs are always tuple in transformers (see doc),
transformers/examples/mm-imdb/run_mmimdb.py,346,Required parameters,
transformers/examples/mm-imdb/run_mmimdb.py,376,Other parameters,
transformers/examples/mm-imdb/run_mmimdb.py,483,Setup distant debugging if needed,
transformers/examples/mm-imdb/run_mmimdb.py,485,Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script,
transformers/examples/mm-imdb/run_mmimdb.py,492,"Setup CUDA, GPU & distributed training",
transformers/examples/mm-imdb/run_mmimdb.py,496,Initializes the distributed backend which will take care of sychronizing nodes/GPUs,
transformers/examples/mm-imdb/run_mmimdb.py,504,Setup logging,
transformers/examples/mm-imdb/run_mmimdb.py,519,Set seed,
transformers/examples/mm-imdb/run_mmimdb.py,522,Load pretrained model and tokenizer,
transformers/examples/mm-imdb/run_mmimdb.py,524,Make sure only the first process in distributed training will download model & vocab,
transformers/examples/mm-imdb/run_mmimdb.py,526,Setup model,
transformers/examples/mm-imdb/run_mmimdb.py,547,Make sure only the first process in distributed training will download model & vocab,
transformers/examples/mm-imdb/run_mmimdb.py,553,Training,
transformers/examples/mm-imdb/run_mmimdb.py,565,"Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()",
transformers/examples/mm-imdb/run_mmimdb.py,567,Create output directory if needed,
transformers/examples/mm-imdb/run_mmimdb.py,572,"Save a trained model, configuration and tokenizer using `save_pretrained()`.",
transformers/examples/mm-imdb/run_mmimdb.py,573,They can then be reloaded using `from_pretrained()`,
transformers/examples/mm-imdb/run_mmimdb.py,576,Take care of distributed/parallel training,
transformers/examples/mm-imdb/run_mmimdb.py,580,Good practice: save your training arguments together with the trained model,
transformers/examples/mm-imdb/run_mmimdb.py,583,Load a trained model and vocabulary that you have fine-tuned,
transformers/examples/mm-imdb/run_mmimdb.py,589,Evaluation,
transformers/examples/mm-imdb/run_mmimdb.py,598,Reduce logging,
transformers/examples/mm-imdb/utils_mmimdb.py,1,coding=utf-8,
transformers/examples/mm-imdb/utils_mmimdb.py,2,"Copyright (c) Facebook, Inc. and its affiliates.",
transformers/examples/mm-imdb/utils_mmimdb.py,3,Copyright (c) HuggingFace Inc. team.,
transformers/examples/mm-imdb/utils_mmimdb.py,4,,
transformers/examples/mm-imdb/utils_mmimdb.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/mm-imdb/utils_mmimdb.py,6,you may not use this file except in compliance with the License.,
transformers/examples/mm-imdb/utils_mmimdb.py,7,You may obtain a copy of the License at,
transformers/examples/mm-imdb/utils_mmimdb.py,8,,
transformers/examples/mm-imdb/utils_mmimdb.py,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/mm-imdb/utils_mmimdb.py,10,,
transformers/examples/mm-imdb/utils_mmimdb.py,11,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/mm-imdb/utils_mmimdb.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/mm-imdb/utils_mmimdb.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/mm-imdb/utils_mmimdb.py,14,See the License for the specific language governing permissions and,
transformers/examples/mm-imdb/utils_mmimdb.py,15,limitations under the License.,
transformers/examples/mm-imdb/utils_mmimdb.py,41,Bx3x224x224 -> Bx2048x7x7 -> Bx2048xN -> BxNx2048,
transformers/examples/mm-imdb/utils_mmimdb.py,45,BxNx2048,
transformers/examples/contrib/run_openai_gpt.py,1,coding=utf-8,
transformers/examples/contrib/run_openai_gpt.py,2,"Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.",
transformers/examples/contrib/run_openai_gpt.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/examples/contrib/run_openai_gpt.py,4,,
transformers/examples/contrib/run_openai_gpt.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/contrib/run_openai_gpt.py,6,you may not use this file except in compliance with the License.,
transformers/examples/contrib/run_openai_gpt.py,7,You may obtain a copy of the License at,
transformers/examples/contrib/run_openai_gpt.py,8,,
transformers/examples/contrib/run_openai_gpt.py,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/contrib/run_openai_gpt.py,10,,
transformers/examples/contrib/run_openai_gpt.py,11,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/contrib/run_openai_gpt.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/contrib/run_openai_gpt.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/contrib/run_openai_gpt.py,14,See the License for the specific language governing permissions and,
transformers/examples/contrib/run_openai_gpt.py,15,limitations under the License.,
transformers/examples/contrib/run_openai_gpt.py,67,skip the first line,
transformers/examples/contrib/run_openai_gpt.py,148,Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script,
transformers/examples/contrib/run_openai_gpt.py,170,Load tokenizer and model,
transformers/examples/contrib/run_openai_gpt.py,171,This loading functions also add new tokens and embeddings called `special tokens`,
transformers/examples/contrib/run_openai_gpt.py,172,These new embeddings will be fine-tuned on the RocStories dataset,
transformers/examples/contrib/run_openai_gpt.py,181,Load and encode the datasets,
transformers/examples/contrib/run_openai_gpt.py,196,Compute the max input length for the Transformer,
transformers/examples/contrib/run_openai_gpt.py,203,Max size of input for the pre-trained model,
transformers/examples/contrib/run_openai_gpt.py,205,Prepare inputs tensors and dataloaders,
transformers/examples/contrib/run_openai_gpt.py,217,Prepare optimizer,
transformers/examples/contrib/run_openai_gpt.py,262,Save a trained model,
transformers/examples/contrib/run_openai_gpt.py,264,"Save a trained model, configuration and tokenizer",
transformers/examples/contrib/run_openai_gpt.py,265,Only save the model itself,
transformers/examples/contrib/run_openai_gpt.py,267,"If we save using the predefined names, we can load using `from_pretrained`",
transformers/examples/contrib/run_openai_gpt.py,275,Load a trained model and vocabulary that you have fine-tuned,
transformers/examples/contrib/run_camembert.py,8,Adapted from https://github.com/pytorch/fairseq/blob/master/fairseq/models/roberta/hub_interface.py,
transformers/examples/contrib/run_camembert.py,10,Batch size 1,
transformers/examples/contrib/run_camembert.py,11,The last hidden-state is the first element of the output tuple,
transformers/examples/contrib/run_swag.py,1,coding=utf-8,
transformers/examples/contrib/run_swag.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/examples/contrib/run_swag.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/examples/contrib/run_swag.py,4,,
transformers/examples/contrib/run_swag.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/contrib/run_swag.py,6,you may not use this file except in compliance with the License.,
transformers/examples/contrib/run_swag.py,7,You may obtain a copy of the License at,
transformers/examples/contrib/run_swag.py,8,,
transformers/examples/contrib/run_swag.py,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/contrib/run_swag.py,10,,
transformers/examples/contrib/run_swag.py,11,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/contrib/run_swag.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/contrib/run_swag.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/contrib/run_swag.py,14,See the License for the specific language governing permissions and,
transformers/examples/contrib/run_swag.py,15,limitations under the License.,
transformers/examples/contrib/run_swag.py,115,"in the swag dataset, the",
transformers/examples/contrib/run_swag.py,116,common beginning of each,
transformers/examples/contrib/run_swag.py,117,"choice is stored in ""sent2"".",
transformers/examples/contrib/run_swag.py,124,we skip the line with the column names,
transformers/examples/contrib/run_swag.py,133,"Swag is a multiple choice task. To perform this task using Bert,",
transformers/examples/contrib/run_swag.py,134,"we will use the formatting proposed in ""Improving Language",
transformers/examples/contrib/run_swag.py,135,"Understanding by Generative Pre-Training"" and suggested by",
transformers/examples/contrib/run_swag.py,136,@jacobdevlin-google in this issue,
transformers/examples/contrib/run_swag.py,137,https://github.com/google-research/bert/issues/38.,
transformers/examples/contrib/run_swag.py,138,,
transformers/examples/contrib/run_swag.py,139,Each choice will correspond to a sample on which we run the,
transformers/examples/contrib/run_swag.py,140,"inference. For a given Swag example, we will create the 4",
transformers/examples/contrib/run_swag.py,141,following inputs:,
transformers/examples/contrib/run_swag.py,142,- [CLS] context [SEP] choice_1 [SEP],
transformers/examples/contrib/run_swag.py,143,- [CLS] context [SEP] choice_2 [SEP],
transformers/examples/contrib/run_swag.py,144,- [CLS] context [SEP] choice_3 [SEP],
transformers/examples/contrib/run_swag.py,145,- [CLS] context [SEP] choice_4 [SEP],
transformers/examples/contrib/run_swag.py,146,The model will output a single value for each input. To get the,
transformers/examples/contrib/run_swag.py,147,"final decision of the model, we will run a softmax over these 4",
transformers/examples/contrib/run_swag.py,148,outputs.,
transformers/examples/contrib/run_swag.py,156,We create a copy of the context tokens in order to be,
transformers/examples/contrib/run_swag.py,157,able to shrink it according to ending_tokens,
transformers/examples/contrib/run_swag.py,160,Modifies `context_tokens_choice` and `ending_tokens` in,
transformers/examples/contrib/run_swag.py,161,place so that the total length is less than the,
transformers/examples/contrib/run_swag.py,162,"specified length.  Account for [CLS], [SEP], [SEP] with",
transformers/examples/contrib/run_swag.py,163,"""- 3""",
transformers/examples/contrib/run_swag.py,172,Zero-pad up to the sequence length.,
transformers/examples/contrib/run_swag.py,205,This is a simple heuristic which will always truncate the longer sequence,
transformers/examples/contrib/run_swag.py,206,one token at a time. This makes more sense than truncating an equal percent,
transformers/examples/contrib/run_swag.py,207,"of tokens from each, since if one sequence is very short then each token",
transformers/examples/contrib/run_swag.py,208,that's truncated likely contains more information than a longer sequence.,
transformers/examples/contrib/run_swag.py,238,"Make sure only the first process in distributed training process the dataset, and the others will use the cache",
transformers/examples/contrib/run_swag.py,240,Load data features from cache or dataset file,
transformers/examples/contrib/run_swag.py,263,"Make sure only the first process in distributed training process the dataset, and the others will use the cache",
transformers/examples/contrib/run_swag.py,265,Convert to Tensors and build dataset,
transformers/examples/contrib/run_swag.py,296,Prepare optimizer and schedule (linear warmup and decay),
transformers/examples/contrib/run_swag.py,316,multi-gpu training (should be after apex fp16 initialization),
transformers/examples/contrib/run_swag.py,320,Distributed training (should be after apex fp16 initialization),
transformers/examples/contrib/run_swag.py,326,Train!,
transformers/examples/contrib/run_swag.py,344,Added here for reproductibility,
transformers/examples/contrib/run_swag.py,353,"'token_type_ids':  None if args.model_type == 'xlm' else batch[2],",
transformers/examples/contrib/run_swag.py,357,"if args.model_type in ['xlnet', 'xlm']:",
transformers/examples/contrib/run_swag.py,358,"inputs.update({'cls_index': batch[5],",
transformers/examples/contrib/run_swag.py,359,'p_mask':       batch[6]}),
transformers/examples/contrib/run_swag.py,361,model outputs are always tuple in transformers (see doc),
transformers/examples/contrib/run_swag.py,364,mean() to average on multi-gpu parallel (not distributed) training,
transformers/examples/contrib/run_swag.py,379,Update learning rate schedule,
transformers/examples/contrib/run_swag.py,384,Log metrics,
transformers/examples/contrib/run_swag.py,387,Only evaluate when single GPU otherwise metrics may not average well,
transformers/examples/contrib/run_swag.py,396,Save model checkpoint,
transformers/examples/contrib/run_swag.py,402,Take care of distributed/parallel training,
transformers/examples/contrib/run_swag.py,428,Note that DistributedSampler samples randomly,
transformers/examples/contrib/run_swag.py,432,Eval!,
transformers/examples/contrib/run_swag.py,447,'token_type_ids': None if args.model_type == 'xlm' else batch[2]  # XLM don't use segment_ids,
transformers/examples/contrib/run_swag.py,452,"if args.model_type in ['xlnet', 'xlm']:",
transformers/examples/contrib/run_swag.py,453,"inputs.update({'cls_index': batch[4],",
transformers/examples/contrib/run_swag.py,454,'p_mask':    batch[5]}),
transformers/examples/contrib/run_swag.py,484,Required parameters,
transformers/examples/contrib/run_swag.py,517,Other parameters,
transformers/examples/contrib/run_swag.py,613,Setup distant debugging if needed,
transformers/examples/contrib/run_swag.py,615,Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script,
transformers/examples/contrib/run_swag.py,622,"Setup CUDA, GPU & distributed training",
transformers/examples/contrib/run_swag.py,626,Initializes the distributed backend which will take care of sychronizing nodes/GPUs,
transformers/examples/contrib/run_swag.py,633,Setup logging,
transformers/examples/contrib/run_swag.py,648,Set seed,
transformers/examples/contrib/run_swag.py,651,Load pretrained model and tokenizer,
transformers/examples/contrib/run_swag.py,653,Make sure only the first process in distributed training will download model & vocab,
transformers/examples/contrib/run_swag.py,666,Make sure only the first process in distributed training will download model & vocab,
transformers/examples/contrib/run_swag.py,672,Training,
transformers/examples/contrib/run_swag.py,678,Save the trained model and the tokenizer,
transformers/examples/contrib/run_swag.py,680,Create output directory if needed,
transformers/examples/contrib/run_swag.py,685,"Save a trained model, configuration and tokenizer using `save_pretrained()`.",
transformers/examples/contrib/run_swag.py,686,They can then be reloaded using `from_pretrained()`,
transformers/examples/contrib/run_swag.py,689,Take care of distributed/parallel training,
transformers/examples/contrib/run_swag.py,693,Good practice: save your training arguments together with the trained model,
transformers/examples/contrib/run_swag.py,696,Load a trained model and vocabulary that you have fine-tuned,
transformers/examples/contrib/run_swag.py,701,Evaluation - we can ask to evaluate all the checkpoints (sub-directories) in a directory,
transformers/examples/contrib/run_swag.py,707,"if do_train is False and do_eval is true, load model directly from pretrained.",
transformers/examples/contrib/run_swag.py,714,Reduce model loading logs,
transformers/examples/contrib/run_swag.py,719,Reload the model,
transformers/examples/contrib/run_swag.py,725,Evaluate,
transformers/examples/contrib/run_transfo_xl.py,1,coding=utf-8,
transformers/examples/contrib/run_transfo_xl.py,2,"Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.",
transformers/examples/contrib/run_transfo_xl.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/examples/contrib/run_transfo_xl.py,4,,
transformers/examples/contrib/run_transfo_xl.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/contrib/run_transfo_xl.py,6,you may not use this file except in compliance with the License.,
transformers/examples/contrib/run_transfo_xl.py,7,You may obtain a copy of the License at,
transformers/examples/contrib/run_transfo_xl.py,8,,
transformers/examples/contrib/run_transfo_xl.py,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/contrib/run_transfo_xl.py,10,,
transformers/examples/contrib/run_transfo_xl.py,11,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/contrib/run_transfo_xl.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/contrib/run_transfo_xl.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/contrib/run_transfo_xl.py,14,See the License for the specific language governing permissions and,
transformers/examples/contrib/run_transfo_xl.py,15,limitations under the License.,
transformers/examples/contrib/run_transfo_xl.py,61,Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script,
transformers/examples/contrib/run_transfo_xl.py,71,Load a pre-processed dataset,
transformers/examples/contrib/run_transfo_xl.py,72,You can also build the corpus yourself using TransfoXLCorpus methods,
transformers/examples/contrib/run_transfo_xl.py,73,The pre-processing involve computing word frequencies to prepare the Adaptive input and SoftMax,
transformers/examples/contrib/run_transfo_xl.py,74,and tokenizing the dataset,
transformers/examples/contrib/run_transfo_xl.py,75,The pre-processed corpus is a convertion (using the conversion script ),
transformers/examples/contrib/run_transfo_xl.py,81,Load a pre-trained model,
transformers/examples/contrib/run_transfo_xl.py,97,,
transformers/examples/contrib/run_transfo_xl.py,98,Evaluation code,
transformers/examples/contrib/run_transfo_xl.py,99,,
transformers/examples/contrib/run_transfo_xl.py,101,Turn on evaluation mode which disables dropout.,
transformers/examples/contrib/run_transfo_xl.py,117,Run on test data.,
transformers/examples/hans/utils_hans.py,1,coding=utf-8,
transformers/examples/hans/utils_hans.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/examples/hans/utils_hans.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/examples/hans/utils_hans.py,4,,
transformers/examples/hans/utils_hans.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/hans/utils_hans.py,6,you may not use this file except in compliance with the License.,
transformers/examples/hans/utils_hans.py,7,You may obtain a copy of the License at,
transformers/examples/hans/utils_hans.py,8,,
transformers/examples/hans/utils_hans.py,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/hans/utils_hans.py,10,,
transformers/examples/hans/utils_hans.py,11,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/hans/utils_hans.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/hans/utils_hans.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/hans/utils_hans.py,14,See the License for the specific language governing permissions and,
transformers/examples/hans/utils_hans.py,15,limitations under the License.,
transformers/examples/hans/hans_processors.py,1,coding=utf-8,
transformers/examples/hans/hans_processors.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/examples/hans/hans_processors.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/examples/hans/hans_processors.py,4,,
transformers/examples/hans/hans_processors.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/hans/hans_processors.py,6,you may not use this file except in compliance with the License.,
transformers/examples/hans/hans_processors.py,7,You may obtain a copy of the License at,
transformers/examples/hans/hans_processors.py,8,,
transformers/examples/hans/hans_processors.py,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/hans/hans_processors.py,10,,
transformers/examples/hans/hans_processors.py,11,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/hans/hans_processors.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/hans/hans_processors.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/hans/hans_processors.py,14,See the License for the specific language governing permissions and,
transformers/examples/hans/hans_processors.py,15,limitations under the License.,
transformers/examples/hans/hans_processors.py,92,The mask has 1 for real tokens and 0 for padding tokens. Only real,
transformers/examples/hans/hans_processors.py,93,tokens are attended to.,
transformers/examples/hans/hans_processors.py,96,Zero-pad up to the sequence length.,
transformers/examples/hans/test_hans.py,1,coding=utf-8,
transformers/examples/hans/test_hans.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/examples/hans/test_hans.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/examples/hans/test_hans.py,4,,
transformers/examples/hans/test_hans.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/hans/test_hans.py,6,you may not use this file except in compliance with the License.,
transformers/examples/hans/test_hans.py,7,You may obtain a copy of the License at,
transformers/examples/hans/test_hans.py,8,,
transformers/examples/hans/test_hans.py,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/hans/test_hans.py,10,,
transformers/examples/hans/test_hans.py,11,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/hans/test_hans.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/hans/test_hans.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/hans/test_hans.py,14,See the License for the specific language governing permissions and,
transformers/examples/hans/test_hans.py,15,limitations under the License.,
transformers/examples/hans/test_hans.py,109,Prepare optimizer and schedule (linear warmup and decay),
transformers/examples/hans/test_hans.py,130,multi-gpu training (should be after apex fp16 initialization),
transformers/examples/hans/test_hans.py,134,Distributed training (should be after apex fp16 initialization),
transformers/examples/hans/test_hans.py,140,Train!,
transformers/examples/hans/test_hans.py,158,Added here for reproductibility (even between python 2 and 3),
transformers/examples/hans/test_hans.py,168,"XLM, DistilBERT and RoBERTa don't use segment_ids",
transformers/examples/hans/test_hans.py,170,model outputs are always tuple in transformers (see doc),
transformers/examples/hans/test_hans.py,173,mean() to average on multi-gpu parallel training,
transformers/examples/hans/test_hans.py,191,Update learning rate schedule,
transformers/examples/hans/test_hans.py,199,Only evaluate when single GPU otherwise metrics may not average well,
transformers/examples/hans/test_hans.py,213,"print(json.dumps({**logs, **{'step': global_step}}))",
transformers/examples/hans/test_hans.py,216,Save model checkpoint,
transformers/examples/hans/test_hans.py,222,Take care of distributed/parallel training,
transformers/examples/hans/test_hans.py,241,"Loop to handle MNLI double evaluation (matched, mis-matched)",
transformers/examples/hans/test_hans.py,253,Note that DistributedSampler samples randomly,
transformers/examples/hans/test_hans.py,257,multi-gpu eval,
transformers/examples/hans/test_hans.py,261,Eval!,
transformers/examples/hans/test_hans.py,278,"XLM, DistilBERT and RoBERTa don't use segment_ids",
transformers/examples/hans/test_hans.py,310,"Make sure only the first process in distributed training process the dataset, and the others will use the cache",
transformers/examples/hans/test_hans.py,314,Load data features from cache or dataset file,
transformers/examples/hans/test_hans.py,333,HACK(label indices are swapped in RoBERTa pretrained model),
transformers/examples/hans/test_hans.py,344,pad on the left for xlnet,
transformers/examples/hans/test_hans.py,353,"Make sure only the first process in distributed training process the dataset, and the others will use the cache",
transformers/examples/hans/test_hans.py,355,Convert to Tensors and build dataset,
transformers/examples/hans/test_hans.py,372,Required parameters,
transformers/examples/hans/test_hans.py,409,Other parameters,
transformers/examples/hans/test_hans.py,511,Setup distant debugging if needed,
transformers/examples/hans/test_hans.py,513,Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script,
transformers/examples/hans/test_hans.py,520,"Setup CUDA, GPU & distributed training",
transformers/examples/hans/test_hans.py,524,Initializes the distributed backend which will take care of sychronizing nodes/GPUs,
transformers/examples/hans/test_hans.py,531,Setup logging,
transformers/examples/hans/test_hans.py,546,Set seed,
transformers/examples/hans/test_hans.py,549,Prepare GLUE task,
transformers/examples/hans/test_hans.py,558,Load pretrained model and tokenizer,
transformers/examples/hans/test_hans.py,560,Make sure only the first process in distributed training will download model & vocab,
transformers/examples/hans/test_hans.py,583,Make sure only the first process in distributed training will download model & vocab,
transformers/examples/hans/test_hans.py,589,Training,
transformers/examples/hans/test_hans.py,595,"Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()",
transformers/examples/hans/test_hans.py,597,Create output directory if needed,
transformers/examples/hans/test_hans.py,602,"Save a trained model, configuration and tokenizer using `save_pretrained()`.",
transformers/examples/hans/test_hans.py,603,They can then be reloaded using `from_pretrained()`,
transformers/examples/hans/test_hans.py,606,Take care of distributed/parallel training,
transformers/examples/hans/test_hans.py,610,Good practice: save your training arguments together with the trained model,
transformers/examples/hans/test_hans.py,613,Load a trained model and vocabulary that you have fine-tuned,
transformers/examples/hans/test_hans.py,618,Evaluation,
transformers/examples/hans/test_hans.py,627,Reduce logging,
transformers/examples/glue/run_pl_glue.py,75,We test on dev set to compare to benchmarks without having to submit to GLUE server,
transformers/examples/glue/run_pl_glue.py,133,updating to test_epoch_end instead of deprecated test_end,
transformers/examples/glue/run_pl_glue.py,136,Converting to the dic required by pl,
transformers/examples/glue/run_pl_glue.py,137,https://github.com/PyTorchLightning/pytorch-lightning/blob/master/\,
transformers/examples/glue/run_pl_glue.py,138,pytorch_lightning/trainer/logging.py#L139,
transformers/examples/glue/run_pl_glue.py,140,`val_loss` is the key returned by `self._eval_end()` but actually refers to `test_loss`,
transformers/examples/glue/run_pl_glue.py,145,Add NER specific options,
transformers/examples/glue/run_pl_glue.py,180,"If output_dir not provided, a folder will be generated in pwd",
transformers/examples/glue/run_pl_glue.py,188,"Optionally, predict on dev set and write to output_dir",
transformers/examples/ner/run_tf_ner.py,1,coding=utf-8,
transformers/examples/ner/run_tf_ner.py,249,Log metrics,
transformers/examples/ner/run_tf_ner.py,252,Only evaluate when single GPU otherwise metrics may not average well,
transformers/examples/ner/run_tf_ner.py,286,Save model checkpoint,
transformers/examples/ner/run_tf_ner.py,410,Load data features from cache or dataset file,
transformers/examples/ner/run_tf_ner.py,429,xlnet has a cls token at the end,
transformers/examples/ner/run_tf_ner.py,434,"roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805",
transformers/examples/ner/run_tf_ner.py,436,pad on the left for xlnet,
transformers/examples/ner/run_tf_ner.py,508,Training,
transformers/examples/ner/run_tf_ner.py,549,Evaluation,
transformers/examples/ner/run_ner.py,1,coding=utf-8,
transformers/examples/ner/run_ner.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/examples/ner/run_ner.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/examples/ner/run_ner.py,4,,
transformers/examples/ner/run_ner.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/ner/run_ner.py,6,you may not use this file except in compliance with the License.,
transformers/examples/ner/run_ner.py,7,You may obtain a copy of the License at,
transformers/examples/ner/run_ner.py,8,,
transformers/examples/ner/run_ner.py,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/ner/run_ner.py,10,,
transformers/examples/ner/run_ner.py,11,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/ner/run_ner.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/ner/run_ner.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/ner/run_ner.py,14,See the License for the specific language governing permissions and,
transformers/examples/ner/run_ner.py,15,limitations under the License.,
transformers/examples/ner/run_ner.py,84,Prepare optimizer and schedule (linear warmup and decay),
transformers/examples/ner/run_ner.py,98,Check if saved optimizer or scheduler states exist,
transformers/examples/ner/run_ner.py,102,Load in optimizer and scheduler states,
transformers/examples/ner/run_ner.py,113,multi-gpu training (should be after apex fp16 initialization),
transformers/examples/ner/run_ner.py,117,Distributed training (should be after apex fp16 initialization),
transformers/examples/ner/run_ner.py,123,Train!,
transformers/examples/ner/run_ner.py,140,Check if continuing training from a checkpoint,
transformers/examples/ner/run_ner.py,142,set global_step to gobal_step of last saved checkpoint from model path,
transformers/examples/ner/run_ner.py,160,Added here for reproductibility,
transformers/examples/ner/run_ner.py,165,Skip past any already trained steps if resuming training,
transformers/examples/ner/run_ner.py,176,"XLM and RoBERTa don""t use segment_ids",
transformers/examples/ner/run_ner.py,179,model outputs are always tuple in pytorch-transformers (see doc),
transformers/examples/ner/run_ner.py,182,mean() to average on multi-gpu parallel training,
transformers/examples/ner/run_ner.py,200,Update learning rate schedule,
transformers/examples/ner/run_ner.py,205,Log metrics,
transformers/examples/ner/run_ner.py,208,Only evaluate when single GPU otherwise metrics may not average well,
transformers/examples/ner/run_ner.py,217,Save model checkpoint,
transformers/examples/ner/run_ner.py,223,Take care of distributed/parallel training,
transformers/examples/ner/run_ner.py,251,Note that DistributedSampler samples randomly,
transformers/examples/ner/run_ner.py,255,multi-gpu evaluate,
transformers/examples/ner/run_ner.py,259,Eval!,
transformers/examples/ner/run_ner.py,276,"XLM and RoBERTa don""t use segment_ids",
transformers/examples/ner/run_ner.py,281,mean() to average on multi-gpu parallel evaluating,
transformers/examples/ner/run_ner.py,322,"Make sure only the first process in distributed training process the dataset, and the others will use the cache",
transformers/examples/ner/run_ner.py,324,Load data features from cache or dataset file,
transformers/examples/ner/run_ner.py,343,xlnet has a cls token at the end,
transformers/examples/ner/run_ner.py,348,"roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805",
transformers/examples/ner/run_ner.py,350,pad on the left for xlnet,
transformers/examples/ner/run_ner.py,360,"Make sure only the first process in distributed training process the dataset, and the others will use the cache",
transformers/examples/ner/run_ner.py,362,Convert to Tensors and build dataset,
transformers/examples/ner/run_ner.py,375,Required parameters,
transformers/examples/ner/run_ner.py,405,Other parameters,
transformers/examples/ner/run_ner.py,522,Setup distant debugging if needed,
transformers/examples/ner/run_ner.py,524,Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script,
transformers/examples/ner/run_ner.py,531,"Setup CUDA, GPU & distributed training",
transformers/examples/ner/run_ner.py,535,Initializes the distributed backend which will take care of sychronizing nodes/GPUs,
transformers/examples/ner/run_ner.py,542,Setup logging,
transformers/examples/ner/run_ner.py,557,Set seed,
transformers/examples/ner/run_ner.py,560,Prepare CONLL-2003 task,
transformers/examples/ner/run_ner.py,563,Use cross entropy ignore index as padding label id so that only real label ids contribute to the loss later,
transformers/examples/ner/run_ner.py,566,Load pretrained model and tokenizer,
transformers/examples/ner/run_ner.py,568,Make sure only the first process in distributed training will download model & vocab,
transformers/examples/ner/run_ner.py,593,Make sure only the first process in distributed training will download model & vocab,
transformers/examples/ner/run_ner.py,599,Training,
transformers/examples/ner/run_ner.py,605,"Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()",
transformers/examples/ner/run_ner.py,607,Create output directory if needed,
transformers/examples/ner/run_ner.py,612,"Save a trained model, configuration and tokenizer using `save_pretrained()`.",
transformers/examples/ner/run_ner.py,613,They can then be reloaded using `from_pretrained()`,
transformers/examples/ner/run_ner.py,616,Take care of distributed/parallel training,
transformers/examples/ner/run_ner.py,620,Good practice: save your training arguments together with the trained model,
transformers/examples/ner/run_ner.py,623,Evaluation,
transformers/examples/ner/run_ner.py,632,Reduce logging,
transformers/examples/ner/run_ner.py,652,Save results,
transformers/examples/ner/run_ner.py,657,Save predictions,
transformers/examples/ner/run_pl_ner.py,41,"XLM and RoBERTa don""t use segment_ids",
transformers/examples/ner/run_pl_ner.py,94,"XLM and RoBERTa don""t use segment_ids",
transformers/examples/ner/run_pl_ner.py,130,todo: update to validation_epoch_end instead of deprecated validation_end,
transformers/examples/ner/run_pl_ner.py,131,when stable,
transformers/examples/ner/run_pl_ner.py,137,updating to test_epoch_end instead of deprecated test_end,
transformers/examples/ner/run_pl_ner.py,140,Converting to the dict required by pl,
transformers/examples/ner/run_pl_ner.py,141,https://github.com/PyTorchLightning/pytorch-lightning/blob/master/\,
transformers/examples/ner/run_pl_ner.py,142,pytorch_lightning/trainer/logging.py#L139,
transformers/examples/ner/run_pl_ner.py,144,`val_loss` is the key returned by `self._eval_end()` but actually refers to `test_loss`,
transformers/examples/ner/run_pl_ner.py,149,Add NER specific options,
transformers/examples/ner/run_pl_ner.py,190,See https://github.com/huggingface/transformers/issues/3159,
transformers/examples/ner/run_pl_ner.py,191,pl use this format to create a checkpoint:,
transformers/examples/ner/run_pl_ner.py,192,https://github.com/PyTorchLightning/pytorch-lightning/blob/master\,
transformers/examples/ner/run_pl_ner.py,193,/pytorch_lightning/callbacks/model_checkpoint.py#L169,
transformers/examples/ner/utils_ner.py,1,coding=utf-8,
transformers/examples/ner/utils_ner.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/examples/ner/utils_ner.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/examples/ner/utils_ner.py,4,,
transformers/examples/ner/utils_ner.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/ner/utils_ner.py,6,you may not use this file except in compliance with the License.,
transformers/examples/ner/utils_ner.py,7,You may obtain a copy of the License at,
transformers/examples/ner/utils_ner.py,8,,
transformers/examples/ner/utils_ner.py,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/ner/utils_ner.py,10,,
transformers/examples/ner/utils_ner.py,11,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/ner/utils_ner.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/ner/utils_ner.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/ner/utils_ner.py,14,See the License for the specific language governing permissions and,
transformers/examples/ner/utils_ner.py,15,limitations under the License.,
transformers/examples/ner/utils_ner.py,73,"Examples could have no label for mode = ""test""",
transformers/examples/ner/utils_ner.py,116,"bert-base-multilingual-cased sometimes output ""nothing ([]) when calling tokenize with just a space.",
transformers/examples/ner/utils_ner.py,119,"Use the real label id for the first token of the word, and padding ids for the remaining tokens",
transformers/examples/ner/utils_ner.py,122,"Account for [CLS] and [SEP] with ""- 2"" and with ""- 3"" for RoBERTa.",
transformers/examples/ner/utils_ner.py,128,The convention in BERT is:,
transformers/examples/ner/utils_ner.py,129,(a) For sequence pairs:,
transformers/examples/ner/utils_ner.py,130,tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP],
transformers/examples/ner/utils_ner.py,131,type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1,
transformers/examples/ner/utils_ner.py,132,(b) For single sequences:,
transformers/examples/ner/utils_ner.py,133,tokens:   [CLS] the dog is hairy . [SEP],
transformers/examples/ner/utils_ner.py,134,type_ids:   0   0   0   0  0     0   0,
transformers/examples/ner/utils_ner.py,135,,
transformers/examples/ner/utils_ner.py,136,"Where ""type_ids"" are used to indicate whether this is the first",
transformers/examples/ner/utils_ner.py,137,sequence or the second sequence. The embedding vectors for `type=0` and,
transformers/examples/ner/utils_ner.py,138,`type=1` were learned during pre-training and are added to the wordpiece,
transformers/examples/ner/utils_ner.py,139,embedding vector (and position vector). This is not *strictly* necessary,
transformers/examples/ner/utils_ner.py,140,"since the [SEP] token unambiguously separates the sequences, but it makes",
transformers/examples/ner/utils_ner.py,141,it easier for the model to learn the concept of sequences.,
transformers/examples/ner/utils_ner.py,142,,
transformers/examples/ner/utils_ner.py,143,"For classification tasks, the first vector (corresponding to [CLS]) is",
transformers/examples/ner/utils_ner.py,144,"used as as the ""sentence vector"". Note that this only makes sense because",
transformers/examples/ner/utils_ner.py,145,the entire model is fine-tuned.,
transformers/examples/ner/utils_ner.py,149,roberta uses an extra separator b/w pairs of sentences,
transformers/examples/ner/utils_ner.py,165,The mask has 1 for real tokens and 0 for padding tokens. Only real,
transformers/examples/ner/utils_ner.py,166,tokens are attended to.,
transformers/examples/ner/utils_ner.py,169,Zero-pad up to the sequence length.,
transformers/examples/translation/t5/evaluate_wmt.py,25,update config with summarization specific params,
transformers/examples/summarization/t5/evaluate_cnn.py,25,update config with summarization specific params,
transformers/examples/summarization/bertabs/modeling_bertabs.py,1,MIT License,
transformers/examples/summarization/bertabs/modeling_bertabs.py,3,Copyright (c) 2019 Yang Liu and the HuggingFace team,
transformers/examples/summarization/bertabs/modeling_bertabs.py,5,"Permission is hereby granted, free of charge, to any person obtaining a copy",
transformers/examples/summarization/bertabs/modeling_bertabs.py,6,"of this software and associated documentation files (the ""Software""), to deal",
transformers/examples/summarization/bertabs/modeling_bertabs.py,7,"in the Software without restriction, including without limitation the rights",
transformers/examples/summarization/bertabs/modeling_bertabs.py,8,"to use, copy, modify, merge, publish, distribute, sublicense, and/or sell",
transformers/examples/summarization/bertabs/modeling_bertabs.py,9,"copies of the Software, and to permit persons to whom the Software is",
transformers/examples/summarization/bertabs/modeling_bertabs.py,10,"furnished to do so, subject to the following conditions:",
transformers/examples/summarization/bertabs/modeling_bertabs.py,12,The above copyright notice and this permission notice shall be included in all,
transformers/examples/summarization/bertabs/modeling_bertabs.py,13,copies or substantial portions of the Software.,
transformers/examples/summarization/bertabs/modeling_bertabs.py,15,"THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR",
transformers/examples/summarization/bertabs/modeling_bertabs.py,16,"IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,",
transformers/examples/summarization/bertabs/modeling_bertabs.py,17,FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE,
transformers/examples/summarization/bertabs/modeling_bertabs.py,18,"AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER",
transformers/examples/summarization/bertabs/modeling_bertabs.py,19,"LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,",
transformers/examples/summarization/bertabs/modeling_bertabs.py,20,OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE,
transformers/examples/summarization/bertabs/modeling_bertabs.py,21,SOFTWARE.,
transformers/examples/summarization/bertabs/modeling_bertabs.py,54,"If pre-trained weights are passed for Bert, load these.",
transformers/examples/summarization/bertabs/modeling_bertabs.py,156,Basic attributes.,
transformers/examples/summarization/bertabs/modeling_bertabs.py,162,Build TransformerDecoder.,
transformers/examples/summarization/bertabs/modeling_bertabs.py,169,"forward(input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask)",
transformers/examples/summarization/bertabs/modeling_bertabs.py,170,"def forward(self, input_ids, state, attention_mask=None, memory_lengths=None,",
transformers/examples/summarization/bertabs/modeling_bertabs.py,171,"step=None, cache=None, encoder_attention_mask=None, encoder_hidden_states=None, memory_masks=None):",
transformers/examples/summarization/bertabs/modeling_bertabs.py,187,Name conversion,
transformers/examples/summarization/bertabs/modeling_bertabs.py,192,src_words = state.src,
transformers/examples/summarization/bertabs/modeling_bertabs.py,198,Decoder padding mask,
transformers/examples/summarization/bertabs/modeling_bertabs.py,203,Encoder padding mask,
transformers/examples/summarization/bertabs/modeling_bertabs.py,210,Pass through the embeddings,
transformers/examples/summarization/bertabs/modeling_bertabs.py,213,len x batch x embedding_dim,
transformers/examples/summarization/bertabs/modeling_bertabs.py,244,Decoders in transformers return a tuple. Beam search will fail,
transformers/examples/summarization/bertabs/modeling_bertabs.py,245,if we don't follow this convention.,
transformers/examples/summarization/bertabs/modeling_bertabs.py,246,", state",
transformers/examples/summarization/bertabs/modeling_bertabs.py,306,"Register self.mask as a saved_state in TransformerDecoderLayer, so",
transformers/examples/summarization/bertabs/modeling_bertabs.py,307,it gets TransformerDecoderLayer's cuda behavior automatically.,
transformers/examples/summarization/bertabs/modeling_bertabs.py,346,return output,
transformers/examples/summarization/bertabs/modeling_bertabs.py,458,"1) Project key, value, and query.",
transformers/examples/summarization/bertabs/modeling_bertabs.py,505,2) Calculate and scale scores.,
transformers/examples/summarization/bertabs/modeling_bertabs.py,513,3) Apply attention dropout and compute context vectors.,
transformers/examples/summarization/bertabs/modeling_bertabs.py,656,,
transformers/examples/summarization/bertabs/modeling_bertabs.py,657,TRANSLATOR,
transformers/examples/summarization/bertabs/modeling_bertabs.py,658,The following code is used to generate summaries using the,
transformers/examples/summarization/bertabs/modeling_bertabs.py,659,pre-trained weights and beam search.,
transformers/examples/summarization/bertabs/modeling_bertabs.py,660,,
transformers/examples/summarization/bertabs/modeling_bertabs.py,664,we should be able to refactor the global scorer a lot,
transformers/examples/summarization/bertabs/modeling_bertabs.py,799,Where the beam search lives,
transformers/examples/summarization/bertabs/modeling_bertabs.py,800,I have no idea why it is being called from the method above,
transformers/examples/summarization/bertabs/modeling_bertabs.py,805,The batch object is funny,
transformers/examples/summarization/bertabs/modeling_bertabs.py,806,Instead of just looking at the size of the arguments we encapsulate,
transformers/examples/summarization/bertabs/modeling_bertabs.py,807,a size argument.,
transformers/examples/summarization/bertabs/modeling_bertabs.py,808,Where is it defined?,
transformers/examples/summarization/bertabs/modeling_bertabs.py,819,Tile states and memory beam_size times.,
transformers/examples/summarization/bertabs/modeling_bertabs.py,826,Give full probability to the first beam on the first step.,
transformers/examples/summarization/bertabs/modeling_bertabs.py,829,Structure that holds finished hypotheses.,
transformers/examples/summarization/bertabs/modeling_bertabs.py,830,noqa: F812,
transformers/examples/summarization/bertabs/modeling_bertabs.py,833,noqa: F812,
transformers/examples/summarization/bertabs/modeling_bertabs.py,834,noqa: F812,
transformers/examples/summarization/bertabs/modeling_bertabs.py,841,Decoder forward.,
transformers/examples/summarization/bertabs/modeling_bertabs.py,846,Generator forward.,
transformers/examples/summarization/bertabs/modeling_bertabs.py,853,Multiply probs by the beam probability.,
transformers/examples/summarization/bertabs/modeling_bertabs.py,859,Flatten probs into a list of possibilities.,
transformers/examples/summarization/bertabs/modeling_bertabs.py,882,Recover log probs.,
transformers/examples/summarization/bertabs/modeling_bertabs.py,885,Resolve beam origin and true word ids.,
transformers/examples/summarization/bertabs/modeling_bertabs.py,889,Map beam_index to batch_index in the flat representation.,
transformers/examples/summarization/bertabs/modeling_bertabs.py,893,Append last prediction.,
transformers/examples/summarization/bertabs/modeling_bertabs.py,899,End condition is top beam is finished.,
transformers/examples/summarization/bertabs/modeling_bertabs.py,901,Save finished hypotheses.,
transformers/examples/summarization/bertabs/modeling_bertabs.py,909,Store finished hypotheses for this batch.,
transformers/examples/summarization/bertabs/modeling_bertabs.py,912,"If the batch reached the end, save the n_best hypotheses.",
transformers/examples/summarization/bertabs/modeling_bertabs.py,920,"If all sentences are translated, no need to go further.",
transformers/examples/summarization/bertabs/modeling_bertabs.py,923,Remove finished batches for the next step.,
transformers/examples/summarization/bertabs/modeling_bertabs.py,928,Reorder states.,
transformers/examples/summarization/bertabs/modeling_bertabs.py,978,,
transformers/examples/summarization/bertabs/modeling_bertabs.py,979,Optimizer for training. We keep this here in case we want to add,
transformers/examples/summarization/bertabs/modeling_bertabs.py,980,a finetuning script.,
transformers/examples/summarization/bertabs/modeling_bertabs.py,981,,
transformers/examples/summarization/bertabs/configuration_bertabs.py,1,coding=utf-8,
transformers/examples/summarization/bertabs/configuration_bertabs.py,2,Copyright 2019 The HuggingFace Inc. team.,
transformers/examples/summarization/bertabs/configuration_bertabs.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/examples/summarization/bertabs/configuration_bertabs.py,4,,
transformers/examples/summarization/bertabs/configuration_bertabs.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/summarization/bertabs/configuration_bertabs.py,6,you may not use this file except in compliance with the License.,
transformers/examples/summarization/bertabs/configuration_bertabs.py,7,You may obtain a copy of the License at,
transformers/examples/summarization/bertabs/configuration_bertabs.py,8,,
transformers/examples/summarization/bertabs/configuration_bertabs.py,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/summarization/bertabs/configuration_bertabs.py,10,,
transformers/examples/summarization/bertabs/configuration_bertabs.py,11,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/summarization/bertabs/configuration_bertabs.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/summarization/bertabs/configuration_bertabs.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/summarization/bertabs/configuration_bertabs.py,14,See the License for the specific language governing permissions and,
transformers/examples/summarization/bertabs/configuration_bertabs.py,15,limitations under the License.,
transformers/examples/summarization/bertabs/run_summarization.py,1,! /usr/bin/python3,
transformers/examples/summarization/bertabs/run_summarization.py,59,Default F1_score,
transformers/examples/summarization/bertabs/run_summarization.py,64,these (unused) arguments are defined to keep the compatibility,
transformers/examples/summarization/bertabs/run_summarization.py,65,with the legacy code and will be deleted in a next iteration.,
transformers/examples/summarization/bertabs/run_summarization.py,113,Prepare the summary file's name,
transformers/examples/summarization/bertabs/run_summarization.py,180,,
transformers/examples/summarization/bertabs/run_summarization.py,181,LOAD the dataset,
transformers/examples/summarization/bertabs/run_summarization.py,182,,
transformers/examples/summarization/bertabs/run_summarization.py,209,remove empty_files,
transformers/examples/summarization/bertabs/run_summarization.py,268,EVALUATION options,
transformers/examples/summarization/bertabs/run_summarization.py,275,BEAM SEARCH arguments,
transformers/examples/summarization/bertabs/run_summarization.py,296,Select device (distibuted not available),
transformers/examples/summarization/bertabs/run_summarization.py,299,Check the existence of directories,
transformers/examples/summarization/bertabs/utils_summarization.py,8,------------,
transformers/examples/summarization/bertabs/utils_summarization.py,9,Data loading,
transformers/examples/summarization/bertabs/utils_summarization.py,10,------------,
transformers/examples/summarization/bertabs/utils_summarization.py,73,"for some unknown reason some lines miss a period, add it",
transformers/examples/summarization/bertabs/utils_summarization.py,76,gather article lines,
transformers/examples/summarization/bertabs/utils_summarization.py,86,"if ""@highlight"" is absent from the file we pop",
transformers/examples/summarization/bertabs/utils_summarization.py,87,"all elements until there is None, raising an exception.",
transformers/examples/summarization/bertabs/utils_summarization.py,90,gather summary lines,
transformers/examples/summarization/bertabs/utils_summarization.py,105,--------------------------,
transformers/examples/summarization/bertabs/utils_summarization.py,106,Encoding and preprocessing,
transformers/examples/summarization/bertabs/utils_summarization.py,107,--------------------------,
transformers/examples/summarization/bertabs/test_utils_summarization.py,1,coding=utf-8,
transformers/examples/summarization/bertabs/test_utils_summarization.py,2,Copyright 2019 HuggingFace Inc.,
transformers/examples/summarization/bertabs/test_utils_summarization.py,3,,
transformers/examples/summarization/bertabs/test_utils_summarization.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/summarization/bertabs/test_utils_summarization.py,5,you may not use this file except in compliance with the License.,
transformers/examples/summarization/bertabs/test_utils_summarization.py,6,You may obtain a copy of the License at,
transformers/examples/summarization/bertabs/test_utils_summarization.py,7,,
transformers/examples/summarization/bertabs/test_utils_summarization.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/summarization/bertabs/test_utils_summarization.py,9,,
transformers/examples/summarization/bertabs/test_utils_summarization.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/summarization/bertabs/test_utils_summarization.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/summarization/bertabs/test_utils_summarization.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/summarization/bertabs/test_utils_summarization.py,13,See the License for the specific language governing permissions and,
transformers/examples/summarization/bertabs/test_utils_summarization.py,14,limitations under the License.,
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,1,coding=utf-8,
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,2,Copyright 2018 The HuggingFace Inc. team.,
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,3,,
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,5,you may not use this file except in compliance with the License.,
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,6,You may obtain a copy of the License at,
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,7,,
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,9,,
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,13,See the License for the specific language governing permissions and,
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,14,limitations under the License.,
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,29,The authors' implementation,
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,69,Instantiate the authors' model with the pre-trained weights,
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,96,-------------------,
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,97,Convert the weights,
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,98,-------------------,
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,105,----------------------------------,
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,106,Make sure the outpus are identical,
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,107,----------------------------------,
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,112,prepare the model inputs,
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,120,failsafe to make sure the weights reset does not affect the,
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,121,loaded weights.,
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,124,forward pass,
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,133,The original model does not apply the geneator layer immediatly but rather in,
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,134,the beam search (where it combines softmax + linear layer). Since we already,
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,135,apply the softmax in our generation process we only apply the linear layer here.,
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,136,We make sure that the outputs of the full stack are identical,
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,156,The model has been saved with torch.save(model) and this is bound to the exact,
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,157,directory structure. We save the state_dict instead.,
transformers/examples/summarization/bart/test_bart_examples.py,55,show that articles were trimmed.,
transformers/examples/summarization/bart/test_bart_examples.py,57,trimmed significantly,
transformers/examples/summarization/bart/test_bart_examples.py,59,show that targets were truncated,
transformers/examples/summarization/bart/test_bart_examples.py,60,Truncated,
transformers/examples/summarization/bart/test_bart_examples.py,61,Truncated,
transformers/examples/summarization/bart/evaluate_cnn.py,36,+2 from original because we start at step=1 and stop before max_length,
transformers/examples/summarization/bart/evaluate_cnn.py,37,+1 from original because we start at step=1,
transformers/examples/summarization/bart/run_bart_sum.py,57,NOTE: this generation will not use the cache.,
transformers/examples/summarization/bart/run_bart_sum.py,60,NOTE: these kwargs get more speed and lower quality summaries than those in evaluate_cnn.py.,
transformers/examples/summarization/bart/run_bart_sum.py,85,write predictions and targets for later rouge evaluation.,
transformers/examples/summarization/bart/run_bart_sum.py,130,Add BART specific options,
transformers/examples/summarization/bart/run_bart_sum.py,162,"If output_dir not provided, a folder will be generated in pwd",
transformers/examples/summarization/bart/run_bart_sum.py,170,"Optionally, predict on dev set and write to output_dir",
transformers/examples/distillation/distiller.py,1,coding=utf-8,
transformers/examples/distillation/distiller.py,2,"Copyright 2019-present, the HuggingFace Inc. team and Facebook, Inc.",
transformers/examples/distillation/distiller.py,3,,
transformers/examples/distillation/distiller.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/distillation/distiller.py,5,you may not use this file except in compliance with the License.,
transformers/examples/distillation/distiller.py,6,You may obtain a copy of the License at,
transformers/examples/distillation/distiller.py,7,,
transformers/examples/distillation/distiller.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/distillation/distiller.py,9,,
transformers/examples/distillation/distiller.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/distillation/distiller.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/distillation/distiller.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/distillation/distiller.py,13,See the License for the specific language governing permissions and,
transformers/examples/distillation/distiller.py,14,limitations under the License.,
transformers/examples/distillation/distiller.py,219,"previously `dtype=torch.uint8`, cf pytorch 1.2.0 compatibility",
transformers/examples/distillation/distiller.py,225,mask a number of words == 0 [8] (faster with fp16),
transformers/examples/distillation/distiller.py,247,"previously `mlm_labels[1-pred_mask] = -1`, cf pytorch 1.2.0 compatibility",
transformers/examples/distillation/distiller.py,249,sanity checks,
transformers/examples/distillation/distiller.py,276,"previously `clm_labels[1-attn_mask] = -1`, cf pytorch 1.2.0 compatibility",
transformers/examples/distillation/distiller.py,278,sanity checks,
transformers/examples/distillation/distiller.py,301,number of sentences == 0 [8],
transformers/examples/distillation/distiller.py,313,sequence length == 0 [8],
transformers/examples/distillation/distiller.py,386,"(bs, seq_length, voc_size)",
transformers/examples/distillation/distiller.py,390,"(bs, seq_length, voc_size)",
transformers/examples/distillation/distiller.py,394,"(bs, seq_length, voc_size)",
transformers/examples/distillation/distiller.py,398,"(bs, seq_length, voc_size)",
transformers/examples/distillation/distiller.py,401,https://github.com/peterliht/knowledge-distillation-pytorch/blob/master/model/net.py#L100,
transformers/examples/distillation/distiller.py,402,https://github.com/peterliht/knowledge-distillation-pytorch/issues/2,
transformers/examples/distillation/distiller.py,404,"(bs, seq_lenth, voc_size)",
transformers/examples/distillation/distiller.py,406,"(bs, seq_lenth, voc_size)",
transformers/examples/distillation/distiller.py,407,(bs * seq_length * voc_size) modulo the 1s in mask,
transformers/examples/distillation/distiller.py,408,"(bs * seq_length, voc_size) modulo the 1s in mask",
transformers/examples/distillation/distiller.py,409,(bs * seq_length * voc_size) modulo the 1s in mask,
transformers/examples/distillation/distiller.py,410,"(bs * seq_length, voc_size) modulo the 1s in mask",
transformers/examples/distillation/distiller.py,434,Reproducing batchmean reduction,
transformers/examples/distillation/distiller.py,437,"(bs, seq_length, dim)",
transformers/examples/distillation/distiller.py,438,"(bs, seq_length, dim)",
transformers/examples/distillation/distiller.py,439,"(bs, seq_length, dim)",
transformers/examples/distillation/distiller.py,443,(bs * seq_length * dim),
transformers/examples/distillation/distiller.py,444,"(bs * seq_length, dim)",
transformers/examples/distillation/distiller.py,445,(bs * seq_length * dim),
transformers/examples/distillation/distiller.py,446,"(bs * seq_length, dim)",
transformers/examples/distillation/distiller.py,448,"(bs * seq_length,)",
transformers/examples/distillation/distiller.py,474,Check for NaN,
transformers/examples/distillation/utils.py,1,coding=utf-8,
transformers/examples/distillation/utils.py,2,"Copyright 2019-present, the HuggingFace Inc. team and Facebook, Inc.",
transformers/examples/distillation/utils.py,3,,
transformers/examples/distillation/utils.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/distillation/utils.py,5,you may not use this file except in compliance with the License.,
transformers/examples/distillation/utils.py,6,You may obtain a copy of the License at,
transformers/examples/distillation/utils.py,7,,
transformers/examples/distillation/utils.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/distillation/utils.py,9,,
transformers/examples/distillation/utils.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/distillation/utils.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/distillation/utils.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/distillation/utils.py,13,See the License for the specific language governing permissions and,
transformers/examples/distillation/utils.py,14,limitations under the License.,
transformers/examples/distillation/utils.py,72,number of nodes / node ID,
transformers/examples/distillation/utils.py,80,local job (single GPU),
transformers/examples/distillation/utils.py,92,sanity checks,
transformers/examples/distillation/utils.py,98,define whether this is the master process / if we are in multi-node distributed mode,
transformers/examples/distillation/utils.py,102,summary,
transformers/examples/distillation/utils.py,114,set GPU device,
transformers/examples/distillation/utils.py,117,initialize multi-GPU,
transformers/examples/distillation/run_squad_w_distillation.py,1,coding=utf-8,
transformers/examples/distillation/run_squad_w_distillation.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,
transformers/examples/distillation/run_squad_w_distillation.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",
transformers/examples/distillation/run_squad_w_distillation.py,4,,
transformers/examples/distillation/run_squad_w_distillation.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/distillation/run_squad_w_distillation.py,6,you may not use this file except in compliance with the License.,
transformers/examples/distillation/run_squad_w_distillation.py,7,You may obtain a copy of the License at,
transformers/examples/distillation/run_squad_w_distillation.py,8,,
transformers/examples/distillation/run_squad_w_distillation.py,9,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/distillation/run_squad_w_distillation.py,10,,
transformers/examples/distillation/run_squad_w_distillation.py,11,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/distillation/run_squad_w_distillation.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/distillation/run_squad_w_distillation.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/distillation/run_squad_w_distillation.py,14,See the License for the specific language governing permissions and,
transformers/examples/distillation/run_squad_w_distillation.py,15,limitations under the License.,
transformers/examples/distillation/run_squad_w_distillation.py,110,Prepare optimizer and schedule (linear warmup and decay),
transformers/examples/distillation/run_squad_w_distillation.py,124,Check if saved optimizer or scheduler states exist,
transformers/examples/distillation/run_squad_w_distillation.py,128,Load in optimizer and scheduler states,
transformers/examples/distillation/run_squad_w_distillation.py,140,multi-gpu training (should be after apex fp16 initialization),
transformers/examples/distillation/run_squad_w_distillation.py,144,Distributed training (should be after apex fp16 initialization),
transformers/examples/distillation/run_squad_w_distillation.py,150,Train!,
transformers/examples/distillation/run_squad_w_distillation.py,167,Check if continuing training from a checkpoint,
transformers/examples/distillation/run_squad_w_distillation.py,170,set global_step to gobal_step of last saved checkpoint from model path,
transformers/examples/distillation/run_squad_w_distillation.py,188,Added here for reproductibility,
transformers/examples/distillation/run_squad_w_distillation.py,195,Skip past any already trained steps if resuming training,
transformers/examples/distillation/run_squad_w_distillation.py,220,Distillation loss,
transformers/examples/distillation/run_squad_w_distillation.py,247,mean() to average on multi-gpu parallel (not distributed) training,
transformers/examples/distillation/run_squad_w_distillation.py,265,Update learning rate schedule,
transformers/examples/distillation/run_squad_w_distillation.py,269,Log metrics,
transformers/examples/distillation/run_squad_w_distillation.py,271,Only evaluate when single GPU otherwise metrics may not average well,
transformers/examples/distillation/run_squad_w_distillation.py,281,Save model checkpoint,
transformers/examples/distillation/run_squad_w_distillation.py,287,Take care of distributed/parallel training,
transformers/examples/distillation/run_squad_w_distillation.py,319,Note that DistributedSampler samples randomly,
transformers/examples/distillation/run_squad_w_distillation.py,323,multi-gpu evaluate,
transformers/examples/distillation/run_squad_w_distillation.py,327,Eval!,
transformers/examples/distillation/run_squad_w_distillation.py,342,XLM don't use segment_ids,
transformers/examples/distillation/run_squad_w_distillation.py,355,"Some models (XLNet, XLM) use 5 arguments for their predictions, while the other ""simpler""",
transformers/examples/distillation/run_squad_w_distillation.py,356,models only use two.,
transformers/examples/distillation/run_squad_w_distillation.py,382,Compute predictions,
transformers/examples/distillation/run_squad_w_distillation.py,392,XLNet uses a more complex post-processing procedure,
transformers/examples/distillation/run_squad_w_distillation.py,425,Compute the F1 and exact scores.,
transformers/examples/distillation/run_squad_w_distillation.py,432,"Make sure only the first process in distributed training process the dataset, and the others will use the cache",
transformers/examples/distillation/run_squad_w_distillation.py,435,Load data features from cache or dataset file,
transformers/examples/distillation/run_squad_w_distillation.py,484,"Make sure only the first process in distributed training process the dataset, and the others will use the cache",
transformers/examples/distillation/run_squad_w_distillation.py,495,Required parameters,
transformers/examples/distillation/run_squad_w_distillation.py,518,Distillation parameters (optional),
transformers/examples/distillation/run_squad_w_distillation.py,541,Other parameters,
transformers/examples/distillation/run_squad_w_distillation.py,711,Setup distant debugging if needed,
transformers/examples/distillation/run_squad_w_distillation.py,713,Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script,
transformers/examples/distillation/run_squad_w_distillation.py,720,"Setup CUDA, GPU & distributed training",
transformers/examples/distillation/run_squad_w_distillation.py,724,Initializes the distributed backend which will take care of sychronizing nodes/GPUs,
transformers/examples/distillation/run_squad_w_distillation.py,731,Setup logging,
transformers/examples/distillation/run_squad_w_distillation.py,746,Set seed,
transformers/examples/distillation/run_squad_w_distillation.py,749,Load pretrained model and tokenizer,
transformers/examples/distillation/run_squad_w_distillation.py,751,Make sure only the first process in distributed training will download model & vocab,
transformers/examples/distillation/run_squad_w_distillation.py,789,Make sure only the first process in distributed training will download model & vocab,
transformers/examples/distillation/run_squad_w_distillation.py,796,"Before we do anything with models, we want to ensure that we get fp16 execution of torch.einsum if args.fp16 is set.",
transformers/examples/distillation/run_squad_w_distillation.py,797,"Otherwise it'll default to ""promote"" mode, and we'll get fp32 operations. Note that running `--fp16_opt_level=""O2""` will",
transformers/examples/distillation/run_squad_w_distillation.py,798,"remove the need for this code, but it is still valid.",
transformers/examples/distillation/run_squad_w_distillation.py,807,Training,
transformers/examples/distillation/run_squad_w_distillation.py,813,Save the trained model and the tokenizer,
transformers/examples/distillation/run_squad_w_distillation.py,815,Create output directory if needed,
transformers/examples/distillation/run_squad_w_distillation.py,820,"Save a trained model, configuration and tokenizer using `save_pretrained()`.",
transformers/examples/distillation/run_squad_w_distillation.py,821,They can then be reloaded using `from_pretrained()`,
transformers/examples/distillation/run_squad_w_distillation.py,824,Take care of distributed/parallel training,
transformers/examples/distillation/run_squad_w_distillation.py,828,Good practice: save your training arguments together with the trained model,
transformers/examples/distillation/run_squad_w_distillation.py,831,Load a trained model and vocabulary that you have fine-tuned,
transformers/examples/distillation/run_squad_w_distillation.py,836,Evaluation - we can ask to evaluate all the checkpoints (sub-directories) in a directory,
transformers/examples/distillation/run_squad_w_distillation.py,846,Reduce model loading logs,
transformers/examples/distillation/run_squad_w_distillation.py,851,Reload the model,
transformers/examples/distillation/run_squad_w_distillation.py,856,Evaluate,
transformers/examples/distillation/grouped_batch_sampler.py,1,coding=utf-8,
transformers/examples/distillation/grouped_batch_sampler.py,2,"Copyright 2019-present, the HuggingFace Inc. team and Facebook, Inc.",
transformers/examples/distillation/grouped_batch_sampler.py,3,,
transformers/examples/distillation/grouped_batch_sampler.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/distillation/grouped_batch_sampler.py,5,you may not use this file except in compliance with the License.,
transformers/examples/distillation/grouped_batch_sampler.py,6,You may obtain a copy of the License at,
transformers/examples/distillation/grouped_batch_sampler.py,7,,
transformers/examples/distillation/grouped_batch_sampler.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/distillation/grouped_batch_sampler.py,9,,
transformers/examples/distillation/grouped_batch_sampler.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/distillation/grouped_batch_sampler.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/distillation/grouped_batch_sampler.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/distillation/grouped_batch_sampler.py,13,See the License for the specific language governing permissions and,
transformers/examples/distillation/grouped_batch_sampler.py,14,limitations under the License.,
transformers/examples/distillation/grouped_batch_sampler.py,37,count number of elements per group,
transformers/examples/distillation/grouped_batch_sampler.py,79,TODO,
transformers/examples/distillation/grouped_batch_sampler.py,84,now we have run out of elements that satisfy,
transformers/examples/distillation/grouped_batch_sampler.py,85,"the group criteria, let's return the remaining",
transformers/examples/distillation/grouped_batch_sampler.py,86,elements so that the size of the sampler is,
transformers/examples/distillation/grouped_batch_sampler.py,87,deterministic,
transformers/examples/distillation/grouped_batch_sampler.py,91,"for the remaining batches, group the batches by similar lengths",
transformers/examples/distillation/lm_seqs_dataset.py,1,coding=utf-8,
transformers/examples/distillation/lm_seqs_dataset.py,2,"Copyright 2019-present, the HuggingFace Inc. team and Facebook, Inc.",
transformers/examples/distillation/lm_seqs_dataset.py,3,,
transformers/examples/distillation/lm_seqs_dataset.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/distillation/lm_seqs_dataset.py,5,you may not use this file except in compliance with the License.,
transformers/examples/distillation/lm_seqs_dataset.py,6,You may obtain a copy of the License at,
transformers/examples/distillation/lm_seqs_dataset.py,7,,
transformers/examples/distillation/lm_seqs_dataset.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/distillation/lm_seqs_dataset.py,9,,
transformers/examples/distillation/lm_seqs_dataset.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/distillation/lm_seqs_dataset.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/distillation/lm_seqs_dataset.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/distillation/lm_seqs_dataset.py,13,See the License for the specific language governing permissions and,
transformers/examples/distillation/lm_seqs_dataset.py,14,limitations under the License.,
transformers/examples/distillation/lm_seqs_dataset.py,136,data_len = sum(self.lengths),
transformers/examples/distillation/lm_seqs_dataset.py,137,nb_unique_tokens = len(Counter(list(chain(*self.token_ids)))),
transformers/examples/distillation/lm_seqs_dataset.py,138,logger.info(f'{data_len} tokens ({nb_unique_tokens} unique)'),
transformers/examples/distillation/lm_seqs_dataset.py,140,unk_idx = self.params.special_tok_ids['unk_token'],
transformers/examples/distillation/lm_seqs_dataset.py,141,nb_unkown = sum([(t==unk_idx).sum() for t in self.token_ids]),
transformers/examples/distillation/lm_seqs_dataset.py,142,logger.info(f'{nb_unkown} unknown tokens (covering {100*nb_unkown/data_len:.2f}% of the data)'),
transformers/examples/distillation/lm_seqs_dataset.py,152,Max for paddings,
transformers/examples/distillation/lm_seqs_dataset.py,155,Pad token ids,
transformers/examples/distillation/lm_seqs_dataset.py,164,"(bs, max_seq_len_)",
transformers/examples/distillation/lm_seqs_dataset.py,165,(bs),
transformers/examples/distillation/train.py,1,coding=utf-8,
transformers/examples/distillation/train.py,2,"Copyright 2019-present, the HuggingFace Inc. team.",
transformers/examples/distillation/train.py,3,,
transformers/examples/distillation/train.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/distillation/train.py,5,you may not use this file except in compliance with the License.,
transformers/examples/distillation/train.py,6,You may obtain a copy of the License at,
transformers/examples/distillation/train.py,7,,
transformers/examples/distillation/train.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/distillation/train.py,9,,
transformers/examples/distillation/train.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/distillation/train.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/distillation/train.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/distillation/train.py,13,See the License for the specific language governing permissions and,
transformers/examples/distillation/train.py,14,limitations under the License.,
transformers/examples/distillation/train.py,222,ARGS,
transformers/examples/distillation/train.py,239,SAVE PARAMS,
transformers/examples/distillation/train.py,248,TOKENIZER,
transformers/examples/distillation/train.py,258,DATA LOADER,
transformers/examples/distillation/train.py,270,do not predict special tokens,
transformers/examples/distillation/train.py,278,STUDENT,
transformers/examples/distillation/train.py,293,TEACHER,
transformers/examples/distillation/train.py,299,FREEZING,
transformers/examples/distillation/train.py,305,SANITY CHECKS,
transformers/examples/distillation/train.py,312,DISTILLER,
transformers/examples/distillation/scripts/extract.py,1,coding=utf-8,
transformers/examples/distillation/scripts/extract.py,2,"Copyright 2019-present, the HuggingFace Inc. team.",
transformers/examples/distillation/scripts/extract.py,3,,
transformers/examples/distillation/scripts/extract.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/distillation/scripts/extract.py,5,you may not use this file except in compliance with the License.,
transformers/examples/distillation/scripts/extract.py,6,You may obtain a copy of the License at,
transformers/examples/distillation/scripts/extract.py,7,,
transformers/examples/distillation/scripts/extract.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/distillation/scripts/extract.py,9,,
transformers/examples/distillation/scripts/extract.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/distillation/scripts/extract.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/distillation/scripts/extract.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/distillation/scripts/extract.py,13,See the License for the specific language governing permissions and,
transformers/examples/distillation/scripts/extract.py,14,limitations under the License.,
transformers/examples/distillation/scripts/extract.py,46,Embeddings,
transformers/examples/distillation/scripts/extract.py,58,Transformer Blocks,
transformers/examples/distillation/scripts/extract.py,85,Language Modeling Head ###s,
transformers/examples/distillation/scripts/extract_distilbert.py,1,coding=utf-8,
transformers/examples/distillation/scripts/extract_distilbert.py,2,"Copyright 2019-present, the HuggingFace Inc. team.",
transformers/examples/distillation/scripts/extract_distilbert.py,3,,
transformers/examples/distillation/scripts/extract_distilbert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/distillation/scripts/extract_distilbert.py,5,you may not use this file except in compliance with the License.,
transformers/examples/distillation/scripts/extract_distilbert.py,6,You may obtain a copy of the License at,
transformers/examples/distillation/scripts/extract_distilbert.py,7,,
transformers/examples/distillation/scripts/extract_distilbert.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/distillation/scripts/extract_distilbert.py,9,,
transformers/examples/distillation/scripts/extract_distilbert.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/distillation/scripts/extract_distilbert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/distillation/scripts/extract_distilbert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/distillation/scripts/extract_distilbert.py,13,See the License for the specific language governing permissions and,
transformers/examples/distillation/scripts/extract_distilbert.py,14,limitations under the License.,
transformers/examples/distillation/scripts/token_counts.py,1,coding=utf-8,
transformers/examples/distillation/scripts/token_counts.py,2,"Copyright 2019-present, the HuggingFace Inc. team.",
transformers/examples/distillation/scripts/token_counts.py,3,,
transformers/examples/distillation/scripts/token_counts.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/distillation/scripts/token_counts.py,5,you may not use this file except in compliance with the License.,
transformers/examples/distillation/scripts/token_counts.py,6,You may obtain a copy of the License at,
transformers/examples/distillation/scripts/token_counts.py,7,,
transformers/examples/distillation/scripts/token_counts.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/distillation/scripts/token_counts.py,9,,
transformers/examples/distillation/scripts/token_counts.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/distillation/scripts/token_counts.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/distillation/scripts/token_counts.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/distillation/scripts/token_counts.py,13,See the License for the specific language governing permissions and,
transformers/examples/distillation/scripts/token_counts.py,14,limitations under the License.,
transformers/examples/distillation/scripts/binarized_data.py,1,coding=utf-8,
transformers/examples/distillation/scripts/binarized_data.py,2,"Copyright 2019-present, the HuggingFace Inc. team.",
transformers/examples/distillation/scripts/binarized_data.py,3,,
transformers/examples/distillation/scripts/binarized_data.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/distillation/scripts/binarized_data.py,5,you may not use this file except in compliance with the License.,
transformers/examples/distillation/scripts/binarized_data.py,6,You may obtain a copy of the License at,
transformers/examples/distillation/scripts/binarized_data.py,7,,
transformers/examples/distillation/scripts/binarized_data.py,8,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/distillation/scripts/binarized_data.py,9,,
transformers/examples/distillation/scripts/binarized_data.py,10,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/distillation/scripts/binarized_data.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/distillation/scripts/binarized_data.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/distillation/scripts/binarized_data.py,13,See the License for the specific language governing permissions and,
transformers/examples/distillation/scripts/binarized_data.py,14,limitations under the License.,
transformers/examples/distillation/scripts/binarized_data.py,48,`[CLS]`,
transformers/examples/distillation/scripts/binarized_data.py,49,`[SEP]`,
transformers/examples/distillation/scripts/binarized_data.py,52,`<s>`,
transformers/examples/distillation/scripts/binarized_data.py,53,`</s>`,
transformers/examples/distillation/scripts/binarized_data.py,56,`<|endoftext|>`,
transformers/examples/distillation/scripts/binarized_data.py,57,`<|endoftext|>`,
transformers/examples/pplm/pplm_classification_head.py,11,"self.mlp1 = torch.nn.Linear(embed_size, embed_size)",
transformers/examples/pplm/pplm_classification_head.py,12,"self.mlp2 = (torch.nn.Linear(embed_size, class_size))",
transformers/examples/pplm/pplm_classification_head.py,16,hidden_state = F.relu(self.mlp1(hidden_state)),
transformers/examples/pplm/pplm_classification_head.py,17,hidden_state = self.mlp2(hidden_state),
transformers/examples/pplm/run_pplm_discrim_train.py,1,! /usr/bin/env python3,
transformers/examples/pplm/run_pplm_discrim_train.py,2,coding=utf-8,
transformers/examples/pplm/run_pplm_discrim_train.py,4,"Copyright (c) 2019 Uber Technologies, Inc.",
transformers/examples/pplm/run_pplm_discrim_train.py,5,,
transformers/examples/pplm/run_pplm_discrim_train.py,6,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/pplm/run_pplm_discrim_train.py,7,you may not use this file except in compliance with the License.,
transformers/examples/pplm/run_pplm_discrim_train.py,8,You may obtain a copy of the License at,
transformers/examples/pplm/run_pplm_discrim_train.py,9,,
transformers/examples/pplm/run_pplm_discrim_train.py,10,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/pplm/run_pplm_discrim_train.py,11,,
transformers/examples/pplm/run_pplm_discrim_train.py,12,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/pplm/run_pplm_discrim_train.py,13,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/pplm/run_pplm_discrim_train.py,14,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/pplm/run_pplm_discrim_train.py,15,See the License for the specific language governing permissions and,
transformers/examples/pplm/run_pplm_discrim_train.py,16,limitations under the License.,
transformers/examples/pplm/run_pplm_discrim_train.py,105,padding value = 0,
transformers/examples/pplm/run_pplm_discrim_train.py,169,sum up batch loss,
transformers/examples/pplm/run_pplm_discrim_train.py,171,get the index of the max log-probability,
transformers/examples/pplm/run_pplm_discrim_train.py,363,"if dataset == ""generic"":",
transformers/examples/pplm/run_pplm_discrim_train.py,364,This assumes the input dataset is a TSV with the following structure:,
transformers/examples/pplm/run_pplm_discrim_train.py,365,class \t text,
transformers/examples/pplm/run_pplm_discrim_train.py,471,"torch.save(discriminator.state_dict(),",
transformers/examples/pplm/run_pplm_discrim_train.py,472,"""{}_discriminator_{}.pt"".format(",
transformers/examples/pplm/run_pplm_discrim_train.py,473,"args.dataset, epoch + 1",
transformers/examples/pplm/run_pplm_discrim_train.py,474,)),
transformers/examples/pplm/run_pplm.py,1,! /usr/bin/env python3,
transformers/examples/pplm/run_pplm.py,2,coding=utf-8,
transformers/examples/pplm/run_pplm.py,4,"Copyright (c) 2019 Uber Technologies, Inc.",
transformers/examples/pplm/run_pplm.py,5,,
transformers/examples/pplm/run_pplm.py,6,"Licensed under the Apache License, Version 2.0 (the ""License"");",
transformers/examples/pplm/run_pplm.py,7,you may not use this file except in compliance with the License.,
transformers/examples/pplm/run_pplm.py,8,You may obtain a copy of the License at,
transformers/examples/pplm/run_pplm.py,9,,
transformers/examples/pplm/run_pplm.py,10,http://www.apache.org/licenses/LICENSE-2.0,
transformers/examples/pplm/run_pplm.py,11,,
transformers/examples/pplm/run_pplm.py,12,"Unless required by applicable law or agreed to in writing, software",
transformers/examples/pplm/run_pplm.py,13,"distributed under the License is distributed on an ""AS IS"" BASIS,",
transformers/examples/pplm/run_pplm.py,14,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
transformers/examples/pplm/run_pplm.py,15,See the License for the specific language governing permissions and,
transformers/examples/pplm/run_pplm.py,16,limitations under the License.,
transformers/examples/pplm/run_pplm.py,124,Generate inital perturbed past,
transformers/examples/pplm/run_pplm.py,135,TODO fix this comment (SUMANTH),
transformers/examples/pplm/run_pplm.py,136,Generate a mask is gradient perturbated is based on a past window,
transformers/examples/pplm/run_pplm.py,154,accumulate perturbations for num_iterations,
transformers/examples/pplm/run_pplm.py,163,Compute hidden using perturbed past,
transformers/examples/pplm/run_pplm.py,169,TODO: Check the layer-norm consistency of this with trained discriminator (Sumanth),
transformers/examples/pplm/run_pplm.py,185,TODO why we need to do this assignment and not just using unpert_past? (Sumanth),
transformers/examples/pplm/run_pplm.py,216,compute gradients,
transformers/examples/pplm/run_pplm.py,219,calculate gradient norms,
transformers/examples/pplm/run_pplm.py,230,normalize gradients,
transformers/examples/pplm/run_pplm.py,236,accumulate gradient,
transformers/examples/pplm/run_pplm.py,239,"reset gradients, just to make sure",
transformers/examples/pplm/run_pplm.py,243,removing past from the graph,
transformers/examples/pplm/run_pplm.py,249,apply the accumulated perturbations to the past,
transformers/examples/pplm/run_pplm.py,458,collect one hot vectors for bags of words,
transformers/examples/pplm/run_pplm.py,467,"Get past/probs for current output, except for last word",
transformers/examples/pplm/run_pplm.py,468,Note that GPT takes 2 inputs: past + current_token,
transformers/examples/pplm/run_pplm.py,470,run model forward to obtain unperturbed,
transformers/examples/pplm/run_pplm.py,479,check if we are abowe grad max length,
transformers/examples/pplm/run_pplm.py,485,modify the past if necessary,
transformers/examples/pplm/run_pplm.py,520,+ SMALL_CONST,
transformers/examples/pplm/run_pplm.py,539,Fuse the modified model and original model,
transformers/examples/pplm/run_pplm.py,544,+ SMALL_CONST,
transformers/examples/pplm/run_pplm.py,545,+ SMALL_CONST,
transformers/examples/pplm/run_pplm.py,547,rescale,
transformers/examples/pplm/run_pplm.py,552,+ SMALL_CONST,
transformers/examples/pplm/run_pplm.py,555,sample or greedy,
transformers/examples/pplm/run_pplm.py,562,update context/output_so_far appending the new token,
transformers/examples/pplm/run_pplm.py,610,set Random seed,
transformers/examples/pplm/run_pplm.py,614,set the device,
transformers/examples/pplm/run_pplm.py,624,load pretrained model,
transformers/examples/pplm/run_pplm.py,629,load tokenizer,
transformers/examples/pplm/run_pplm.py,632,Freeze GPT-2 weights,
transformers/examples/pplm/run_pplm.py,636,figure out conditioning text,
transformers/examples/pplm/run_pplm.py,650,generate unperturbed and perturbed texts,
transformers/examples/pplm/run_pplm.py,652,full_text_generation returns:,
transformers/examples/pplm/run_pplm.py,653,"unpert_gen_tok_text, pert_gen_tok_texts, discrim_losses, losses_in_time",
transformers/examples/pplm/run_pplm.py,679,untokenize unperturbed text,
transformers/examples/pplm/run_pplm.py,693,filtering all words in the list composed of more than 1 token,
transformers/examples/pplm/run_pplm.py,695,w[0] because we are sure w has only 1 item because previous fitler,
transformers/examples/pplm/run_pplm.py,698,iterate through the perturbed texts,
transformers/examples/pplm/run_pplm.py,701,untokenize unperturbed text,
transformers/examples/pplm/run_pplm.py,722,"keep the prefix, perturbed seq, original seq for each index",
transformers/docs/source/conf.py,1,-*- coding: utf-8 -*-,
transformers/docs/source/conf.py,2,,
transformers/docs/source/conf.py,3,Configuration file for the Sphinx documentation builder.,
transformers/docs/source/conf.py,4,,
transformers/docs/source/conf.py,5,This file does only contain a selection of the most common options. For a,
transformers/docs/source/conf.py,6,full list see the documentation:,
transformers/docs/source/conf.py,7,http://www.sphinx-doc.org/en/master/config,
transformers/docs/source/conf.py,9,-- Path setup --------------------------------------------------------------,
transformers/docs/source/conf.py,11,"If extensions (or modules to document with autodoc) are in another directory,",
transformers/docs/source/conf.py,12,add these directories to sys.path here. If the directory is relative to the,
transformers/docs/source/conf.py,13,"documentation root, use os.path.abspath to make it absolute, like shown here.",
transformers/docs/source/conf.py,14,,
transformers/docs/source/conf.py,20,-- Project information -----------------------------------------------------,
transformers/docs/source/conf.py,26,The short X.Y version,
transformers/docs/source/conf.py,28,"The full version, including alpha/beta/rc tags",
transformers/docs/source/conf.py,32,-- General configuration ---------------------------------------------------,
transformers/docs/source/conf.py,34,"If your documentation needs a minimal Sphinx version, state it here.",
transformers/docs/source/conf.py,35,,
transformers/docs/source/conf.py,36,needs_sphinx = '1.0',
transformers/docs/source/conf.py,38,"Add any Sphinx extension module names here, as strings. They can be",
transformers/docs/source/conf.py,39,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,
transformers/docs/source/conf.py,40,ones.,
transformers/docs/source/conf.py,50,"Add any paths that contain templates here, relative to this directory.",
transformers/docs/source/conf.py,53,The suffix(es) of source filenames.,
transformers/docs/source/conf.py,54,You can specify multiple suffix as a list of string:,
transformers/docs/source/conf.py,55,,
transformers/docs/source/conf.py,57,source_suffix = '.rst',
transformers/docs/source/conf.py,59,The master toctree document.,
transformers/docs/source/conf.py,62,The language for content autogenerated by Sphinx. Refer to documentation,
transformers/docs/source/conf.py,63,for a list of supported languages.,
transformers/docs/source/conf.py,64,,
transformers/docs/source/conf.py,65,This is also used if you do content translation via gettext catalogs.,
transformers/docs/source/conf.py,66,"Usually you set ""language"" from the command line for these cases.",
transformers/docs/source/conf.py,69,"List of patterns, relative to source directory, that match files and",
transformers/docs/source/conf.py,70,directories to ignore when looking for source files.,
transformers/docs/source/conf.py,71,This pattern also affects html_static_path and html_extra_path.,
transformers/docs/source/conf.py,74,The name of the Pygments (syntax highlighting) style to use.,
transformers/docs/source/conf.py,78,-- Options for HTML output -------------------------------------------------,
transformers/docs/source/conf.py,80,The theme to use for HTML and HTML Help pages.  See the documentation for,
transformers/docs/source/conf.py,81,a list of builtin themes.,
transformers/docs/source/conf.py,82,,
transformers/docs/source/conf.py,85,Theme options are theme-specific and customize the look and feel of a theme,
transformers/docs/source/conf.py,86,"further.  For a list of options available for each theme, see the",
transformers/docs/source/conf.py,87,documentation.,
transformers/docs/source/conf.py,88,,
transformers/docs/source/conf.py,93,"Add any paths that contain custom static files (such as style sheets) here,",
transformers/docs/source/conf.py,94,"relative to this directory. They are copied after the builtin static files,",
transformers/docs/source/conf.py,95,"so a file named ""default.css"" will overwrite the builtin ""default.css"".",
transformers/docs/source/conf.py,98,"Custom sidebar templates, must be a dictionary that maps document names",
transformers/docs/source/conf.py,99,to template names.,
transformers/docs/source/conf.py,100,,
transformers/docs/source/conf.py,101,The default sidebars (for documents that don't match any pattern) are,
transformers/docs/source/conf.py,102,defined by theme itself.  Builtin themes are using these templates by,
transformers/docs/source/conf.py,103,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
transformers/docs/source/conf.py,104,'searchbox.html']``.,
transformers/docs/source/conf.py,105,,
transformers/docs/source/conf.py,106,html_sidebars = {},
transformers/docs/source/conf.py,108,This must be the name of an image file (path relative to the configuration,
transformers/docs/source/conf.py,109,directory) that is the favicon of the docs. Modern browsers use this as,
transformers/docs/source/conf.py,110,"the icon for tabs, windows and bookmarks. It should be a Windows-style",
transformers/docs/source/conf.py,111,icon file (.ico).,
transformers/docs/source/conf.py,115,-- Options for HTMLHelp output ---------------------------------------------,
transformers/docs/source/conf.py,117,Output file base name for HTML help builder.,
transformers/docs/source/conf.py,121,-- Options for LaTeX output ------------------------------------------------,
transformers/docs/source/conf.py,124,The paper size ('letterpaper' or 'a4paper').,
transformers/docs/source/conf.py,125,,
transformers/docs/source/conf.py,126,"'papersize': 'letterpaper',",
transformers/docs/source/conf.py,128,"The font size ('10pt', '11pt' or '12pt').",
transformers/docs/source/conf.py,129,,
transformers/docs/source/conf.py,130,"'pointsize': '10pt',",
transformers/docs/source/conf.py,132,Additional stuff for the LaTeX preamble.,
transformers/docs/source/conf.py,133,,
transformers/docs/source/conf.py,134,"'preamble': '',",
transformers/docs/source/conf.py,136,Latex figure (float) alignment,
transformers/docs/source/conf.py,137,,
transformers/docs/source/conf.py,138,"'figure_align': 'htbp',",
transformers/docs/source/conf.py,141,Grouping the document tree into LaTeX files. List of tuples,
transformers/docs/source/conf.py,142,"(source start file, target name, title,",
transformers/docs/source/conf.py,143,"author, documentclass [howto, manual, or own class]).",
transformers/docs/source/conf.py,150,-- Options for manual page output ------------------------------------------,
transformers/docs/source/conf.py,152,One entry per manual page. List of tuples,
transformers/docs/source/conf.py,153,"(source start file, name, description, authors, manual section).",
transformers/docs/source/conf.py,160,-- Options for Texinfo output ----------------------------------------------,
transformers/docs/source/conf.py,162,Grouping the document tree into Texinfo files. List of tuples,
transformers/docs/source/conf.py,163,"(source start file, target name, title, author,",
transformers/docs/source/conf.py,164,"dir menu entry, description, category)",
transformers/docs/source/conf.py,172,-- Options for Epub output -------------------------------------------------,
transformers/docs/source/conf.py,174,Bibliographic Dublin Core info.,
transformers/docs/source/conf.py,177,The unique identifier of the text. This can be a ISBN number,
transformers/docs/source/conf.py,178,or the project homepage.,
transformers/docs/source/conf.py,179,,
transformers/docs/source/conf.py,180,epub_identifier = '',
transformers/docs/source/conf.py,182,A unique identification for the text.,
transformers/docs/source/conf.py,183,,
transformers/docs/source/conf.py,184,epub_uid = '',
transformers/docs/source/conf.py,186,A list of files that should not be packed into the epub file.,
transformers/docs/source/conf.py,194,-- Extension configuration -------------------------------------------------,
