file path,line #,comment,satd
Mask_RCNN/setup.py,27,parse_requirements() returns generator of pip.req.InstallRequirement objects,
Mask_RCNN/mrcnn/model.py,28,Requires TensorFlow 1.3+ and Keras 2.0.8+.,
Mask_RCNN/mrcnn/model.py,34,,
Mask_RCNN/mrcnn/model.py,35,Utility Functions,
Mask_RCNN/mrcnn/model.py,36,,
Mask_RCNN/mrcnn/model.py,80,Currently supports ResNet only,
Mask_RCNN/mrcnn/model.py,88,,
Mask_RCNN/mrcnn/model.py,89,Resnet Graph,
Mask_RCNN/mrcnn/model.py,90,,
Mask_RCNN/mrcnn/model.py,92,Code adopted from:,
Mask_RCNN/mrcnn/model.py,93,https://github.com/fchollet/deep-learning-models/blob/master/resnet50.py,
Mask_RCNN/mrcnn/model.py,178,Stage 1,
Mask_RCNN/mrcnn/model.py,184,Stage 2,
Mask_RCNN/mrcnn/model.py,188,Stage 3,
Mask_RCNN/mrcnn/model.py,193,Stage 4,
Mask_RCNN/mrcnn/model.py,199,Stage 5,
Mask_RCNN/mrcnn/model.py,209,,
Mask_RCNN/mrcnn/model.py,210,Proposal Layer,
Mask_RCNN/mrcnn/model.py,211,,
Mask_RCNN/mrcnn/model.py,218,"Convert to y, x, h, w",
Mask_RCNN/mrcnn/model.py,223,Apply deltas,
Mask_RCNN/mrcnn/model.py,228,"Convert back to y1, x1, y2, x2",
Mask_RCNN/mrcnn/model.py,242,Split,
Mask_RCNN/mrcnn/model.py,245,Clip,
Mask_RCNN/mrcnn/model.py,277,"Box Scores. Use the foreground class confidence. [Batch, num_rois, 1]",
Mask_RCNN/mrcnn/model.py,279,"Box deltas [batch, num_rois, 4]",
Mask_RCNN/mrcnn/model.py,282,Anchors,
Mask_RCNN/mrcnn/model.py,285,Improve performance by trimming to top anchors by score,
Mask_RCNN/mrcnn/model.py,286,and doing the rest on the smaller subset.,
Mask_RCNN/mrcnn/model.py,298,Apply deltas to anchors to get refined anchors.,
Mask_RCNN/mrcnn/model.py,299,"[batch, N, (y1, x1, y2, x2)]",
Mask_RCNN/mrcnn/model.py,305,"Clip to image boundaries. Since we're in normalized coordinates,",
Mask_RCNN/mrcnn/model.py,306,"clip to 0..1 range. [batch, N, (y1, x1, y2, x2)]",
Mask_RCNN/mrcnn/model.py,313,Filter out small boxes,
Mask_RCNN/mrcnn/model.py,314,"According to Xinlei Chen's paper, this reduces detection accuracy",
Mask_RCNN/mrcnn/model.py,315,"for small objects, so we're skipping it.",
Mask_RCNN/mrcnn/model.py,317,Non-max suppression,
Mask_RCNN/mrcnn/model.py,323,Pad if needed,
Mask_RCNN/mrcnn/model.py,335,,
Mask_RCNN/mrcnn/model.py,336,ROIAlign Layer,
Mask_RCNN/mrcnn/model.py,337,,
Mask_RCNN/mrcnn/model.py,369,"Crop boxes [batch, num_boxes, (y1, x1, y2, x2)] in normalized coords",
Mask_RCNN/mrcnn/model.py,372,Image meta,
Mask_RCNN/mrcnn/model.py,373,Holds details about the image. See compose_image_meta(),
Mask_RCNN/mrcnn/model.py,376,Feature Maps. List of feature maps from different level of the,
Mask_RCNN/mrcnn/model.py,377,"feature pyramid. Each is [batch, height, width, channels]",
Mask_RCNN/mrcnn/model.py,380,Assign each ROI to a level in the pyramid based on the ROI area.,
Mask_RCNN/mrcnn/model.py,384,Use shape of first image. Images in a batch must have the same size.,
Mask_RCNN/mrcnn/model.py,386,Equation 1 in the Feature Pyramid Networks paper. Account for,
Mask_RCNN/mrcnn/model.py,387,the fact that our coordinates are normalized here.,
Mask_RCNN/mrcnn/model.py,388,e.g. a 224x224 ROI (in pixels) maps to P4,
Mask_RCNN/mrcnn/model.py,395,Loop through levels and apply ROI pooling to each. P2 to P5.,
Mask_RCNN/mrcnn/model.py,402,Box indices for crop_and_resize.,
Mask_RCNN/mrcnn/model.py,405,Keep track of which box is mapped to which level,
Mask_RCNN/mrcnn/model.py,408,Stop gradient propogation to ROI proposals,
Mask_RCNN/mrcnn/model.py,412,Crop and Resize,
Mask_RCNN/mrcnn/model.py,413,"From Mask R-CNN paper: ""We sample four regular locations, so",
Mask_RCNN/mrcnn/model.py,414,"that we can evaluate either max or average pooling. In fact,",
Mask_RCNN/mrcnn/model.py,415,interpolating only a single value at each bin center (without,
Mask_RCNN/mrcnn/model.py,416,"pooling) is nearly as effective.""",
Mask_RCNN/mrcnn/model.py,417,,
Mask_RCNN/mrcnn/model.py,418,"Here we use the simplified approach of a single value per bin,",
Mask_RCNN/mrcnn/model.py,419,which is how it's done in tf.crop_and_resize(),
Mask_RCNN/mrcnn/model.py,420,"Result: [batch * num_boxes, pool_height, pool_width, channels]",
Mask_RCNN/mrcnn/model.py,425,Pack pooled features into one tensor,
Mask_RCNN/mrcnn/model.py,428,Pack box_to_level mapping into one array and add another,
Mask_RCNN/mrcnn/model.py,429,column representing the order of pooled boxes,
Mask_RCNN/mrcnn/model.py,435,Rearrange pooled features to match the order of the original boxes,
Mask_RCNN/mrcnn/model.py,436,Sort box_to_level by batch then box index,
Mask_RCNN/mrcnn/model.py,437,"TF doesn't have a way to sort by two columns, so merge them and sort.",
Mask_RCNN/mrcnn/model.py,444,Re-add the batch dimension,
Mask_RCNN/mrcnn/model.py,453,,
Mask_RCNN/mrcnn/model.py,454,Detection Target Layer,
Mask_RCNN/mrcnn/model.py,455,,
Mask_RCNN/mrcnn/model.py,461,1. Tile boxes2 and repeat boxes1. This allows us to compare,
Mask_RCNN/mrcnn/model.py,462,every boxes1 against every boxes2 without loops.,
Mask_RCNN/mrcnn/model.py,463,TF doesn't have an equivalent to np.repeat() so simulate it,
Mask_RCNN/mrcnn/model.py,464,using tf.tile() and tf.reshape.,
Mask_RCNN/mrcnn/model.py,468,2. Compute intersections,
Mask_RCNN/mrcnn/model.py,476,3. Compute unions,
Mask_RCNN/mrcnn/model.py,480,"4. Compute IoU and reshape to [boxes1, boxes2]",
Mask_RCNN/mrcnn/model.py,507,Assertions,
Mask_RCNN/mrcnn/model.py,515,Remove zero padding,
Mask_RCNN/mrcnn/model.py,523,Handle COCO crowds,
Mask_RCNN/mrcnn/model.py,524,A crowd box in COCO is a bounding box around several instances. Exclude,
Mask_RCNN/mrcnn/model.py,525,them from training. A crowd box is given a negative class ID.,
Mask_RCNN/mrcnn/model.py,533,"Compute overlaps matrix [proposals, gt_boxes]",
Mask_RCNN/mrcnn/model.py,536,"Compute overlaps with crowd boxes [proposals, crowd_boxes]",
Mask_RCNN/mrcnn/model.py,541,Determine positive and negative ROIs,
Mask_RCNN/mrcnn/model.py,543,1. Positive ROIs are those with >= 0.5 IoU with a GT box,
Mask_RCNN/mrcnn/model.py,546,2. Negative ROIs are those with < 0.5 with every GT box. Skip crowds.,
Mask_RCNN/mrcnn/model.py,549,Subsample ROIs. Aim for 33% positive,
Mask_RCNN/mrcnn/model.py,550,Positive ROIs,
Mask_RCNN/mrcnn/model.py,555,Negative ROIs. Add enough to maintain positive:negative ratio.,
Mask_RCNN/mrcnn/model.py,559,Gather selected ROIs,
Mask_RCNN/mrcnn/model.py,563,Assign positive ROIs to GT boxes.,
Mask_RCNN/mrcnn/model.py,573,Compute bbox refinement for positive ROIs,
Mask_RCNN/mrcnn/model.py,577,Assign positive ROIs to GT masks,
Mask_RCNN/mrcnn/model.py,578,"Permute masks to [N, height, width, 1]",
Mask_RCNN/mrcnn/model.py,580,Pick the right mask for each ROI,
Mask_RCNN/mrcnn/model.py,583,Compute mask targets,
Mask_RCNN/mrcnn/model.py,586,Transform ROI coordinates from normalized image space,
Mask_RCNN/mrcnn/model.py,587,to normalized mini-mask space.,
Mask_RCNN/mrcnn/model.py,601,Remove the extra dimension from masks.,
Mask_RCNN/mrcnn/model.py,604,Threshold mask pixels at 0.5 to have GT masks be 0 or 1 to use with,
Mask_RCNN/mrcnn/model.py,605,binary cross entropy loss.,
Mask_RCNN/mrcnn/model.py,608,Append negative ROIs and pad bbox deltas and masks that,
Mask_RCNN/mrcnn/model.py,609,are not used for negative ROIs with zeros.,
Mask_RCNN/mrcnn/model.py,657,Slice the batch and run a graph for each slice,
Mask_RCNN/mrcnn/model.py,658,TODO: Rename target_bbox to target_deltas for clarity,
Mask_RCNN/mrcnn/model.py,669,rois,
Mask_RCNN/mrcnn/model.py,670,class_ids,
Mask_RCNN/mrcnn/model.py,671,deltas,
Mask_RCNN/mrcnn/model.py,673,masks,
Mask_RCNN/mrcnn/model.py,680,,
Mask_RCNN/mrcnn/model.py,681,Detection Layer,
Mask_RCNN/mrcnn/model.py,682,,
Mask_RCNN/mrcnn/model.py,699,Class IDs per ROI,
Mask_RCNN/mrcnn/model.py,701,Class probability of the top class of each ROI,
Mask_RCNN/mrcnn/model.py,704,Class-specific bounding box deltas,
Mask_RCNN/mrcnn/model.py,706,Apply bounding box deltas,
Mask_RCNN/mrcnn/model.py,707,"Shape: [boxes, (y1, x1, y2, x2)] in normalized coordinates",
Mask_RCNN/mrcnn/model.py,710,Clip boxes to image window,
Mask_RCNN/mrcnn/model.py,713,TODO: Filter out boxes with zero area,
Mask_RCNN/mrcnn/model.py,715,Filter out background boxes,
Mask_RCNN/mrcnn/model.py,717,Filter out low confidence boxes,
Mask_RCNN/mrcnn/model.py,724,Apply per-class NMS,
Mask_RCNN/mrcnn/model.py,725,1. Prepare variables,
Mask_RCNN/mrcnn/model.py,733,Indices of ROIs of the given class,
Mask_RCNN/mrcnn/model.py,735,Apply NMS,
Mask_RCNN/mrcnn/model.py,741,Map indices,
Mask_RCNN/mrcnn/model.py,743,Pad with -1 so returned tensors have the same shape,
Mask_RCNN/mrcnn/model.py,747,Set shape so map_fn() can infer result shape,
Mask_RCNN/mrcnn/model.py,751,2. Map over class IDs,
Mask_RCNN/mrcnn/model.py,754,"3. Merge results into one list, and remove -1 padding",
Mask_RCNN/mrcnn/model.py,757,4. Compute intersection between keep and nms_keep,
Mask_RCNN/mrcnn/model.py,761,Keep top detections,
Mask_RCNN/mrcnn/model.py,768,"Arrange output as [N, (y1, x1, y2, x2, class_id, score)]",
Mask_RCNN/mrcnn/model.py,769,Coordinates are normalized.,
Mask_RCNN/mrcnn/model.py,776,Pad with zeros if detections < DETECTION_MAX_INSTANCES,
Mask_RCNN/mrcnn/model.py,801,Get windows of images in normalized coordinates. Windows are the area,
Mask_RCNN/mrcnn/model.py,802,in the image that excludes the padding.,
Mask_RCNN/mrcnn/model.py,803,Use the shape of the first image in the batch to normalize the window,
Mask_RCNN/mrcnn/model.py,804,because we know that all images get resized to the same size.,
Mask_RCNN/mrcnn/model.py,809,Run detection refinement graph on each item in the batch,
Mask_RCNN/mrcnn/model.py,815,Reshape output,
Mask_RCNN/mrcnn/model.py,816,"[batch, num_detections, (y1, x1, y2, x2, class_id, class_score)] in",
Mask_RCNN/mrcnn/model.py,817,normalized coordinates,
Mask_RCNN/mrcnn/model.py,826,,
Mask_RCNN/mrcnn/model.py,827,Region Proposal Network (RPN),
Mask_RCNN/mrcnn/model.py,828,,
Mask_RCNN/mrcnn/model.py,844,TODO: check if stride of 2 causes alignment issues if the feature map,
Mask_RCNN/mrcnn/model.py,845,is not even.,
Mask_RCNN/mrcnn/model.py,846,Shared convolutional base of the RPN,
Mask_RCNN/mrcnn/model.py,851,"Anchor Score. [batch, height, width, anchors per location * 2].",
Mask_RCNN/mrcnn/model.py,855,"Reshape to [batch, anchors, 2]",
Mask_RCNN/mrcnn/model.py,859,Softmax on last dimension of BG/FG.,
Mask_RCNN/mrcnn/model.py,863,"Bounding box refinement. [batch, H, W, anchors per location * depth]",
Mask_RCNN/mrcnn/model.py,864,"where depth is [x, y, log(w), log(h)]",
Mask_RCNN/mrcnn/model.py,868,"Reshape to [batch, anchors, 4]",
Mask_RCNN/mrcnn/model.py,896,,
Mask_RCNN/mrcnn/model.py,897,Feature Pyramid Network Heads,
Mask_RCNN/mrcnn/model.py,898,,
Mask_RCNN/mrcnn/model.py,922,ROI Pooling,
Mask_RCNN/mrcnn/model.py,923,"Shape: [batch, num_rois, POOL_SIZE, POOL_SIZE, channels]",
Mask_RCNN/mrcnn/model.py,926,Two 1024 FC layers (implemented with Conv2D for consistency),
Mask_RCNN/mrcnn/model.py,939,Classifier head,
Mask_RCNN/mrcnn/model.py,945,BBox head,
Mask_RCNN/mrcnn/model.py,946,"[batch, num_rois, NUM_CLASSES * (dy, dx, log(dh), log(dw))]",
Mask_RCNN/mrcnn/model.py,949,"Reshape to [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))]",
Mask_RCNN/mrcnn/model.py,971,ROI Pooling,
Mask_RCNN/mrcnn/model.py,972,"Shape: [batch, num_rois, MASK_POOL_SIZE, MASK_POOL_SIZE, channels]",
Mask_RCNN/mrcnn/model.py,976,Conv layers,
Mask_RCNN/mrcnn/model.py,1008,,
Mask_RCNN/mrcnn/model.py,1009,Loss Functions,
Mask_RCNN/mrcnn/model.py,1010,,
Mask_RCNN/mrcnn/model.py,1029,Squeeze last dim to simplify,
Mask_RCNN/mrcnn/model.py,1031,Get anchor classes. Convert the -1/+1 match to 0/1 values.,
Mask_RCNN/mrcnn/model.py,1033,"Positive and Negative anchors contribute to the loss,",
Mask_RCNN/mrcnn/model.py,1034,but neutral anchors (match value = 0) don't.,
Mask_RCNN/mrcnn/model.py,1036,Pick rows that contribute to the loss and filter out the rest.,
Mask_RCNN/mrcnn/model.py,1039,Cross entropy loss,
Mask_RCNN/mrcnn/model.py,1057,"Positive anchors contribute to the loss, but negative and",
Mask_RCNN/mrcnn/model.py,1058,neutral anchors (match value of 0 or -1) don't.,
Mask_RCNN/mrcnn/model.py,1062,Pick bbox deltas that contribute to the loss,
Mask_RCNN/mrcnn/model.py,1065,Trim target bounding box deltas to the same length as rpn_bbox.,
Mask_RCNN/mrcnn/model.py,1087,"During model building, Keras calls this function with",
Mask_RCNN/mrcnn/model.py,1088,target_class_ids of type float32. Unclear why. Cast it,
Mask_RCNN/mrcnn/model.py,1089,to int to get around it.,
Mask_RCNN/mrcnn/model.py,1092,Find predictions of classes that are not in the dataset.,
Mask_RCNN/mrcnn/model.py,1094,TODO: Update this line to work with batch > 1. Right now it assumes all,
Mask_RCNN/mrcnn/model.py,1095,images in a batch have the same active_class_ids,
Mask_RCNN/mrcnn/model.py,1098,Loss,
Mask_RCNN/mrcnn/model.py,1102,Erase losses of predictions of classes that are not in the active,
Mask_RCNN/mrcnn/model.py,1103,classes of the image.,
Mask_RCNN/mrcnn/model.py,1106,Computer loss mean. Use only predictions that contribute,
Mask_RCNN/mrcnn/model.py,1107,to the loss to get a correct mean.,
Mask_RCNN/mrcnn/model.py,1119,Reshape to merge batch and roi dimensions for simplicity.,
Mask_RCNN/mrcnn/model.py,1124,Only positive ROIs contribute to the loss. And only,
Mask_RCNN/mrcnn/model.py,1125,the right class_id of each ROI. Get their indices.,
Mask_RCNN/mrcnn/model.py,1131,Gather the deltas (predicted and true) that contribute to loss,
Mask_RCNN/mrcnn/model.py,1135,Smooth-L1 Loss,
Mask_RCNN/mrcnn/model.py,1152,Reshape for simplicity. Merge first two dimensions into one.,
Mask_RCNN/mrcnn/model.py,1159,"Permute predicted masks to [N, num_classes, height, width]",
Mask_RCNN/mrcnn/model.py,1162,Only positive ROIs contribute to the loss. And only,
Mask_RCNN/mrcnn/model.py,1163,the class specific mask of each ROI.,
Mask_RCNN/mrcnn/model.py,1169,Gather the masks (predicted and true) that contribute to loss,
Mask_RCNN/mrcnn/model.py,1173,"Compute binary cross entropy. If no positive ROIs, then return 0.",
Mask_RCNN/mrcnn/model.py,1174,"shape: [batch, roi, num_classes]",
Mask_RCNN/mrcnn/model.py,1182,,
Mask_RCNN/mrcnn/model.py,1183,Data Generator,
Mask_RCNN/mrcnn/model.py,1184,,
Mask_RCNN/mrcnn/model.py,1210,Load image and mask,
Mask_RCNN/mrcnn/model.py,1222,Random horizontal flips.,
Mask_RCNN/mrcnn/model.py,1223,TODO: will be removed in a future update in favor of augmentation,
Mask_RCNN/mrcnn/model.py,1230,Augmentation,
Mask_RCNN/mrcnn/model.py,1231,This requires the imgaug lib (https://github.com/aleju/imgaug),
Mask_RCNN/mrcnn/model.py,1235,Augmenters that are safe to apply to masks,
Mask_RCNN/mrcnn/model.py,1236,"Some, such as Affine, have settings that make them unsafe, so always",
Mask_RCNN/mrcnn/model.py,1237,test your augmentation on masks,
Mask_RCNN/mrcnn/model.py,1246,Store shapes before augmentation to compare,
Mask_RCNN/mrcnn/model.py,1249,Make augmenters deterministic to apply similarly to images and masks,
Mask_RCNN/mrcnn/model.py,1252,Change mask to np.uint8 because imgaug doesn't support np.bool,
Mask_RCNN/mrcnn/model.py,1255,Verify that shapes didn't change,
Mask_RCNN/mrcnn/model.py,1258,Change mask back to bool,
Mask_RCNN/mrcnn/model.py,1261,Note that some boxes might be all zeros if the corresponding mask got cropped out.,
Mask_RCNN/mrcnn/model.py,1262,and here is to filter them out,
Mask_RCNN/mrcnn/model.py,1266,Bounding boxes. Note that some boxes might be all zeros,
Mask_RCNN/mrcnn/model.py,1267,if the corresponding mask got cropped out.,
Mask_RCNN/mrcnn/model.py,1268,"bbox: [num_instances, (y1, x1, y2, x2)]",
Mask_RCNN/mrcnn/model.py,1271,Active classes,
Mask_RCNN/mrcnn/model.py,1272,"Different datasets have different classes, so track the",
Mask_RCNN/mrcnn/model.py,1273,classes supported in the dataset of this image.,
Mask_RCNN/mrcnn/model.py,1278,Resize masks to smaller size to reduce memory usage,
Mask_RCNN/mrcnn/model.py,1282,Image meta data,
Mask_RCNN/mrcnn/model.py,1317,It's common to add GT Boxes to ROIs but we don't do that here because,
Mask_RCNN/mrcnn/model.py,1318,"according to XinLei Chen's paper, it doesn't help.",
Mask_RCNN/mrcnn/model.py,1320,Trim empty padding in gt_boxes and gt_masks parts,
Mask_RCNN/mrcnn/model.py,1327,Compute areas of ROIs and ground truth boxes.,
Mask_RCNN/mrcnn/model.py,1333,"Compute overlaps [rpn_rois, gt_boxes]",
Mask_RCNN/mrcnn/model.py,1340,Assign ROIs to GT boxes,
Mask_RCNN/mrcnn/model.py,1344,GT box assigned to each ROI,
Mask_RCNN/mrcnn/model.py,1348,Positive ROIs are those with >= 0.5 IoU with a GT box.,
Mask_RCNN/mrcnn/model.py,1351,Negative ROIs are those with max IoU 0.1-0.5 (hard example mining),
Mask_RCNN/mrcnn/model.py,1352,"TODO: To hard example mine or not to hard example mine, that's the question",
Mask_RCNN/mrcnn/model.py,1353,bg_ids = np.where((rpn_roi_iou_max >= 0.1) & (rpn_roi_iou_max < 0.5))[0],
Mask_RCNN/mrcnn/model.py,1356,Subsample ROIs. Aim for 33% foreground.,
Mask_RCNN/mrcnn/model.py,1357,FG,
Mask_RCNN/mrcnn/model.py,1363,BG,
Mask_RCNN/mrcnn/model.py,1369,Combine indices of ROIs to keep,
Mask_RCNN/mrcnn/model.py,1371,Need more?,
Mask_RCNN/mrcnn/model.py,1374,Looks like we don't have enough samples to maintain the desired,
Mask_RCNN/mrcnn/model.py,1375,balance. Reduce requirements and fill in the rest. This is,
Mask_RCNN/mrcnn/model.py,1376,likely different from the Mask RCNN paper.,
Mask_RCNN/mrcnn/model.py,1378,There is a small chance we have neither fg nor bg samples.,
Mask_RCNN/mrcnn/model.py,1380,Pick bg regions with easier IoU threshold,
Mask_RCNN/mrcnn/model.py,1387,Fill the rest with repeated bg rois.,
Mask_RCNN/mrcnn/model.py,1395,Reset the gt boxes assigned to BG ROIs.,
Mask_RCNN/mrcnn/model.py,1399,"For each kept ROI, assign a class_id, and for FG ROIs also add bbox refinement.",
Mask_RCNN/mrcnn/model.py,1405,"Class-aware bbox deltas. [y, x, log(h), log(w)]",
Mask_RCNN/mrcnn/model.py,1411,Normalize bbox refinements,
Mask_RCNN/mrcnn/model.py,1414,Generate class-specific target masks,
Mask_RCNN/mrcnn/model.py,1424,"Create a mask placeholder, the size of the image",
Mask_RCNN/mrcnn/model.py,1426,GT box,
Mask_RCNN/mrcnn/model.py,1430,Resize mini mask to size of GT box,
Mask_RCNN/mrcnn/model.py,1433,Place the mini batch in the placeholder,
Mask_RCNN/mrcnn/model.py,1436,Pick part of the mask and resize it,
Mask_RCNN/mrcnn/model.py,1458,"RPN Match: 1 = positive anchor, -1 = negative anchor, 0 = neutral",
Mask_RCNN/mrcnn/model.py,1460,"RPN bounding boxes: [max anchors per image, (dy, dx, log(dh), log(dw))]",
Mask_RCNN/mrcnn/model.py,1463,Handle COCO crowds,
Mask_RCNN/mrcnn/model.py,1464,A crowd box in COCO is a bounding box around several instances. Exclude,
Mask_RCNN/mrcnn/model.py,1465,them from training. A crowd box is given a negative class ID.,
Mask_RCNN/mrcnn/model.py,1468,Filter out crowds from ground truth class IDs and boxes,
Mask_RCNN/mrcnn/model.py,1473,"Compute overlaps with crowd boxes [anchors, crowds]",
Mask_RCNN/mrcnn/model.py,1478,All anchors don't intersect a crowd,
Mask_RCNN/mrcnn/model.py,1481,"Compute overlaps [num_anchors, num_gt_boxes]",
Mask_RCNN/mrcnn/model.py,1484,Match anchors to GT Boxes,
Mask_RCNN/mrcnn/model.py,1485,If an anchor overlaps a GT box with IoU >= 0.7 then it's positive.,
Mask_RCNN/mrcnn/model.py,1486,If an anchor overlaps a GT box with IoU < 0.3 then it's negative.,
Mask_RCNN/mrcnn/model.py,1487,"Neutral anchors are those that don't match the conditions above,",
Mask_RCNN/mrcnn/model.py,1488,and they don't influence the loss function.,
Mask_RCNN/mrcnn/model.py,1489,"However, don't keep any GT box unmatched (rare, but happens). Instead,",
Mask_RCNN/mrcnn/model.py,1490,match it to the closest anchor (even if its max IoU is < 0.3).,
Mask_RCNN/mrcnn/model.py,1491,,
Mask_RCNN/mrcnn/model.py,1492,1. Set negative anchors first. They get overwritten below if a GT box is,
Mask_RCNN/mrcnn/model.py,1493,matched to them. Skip boxes in crowd areas.,
Mask_RCNN/mrcnn/model.py,1497,2. Set an anchor for each GT box (regardless of IoU value).,
Mask_RCNN/mrcnn/model.py,1498,If multiple anchors have the same IoU match all of them,
Mask_RCNN/mrcnn/model.py,1501,3. Set anchors with high overlap as positive.,
Mask_RCNN/mrcnn/model.py,1504,Subsample to balance positive and negative anchors,
Mask_RCNN/mrcnn/model.py,1505,Don't let positives be more than half the anchors,
Mask_RCNN/mrcnn/model.py,1509,Reset the extra ones to neutral,
Mask_RCNN/mrcnn/model.py,1512,Same for negative proposals,
Mask_RCNN/mrcnn/model.py,1517,Rest the extra ones to neutral,
Mask_RCNN/mrcnn/model.py,1521,"For positive anchors, compute shift and scale needed to transform them",
Mask_RCNN/mrcnn/model.py,1522,to match the corresponding GT boxes.,
Mask_RCNN/mrcnn/model.py,1524,index into rpn_bbox,
Mask_RCNN/mrcnn/model.py,1525,TODO: use box_refinement() rather than duplicating the code here,
Mask_RCNN/mrcnn/model.py,1527,Closest gt box (it might have IoU < 0.7),
Mask_RCNN/mrcnn/model.py,1530,Convert coordinates to center plus width/height.,
Mask_RCNN/mrcnn/model.py,1531,GT Box,
Mask_RCNN/mrcnn/model.py,1536,Anchor,
Mask_RCNN/mrcnn/model.py,1542,Compute the bbox refinement that the RPN should predict.,
Mask_RCNN/mrcnn/model.py,1549,Normalize,
Mask_RCNN/mrcnn/model.py,1567,placeholder,
Mask_RCNN/mrcnn/model.py,1570,Generate random ROIs around GT boxes (90% of count),
Mask_RCNN/mrcnn/model.py,1576,random boundaries,
Mask_RCNN/mrcnn/model.py,1582,"To avoid generating boxes with zero area, we generate double what",
Mask_RCNN/mrcnn/model.py,1583,we need and filter out the extra. If we get fewer valid boxes,
Mask_RCNN/mrcnn/model.py,1584,"than we need, we loop and try again.",
Mask_RCNN/mrcnn/model.py,1588,Filter out zero area boxes,
Mask_RCNN/mrcnn/model.py,1597,Sort on axis 1 to ensure x1 <= x2 and y1 <= y2 and then reshape,
Mask_RCNN/mrcnn/model.py,1598,"into x1, y1, x2, y2 order",
Mask_RCNN/mrcnn/model.py,1604,Generate random ROIs anywhere in the image (10% of count),
Mask_RCNN/mrcnn/model.py,1606,"To avoid generating boxes with zero area, we generate double what",
Mask_RCNN/mrcnn/model.py,1607,we need and filter out the extra. If we get fewer valid boxes,
Mask_RCNN/mrcnn/model.py,1608,"than we need, we loop and try again.",
Mask_RCNN/mrcnn/model.py,1612,Filter out zero area boxes,
Mask_RCNN/mrcnn/model.py,1621,Sort on axis 1 to ensure x1 <= x2 and y1 <= y2 and then reshape,
Mask_RCNN/mrcnn/model.py,1622,"into x1, y1, x2, y2 order",
Mask_RCNN/mrcnn/model.py,1673,batch item index,
Mask_RCNN/mrcnn/model.py,1679,Anchors,
Mask_RCNN/mrcnn/model.py,1680,"[anchor_count, (y1, x1, y2, x2)]",
Mask_RCNN/mrcnn/model.py,1688,Keras requires a generator to run indefinitely.,
Mask_RCNN/mrcnn/model.py,1691,Increment index to pick next image. Shuffle if at the start of an epoch.,
Mask_RCNN/mrcnn/model.py,1696,Get GT bounding boxes and masks for image.,
Mask_RCNN/mrcnn/model.py,1699,If the image source is not to be augmented pass None as augmentation,
Mask_RCNN/mrcnn/model.py,1711,Skip images that have no instances. This can happen in cases,
Mask_RCNN/mrcnn/model.py,1712,where we train on a subset of classes and the image doesn't,
Mask_RCNN/mrcnn/model.py,1713,have any of the classes we care about.,
Mask_RCNN/mrcnn/model.py,1717,RPN Targets,
Mask_RCNN/mrcnn/model.py,1721,Mask R-CNN Targets,
Mask_RCNN/mrcnn/model.py,1730,Init batch arrays,
Mask_RCNN/mrcnn/model.py,1760,"If more instances than fits in the array, sub-sample from them.",
Mask_RCNN/mrcnn/model.py,1768,Add to batch,
Mask_RCNN/mrcnn/model.py,1785,Batch full?,
Mask_RCNN/mrcnn/model.py,1795,Keras requires that output and targets have the same number of dimensions,
Mask_RCNN/mrcnn/model.py,1803,start a new batch,
Mask_RCNN/mrcnn/model.py,1808,Log it and skip the image,
Mask_RCNN/mrcnn/model.py,1816,,
Mask_RCNN/mrcnn/model.py,1817,MaskRCNN Class,
Mask_RCNN/mrcnn/model.py,1818,,
Mask_RCNN/mrcnn/model.py,1847,Image size must be dividable by 2 multiple times,
Mask_RCNN/mrcnn/model.py,1854,Inputs,
Mask_RCNN/mrcnn/model.py,1860,RPN GT,
Mask_RCNN/mrcnn/model.py,1866,"Detection GT (class IDs, bounding boxes, and masks)",
Mask_RCNN/mrcnn/model.py,1867,1. GT Class IDs (zero padded),
Mask_RCNN/mrcnn/model.py,1870,2. GT Boxes in pixels (zero padded),
Mask_RCNN/mrcnn/model.py,1871,"[batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)] in image coordinates",
Mask_RCNN/mrcnn/model.py,1874,Normalize coordinates,
Mask_RCNN/mrcnn/model.py,1877,3. GT Masks (zero padded),
Mask_RCNN/mrcnn/model.py,1878,"[batch, height, width, MAX_GT_INSTANCES]",
Mask_RCNN/mrcnn/model.py,1889,Anchors in normalized coordinates,
Mask_RCNN/mrcnn/model.py,1892,Build the shared convolutional layers.,
Mask_RCNN/mrcnn/model.py,1893,Bottom-up Layers,
Mask_RCNN/mrcnn/model.py,1894,"Returns a list of the last layers of each stage, 5 in total.",
Mask_RCNN/mrcnn/model.py,1895,"Don't create the thead (stage 5), so we pick the 4th item in the list.",
Mask_RCNN/mrcnn/model.py,1902,Top-down Layers,
Mask_RCNN/mrcnn/model.py,1903,TODO: add assert to varify feature map sizes match what's in config,
Mask_RCNN/mrcnn/model.py,1914,Attach 3x3 conv to all P layers to get the final feature maps.,
Mask_RCNN/mrcnn/model.py,1919,P6 is used for the 5th anchor scale in RPN. Generated by,
Mask_RCNN/mrcnn/model.py,1920,subsampling from P5 with stride of 2.,
Mask_RCNN/mrcnn/model.py,1923,"Note that P6 is used in RPN, but not in the classifier heads.",
Mask_RCNN/mrcnn/model.py,1927,Anchors,
Mask_RCNN/mrcnn/model.py,1930,Duplicate across the batch dimension because Keras requires it,
Mask_RCNN/mrcnn/model.py,1931,TODO: can this be optimized to avoid duplicating the anchors?,
Mask_RCNN/mrcnn/model.py,1933,A hack to get around Keras's bad support for constants,
Mask_RCNN/mrcnn/model.py,1938,RPN Model,
Mask_RCNN/mrcnn/model.py,1941,Loop through pyramid layers,
Mask_RCNN/mrcnn/model.py,1942,list of lists,
Mask_RCNN/mrcnn/model.py,1945,Concatenate layer outputs,
Mask_RCNN/mrcnn/model.py,1946,Convert from list of lists of level outputs to list of lists,
Mask_RCNN/mrcnn/model.py,1947,of outputs across levels.,
Mask_RCNN/mrcnn/model.py,1948,"e.g. [[a1, b1, c1], [a2, b2, c2]] => [[a1, a2], [b1, b2], [c1, c2]]",
Mask_RCNN/mrcnn/model.py,1956,Generate proposals,
Mask_RCNN/mrcnn/model.py,1957,"Proposals are [batch, N, (y1, x1, y2, x2)] in normalized coordinates",
Mask_RCNN/mrcnn/model.py,1958,and zero padded.,
Mask_RCNN/mrcnn/model.py,1968,Class ID mask to mark class IDs supported by the dataset the image,
Mask_RCNN/mrcnn/model.py,1969,came from.,
Mask_RCNN/mrcnn/model.py,1975,Ignore predicted ROIs and use ROIs provided as an input.,
Mask_RCNN/mrcnn/model.py,1978,Normalize coordinates,
Mask_RCNN/mrcnn/model.py,1984,Generate detection targets,
Mask_RCNN/mrcnn/model.py,1985,Subsamples proposals and generates target outputs for training,
Mask_RCNN/mrcnn/model.py,1986,"Note that proposal class IDs, gt_boxes, and gt_masks are zero",
Mask_RCNN/mrcnn/model.py,1987,"padded. Equally, returned rois and targets are zero padded.",
Mask_RCNN/mrcnn/model.py,1992,Network Heads,
Mask_RCNN/mrcnn/model.py,1993,TODO: verify that this handles zero padded ROIs,
Mask_RCNN/mrcnn/model.py,2006,TODO: clean up (use tf.identify if necessary),
Mask_RCNN/mrcnn/model.py,2009,Losses,
Mask_RCNN/mrcnn/model.py,2021,Model,
Mask_RCNN/mrcnn/model.py,2032,Network Heads,
Mask_RCNN/mrcnn/model.py,2033,Proposal classifier and BBox regressor heads,
Mask_RCNN/mrcnn/model.py,2040,Detections,
Mask_RCNN/mrcnn/model.py,2041,"output is [batch, num_detections, (y1, x1, y2, x2, class_id, score)] in",
Mask_RCNN/mrcnn/model.py,2042,normalized coordinates,
Mask_RCNN/mrcnn/model.py,2046,Create masks for detections,
Mask_RCNN/mrcnn/model.py,2059,Add multi-GPU support.,
Mask_RCNN/mrcnn/model.py,2072,Get directory names. Each directory corresponds to a model,
Mask_RCNN/mrcnn/model.py,2082,Pick last directory,
Mask_RCNN/mrcnn/model.py,2084,Find the last checkpoint,
Mask_RCNN/mrcnn/model.py,2102,Conditional import to support versions of Keras before 2.2,
Mask_RCNN/mrcnn/model.py,2103,TODO: remove in about 6 months (end of 2018),
Mask_RCNN/mrcnn/model.py,2107,Keras before 2.2 used the 'topology' namespace.,
Mask_RCNN/mrcnn/model.py,2119,"In multi-GPU training, we wrap the model. Get layers",
Mask_RCNN/mrcnn/model.py,2120,of the inner model because they have the weights.,
Mask_RCNN/mrcnn/model.py,2125,Exclude some layers,
Mask_RCNN/mrcnn/model.py,2136,Update the log directory,
Mask_RCNN/mrcnn/model.py,2157,Optimizer object,
Mask_RCNN/mrcnn/model.py,2161,Add Losses,
Mask_RCNN/mrcnn/model.py,2162,"First, clear previously set losses to avoid duplication",
Mask_RCNN/mrcnn/model.py,2177,Add L2 Regularization,
Mask_RCNN/mrcnn/model.py,2178,Skip gamma and beta weights of batch normalization layers.,
Mask_RCNN/mrcnn/model.py,2185,Compile,
Mask_RCNN/mrcnn/model.py,2190,Add metrics for losses,
Mask_RCNN/mrcnn/model.py,2205,Print message on the first call (but not on recursive calls),
Mask_RCNN/mrcnn/model.py,2211,"In multi-GPU training, we wrap the model. Get layers",
Mask_RCNN/mrcnn/model.py,2212,of the inner model because they have the weights.,
Mask_RCNN/mrcnn/model.py,2217,Is the layer a model?,
Mask_RCNN/mrcnn/model.py,2226,Is it trainable?,
Mask_RCNN/mrcnn/model.py,2228,"Update layer. If layer is a container, update inner layer.",
Mask_RCNN/mrcnn/model.py,2233,Print trainable layer names,
Mask_RCNN/mrcnn/model.py,2246,Set date and epoch counter as if starting a new model,
Mask_RCNN/mrcnn/model.py,2250,If we have a model path with date and epochs use them,
Mask_RCNN/mrcnn/model.py,2252,Continue from we left of. Get epoch and date from the file name,
Mask_RCNN/mrcnn/model.py,2253,A sample model path might look like:,
Mask_RCNN/mrcnn/model.py,2254,\path\to\logs\coco20171029T2315\mask_rcnn_coco_0001.h5 (Windows),
Mask_RCNN/mrcnn/model.py,2255,/path/to/logs/coco20171029T2315/mask_rcnn_coco_0001.h5 (Linux),
Mask_RCNN/mrcnn/model.py,2261,"Epoch number in file is 1-based, and in Keras code it's 0-based.",
Mask_RCNN/mrcnn/model.py,2262,"So, adjust for that then increment by one to start from the next epoch",
Mask_RCNN/mrcnn/model.py,2266,Directory for training logs,
Mask_RCNN/mrcnn/model.py,2270,Path to save after each epoch. Include placeholders that get filled by Keras.,
Mask_RCNN/mrcnn/model.py,2312,Pre-defined layer regular expressions,
Mask_RCNN/mrcnn/model.py,2314,all layers but the backbone,
Mask_RCNN/mrcnn/model.py,2316,From a specific Resnet stage and up,
Mask_RCNN/mrcnn/model.py,2320,All layers,
Mask_RCNN/mrcnn/model.py,2326,Data generators,
Mask_RCNN/mrcnn/model.py,2334,Create log_dir if it does not exist,
Mask_RCNN/mrcnn/model.py,2338,Callbacks,
Mask_RCNN/mrcnn/model.py,2346,Add custom callbacks to the list,
Mask_RCNN/mrcnn/model.py,2350,Train,
Mask_RCNN/mrcnn/model.py,2356,Work-around for Windows: Keras fails on Windows when using,
Mask_RCNN/mrcnn/model.py,2357,multiprocessing workers. See discussion here:,
Mask_RCNN/mrcnn/model.py,2358,https://github.com/matterport/Mask_RCNN/issues/13#issuecomment-353124009,
Mask_RCNN/mrcnn/model.py,2394,Resize image,
Mask_RCNN/mrcnn/model.py,2395,TODO: move resizing to mold_image(),
Mask_RCNN/mrcnn/model.py,2403,Build image_meta,
Mask_RCNN/mrcnn/model.py,2407,Append,
Mask_RCNN/mrcnn/model.py,2411,Pack into arrays,
Mask_RCNN/mrcnn/model.py,2436,How many detections do we have?,
Mask_RCNN/mrcnn/model.py,2437,Detections array is padded with zeros. Find the first class_id == 0.,
Mask_RCNN/mrcnn/model.py,2441,"Extract boxes, class_ids, scores, and class-specific masks",
Mask_RCNN/mrcnn/model.py,2447,Translate normalized coordinates in the resized image to pixel,
Mask_RCNN/mrcnn/model.py,2448,coordinates in the original image before resizing,
Mask_RCNN/mrcnn/model.py,2452,window height,
Mask_RCNN/mrcnn/model.py,2453,window width,
Mask_RCNN/mrcnn/model.py,2455,Convert boxes to normalized coordinates on the window,
Mask_RCNN/mrcnn/model.py,2457,Convert boxes to pixel coordinates on the original image,
Mask_RCNN/mrcnn/model.py,2460,Filter out detections with zero area. Happens in early training when,
Mask_RCNN/mrcnn/model.py,2461,network weights are still random,
Mask_RCNN/mrcnn/model.py,2471,Resize masks to original image size and set boundary threshold.,
Mask_RCNN/mrcnn/model.py,2474,Convert neural network mask to full size mask,
Mask_RCNN/mrcnn/model.py,2502,Mold inputs to format expected by the neural network,
Mask_RCNN/mrcnn/model.py,2505,Validate image sizes,
Mask_RCNN/mrcnn/model.py,2506,All images in a batch MUST be of the same size,
Mask_RCNN/mrcnn/model.py,2512,Anchors,
Mask_RCNN/mrcnn/model.py,2514,Duplicate across the batch dimension because Keras requires it,
Mask_RCNN/mrcnn/model.py,2515,TODO: can this be optimized to avoid duplicating the anchors?,
Mask_RCNN/mrcnn/model.py,2522,Run object detection,
Mask_RCNN/mrcnn/model.py,2525,Process detections,
Mask_RCNN/mrcnn/model.py,2563,Validate image sizes,
Mask_RCNN/mrcnn/model.py,2564,All images in a batch MUST be of the same size,
Mask_RCNN/mrcnn/model.py,2569,Anchors,
Mask_RCNN/mrcnn/model.py,2571,Duplicate across the batch dimension because Keras requires it,
Mask_RCNN/mrcnn/model.py,2572,TODO: can this be optimized to avoid duplicating the anchors?,
Mask_RCNN/mrcnn/model.py,2579,Run object detection,
Mask_RCNN/mrcnn/model.py,2582,Process detections,
Mask_RCNN/mrcnn/model.py,2601,Cache anchors and reuse if image shape is the same,
Mask_RCNN/mrcnn/model.py,2605,Generate Anchors,
Mask_RCNN/mrcnn/model.py,2612,Keep a copy of the latest anchors in pixel coordinates because,
Mask_RCNN/mrcnn/model.py,2613,it's used in inspect_model notebooks.,
Mask_RCNN/mrcnn/model.py,2614,TODO: Remove this after the notebook are refactored to not use it,
Mask_RCNN/mrcnn/model.py,2616,Normalize coordinates,
Mask_RCNN/mrcnn/model.py,2628,Put a limit on how deep we go to avoid very long loops,
Mask_RCNN/mrcnn/model.py,2631,Convert name to a regex and allow matching a number prefix,
Mask_RCNN/mrcnn/model.py,2632,because Keras adds them automatically,
Mask_RCNN/mrcnn/model.py,2660,Loop through all layers,
Mask_RCNN/mrcnn/model.py,2662,"If layer is a wrapper, find inner trainable layer",
Mask_RCNN/mrcnn/model.py,2664,Include layer if it has weights,
Mask_RCNN/mrcnn/model.py,2684,Organize desired outputs into an ordered dict,
Mask_RCNN/mrcnn/model.py,2689,Build a Keras function to run parts of the computation graph,
Mask_RCNN/mrcnn/model.py,2695,Prepare inputs,
Mask_RCNN/mrcnn/model.py,2701,Anchors,
Mask_RCNN/mrcnn/model.py,2703,Duplicate across the batch dimension because Keras requires it,
Mask_RCNN/mrcnn/model.py,2704,TODO: can this be optimized to avoid duplicating the anchors?,
Mask_RCNN/mrcnn/model.py,2708,Run inference,
Mask_RCNN/mrcnn/model.py,2713,Pack the generated Numpy arrays into a a dict and log the results.,
Mask_RCNN/mrcnn/model.py,2721,,
Mask_RCNN/mrcnn/model.py,2722,Data Formatting,
Mask_RCNN/mrcnn/model.py,2723,,
Mask_RCNN/mrcnn/model.py,2740,size=1,
Mask_RCNN/mrcnn/model.py,2741,size=3,
Mask_RCNN/mrcnn/model.py,2742,size=3,
Mask_RCNN/mrcnn/model.py,2743,"size=4 (y1, x1, y2, x2) in image cooredinates",
Mask_RCNN/mrcnn/model.py,2744,size=1,
Mask_RCNN/mrcnn/model.py,2745,size=num_classes,
Mask_RCNN/mrcnn/model.py,2761,"(y1, x1, y2, x2) window of image in in pixels",
Mask_RCNN/mrcnn/model.py,2785,"(y1, x1, y2, x2) window of image in in pixels",
Mask_RCNN/mrcnn/model.py,2811,,
Mask_RCNN/mrcnn/model.py,2812,Miscellenous Graph Functions,
Mask_RCNN/mrcnn/model.py,2813,,
Mask_RCNN/mrcnn/utils.py,26,URL from which to download the latest COCO trained weights,
Mask_RCNN/mrcnn/utils.py,30,,
Mask_RCNN/mrcnn/utils.py,31,Bounding Boxes,
Mask_RCNN/mrcnn/utils.py,32,,
Mask_RCNN/mrcnn/utils.py,43,Bounding box.,
Mask_RCNN/mrcnn/utils.py,49,x2 and y2 should not be part of the box. Increment by 1.,
Mask_RCNN/mrcnn/utils.py,53,No mask for this instance. Might happen due to,
Mask_RCNN/mrcnn/utils.py,54,resizing or cropping. Set bbox to zeros,
Mask_RCNN/mrcnn/utils.py,70,Calculate intersection areas,
Mask_RCNN/mrcnn/utils.py,87,Areas of anchors and GT boxes,
Mask_RCNN/mrcnn/utils.py,91,"Compute overlaps to generate matrix [boxes1 count, boxes2 count]",
Mask_RCNN/mrcnn/utils.py,92,Each cell contains the IoU value.,
Mask_RCNN/mrcnn/utils.py,105,If either set of masks is empty return empty result,
Mask_RCNN/mrcnn/utils.py,108,flatten masks and compute their areas,
Mask_RCNN/mrcnn/utils.py,114,intersections and union,
Mask_RCNN/mrcnn/utils.py,132,Compute box areas,
Mask_RCNN/mrcnn/utils.py,139,Get indicies of boxes sorted by scores (highest first),
Mask_RCNN/mrcnn/utils.py,144,Pick top box and add its index to the list,
Mask_RCNN/mrcnn/utils.py,147,Compute IoU of the picked box with the rest,
Mask_RCNN/mrcnn/utils.py,149,Identify boxes with IoU over the threshold. This,
Mask_RCNN/mrcnn/utils.py,150,"returns indices into ixs[1:], so add 1 to get",
Mask_RCNN/mrcnn/utils.py,151,indices into ixs.,
Mask_RCNN/mrcnn/utils.py,153,Remove indices of the picked and overlapped boxes.,
Mask_RCNN/mrcnn/utils.py,165,"Convert to y, x, h, w",
Mask_RCNN/mrcnn/utils.py,170,Apply deltas,
Mask_RCNN/mrcnn/utils.py,175,"Convert back to y1, x1, y2, x2",
Mask_RCNN/mrcnn/utils.py,235,,
Mask_RCNN/mrcnn/utils.py,236,Dataset,
Mask_RCNN/mrcnn/utils.py,237,,
Mask_RCNN/mrcnn/utils.py,258,Background is always the first class,
Mask_RCNN/mrcnn/utils.py,264,Does the class exist already?,
Mask_RCNN/mrcnn/utils.py,267,"source.class_id combination already available, skip",
Mask_RCNN/mrcnn/utils.py,269,Add the class,
Mask_RCNN/mrcnn/utils.py,305,Build (or rebuild) everything else from the info dicts.,
Mask_RCNN/mrcnn/utils.py,312,Mapping from source class and image IDs to internal IDs,
Mask_RCNN/mrcnn/utils.py,318,Map sources to class_ids they support,
Mask_RCNN/mrcnn/utils.py,321,Loop over datasets,
Mask_RCNN/mrcnn/utils.py,324,Find classes that belong to this dataset,
Mask_RCNN/mrcnn/utils.py,326,Include BG class in all datasets,
Mask_RCNN/mrcnn/utils.py,358,Load image,
Mask_RCNN/mrcnn/utils.py,360,If grayscale. Convert to RGB for consistency.,
Mask_RCNN/mrcnn/utils.py,363,"If has an alpha channel, remove it for consistency",
Mask_RCNN/mrcnn/utils.py,380,Override this function to load a mask from your dataset.,
Mask_RCNN/mrcnn/utils.py,381,"Otherwise, it returns an empty mask.",
Mask_RCNN/mrcnn/utils.py,420,Keep track of image dtype and return results in the same dtype,
Mask_RCNN/mrcnn/utils.py,422,"Default window (y1, x1, y2, x2) and default scale == 1.",
Mask_RCNN/mrcnn/utils.py,432,Scale?,
Mask_RCNN/mrcnn/utils.py,434,Scale up but not down,
Mask_RCNN/mrcnn/utils.py,439,Does it exceed max dim?,
Mask_RCNN/mrcnn/utils.py,445,Resize image using bilinear interpolation,
Mask_RCNN/mrcnn/utils.py,450,Need padding or cropping?,
Mask_RCNN/mrcnn/utils.py,452,Get new height and width,
Mask_RCNN/mrcnn/utils.py,463,Both sides must be divisible by 64,
Mask_RCNN/mrcnn/utils.py,465,Height,
Mask_RCNN/mrcnn/utils.py,472,Width,
Mask_RCNN/mrcnn/utils.py,483,Pick a random crop,
Mask_RCNN/mrcnn/utils.py,504,"Suppress warning from scipy 0.13.0, the output shape of zoom() is",
Mask_RCNN/mrcnn/utils.py,505,calculated with round() instead of int(),
Mask_RCNN/mrcnn/utils.py,525,Pick slice and cast to bool in case load_mask() returned wrong dtype,
Mask_RCNN/mrcnn/utils.py,531,Resize with bilinear interpolation,
Mask_RCNN/mrcnn/utils.py,549,Resize with bilinear interpolation,
Mask_RCNN/mrcnn/utils.py,555,TODO: Build and use this function to reduce code duplication,
Mask_RCNN/mrcnn/utils.py,573,Put the mask in the right location.,
Mask_RCNN/mrcnn/utils.py,579,,
Mask_RCNN/mrcnn/utils.py,580,Anchors,
Mask_RCNN/mrcnn/utils.py,581,,
Mask_RCNN/mrcnn/utils.py,593,Get all combinations of scales and ratios,
Mask_RCNN/mrcnn/utils.py,598,Enumerate heights and widths from scales and ratios,
Mask_RCNN/mrcnn/utils.py,602,Enumerate shifts in feature space,
Mask_RCNN/mrcnn/utils.py,607,"Enumerate combinations of shifts, widths, and heights",
Mask_RCNN/mrcnn/utils.py,611,"Reshape to get a list of (y, x) and a list of (h, w)",
Mask_RCNN/mrcnn/utils.py,616,"Convert to corner coordinates (y1, x1, y2, x2)",
Mask_RCNN/mrcnn/utils.py,633,Anchors,
Mask_RCNN/mrcnn/utils.py,634,"[anchor_count, (y1, x1, y2, x2)]",
Mask_RCNN/mrcnn/utils.py,642,,
Mask_RCNN/mrcnn/utils.py,643,Miscellaneous,
Mask_RCNN/mrcnn/utils.py,644,,
Mask_RCNN/mrcnn/utils.py,668,Trim zero padding,
Mask_RCNN/mrcnn/utils.py,669,TODO: cleaner to do zero unpadding upstream,
Mask_RCNN/mrcnn/utils.py,674,Sort predictions by score from high to low,
Mask_RCNN/mrcnn/utils.py,681,"Compute IoU overlaps [pred_masks, gt_masks]",
Mask_RCNN/mrcnn/utils.py,684,Loop through predictions and find matching ground truth boxes,
Mask_RCNN/mrcnn/utils.py,689,Find best matching ground truth box,
Mask_RCNN/mrcnn/utils.py,690,1. Sort matches by score,
Mask_RCNN/mrcnn/utils.py,692,2. Remove low scores,
Mask_RCNN/mrcnn/utils.py,696,3. Find the match,
Mask_RCNN/mrcnn/utils.py,698,"If ground truth box is already matched, go to next one",
Mask_RCNN/mrcnn/utils.py,701,"If we reach IoU smaller than the threshold, end the loop",
Mask_RCNN/mrcnn/utils.py,705,Do we have a match?,
Mask_RCNN/mrcnn/utils.py,726,Get matches and overlaps,
Mask_RCNN/mrcnn/utils.py,732,Compute precision and recall at each prediction box step,
Mask_RCNN/mrcnn/utils.py,736,Pad with start and end values to simplify the math,
Mask_RCNN/mrcnn/utils.py,740,"Ensure precision values decrease but don't increase. This way, the",
Mask_RCNN/mrcnn/utils.py,741,precision value at each recall threshold is the maximum it can be,
Mask_RCNN/mrcnn/utils.py,742,"for all following recall thresholds, as specified by the VOC paper.",
Mask_RCNN/mrcnn/utils.py,746,Compute mean AP over recall range,
Mask_RCNN/mrcnn/utils.py,758,Default is 0.5 to 0.95 with increments of 0.05,
Mask_RCNN/mrcnn/utils.py,761,Compute AP over range of IoU thresholds,
Mask_RCNN/mrcnn/utils.py,785,Measure overlaps,
Mask_RCNN/mrcnn/utils.py,796,## Batch Slicing,
Mask_RCNN/mrcnn/utils.py,797,"Some custom layers support a batch size of 1 only, and require a lot of work",
Mask_RCNN/mrcnn/utils.py,798,to support batches greater than 1. This function slices an input tensor,
Mask_RCNN/mrcnn/utils.py,799,"across the batch dimension and feeds batches of size 1. Effectively,",
Mask_RCNN/mrcnn/utils.py,800,an easy way to support batches > 1 quickly with little code modification.,
Mask_RCNN/mrcnn/utils.py,801,"In the long run, it's more efficient to modify the code to support large",
Mask_RCNN/mrcnn/utils.py,802,batches and getting rid of this function. Consider this a temporary solution,
Mask_RCNN/mrcnn/utils.py,824,Change outputs from a list of slices where each is,
Mask_RCNN/mrcnn/utils.py,825,a list of outputs to a list of outputs and each has,
Mask_RCNN/mrcnn/utils.py,826,a list of slices,
Mask_RCNN/mrcnn/utils.py,897,New in 0.14: anti_aliasing. Default it to False for backward,
Mask_RCNN/mrcnn/utils.py,898,compatibility with skimage 0.13.,
Mask_RCNN/mrcnn/config.py,13,Base Configuration Class,
Mask_RCNN/mrcnn/config.py,14,"Don't use this class directly. Instead, sub-class it and override",
Mask_RCNN/mrcnn/config.py,15,the configurations you need to change.,
Mask_RCNN/mrcnn/config.py,22,"Name the configurations. For example, 'COCO', 'Experiment 3', ...etc.",
Mask_RCNN/mrcnn/config.py,23,Useful if your code needs to do things differently depending on which,
Mask_RCNN/mrcnn/config.py,24,experiment is running.,
Mask_RCNN/mrcnn/config.py,25,Override in sub-classes,
Mask_RCNN/mrcnn/config.py,27,"NUMBER OF GPUs to use. When using only a CPU, this needs to be set to 1.",
Mask_RCNN/mrcnn/config.py,30,Number of images to train with on each GPU. A 12GB GPU can typically,
Mask_RCNN/mrcnn/config.py,31,handle 2 images of 1024x1024px.,
Mask_RCNN/mrcnn/config.py,32,Adjust based on your GPU memory and image sizes. Use the highest,
Mask_RCNN/mrcnn/config.py,33,number that your GPU can handle for best performance.,
Mask_RCNN/mrcnn/config.py,36,Number of training steps per epoch,
Mask_RCNN/mrcnn/config.py,37,This doesn't need to match the size of the training set. Tensorboard,
Mask_RCNN/mrcnn/config.py,38,"updates are saved at the end of each epoch, so setting this to a",
Mask_RCNN/mrcnn/config.py,39,smaller number means getting more frequent TensorBoard updates.,
Mask_RCNN/mrcnn/config.py,40,Validation stats are also calculated at each epoch end and they,
Mask_RCNN/mrcnn/config.py,41,"might take a while, so don't set this too small to avoid spending",
Mask_RCNN/mrcnn/config.py,42,a lot of time on validation stats.,
Mask_RCNN/mrcnn/config.py,45,Number of validation steps to run at the end of every training epoch.,
Mask_RCNN/mrcnn/config.py,46,"A bigger number improves accuracy of validation stats, but slows",
Mask_RCNN/mrcnn/config.py,47,down the training.,
Mask_RCNN/mrcnn/config.py,50,Backbone network architecture,
Mask_RCNN/mrcnn/config.py,51,"Supported values are: resnet50, resnet101.",
Mask_RCNN/mrcnn/config.py,52,You can also provide a callable that should have the signature,
Mask_RCNN/mrcnn/config.py,53,"of model.resnet_graph. If you do so, you need to supply a callable",
Mask_RCNN/mrcnn/config.py,54,to COMPUTE_BACKBONE_SHAPE as well,
Mask_RCNN/mrcnn/config.py,57,Only useful if you supply a callable to BACKBONE. Should compute,
Mask_RCNN/mrcnn/config.py,58,the shape of each layer of the FPN Pyramid.,
Mask_RCNN/mrcnn/config.py,59,See model.compute_backbone_shapes,
Mask_RCNN/mrcnn/config.py,62,The strides of each layer of the FPN Pyramid. These values,
Mask_RCNN/mrcnn/config.py,63,are based on a Resnet101 backbone.,
Mask_RCNN/mrcnn/config.py,66,Size of the fully-connected layers in the classification graph,
Mask_RCNN/mrcnn/config.py,69,Size of the top-down layers used to build the feature pyramid,
Mask_RCNN/mrcnn/config.py,72,Number of classification classes (including background),
Mask_RCNN/mrcnn/config.py,73,Override in sub-classes,
Mask_RCNN/mrcnn/config.py,75,Length of square anchor side in pixels,
Mask_RCNN/mrcnn/config.py,78,Ratios of anchors at each cell (width/height),
Mask_RCNN/mrcnn/config.py,79,"A value of 1 represents a square anchor, and 0.5 is a wide anchor",
Mask_RCNN/mrcnn/config.py,82,Anchor stride,
Mask_RCNN/mrcnn/config.py,83,If 1 then anchors are created for each cell in the backbone feature map.,
Mask_RCNN/mrcnn/config.py,84,"If 2, then anchors are created for every other cell, and so on.",
Mask_RCNN/mrcnn/config.py,87,Non-max suppression threshold to filter RPN proposals.,
Mask_RCNN/mrcnn/config.py,88,You can increase this during training to generate more propsals.,
Mask_RCNN/mrcnn/config.py,91,How many anchors per image to use for RPN training,
Mask_RCNN/mrcnn/config.py,94,ROIs kept after tf.nn.top_k and before non-maximum suppression,
Mask_RCNN/mrcnn/config.py,97,ROIs kept after non-maximum suppression (training and inference),
Mask_RCNN/mrcnn/config.py,101,"If enabled, resizes instance masks to a smaller size to reduce",
Mask_RCNN/mrcnn/config.py,102,memory load. Recommended when using high-resolution images.,
Mask_RCNN/mrcnn/config.py,104,"(height, width) of the mini-mask",
Mask_RCNN/mrcnn/config.py,106,Input image resizing,
Mask_RCNN/mrcnn/config.py,107,"Generally, use the ""square"" resizing mode for training and predicting",
Mask_RCNN/mrcnn/config.py,108,"and it should work well in most cases. In this mode, images are scaled",
Mask_RCNN/mrcnn/config.py,109,"up such that the small side is = IMAGE_MIN_DIM, but ensuring that the",
Mask_RCNN/mrcnn/config.py,110,scaling doesn't make the long side > IMAGE_MAX_DIM. Then the image is,
Mask_RCNN/mrcnn/config.py,111,padded with zeros to make it a square so multiple images can be put,
Mask_RCNN/mrcnn/config.py,112,in one batch.,
Mask_RCNN/mrcnn/config.py,113,Available resizing modes:,
Mask_RCNN/mrcnn/config.py,114,none:   No resizing or padding. Return the image unchanged.,
Mask_RCNN/mrcnn/config.py,115,square: Resize and pad with zeros to get a square image,
Mask_RCNN/mrcnn/config.py,116,"of size [max_dim, max_dim].",
Mask_RCNN/mrcnn/config.py,117,pad64:  Pads width and height with zeros to make them multiples of 64.,
Mask_RCNN/mrcnn/config.py,118,"If IMAGE_MIN_DIM or IMAGE_MIN_SCALE are not None, then it scales",
Mask_RCNN/mrcnn/config.py,119,up before padding. IMAGE_MAX_DIM is ignored in this mode.,
Mask_RCNN/mrcnn/config.py,120,The multiple of 64 is needed to ensure smooth scaling of feature,
Mask_RCNN/mrcnn/config.py,121,maps up and down the 6 levels of the FPN pyramid (2**6=64).,
Mask_RCNN/mrcnn/config.py,122,"crop:   Picks random crops from the image. First, scales the image based",
Mask_RCNN/mrcnn/config.py,123,"on IMAGE_MIN_DIM and IMAGE_MIN_SCALE, then picks a random crop of",
Mask_RCNN/mrcnn/config.py,124,size IMAGE_MIN_DIM x IMAGE_MIN_DIM. Can be used in training only.,
Mask_RCNN/mrcnn/config.py,125,IMAGE_MAX_DIM is not used in this mode.,
Mask_RCNN/mrcnn/config.py,129,Minimum scaling ratio. Checked after MIN_IMAGE_DIM and can force further,
Mask_RCNN/mrcnn/config.py,130,"up scaling. For example, if set to 2 then images are scaled up to double",
Mask_RCNN/mrcnn/config.py,131,"the width and height, or more, even if MIN_IMAGE_DIM doesn't require it.",
Mask_RCNN/mrcnn/config.py,132,"However, in 'square' mode, it can be overruled by IMAGE_MAX_DIM.",
Mask_RCNN/mrcnn/config.py,134,"Number of color channels per image. RGB = 3, grayscale = 1, RGB-D = 4",
Mask_RCNN/mrcnn/config.py,135,Changing this requires other changes in the code. See the WIKI for more,
Mask_RCNN/mrcnn/config.py,136,details: https://github.com/matterport/Mask_RCNN/wiki,
Mask_RCNN/mrcnn/config.py,139,Image mean (RGB),
Mask_RCNN/mrcnn/config.py,142,Number of ROIs per image to feed to classifier/mask heads,
Mask_RCNN/mrcnn/config.py,143,The Mask RCNN paper uses 512 but often the RPN doesn't generate,
Mask_RCNN/mrcnn/config.py,144,enough positive proposals to fill this and keep a positive:negative,
Mask_RCNN/mrcnn/config.py,145,ratio of 1:3. You can increase the number of proposals by adjusting,
Mask_RCNN/mrcnn/config.py,146,the RPN NMS threshold.,
Mask_RCNN/mrcnn/config.py,149,Percent of positive ROIs used to train classifier/mask heads,
Mask_RCNN/mrcnn/config.py,152,Pooled ROIs,
Mask_RCNN/mrcnn/config.py,156,Shape of output mask,
Mask_RCNN/mrcnn/config.py,157,To change this you also need to change the neural network mask branch,
Mask_RCNN/mrcnn/config.py,160,Maximum number of ground truth instances to use in one image,
Mask_RCNN/mrcnn/config.py,163,Bounding box refinement standard deviation for RPN and final detections.,
Mask_RCNN/mrcnn/config.py,167,Max number of final detections,
Mask_RCNN/mrcnn/config.py,170,Minimum probability value to accept a detected instance,
Mask_RCNN/mrcnn/config.py,171,ROIs below this threshold are skipped,
Mask_RCNN/mrcnn/config.py,174,Non-maximum suppression threshold for detection,
Mask_RCNN/mrcnn/config.py,177,Learning rate and momentum,
Mask_RCNN/mrcnn/config.py,178,"The Mask RCNN paper uses lr=0.02, but on TensorFlow it causes",
Mask_RCNN/mrcnn/config.py,179,weights to explode. Likely due to differences in optimizer,
Mask_RCNN/mrcnn/config.py,180,implementation.,
Mask_RCNN/mrcnn/config.py,184,Weight decay regularization,
Mask_RCNN/mrcnn/config.py,187,Loss weights for more precise optimization.,
Mask_RCNN/mrcnn/config.py,188,Can be used for R-CNN training setup.,
Mask_RCNN/mrcnn/config.py,197,Use RPN ROIs or externally generated ROIs for training,
Mask_RCNN/mrcnn/config.py,198,Keep this True for most situations. Set to False if you want to train,
Mask_RCNN/mrcnn/config.py,199,the head branches on ROI generated by code rather than the ROIs from,
Mask_RCNN/mrcnn/config.py,200,"the RPN. For example, to debug the classifier head without having to",
Mask_RCNN/mrcnn/config.py,201,train the RPN.,
Mask_RCNN/mrcnn/config.py,204,Train or freeze batch normalization layers,
Mask_RCNN/mrcnn/config.py,205,None: Train BN layers. This is the normal mode,
Mask_RCNN/mrcnn/config.py,206,False: Freeze BN layers. Good when using a small batch size,
Mask_RCNN/mrcnn/config.py,207,True: (don't use). Set layer in training mode even when predicting,
Mask_RCNN/mrcnn/config.py,208,Defaulting to False since batch size is often small,
Mask_RCNN/mrcnn/config.py,210,Gradient norm clipping,
Mask_RCNN/mrcnn/config.py,215,Effective batch size,
Mask_RCNN/mrcnn/config.py,218,Input image size,
Mask_RCNN/mrcnn/config.py,226,Image meta data length,
Mask_RCNN/mrcnn/config.py,227,See compose_image_meta() for details,
Mask_RCNN/mrcnn/visualize.py,23,Root directory of the project,
Mask_RCNN/mrcnn/visualize.py,26,Import Mask RCNN,
Mask_RCNN/mrcnn/visualize.py,27,To find local version of the library,
Mask_RCNN/mrcnn/visualize.py,31,,
Mask_RCNN/mrcnn/visualize.py,32,Visualization,
Mask_RCNN/mrcnn/visualize.py,33,,
Mask_RCNN/mrcnn/visualize.py,100,Number of instances,
Mask_RCNN/mrcnn/visualize.py,107,"If no axis is passed, create one and automatically call show()",
Mask_RCNN/mrcnn/visualize.py,113,Generate random colors,
Mask_RCNN/mrcnn/visualize.py,116,Show area outside image boundaries.,
Mask_RCNN/mrcnn/visualize.py,127,Bounding box,
Mask_RCNN/mrcnn/visualize.py,129,Skip this instance. Has no bbox. Likely lost in image cropping.,
Mask_RCNN/mrcnn/visualize.py,138,Label,
Mask_RCNN/mrcnn/visualize.py,149,Mask,
Mask_RCNN/mrcnn/visualize.py,154,Mask Polygon,
Mask_RCNN/mrcnn/visualize.py,155,Pad to ensure proper polygons for masks that touch image edges.,
Mask_RCNN/mrcnn/visualize.py,161,"Subtract the padding and flip (y, x) to (x, y)",
Mask_RCNN/mrcnn/visualize.py,177,Match predictions to ground truth,
Mask_RCNN/mrcnn/visualize.py,182,Ground truth = green. Predictions = red,
Mask_RCNN/mrcnn/visualize.py,185,Concatenate GT and predictions,
Mask_RCNN/mrcnn/visualize.py,190,Captions per instance show score/IoU,
Mask_RCNN/mrcnn/visualize.py,196,Set title if not provided,
Mask_RCNN/mrcnn/visualize.py,198,Display,
Mask_RCNN/mrcnn/visualize.py,215,Pick random anchors in case there are too many.,
Mask_RCNN/mrcnn/visualize.py,227,Show area outside image boundaries.,
Mask_RCNN/mrcnn/visualize.py,235,ROI,
Mask_RCNN/mrcnn/visualize.py,241,Refined ROI,
Mask_RCNN/mrcnn/visualize.py,247,Connect the top-left corners of the anchor and proposal for easy visualization,
Mask_RCNN/mrcnn/visualize.py,250,Label,
Mask_RCNN/mrcnn/visualize.py,255,Mask,
Mask_RCNN/mrcnn/visualize.py,262,Print stats,
Mask_RCNN/mrcnn/visualize.py,269,TODO: Replace with matplotlib equivalent?,
Mask_RCNN/mrcnn/visualize.py,288,Pick top prominent classes in this image,
Mask_RCNN/mrcnn/visualize.py,294,Generate images and titles,
Mask_RCNN/mrcnn/visualize.py,297,Pull masks of instances belonging to the same class.,
Mask_RCNN/mrcnn/visualize.py,312,Plot the Precision-Recall curve,
Mask_RCNN/mrcnn/visualize.py,375,Number of boxes,
Mask_RCNN/mrcnn/visualize.py,379,Matplotlib Axis,
Mask_RCNN/mrcnn/visualize.py,383,Generate random colors,
Mask_RCNN/mrcnn/visualize.py,386,Show area outside image boundaries.,
Mask_RCNN/mrcnn/visualize.py,396,Box visibility,
Mask_RCNN/mrcnn/visualize.py,411,Boxes,
Mask_RCNN/mrcnn/visualize.py,414,Skip this instance. Has no bbox. Likely lost in cropping.,
Mask_RCNN/mrcnn/visualize.py,422,Refined boxes,
Mask_RCNN/mrcnn/visualize.py,428,Connect the top-left corners of the anchor and proposal,
Mask_RCNN/mrcnn/visualize.py,432,Captions,
Mask_RCNN/mrcnn/visualize.py,435,"If there are refined boxes, display captions on them",
Mask_RCNN/mrcnn/visualize.py,443,Masks,
Mask_RCNN/mrcnn/visualize.py,447,Mask Polygon,
Mask_RCNN/mrcnn/visualize.py,448,Pad to ensure proper polygons for masks that touch image edges.,
Mask_RCNN/mrcnn/visualize.py,454,"Subtract the padding and flip (y, x) to (x, y)",
Mask_RCNN/mrcnn/visualize.py,482,list of Numpy arrays,
Mask_RCNN/mrcnn/visualize.py,483,list of TF tensors,
Mask_RCNN/mrcnn/visualize.py,486,Detect problematic layers. Exclude biases of conv layers.,
Mask_RCNN/mrcnn/visualize.py,492,Add row,
Mask_RCNN/mrcnn/parallel_model.py,58,Slice inputs. Slice inputs on the CPU to avoid sending a copy,
Mask_RCNN/mrcnn/parallel_model.py,59,of the full inputs to all GPUs. Saves on bandwidth and memory.,
Mask_RCNN/mrcnn/parallel_model.py,69,Run the model call() on each GPU to place the ops there,
Mask_RCNN/mrcnn/parallel_model.py,73,Run a slice of inputs through this replica,
Mask_RCNN/mrcnn/parallel_model.py,80,Create the model replica and get the outputs,
Mask_RCNN/mrcnn/parallel_model.py,84,Save the outputs for merging back together later,
Mask_RCNN/mrcnn/parallel_model.py,88,Merge outputs on CPU,
Mask_RCNN/mrcnn/parallel_model.py,92,Concatenate or average outputs?,
Mask_RCNN/mrcnn/parallel_model.py,93,Outputs usually have a batch dimension and we concatenate,
Mask_RCNN/mrcnn/parallel_model.py,94,"across it. If they don't, then the output is likely a loss",
Mask_RCNN/mrcnn/parallel_model.py,95,or a metric value that gets averaged across the batch.,
Mask_RCNN/mrcnn/parallel_model.py,96,Keras expects losses and metrics to be scalars.,
Mask_RCNN/mrcnn/parallel_model.py,98,Average,
Mask_RCNN/mrcnn/parallel_model.py,101,Concatenate,
Mask_RCNN/mrcnn/parallel_model.py,108,Testing code below. It creates a simple model to train on MNIST and,
Mask_RCNN/mrcnn/parallel_model.py,109,tries to run it on 2 GPUs. It saves the graph so it can be viewed,
Mask_RCNN/mrcnn/parallel_model.py,110,in TensorBoard. Run it as:,
Mask_RCNN/mrcnn/parallel_model.py,111,,
Mask_RCNN/mrcnn/parallel_model.py,112,python3 parallel_model.py,
Mask_RCNN/mrcnn/parallel_model.py,122,Root directory of the project,
Mask_RCNN/mrcnn/parallel_model.py,125,Directory to save logs and trained model,
Mask_RCNN/mrcnn/parallel_model.py,129,"Reset default graph. Keras leaves old ops in the graph,",
Mask_RCNN/mrcnn/parallel_model.py,130,which are ignored for execution but clutter graph,
Mask_RCNN/mrcnn/parallel_model.py,131,visualization in TensorBoard.,
Mask_RCNN/mrcnn/parallel_model.py,146,Load MNIST Data,
Mask_RCNN/mrcnn/parallel_model.py,154,Build data generator and model,
Mask_RCNN/mrcnn/parallel_model.py,158,Add multi-GPU support.,
Mask_RCNN/mrcnn/parallel_model.py,168,Train,
Mask_RCNN/samples/coco/coco.py,34,https://github.com/aleju/imgaug (pip3 install imgaug),
Mask_RCNN/samples/coco/coco.py,36,Download and install the Python COCO tools from https://github.com/waleedka/coco,
Mask_RCNN/samples/coco/coco.py,37,That's a fork from the original https://github.com/pdollar/coco with a bug,
Mask_RCNN/samples/coco/coco.py,38,fix for Python 3.,
Mask_RCNN/samples/coco/coco.py,39,I submitted a pull request https://github.com/cocodataset/cocoapi/pull/50,
Mask_RCNN/samples/coco/coco.py,40,If the PR is merged then use the original repo.,
Mask_RCNN/samples/coco/coco.py,41,"Note: Edit PythonAPI/Makefile and replace ""python"" with ""python3"".",
Mask_RCNN/samples/coco/coco.py,50,Root directory of the project,
Mask_RCNN/samples/coco/coco.py,53,Import Mask RCNN,
Mask_RCNN/samples/coco/coco.py,54,To find local version of the library,
Mask_RCNN/samples/coco/coco.py,58,Path to trained weights file,
Mask_RCNN/samples/coco/coco.py,61,"Directory to save logs and model checkpoints, if not provided",
Mask_RCNN/samples/coco/coco.py,62,through the command line argument --logs,
Mask_RCNN/samples/coco/coco.py,66,,
Mask_RCNN/samples/coco/coco.py,67,Configurations,
Mask_RCNN/samples/coco/coco.py,68,,
Mask_RCNN/samples/coco/coco.py,76,Give the configuration a recognizable name,
Mask_RCNN/samples/coco/coco.py,79,"We use a GPU with 12GB memory, which can fit two images.",
Mask_RCNN/samples/coco/coco.py,80,Adjust down if you use a smaller GPU.,
Mask_RCNN/samples/coco/coco.py,83,Uncomment to train on 8 GPUs (default is 1),
Mask_RCNN/samples/coco/coco.py,84,GPU_COUNT = 8,
Mask_RCNN/samples/coco/coco.py,86,Number of classes (including background),
Mask_RCNN/samples/coco/coco.py,87,COCO has 80 classes,
Mask_RCNN/samples/coco/coco.py,90,,
Mask_RCNN/samples/coco/coco.py,91,Dataset,
Mask_RCNN/samples/coco/coco.py,92,,
Mask_RCNN/samples/coco/coco.py,116,Load all classes or a subset?,
Mask_RCNN/samples/coco/coco.py,118,All classes,
Mask_RCNN/samples/coco/coco.py,121,All images or a subset?,
Mask_RCNN/samples/coco/coco.py,126,Remove duplicates,
Mask_RCNN/samples/coco/coco.py,129,All images,
Mask_RCNN/samples/coco/coco.py,132,Add classes,
Mask_RCNN/samples/coco/coco.py,136,Add images,
Mask_RCNN/samples/coco/coco.py,158,Setup paths and file names,
Mask_RCNN/samples/coco/coco.py,167,"print(""Image paths:""); print(imgDir); print(imgZipFile); print(imgURL)",
Mask_RCNN/samples/coco/coco.py,169,Create main folder if it doesn't exist yet,
Mask_RCNN/samples/coco/coco.py,173,Download images if not available locally,
Mask_RCNN/samples/coco/coco.py,186,Setup annotations data paths,
Mask_RCNN/samples/coco/coco.py,203,"print(""Annotations paths:""); print(annDir); print(annFile); print(annZipFile); print(annURL)",
Mask_RCNN/samples/coco/coco.py,205,Download annotations if not available locally,
Mask_RCNN/samples/coco/coco.py,232,"If not a COCO image, delegate to parent class.",
Mask_RCNN/samples/coco/coco.py,240,"Build mask of shape [height, width, instance_count] and list",
Mask_RCNN/samples/coco/coco.py,241,of class IDs that correspond to each channel of the mask.,
Mask_RCNN/samples/coco/coco.py,248,Some objects are so small that they're less than 1 pixel area,
Mask_RCNN/samples/coco/coco.py,249,and end up rounded out. Skip those objects.,
Mask_RCNN/samples/coco/coco.py,252,"Is it a crowd? If so, use a negative class ID.",
Mask_RCNN/samples/coco/coco.py,254,Use negative class ID for crowds,
Mask_RCNN/samples/coco/coco.py,256,"For crowd masks, annToMask() sometimes returns a mask",
Mask_RCNN/samples/coco/coco.py,257,"smaller than the given dimensions. If so, resize it.",
Mask_RCNN/samples/coco/coco.py,263,Pack instance masks into an array,
Mask_RCNN/samples/coco/coco.py,269,Call super class to return an empty mask,
Mask_RCNN/samples/coco/coco.py,280,The following two functions are from pycocotools with a few changes.,
Mask_RCNN/samples/coco/coco.py,289,polygon -- a single object might consist of multiple parts,
Mask_RCNN/samples/coco/coco.py,290,we merge all parts into one mask rle code,
Mask_RCNN/samples/coco/coco.py,294,uncompressed RLE,
Mask_RCNN/samples/coco/coco.py,297,rle,
Mask_RCNN/samples/coco/coco.py,311,,
Mask_RCNN/samples/coco/coco.py,312,COCO Evaluation,
Mask_RCNN/samples/coco/coco.py,313,,
Mask_RCNN/samples/coco/coco.py,318,"If no results, return an empty list",
Mask_RCNN/samples/coco/coco.py,324,Loop through detections,
Mask_RCNN/samples/coco/coco.py,348,Pick COCO images from the dataset,
Mask_RCNN/samples/coco/coco.py,351,Limit to a subset,
Mask_RCNN/samples/coco/coco.py,355,Get corresponding COCO image IDs.,
Mask_RCNN/samples/coco/coco.py,363,Load image,
Mask_RCNN/samples/coco/coco.py,366,Run detection,
Mask_RCNN/samples/coco/coco.py,371,Convert results to COCO format,
Mask_RCNN/samples/coco/coco.py,372,Cast masks to uint8 because COCO tools errors out on bool,
Mask_RCNN/samples/coco/coco.py,379,Load results. This modifies results with additional attributes.,
Mask_RCNN/samples/coco/coco.py,382,Evaluate,
Mask_RCNN/samples/coco/coco.py,394,,
Mask_RCNN/samples/coco/coco.py,395,Training,
Mask_RCNN/samples/coco/coco.py,396,,
Mask_RCNN/samples/coco/coco.py,402,Parse command line arguments,
Mask_RCNN/samples/coco/coco.py,439,Configurations,
Mask_RCNN/samples/coco/coco.py,444,Set batch size to 1 since we'll be running inference on,
Mask_RCNN/samples/coco/coco.py,445,one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU,
Mask_RCNN/samples/coco/coco.py,452,Create model,
Mask_RCNN/samples/coco/coco.py,460,Select weights file to load,
Mask_RCNN/samples/coco/coco.py,464,Find last trained weights,
Mask_RCNN/samples/coco/coco.py,467,Start from ImageNet trained weights,
Mask_RCNN/samples/coco/coco.py,472,Load weights,
Mask_RCNN/samples/coco/coco.py,476,Train or evaluate,
Mask_RCNN/samples/coco/coco.py,478,Training dataset. Use the training set and 35K from the,
Mask_RCNN/samples/coco/coco.py,479,"validation set, as as in the Mask RCNN paper.",
Mask_RCNN/samples/coco/coco.py,486,Validation dataset,
Mask_RCNN/samples/coco/coco.py,492,Image Augmentation,
Mask_RCNN/samples/coco/coco.py,493,Right/Left flip 50% of the time,
Mask_RCNN/samples/coco/coco.py,496,*** This training schedule is an example. Update to your needs ***,
Mask_RCNN/samples/coco/coco.py,498,Training - Stage 1,
Mask_RCNN/samples/coco/coco.py,506,Training - Stage 2,
Mask_RCNN/samples/coco/coco.py,507,Finetune layers from ResNet stage 4 and up,
Mask_RCNN/samples/coco/coco.py,515,Training - Stage 3,
Mask_RCNN/samples/coco/coco.py,516,Fine tune all layers,
Mask_RCNN/samples/coco/coco.py,525,Validation dataset,
Mask_RCNN/samples/nucleus/nucleus.py,28,Set matplotlib backend,
Mask_RCNN/samples/nucleus/nucleus.py,29,This has to be done before other importa that might,
Mask_RCNN/samples/nucleus/nucleus.py,30,"set it, but only if we're running in script mode",
Mask_RCNN/samples/nucleus/nucleus.py,31,rather than being imported.,
Mask_RCNN/samples/nucleus/nucleus.py,34,Agg backend runs without a display,
Mask_RCNN/samples/nucleus/nucleus.py,46,Root directory of the project,
Mask_RCNN/samples/nucleus/nucleus.py,49,Import Mask RCNN,
Mask_RCNN/samples/nucleus/nucleus.py,50,To find local version of the library,
Mask_RCNN/samples/nucleus/nucleus.py,56,Path to trained weights file,
Mask_RCNN/samples/nucleus/nucleus.py,59,"Directory to save logs and model checkpoints, if not provided",
Mask_RCNN/samples/nucleus/nucleus.py,60,through the command line argument --logs,
Mask_RCNN/samples/nucleus/nucleus.py,63,Results directory,
Mask_RCNN/samples/nucleus/nucleus.py,64,Save submission files here,
Mask_RCNN/samples/nucleus/nucleus.py,67,"The dataset doesn't have a standard train/val split, so I picked",
Mask_RCNN/samples/nucleus/nucleus.py,68,a variety of images to surve as a validation set.,
Mask_RCNN/samples/nucleus/nucleus.py,98,,
Mask_RCNN/samples/nucleus/nucleus.py,99,Configurations,
Mask_RCNN/samples/nucleus/nucleus.py,100,,
Mask_RCNN/samples/nucleus/nucleus.py,104,Give the configuration a recognizable name,
Mask_RCNN/samples/nucleus/nucleus.py,107,Adjust depending on your GPU memory,
Mask_RCNN/samples/nucleus/nucleus.py,110,Number of classes (including background),
Mask_RCNN/samples/nucleus/nucleus.py,111,Background + nucleus,
Mask_RCNN/samples/nucleus/nucleus.py,113,Number of training and validation steps per epoch,
Mask_RCNN/samples/nucleus/nucleus.py,117,Don't exclude based on confidence. Since we have two classes,
Mask_RCNN/samples/nucleus/nucleus.py,118,then 0.5 is the minimum anyway as it picks between nucleus and BG,
Mask_RCNN/samples/nucleus/nucleus.py,121,Backbone network architecture,
Mask_RCNN/samples/nucleus/nucleus.py,122,"Supported values are: resnet50, resnet101",
Mask_RCNN/samples/nucleus/nucleus.py,125,Input image resizing,
Mask_RCNN/samples/nucleus/nucleus.py,126,Random crops of size 512x512,
Mask_RCNN/samples/nucleus/nucleus.py,132,Length of square anchor side in pixels,
Mask_RCNN/samples/nucleus/nucleus.py,135,ROIs kept after non-maximum supression (training and inference),
Mask_RCNN/samples/nucleus/nucleus.py,139,Non-max suppression threshold to filter RPN proposals.,
Mask_RCNN/samples/nucleus/nucleus.py,140,You can increase this during training to generate more propsals.,
Mask_RCNN/samples/nucleus/nucleus.py,143,How many anchors per image to use for RPN training,
Mask_RCNN/samples/nucleus/nucleus.py,146,Image mean (RGB),
Mask_RCNN/samples/nucleus/nucleus.py,149,"If enabled, resizes instance masks to a smaller size to reduce",
Mask_RCNN/samples/nucleus/nucleus.py,150,memory load. Recommended when using high-resolution images.,
Mask_RCNN/samples/nucleus/nucleus.py,152,"(height, width) of the mini-mask",
Mask_RCNN/samples/nucleus/nucleus.py,154,Number of ROIs per image to feed to classifier/mask heads,
Mask_RCNN/samples/nucleus/nucleus.py,155,The Mask RCNN paper uses 512 but often the RPN doesn't generate,
Mask_RCNN/samples/nucleus/nucleus.py,156,enough positive proposals to fill this and keep a positive:negative,
Mask_RCNN/samples/nucleus/nucleus.py,157,ratio of 1:3. You can increase the number of proposals by adjusting,
Mask_RCNN/samples/nucleus/nucleus.py,158,the RPN NMS threshold.,
Mask_RCNN/samples/nucleus/nucleus.py,161,Maximum number of ground truth instances to use in one image,
Mask_RCNN/samples/nucleus/nucleus.py,164,Max number of final detections per image,
Mask_RCNN/samples/nucleus/nucleus.py,169,Set batch size to 1 to run one image at a time,
Mask_RCNN/samples/nucleus/nucleus.py,172,Don't resize imager for inferencing,
Mask_RCNN/samples/nucleus/nucleus.py,174,Non-max suppression threshold to filter RPN proposals.,
Mask_RCNN/samples/nucleus/nucleus.py,175,You can increase this during training to generate more propsals.,
Mask_RCNN/samples/nucleus/nucleus.py,179,,
Mask_RCNN/samples/nucleus/nucleus.py,180,Dataset,
Mask_RCNN/samples/nucleus/nucleus.py,181,,
Mask_RCNN/samples/nucleus/nucleus.py,194,Add classes. We have one class.,
Mask_RCNN/samples/nucleus/nucleus.py,195,"Naming the dataset nucleus, and the class nucleus",
Mask_RCNN/samples/nucleus/nucleus.py,198,Which subset?,
Mask_RCNN/samples/nucleus/nucleus.py,199,"""val"": use hard-coded list above",
Mask_RCNN/samples/nucleus/nucleus.py,200,"""train"": use data from stage1_train minus the hard-coded list above",
Mask_RCNN/samples/nucleus/nucleus.py,201,else: use the data from the specified sub-directory,
Mask_RCNN/samples/nucleus/nucleus.py,208,Get image ids from directory names,
Mask_RCNN/samples/nucleus/nucleus.py,213,Add images,
Mask_RCNN/samples/nucleus/nucleus.py,228,Get mask directory from image path,
Mask_RCNN/samples/nucleus/nucleus.py,231,Read mask files from .png image,
Mask_RCNN/samples/nucleus/nucleus.py,238,"Return mask, and array of class IDs of each instance. Since we have",
Mask_RCNN/samples/nucleus/nucleus.py,239,"one class ID, we return an array of ones",
Mask_RCNN/samples/nucleus/nucleus.py,251,,
Mask_RCNN/samples/nucleus/nucleus.py,252,Training,
Mask_RCNN/samples/nucleus/nucleus.py,253,,
Mask_RCNN/samples/nucleus/nucleus.py,257,Training dataset.,
Mask_RCNN/samples/nucleus/nucleus.py,262,Validation dataset,
Mask_RCNN/samples/nucleus/nucleus.py,267,Image augmentation,
Mask_RCNN/samples/nucleus/nucleus.py,268,http://imgaug.readthedocs.io/en/latest/source/augmenters.html,
Mask_RCNN/samples/nucleus/nucleus.py,279,*** This training schedule is an example. Update to your needs ***,
Mask_RCNN/samples/nucleus/nucleus.py,281,"If starting from imagenet, train heads only for a bit",
Mask_RCNN/samples/nucleus/nucleus.py,282,since they have random weights,
Mask_RCNN/samples/nucleus/nucleus.py,298,,
Mask_RCNN/samples/nucleus/nucleus.py,299,RLE Encoding,
Mask_RCNN/samples/nucleus/nucleus.py,300,,
Mask_RCNN/samples/nucleus/nucleus.py,307,Flatten it column wise,
Mask_RCNN/samples/nucleus/nucleus.py,309,Compute gradient. Equals 1 or -1 at transition points,
Mask_RCNN/samples/nucleus/nucleus.py,311,1-based indicies of transition points (where gradient != 0),
Mask_RCNN/samples/nucleus/nucleus.py,313,Convert second index in each pair to lenth,
Mask_RCNN/samples/nucleus/nucleus.py,330,Reshape and transpose,
Mask_RCNN/samples/nucleus/nucleus.py,338,"If mask is empty, return line with image ID only",
Mask_RCNN/samples/nucleus/nucleus.py,341,Remove mask overlaps,
Mask_RCNN/samples/nucleus/nucleus.py,342,Multiply each instance mask by its score order,
Mask_RCNN/samples/nucleus/nucleus.py,343,then take the maximum across the last dimension,
Mask_RCNN/samples/nucleus/nucleus.py,344,1-based descending,
Mask_RCNN/samples/nucleus/nucleus.py,346,Loop over instance masks,
Mask_RCNN/samples/nucleus/nucleus.py,350,Skip if empty,
Mask_RCNN/samples/nucleus/nucleus.py,358,,
Mask_RCNN/samples/nucleus/nucleus.py,359,Detection,
Mask_RCNN/samples/nucleus/nucleus.py,360,,
Mask_RCNN/samples/nucleus/nucleus.py,366,Create directory,
Mask_RCNN/samples/nucleus/nucleus.py,373,Read dataset,
Mask_RCNN/samples/nucleus/nucleus.py,377,Load over images,
Mask_RCNN/samples/nucleus/nucleus.py,380,Load image and run detection,
Mask_RCNN/samples/nucleus/nucleus.py,382,Detect objects,
Mask_RCNN/samples/nucleus/nucleus.py,384,Encode image to RLE. Returns a string of multiple lines,
Mask_RCNN/samples/nucleus/nucleus.py,388,Save image with masks,
Mask_RCNN/samples/nucleus/nucleus.py,396,Save to csv file,
Mask_RCNN/samples/nucleus/nucleus.py,404,,
Mask_RCNN/samples/nucleus/nucleus.py,405,Command Line,
Mask_RCNN/samples/nucleus/nucleus.py,406,,
Mask_RCNN/samples/nucleus/nucleus.py,411,Parse command line arguments,
Mask_RCNN/samples/nucleus/nucleus.py,432,Validate arguments,
Mask_RCNN/samples/nucleus/nucleus.py,444,Configurations,
Mask_RCNN/samples/nucleus/nucleus.py,451,Create model,
Mask_RCNN/samples/nucleus/nucleus.py,459,Select weights file to load,
Mask_RCNN/samples/nucleus/nucleus.py,462,Download weights file,
Mask_RCNN/samples/nucleus/nucleus.py,466,Find last trained weights,
Mask_RCNN/samples/nucleus/nucleus.py,469,Start from ImageNet trained weights,
Mask_RCNN/samples/nucleus/nucleus.py,474,Load weights,
Mask_RCNN/samples/nucleus/nucleus.py,477,Exclude the last layers because they require a matching,
Mask_RCNN/samples/nucleus/nucleus.py,478,number of classes,
Mask_RCNN/samples/nucleus/nucleus.py,485,Train or evaluate,
Mask_RCNN/samples/shapes/shapes.py,19,Root directory of the project,
Mask_RCNN/samples/shapes/shapes.py,22,Import Mask RCNN,
Mask_RCNN/samples/shapes/shapes.py,23,To find local version of the library,
Mask_RCNN/samples/shapes/shapes.py,33,Give the configuration a recognizable name,
Mask_RCNN/samples/shapes/shapes.py,36,Train on 1 GPU and 8 images per GPU. We can put multiple images on each,
Mask_RCNN/samples/shapes/shapes.py,37,GPU because the images are small. Batch size is 8 (GPUs * images/GPU).,
Mask_RCNN/samples/shapes/shapes.py,41,Number of classes (including background),
Mask_RCNN/samples/shapes/shapes.py,42,background + 3 shapes,
Mask_RCNN/samples/shapes/shapes.py,44,Use small images for faster training. Set the limits of the small side,
Mask_RCNN/samples/shapes/shapes.py,45,"the large side, and that determines the image shape.",
Mask_RCNN/samples/shapes/shapes.py,49,Use smaller anchors because our image and objects are small,
Mask_RCNN/samples/shapes/shapes.py,50,anchor side in pixels,
Mask_RCNN/samples/shapes/shapes.py,52,Reduce training ROIs per image because the images are small and have,
Mask_RCNN/samples/shapes/shapes.py,53,few objects. Aim to allow ROI sampling to pick 33% positive ROIs.,
Mask_RCNN/samples/shapes/shapes.py,56,Use a small epoch since the data is simple,
Mask_RCNN/samples/shapes/shapes.py,59,use small validation steps since the epoch is small,
Mask_RCNN/samples/shapes/shapes.py,74,Add classes,
Mask_RCNN/samples/shapes/shapes.py,79,Add images,
Mask_RCNN/samples/shapes/shapes.py,80,Generate random specifications of images (i.e. color and,
Mask_RCNN/samples/shapes/shapes.py,81,list of shapes sizes and locations). This is more compact than,
Mask_RCNN/samples/shapes/shapes.py,82,actual images. Images are generated on the fly in load_image().,
Mask_RCNN/samples/shapes/shapes.py,121,Handle occlusions,
Mask_RCNN/samples/shapes/shapes.py,127,Map class names to class IDs.,
Mask_RCNN/samples/shapes/shapes.py,133,"Get the center x, y and the size s",
Mask_RCNN/samples/shapes/shapes.py,157,Shape,
Mask_RCNN/samples/shapes/shapes.py,159,Color,
Mask_RCNN/samples/shapes/shapes.py,161,"Center x, y",
Mask_RCNN/samples/shapes/shapes.py,165,Size,
Mask_RCNN/samples/shapes/shapes.py,174,Pick random background color,
Mask_RCNN/samples/shapes/shapes.py,176,Generate a few random shapes and record their,
Mask_RCNN/samples/shapes/shapes.py,177,bounding boxes,
Mask_RCNN/samples/shapes/shapes.py,186,Apply non-max suppression wit 0.3 threshold to avoid,
Mask_RCNN/samples/shapes/shapes.py,187,shapes covering each other,
Mask_RCNN/samples/balloon/balloon.py,37,Root directory of the project,
Mask_RCNN/samples/balloon/balloon.py,40,Import Mask RCNN,
Mask_RCNN/samples/balloon/balloon.py,41,To find local version of the library,
Mask_RCNN/samples/balloon/balloon.py,45,Path to trained weights file,
Mask_RCNN/samples/balloon/balloon.py,48,"Directory to save logs and model checkpoints, if not provided",
Mask_RCNN/samples/balloon/balloon.py,49,through the command line argument --logs,
Mask_RCNN/samples/balloon/balloon.py,52,,
Mask_RCNN/samples/balloon/balloon.py,53,Configurations,
Mask_RCNN/samples/balloon/balloon.py,54,,
Mask_RCNN/samples/balloon/balloon.py,61,Give the configuration a recognizable name,
Mask_RCNN/samples/balloon/balloon.py,64,"We use a GPU with 12GB memory, which can fit two images.",
Mask_RCNN/samples/balloon/balloon.py,65,Adjust down if you use a smaller GPU.,
Mask_RCNN/samples/balloon/balloon.py,68,Number of classes (including background),
Mask_RCNN/samples/balloon/balloon.py,69,Background + balloon,
Mask_RCNN/samples/balloon/balloon.py,71,Number of training steps per epoch,
Mask_RCNN/samples/balloon/balloon.py,74,Skip detections with < 90% confidence,
Mask_RCNN/samples/balloon/balloon.py,78,,
Mask_RCNN/samples/balloon/balloon.py,79,Dataset,
Mask_RCNN/samples/balloon/balloon.py,80,,
Mask_RCNN/samples/balloon/balloon.py,89,Add classes. We have only one class to add.,
Mask_RCNN/samples/balloon/balloon.py,92,Train or validation dataset?,
Mask_RCNN/samples/balloon/balloon.py,96,Load annotations,
Mask_RCNN/samples/balloon/balloon.py,97,VGG Image Annotator (up to version 1.6) saves each image in the form:,
Mask_RCNN/samples/balloon/balloon.py,98,"{ 'filename': '28503151_5b5b7ec140_b.jpg',",
Mask_RCNN/samples/balloon/balloon.py,99,'regions': {,
Mask_RCNN/samples/balloon/balloon.py,100,'0': {,
Mask_RCNN/samples/balloon/balloon.py,101,"'region_attributes': {},",
Mask_RCNN/samples/balloon/balloon.py,102,'shape_attributes': {,
Mask_RCNN/samples/balloon/balloon.py,103,"'all_points_x': [...],",
Mask_RCNN/samples/balloon/balloon.py,104,"'all_points_y': [...],",
Mask_RCNN/samples/balloon/balloon.py,105,"'name': 'polygon'}},",
Mask_RCNN/samples/balloon/balloon.py,106,... more regions ...,
Mask_RCNN/samples/balloon/balloon.py,107,"},",
Mask_RCNN/samples/balloon/balloon.py,108,'size': 100202,
Mask_RCNN/samples/balloon/balloon.py,109,},
Mask_RCNN/samples/balloon/balloon.py,110,We mostly care about the x and y coordinates of each region,
Mask_RCNN/samples/balloon/balloon.py,111,"Note: In VIA 2.0, regions was changed from a dict to a list.",
Mask_RCNN/samples/balloon/balloon.py,113,don't need the dict keys,
Mask_RCNN/samples/balloon/balloon.py,115,The VIA tool saves images in the JSON even if they don't have any,
Mask_RCNN/samples/balloon/balloon.py,116,annotations. Skip unannotated images.,
Mask_RCNN/samples/balloon/balloon.py,119,Add images,
Mask_RCNN/samples/balloon/balloon.py,121,"Get the x, y coordinaets of points of the polygons that make up",
Mask_RCNN/samples/balloon/balloon.py,122,the outline of each object instance. These are stores in the,
Mask_RCNN/samples/balloon/balloon.py,123,shape_attributes (see json format above),
Mask_RCNN/samples/balloon/balloon.py,124,The if condition is needed to support VIA versions 1.x and 2.x.,
Mask_RCNN/samples/balloon/balloon.py,130,load_mask() needs the image size to convert polygons to masks.,
Mask_RCNN/samples/balloon/balloon.py,131,"Unfortunately, VIA doesn't include it in JSON, so we must read",
Mask_RCNN/samples/balloon/balloon.py,132,the image. This is only managable since the dataset is tiny.,
Mask_RCNN/samples/balloon/balloon.py,139,use file name as a unique image id,
Mask_RCNN/samples/balloon/balloon.py,151,"If not a balloon dataset image, delegate to parent class.",
Mask_RCNN/samples/balloon/balloon.py,156,Convert polygons to a bitmap mask of shape,
Mask_RCNN/samples/balloon/balloon.py,157,"[height, width, instance_count]",
Mask_RCNN/samples/balloon/balloon.py,162,Get indexes of pixels inside the polygon and set them to 1,
Mask_RCNN/samples/balloon/balloon.py,166,"Return mask, and array of class IDs of each instance. Since we have",
Mask_RCNN/samples/balloon/balloon.py,167,"one class ID only, we return an array of 1s",
Mask_RCNN/samples/balloon/balloon.py,181,Training dataset.,
Mask_RCNN/samples/balloon/balloon.py,186,Validation dataset,
Mask_RCNN/samples/balloon/balloon.py,191,*** This training schedule is an example. Update to your needs ***,
Mask_RCNN/samples/balloon/balloon.py,192,"Since we're using a very small dataset, and starting from",
Mask_RCNN/samples/balloon/balloon.py,193,"COCO trained weights, we don't need to train too long. Also,",
Mask_RCNN/samples/balloon/balloon.py,194,"no need to train all layers, just the heads should do it.",
Mask_RCNN/samples/balloon/balloon.py,209,Make a grayscale copy of the image. The grayscale copy still,
Mask_RCNN/samples/balloon/balloon.py,210,"has 3 RGB channels, though.",
Mask_RCNN/samples/balloon/balloon.py,212,Copy color pixels from the original color image where mask is set,
Mask_RCNN/samples/balloon/balloon.py,214,"We're treating all instances as one, so collapse the mask into one layer",
Mask_RCNN/samples/balloon/balloon.py,225,Image or video?,
Mask_RCNN/samples/balloon/balloon.py,227,Run model detection and generate the color splash effect,
Mask_RCNN/samples/balloon/balloon.py,229,Read image,
Mask_RCNN/samples/balloon/balloon.py,231,Detect objects,
Mask_RCNN/samples/balloon/balloon.py,233,Color splash,
Mask_RCNN/samples/balloon/balloon.py,235,Save output,
Mask_RCNN/samples/balloon/balloon.py,240,Video capture,
Mask_RCNN/samples/balloon/balloon.py,246,Define codec and create video writer,
Mask_RCNN/samples/balloon/balloon.py,256,Read next image,
Mask_RCNN/samples/balloon/balloon.py,259,"OpenCV returns images as BGR, convert to RGB",
Mask_RCNN/samples/balloon/balloon.py,261,Detect objects,
Mask_RCNN/samples/balloon/balloon.py,263,Color splash,
Mask_RCNN/samples/balloon/balloon.py,265,RGB -> BGR to save image to video,
Mask_RCNN/samples/balloon/balloon.py,267,Add image to video writer,
Mask_RCNN/samples/balloon/balloon.py,274,,
Mask_RCNN/samples/balloon/balloon.py,275,Training,
Mask_RCNN/samples/balloon/balloon.py,276,,
Mask_RCNN/samples/balloon/balloon.py,281,Parse command line arguments,
Mask_RCNN/samples/balloon/balloon.py,305,Validate arguments,
Mask_RCNN/samples/balloon/balloon.py,316,Configurations,
Mask_RCNN/samples/balloon/balloon.py,321,Set batch size to 1 since we'll be running inference on,
Mask_RCNN/samples/balloon/balloon.py,322,one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU,
Mask_RCNN/samples/balloon/balloon.py,328,Create model,
Mask_RCNN/samples/balloon/balloon.py,336,Select weights file to load,
Mask_RCNN/samples/balloon/balloon.py,339,Download weights file,
Mask_RCNN/samples/balloon/balloon.py,343,Find last trained weights,
Mask_RCNN/samples/balloon/balloon.py,346,Start from ImageNet trained weights,
Mask_RCNN/samples/balloon/balloon.py,351,Load weights,
Mask_RCNN/samples/balloon/balloon.py,354,Exclude the last layers because they require a matching,
Mask_RCNN/samples/balloon/balloon.py,355,number of classes,
Mask_RCNN/samples/balloon/balloon.py,362,Train or evaluate,
